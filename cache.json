{"2025-07-10T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2507.07999v1","updated":"2025-07-10T17:59:58Z","published":"2025-07-10T17:59:58Z","title":"Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology","summary":"  Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\nreferencing visual regions, just like human \"thinking with images\". However, no\nbenchmark exists to evaluate these capabilities holistically. To bridge this\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\ndiagnostic benchmark built on three principles: (1) focused visual perception\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\nevaluation, and (3) second-order reasoning to test object interactions and\nspatial hierarchies beyond simple object localization. Prioritizing images with\ndense objects, we initially sample 1K high-quality images from SA-1B, and\nincorporate eight LMM experts to manually annotate questions, candidate\noptions, and answers for each image. After three stages of quality control,\nTreeBench consists of 405 challenging visual question-answering pairs, even the\nmost advanced models struggle with this benchmark, where none of them reach 60%\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\nsupervise localization and reasoning jointly with reinforcement learning,\nenabling accurate localizations and explainable reasoning pathways. Initialized\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.\n","authors":["Haochen Wang","Xiangtai Li","Zilong Huang","Anran Wang","Jiacong Wang","Tao Zhang","Jiani Zheng","Sule Bai","Zijian Kang","Jiashi Feng","Zhuochen Wang","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.07999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07998v1","updated":"2025-07-10T17:59:55Z","published":"2025-07-10T17:59:55Z","title":"PyVision: Agentic Vision with Dynamic Tooling","summary":"  LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning.\n","authors":["Shitian Zhao","Haoquan Zhang","Shaoheng Lin","Ming Li","Qilong Wu","Kaipeng Zhang","Chen Wei"],"pdf_url":"https://arxiv.org/pdf/2507.07998v1.pdf","comment":"26 Pages, 10 Figures, Technical report"},{"id":"http://arxiv.org/abs/2407.14937v2","updated":"2025-07-10T17:58:36Z","published":"2024-07-20T17:05:04Z","title":"Operationalizing a Threat Model for Red-Teaming Large Language Models\n  (LLMs)","summary":"  Creating secure and resilient applications with large language models (LLM)\nrequires anticipating, adjusting to, and countering unforeseen threats.\nRed-teaming has emerged as a critical technique for identifying vulnerabilities\nin real-world LLM implementations. This paper presents a detailed threat model\nand provides a systematization of knowledge (SoK) of red-teaming attacks on\nLLMs. We develop a taxonomy of attacks based on the stages of the LLM\ndevelopment and deployment process and extract various insights from previous\nresearch. In addition, we compile methods for defense and practical red-teaming\nstrategies for practitioners. By delineating prominent attack motifs and\nshedding light on various entry points, this paper provides a framework for\nimproving the security and robustness of LLM-based systems.\n","authors":["Apurv Verma","Satyapriya Krishna","Sebastian Gehrmann","Madhavan Seshadri","Anu Pradhan","Tom Ault","Leslie Barrett","David Rabinowitz","John Doucette","NhatHai Phan"],"pdf_url":"https://arxiv.org/pdf/2407.14937v2.pdf","comment":"Transactions of Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2507.07988v1","updated":"2025-07-10T17:58:26Z","published":"2025-07-10T17:58:26Z","title":"Automating Expert-Level Medical Reasoning Evaluation of Large Language\n  Models","summary":"  As large language models (LLMs) become increasingly integrated into clinical\ndecision-making, ensuring transparent and trustworthy reasoning is essential.\nHowever, existing evaluation strategies of LLMs' medical reasoning capability\neither suffer from unsatisfactory assessment or poor scalability, and a\nrigorous benchmark remains lacking. To address this, we introduce\nMedThink-Bench, a benchmark designed for rigorous, explainable, and scalable\nassessment of LLMs' medical reasoning. MedThink-Bench comprises 500 challenging\nquestions across ten medical domains, each annotated with expert-crafted\nstep-by-step rationales. Building on this, we propose LLM-w-Ref, a novel\nevaluation framework that leverages fine-grained rationales and LLM-as-a-Judge\nmechanisms to assess intermediate reasoning with expert-level fidelity while\nmaintaining scalability. Experiments show that LLM-w-Ref exhibits a strong\npositive correlation with expert judgments. Benchmarking twelve\nstate-of-the-art LLMs, we find that smaller models (e.g., MedGemma-27B) can\nsurpass larger proprietary counterparts (e.g., OpenAI-o3). Overall,\nMedThink-Bench offers a foundational tool for evaluating LLMs' medical\nreasoning, advancing their safe and responsible deployment in clinical\npractice.\n","authors":["Shuang Zhou","Wenya Xie","Jiaxi Li","Zaifu Zhan","Meijia Song","Han Yang","Cheyenna Espinoza","Lindsay Welton","Xinnie Mai","Yanwei Jin","Zidu Xu","Yuen-Hei Chung","Yiyun Xing","Meng-Han Tsai","Emma Schaffer","Yucheng Shi","Ninghao Liu","Zirui Liu","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.07988v1.pdf","comment":"22 pages,6 figures"},{"id":"http://arxiv.org/abs/2507.07983v1","updated":"2025-07-10T17:56:03Z","published":"2025-07-10T17:56:03Z","title":"Performance and Practical Considerations of Large and Small Language\n  Models in Clinical Decision Support in Rheumatology","summary":"  Large language models (LLMs) show promise for supporting clinical\ndecision-making in complex fields such as rheumatology. Our evaluation shows\nthat smaller language models (SLMs), combined with retrieval-augmented\ngeneration (RAG), achieve higher diagnostic and therapeutic performance than\nlarger models, while requiring substantially less energy and enabling\ncost-efficient, local deployment. These features are attractive for\nresource-limited healthcare. However, expert oversight remains essential, as no\nmodel consistently reached specialist-level accuracy in rheumatology.\n","authors":["Sabine Felde","Rüdiger Buchkremer","Gamal Chehab","Christian Thielscher","Jörg HW Distler","Matthias Schneider","Jutta G. Richter"],"pdf_url":"https://arxiv.org/pdf/2507.07983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07981v1","updated":"2025-07-10T17:55:05Z","published":"2025-07-10T17:55:05Z","title":"Why is Your Language Model a Poor Implicit Reward Model?","summary":"  Reward models are key to language model post-training and inference\npipelines. Conveniently, recent work showed that every language model defines\nan implicit reward model (IM-RM), without requiring any architectural changes.\nHowever, such IM-RMs tend to generalize worse, especially out-of-distribution,\ncompared to explicit reward models (EX-RMs) that apply a dedicated linear head\nover the hidden representations of a language model. The existence of a\ngeneralization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They\ncan be trained using the same data, loss function, and language model, and\ndiffer only in how the reward is computed. Towards a fundamental understanding\nof the implicit biases underlying different reward model types, we investigate\nthe root cause of this gap. Our main finding, backed by theory and experiments,\nis that IM-RMs rely more heavily on superficial token-level cues. Consequently,\nthey often generalize worse than EX-RMs under token-level distribution shifts,\nas well as in-distribution. Furthermore, we provide evidence against\nalternative hypotheses for the generalization gap. Most notably, we challenge\nthe intuitive claim that IM-RMs struggle in tasks where generation is harder\nthan verification because they can operate both as a verifier and a generator.\nTaken together, our results highlight that seemingly minor design choices can\nsubstantially impact the generalization behavior of reward models.\n","authors":["Noam Razin","Yong Lin","Jiarui Yao","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2507.07981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18603v2","updated":"2025-07-10T17:52:43Z","published":"2024-12-24T18:56:46Z","title":"Long-Form Speech Generation with Spoken Language Models","summary":"  We consider the generative modeling of speech over multiple minutes, a\nrequirement for long-form multimedia generation and audio-native voice\nassistants. However, textless spoken language models struggle to generate\nplausible speech past tens of seconds, due to high temporal resolution of\nspeech tokens causing loss of coherence, architectural issues with\nlong-sequence training or extrapolation, and memory costs at inference time.\nFrom these considerations we derive SpeechSSM, the first speech language model\nfamily to learn from and sample long-form spoken audio (e.g., 16 minutes of\nread or extemporaneous speech) in a single decoding session without text\nintermediates. SpeechSSMs leverage recent advances in linear-time sequence\nmodeling to greatly surpass current Transformer spoken LMs in coherence and\nefficiency on multi-minute generations while still matching them at the\nutterance level. As we found current spoken language evaluations uninformative,\nespecially in this new long-form setting, we also introduce: LibriSpeech-Long,\na benchmark for long-form speech evaluation; new embedding-based and LLM-judged\nmetrics; and quality measurements over length and time. Speech samples, the\nLibriSpeech-Long dataset, and any future code or model releases can be found at\nhttps://google.github.io/tacotron/publications/speechssm/.\n","authors":["Se Jin Park","Julian Salazar","Aren Jansen","Keisuke Kinoshita","Yong Man Ro","RJ Skerry-Ryan"],"pdf_url":"https://arxiv.org/pdf/2412.18603v2.pdf","comment":"Accepted to ICML 2025 (oral)"},{"id":"http://arxiv.org/abs/2506.04462v2","updated":"2025-07-10T17:50:21Z","published":"2025-06-04T21:29:07Z","title":"Watermarking Degrades Alignment in Language Models: Analysis and\n  Mitigation","summary":"  Watermarking techniques for large language models (LLMs) can significantly\nimpact output quality, yet their effects on truthfulness, safety, and\nhelpfulness remain critically underexamined. This paper presents a systematic\nanalysis of how two popular watermarking approaches-Gumbel and KGW-affect these\ncore alignment properties across four aligned LLMs. Our experiments reveal two\ndistinct degradation patterns: guard attenuation, where enhanced helpfulness\nundermines model safety, and guard amplification, where excessive caution\nreduces model helpfulness. These patterns emerge from watermark-induced shifts\nin token distribution, surfacing the fundamental tension that exists between\nalignment objectives.\n  To mitigate these degradations, we propose Alignment Resampling (AR), an\ninference-time sampling method that uses an external reward model to restore\nalignment. We establish a theoretical lower bound on the improvement in\nexpected reward score as the sample size is increased and empirically\ndemonstrate that sampling just 2-4 watermarked generations effectively recovers\nor surpasses baseline (unwatermarked) alignment scores. To overcome the limited\nresponse diversity of standard Gumbel watermarking, our modified implementation\nsacrifices strict distortion-freeness while maintaining robust detectability,\nensuring compatibility with AR. Experimental results confirm that AR\nsuccessfully recovers baseline alignment in both watermarking approaches, while\nmaintaining strong watermark detectability. This work reveals the critical\nbalance between watermark strength and model alignment, providing a simple\ninference-time solution to responsibly deploy watermarked LLMs in practice.\n","authors":["Apurv Verma","NhatHai Phan","Shubhendu Trivedi"],"pdf_url":"https://arxiv.org/pdf/2506.04462v2.pdf","comment":"Published at the 1st Workshop on GenAI Watermarking, collocated with\n  ICLR 2025. OpenReview: https://openreview.net/forum?id=SIBkIV48gF"},{"id":"http://arxiv.org/abs/2507.07966v1","updated":"2025-07-10T17:47:40Z","published":"2025-07-10T17:47:40Z","title":"Scaling RL to Long Videos","summary":"  We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).\n","authors":["Yukang Chen","Wei Huang","Baifeng Shi","Qinghao Hu","Hanrong Ye","Ligeng Zhu","Zhijian Liu","Pavlo Molchanov","Jan Kautz","Xiaojuan Qi","Sifei Liu","Hongxu Yin","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2507.07966v1.pdf","comment":"Code and models are available at https://github.com/NVlabs/Long-RL"},{"id":"http://arxiv.org/abs/2507.07957v1","updated":"2025-07-10T17:40:11Z","published":"2025-07-10T17:40:11Z","title":"MIRIX: Multi-Agent Memory System for LLM-Based Agents","summary":"  Although memory capabilities of AI agents are gaining increasing attention,\nexisting solutions remain fundamentally limited. Most rely on flat, narrowly\nscoped memory components, constraining their ability to personalize, abstract,\nand reliably recall user-specific information over time. To this end, we\nintroduce MIRIX, a modular, multi-agent memory system that redefines the future\nof AI memory by solving the field's most critical challenge: enabling language\nmodels to truly remember. Unlike prior approaches, MIRIX transcends text to\nembrace rich visual and multimodal experiences, making memory genuinely useful\nin real-world scenarios. MIRIX consists of six distinct, carefully structured\nmemory types: Core, Episodic, Semantic, Procedural, Resource Memory, and\nKnowledge Vault, coupled with a multi-agent framework that dynamically controls\nand coordinates updates and retrieval. This design enables agents to persist,\nreason over, and accurately retrieve diverse, long-term user data at scale. We\nvalidate MIRIX in two demanding settings. First, on ScreenshotVQA, a\nchallenging multimodal benchmark comprising nearly 20,000 high-resolution\ncomputer screenshots per sequence, requiring deep contextual understanding and\nwhere no existing memory systems can be applied, MIRIX achieves 35% higher\naccuracy than the RAG baseline while reducing storage requirements by 99.9%.\nSecond, on LOCOMO, a long-form conversation benchmark with single-modal textual\ninput, MIRIX attains state-of-the-art performance of 85.4%, far surpassing\nexisting baselines. These results show that MIRIX sets a new performance\nstandard for memory-augmented LLM agents. To allow users to experience our\nmemory system, we provide a packaged application powered by MIRIX. It monitors\nthe screen in real time, builds a personalized memory base, and offers\nintuitive visualization and secure local storage to ensure privacy.\n","authors":["Yu Wang","Xi Chen"],"pdf_url":"https://arxiv.org/pdf/2507.07957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07939v1","updated":"2025-07-10T17:23:42Z","published":"2025-07-10T17:23:42Z","title":"SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement\n  and Entropy-aware Alignment","summary":"  While Vision-Language Models (VLMs) have shown promising progress in general\nmultimodal tasks, they often struggle in industrial anomaly detection and\nreasoning, particularly in delivering interpretable explanations and\ngeneralizing to unseen categories. This limitation stems from the inherently\ndomain-specific nature of anomaly detection, which hinders the applicability of\nexisting VLMs in industrial scenarios that require precise, structured, and\ncontext-aware analysis. To address these challenges, we propose SAGE, a\nVLM-based framework that enhances anomaly reasoning through Self-Guided Fact\nEnhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE\nintegrates domain-specific knowledge into visual reasoning via fact extraction\nand fusion, while E-DPO aligns model outputs with expert preferences using\nentropy-aware optimization. Additionally, we introduce AD-PL, a\npreference-optimized dataset tailored for industrial anomaly reasoning,\nconsisting of 28,415 question-answering instances with expert-ranked responses.\nTo evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation\n(MLE), a quantitative framework analyzing model logic and consistency. SAGE\ndemonstrates superior performance on industrial anomaly datasets under\nzero-shot and one-shot settings. The code, model and dataset are available at\nhttps://github.com/amoreZgx1n/SAGE.\n","authors":["Guoxin Zang","Xue Li","Donglin Di","Lanshun Nie","Dechen Zhan","Yang Song","Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2507.07939v1.pdf","comment":"Accepted by ACMMM2025"},{"id":"http://arxiv.org/abs/2505.20625v2","updated":"2025-07-10T17:12:27Z","published":"2025-05-27T02:05:42Z","title":"Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven\n  Collaboration","summary":"  Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). Existing works leverage agent-based divide-and-conquer\nmethods for processing long contexts. But these methods face crucial\nlimitations, including prohibitive accumulated latency and amplified\ninformation loss from excessive agent invocations, and the disruption of\ninherent textual dependencies by immoderate partitioning. In this paper, we\npropose a novel multi-agent framework XpandA (Expand-Agent) coupled with\nquestion-driven workflow and dynamic partitioning for robust long-context\nprocessing. XpandA overcomes these limitations through: 1) dynamic partitioning\nof long texts, which adaptively modulates the filling rate of context windows\nfor input sequences of vastly varying lengths; 2) question-guided protocol to\nupdate flat information ensembles within centralized shared memory,\nconstructing consistent inter-agent knowledge across partitions; and 3)\nselectively replaying specific partitions based on the state-tracking of\nquestion-information couples to promote the resolution of inverted-order\nstructures across partitions (e.g., flashbacks). We perform a comprehensive\nevaluation of XpandA on multiple long-context benchmarks with length varying\nfrom 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long\nsequences and its significant effectiveness in enhancing the long-context\ncapabilities of various LLMs by achieving 20\\% improvements and 1.5x inference\nspeedup over baselines of full-context, RAG and previous agent-based methods.\n","authors":["Sibo Xiao","Zixin Lin","Wenyang Gao","Hui Chen","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.20625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10955v2","updated":"2025-07-10T16:59:24Z","published":"2024-09-17T07:44:06Z","title":"Investigating Context-Faithfulness in Large Language Models: The Roles\n  of Memory Strength and Evidence Style","summary":"  Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by\nincorporating external information into the response generation process.\nHowever, how context-faithful LLMs are and what factors influence LLMs' context\nfaithfulness remain largely unexplored. In this study, we investigate the\nimpact of memory strength and evidence presentation on LLMs' receptiveness to\nexternal evidence. We quantify the memory strength of LLMs by measuring the\ndivergence in LLMs' responses to different paraphrases of the same question,\nwhich is not considered by previous works. We also generate evidence in various\nstyles to examine LLMs' behavior. Our results show that for questions with high\nmemory strength, LLMs are more likely to rely on internal memory. Furthermore,\npresenting paraphrased evidence significantly increases LLMs' receptiveness\ncompared to simple repetition or adding details. These findings provide key\ninsights for improving retrieval-augmented generation and context-aware LLMs.\nOur code is available at https://github.com/liyp0095/ContextFaithful.\n","authors":["Yuepei Li","Kang Zhou","Qiao Qiao","Bach Nguyen","Qing Wang","Qi Li"],"pdf_url":"https://arxiv.org/pdf/2409.10955v2.pdf","comment":"This work is published at ACL 2025"},{"id":"http://arxiv.org/abs/2507.07910v1","updated":"2025-07-10T16:44:33Z","published":"2025-07-10T16:44:33Z","title":"DTECT: Dynamic Topic Explorer & Context Tracker","summary":"  The explosive growth of textual data over time presents a significant\nchallenge in uncovering evolving themes and trends. Existing dynamic topic\nmodeling techniques, while powerful, often exist in fragmented pipelines that\nlack robust support for interpretation and user-friendly exploration. We\nintroduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end\nsystem that bridges the gap between raw textual data and meaningful temporal\ninsights. DTECT provides a unified workflow that supports data preprocessing,\nmultiple model architectures, and dedicated evaluation metrics to analyze the\ntopic quality of temporal topic models. It significantly enhances\ninterpretability by introducing LLM-driven automatic topic labeling, trend\nanalysis via temporally salient words, interactive visualizations with\ndocument-level summarization, and a natural language chat interface for\nintuitive data querying. By integrating these features into a single, cohesive\nplatform, DTECT empowers users to more effectively track and understand\nthematic dynamics. DTECT is open-source and available at\nhttps://github.com/AdhyaSuman/DTECT.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2507.07910v1.pdf","comment":"Code: https://github.com/AdhyaSuman/DTECT | Demo:\n  https://huggingface.co/spaces/AdhyaSuman/DTECT | Video:\n  https://youtu.be/B8nNfxFoJAU"},{"id":"http://arxiv.org/abs/2507.06203v2","updated":"2025-07-10T16:43:36Z","published":"2025-07-08T17:29:07Z","title":"A Survey on Latent Reasoning","summary":"  Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/.\n","authors":["Rui-Jie Zhu","Tianhao Peng","Tianhao Cheng","Xingwei Qu","Jinfa Huang","Dawei Zhu","Hao Wang","Kaiwen Xue","Xuanliang Zhang","Yong Shan","Tianle Cai","Taylor Kergan","Assel Kembay","Andrew Smith","Chenghua Lin","Binh Nguyen","Yuqi Pan","Yuhong Chou","Zefan Cai","Zhenhe Wu","Yongchi Zhao","Tianyu Liu","Jian Yang","Wangchunshu Zhou","Chujie Zheng","Chongxuan Li","Yuyin Zhou","Zhoujun Li","Zhaoxiang Zhang","Jiaheng Liu","Ge Zhang","Wenhao Huang","Jason Eshraghian"],"pdf_url":"https://arxiv.org/pdf/2507.06203v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07887v1","updated":"2025-07-10T16:17:40Z","published":"2025-07-10T16:17:40Z","title":"Automating MD simulations for Proteins using Large language Models:\n  NAMD-Agent","summary":"  Molecular dynamics simulations are an essential tool in understanding protein\nstructure, dynamics, and function at the atomic level. However, preparing high\nquality input files for MD simulations can be a time consuming and error prone\nprocess. In this work, we introduce an automated pipeline that leverages Large\nLanguage Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with\npython scripting and Selenium based web automation to streamline the generation\nof MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based\ninterface for preparing simulation-ready inputs for NAMD. By integrating\nGemini's code generation and iterative refinement capabilities, simulation\nscripts are automatically written, executed, and revised to navigate CHARMM\nGUI, extract appropriate parameters, and produce the required NAMD input files.\nPost processing is performed using additional software to further refine the\nsimulation outputs, thereby enabling a complete and largely hands free\nworkflow. Our results demonstrate that this approach reduces setup time,\nminimizes manual errors, and offers a scalable solution for handling multiple\nprotein systems in parallel. This automated framework paves the way for broader\napplication of LLMs in computational structural biology, offering a robust and\nadaptable platform for future developments in simulation automation.\n","authors":["Achuth Chandrasekhar","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2507.07887v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2307.10016v2","updated":"2025-07-10T16:03:19Z","published":"2023-07-19T14:55:50Z","title":"When Dialects Collide: How Socioeconomic Mixing Affects Language Use","summary":"  The socioeconomic background of people and how they use standard forms of\nlanguage are not independent, as demonstrated in various sociolinguistic\nstudies. However, the extent to which these correlations may be influenced by\nthe mixing of people from different socioeconomic classes remains relatively\nunexplored from a quantitative perspective. In this work we leverage geotagged\ntweets and transferable computational methods to map deviations from standard\nEnglish on a large scale, in seven thousand administrative areas of England and\nWales. We combine these data with high-resolution income maps to assign a proxy\nsocioeconomic indicator to home-located users. Strikingly, across eight\nmetropolitan areas we find a consistent pattern suggesting that the more\ndifferent socioeconomic classes mix, the less interdependent the frequency of\ntheir departures from standard grammar and their income become. Further, we\npropose an agent-based model of linguistic variety adoption that sheds light on\nthe mechanisms that produce the observations seen in the data.\n","authors":["Thomas Louf","José J. Ramasco","David Sánchez","Márton Karsai"],"pdf_url":"https://arxiv.org/pdf/2307.10016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.19598v2","updated":"2025-07-10T15:58:00Z","published":"2025-05-26T07:08:38Z","title":"Evaluating Robustness of Large Audio Language Models to Audio Injection:\n  An Empirical Study","summary":"  Large Audio-Language Models (LALMs) are increasingly deployed in real-world\napplications, yet their robustness against malicious audio injection attacks\nremains underexplored. This study systematically evaluates five leading LALMs\nacross four attack scenarios: Audio Interference Attack, Instruction Following\nAttack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics\nlike Defense Success Rate, Context Robustness Score, and Judgment Robustness\nIndex, their vulnerabilities and resilience were quantitatively assessed.\nExperimental results reveal significant performance disparities among models;\nno single model consistently outperforms others across all attack types. The\nposition of malicious content critically influences attack effectiveness,\nparticularly when placed at the beginning of sequences. A negative correlation\nbetween instruction-following capability and robustness suggests models\nadhering strictly to instructions may be more susceptible, contrasting with\ngreater resistance by safety-aligned models. Additionally, system prompts show\nmixed effectiveness, indicating the need for tailored strategies. This work\nintroduces a benchmark framework and highlights the importance of integrating\nrobustness into training pipelines. Findings emphasize developing multi-modal\ndefenses and architectural designs that decouple capability from susceptibility\nfor secure LALMs deployment.\n","authors":["Guanyu Hou","Jiaming He","Yinhang Zhou","Ji Guo","Yitong Qiao","Rui Zhang","Wenbo Jiang"],"pdf_url":"https://arxiv.org/pdf/2505.19598v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07870v1","updated":"2025-07-10T15:52:04Z","published":"2025-07-10T15:52:04Z","title":"DocCHA: Towards LLM-Augmented Interactive Online diagnosis System","summary":"  Despite the impressive capabilities of Large Language Models (LLMs), existing\nConversational Health Agents (CHAs) remain static and brittle, incapable of\nadaptive multi-turn reasoning, symptom clarification, or transparent\ndecision-making. This hinders their real-world applicability in clinical\ndiagnosis, where iterative and structured dialogue is essential. We propose\nDocCHA, a confidence-aware, modular framework that emulates clinical reasoning\nby decomposing the diagnostic process into three stages: (1) symptom\nelicitation, (2) history acquisition, and (3) causal graph construction. Each\nmodule uses interpretable confidence scores to guide adaptive questioning,\nprioritize informative clarifications, and refine weak reasoning links.\n  Evaluated on two real-world Chinese consultation datasets (IMCS21, DX),\nDocCHA consistently outperforms strong prompting-based LLM baselines (GPT-3.5,\nGPT-4o, LLaMA-3), achieving up to 5.18 percent higher diagnostic accuracy and\nover 30 percent improvement in symptom recall, with only modest increase in\ndialogue turns. These results demonstrate the effectiveness of DocCHA in\nenabling structured, transparent, and efficient diagnostic conversations --\npaving the way for trustworthy LLM-powered clinical assistants in multilingual\nand resource-constrained settings.\n","authors":["Xinyi Liu","Dachun Sun","Yi R. Fung","Dilek Hakkani-Tür","Tarek Abdelzaher"],"pdf_url":"https://arxiv.org/pdf/2507.07870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07868v1","updated":"2025-07-10T15:48:23Z","published":"2025-07-10T15:48:23Z","title":"Alpay Algebra V: Multi-Layered Semantic Games and Transfinite\n  Fixed-Point Simulation","summary":"  This paper extends the self-referential framework of Alpay Algebra into a\nmulti-layered semantic game architecture where transfinite fixed-point\nconvergence encompasses hierarchical sub-games at each iteration level.\nBuilding upon Alpay Algebra IV's empathetic embedding concept, we introduce a\nnested game-theoretic structure where the alignment process between AI systems\nand documents becomes a meta-game containing embedded decision problems. We\nformalize this through a composite operator $\\phi(\\cdot, \\gamma(\\cdot))$ where\n$\\phi$ drives the main semantic convergence while $\\gamma$ resolves local\nsub-games. The resulting framework demonstrates that game-theoretic reasoning\nemerges naturally from fixed-point iteration rather than being imposed\nexternally. We prove a Game Theorem establishing existence and uniqueness of\nsemantic equilibria under realistic cognitive simulation assumptions. Our\nverification suite includes adaptations of Banach's fixed-point theorem to\ntransfinite contexts, a novel $\\phi$-topology based on the\nKozlov-Maz'ya-Rossmann formula for handling semantic singularities, and\ncategorical consistency tests via the Yoneda lemma. The paper itself functions\nas a semantic artifact designed to propagate its fixed-point patterns in AI\nembedding spaces -- a deliberate instantiation of the \"semantic virus\" concept\nit theorizes. All results are grounded in category theory, information theory,\nand realistic AI cognition models, ensuring practical applicability beyond pure\nmathematical abstraction.\n","authors":["Bugra Kilictas","Faruk Alpay"],"pdf_url":"https://arxiv.org/pdf/2507.07868v1.pdf","comment":"18 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.06167v3","updated":"2025-07-10T15:41:04Z","published":"2025-07-08T16:47:16Z","title":"Skywork-R1V3 Technical Report","summary":"  We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.\n","authors":["Wei Shen","Jiangbo Pei","Yi Peng","Xuchen Song","Yang Liu","Jian Peng","Haofeng Sun","Yunzhuo Hao","Peiyu Wang","Jianhao Zhang","Yahui Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.06167v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07847v1","updated":"2025-07-10T15:26:59Z","published":"2025-07-10T15:26:59Z","title":"From Ambiguity to Accuracy: The Transformative Effect of Coreference\n  Resolution on Retrieval-Augmented Generation systems","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in\nnatural language processing (NLP), improving factual consistency and reducing\nhallucinations by integrating external document retrieval with large language\nmodels (LLMs). However, the effectiveness of RAG is often hindered by\ncoreferential complexity in retrieved documents, introducing ambiguity that\ndisrupts in-context learning. In this study, we systematically investigate how\nentity coreference affects both document retrieval and generative performance\nin RAG-based systems, focusing on retrieval relevance, contextual\nunderstanding, and overall response quality. We demonstrate that coreference\nresolution enhances retrieval effectiveness and improves question-answering\n(QA) performance. Through comparative analysis of different pooling strategies\nin retrieval tasks, we find that mean pooling demonstrates superior context\ncapturing ability after applying coreference resolution. In QA tasks, we\ndiscover that smaller models benefit more from the disambiguation process,\nlikely due to their limited inherent capacity for handling referential\nambiguity. With these findings, this study aims to provide a deeper\nunderstanding of the challenges posed by coreferential complexity in RAG,\nproviding guidance for improving retrieval and generation in\nknowledge-intensive AI applications.\n","authors":["Youngjoon Jang","Seongtae Hong","Junyoung Son","Sungjin Park","Chanjun Park","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2507.07847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18865v2","updated":"2025-07-10T15:21:39Z","published":"2024-04-29T16:52:57Z","title":"Truth-value judgment in language models: 'truth directions' are context\n  sensitive","summary":"  Recent work has demonstrated that the latent spaces of large language models\n(LLMs) contain directions predictive of the truth of sentences. Multiple\nmethods recover such directions and build probes that are described as\nuncovering a model's \"knowledge\" or \"beliefs\". We investigate this phenomenon,\nlooking closely at the impact of context on the probes. Our experiments\nestablish where in the LLM the probe's predictions are (most) sensitive to the\npresence of related sentences, and how to best characterize this kind of\nsensitivity. We do so by measuring different types of consistency errors that\noccur after probing an LLM whose inputs consist of hypotheses preceded by\n(negated) supporting and contradicting sentences. We also perform a causal\nintervention experiment, investigating whether moving the representation of a\npremise along these truth-value directions influences the position of an\nentailed or contradicted sentence along that same direction. We find that the\nprobes we test are generally context sensitive, but that contexts which should\nnot affect the truth often still impact the probe outputs. Our experiments show\nthat the type of errors depend on the layer, the model, and the kind of data.\nFinally, our results suggest that truth-value directions are causal mediators\nin the inference process that incorporates in-context information.\n","authors":["Stefan F. Schouten","Peter Bloem","Ilia Markov","Piek Vossen"],"pdf_url":"https://arxiv.org/pdf/2404.18865v2.pdf","comment":"COLM 2025"},{"id":"http://arxiv.org/abs/2502.12896v5","updated":"2025-07-10T15:12:15Z","published":"2025-02-18T14:32:44Z","title":"None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks","summary":"  In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers.\n","authors":["Eva Sánchez Salido","Julio Gonzalo","Guillermo Marco"],"pdf_url":"https://arxiv.org/pdf/2502.12896v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07618v4","updated":"2025-07-10T15:03:47Z","published":"2024-11-12T07:54:13Z","title":"Constrain Alignment with Sparse Autoencoders","summary":"  The alignment of large language models (LLMs) with human preferences remains\na key challenge. While post-training techniques like Reinforcement Learning\nfrom Human Feedback (RLHF) and Direct Preference Optimization (DPO) have\nachieved notable success, they often introduce computational inefficiencies and\ntraining instability. In this paper, we propose Feature-level constrained\nPreference Optimization (FPO), a novel method designed to simplify the\nalignment process while ensuring stability. FPO leverages pre-trained Sparse\nAutoencoders (SAEs) and introduces feature-level constraints, allowing for\nefficient, sparsity-enforced alignment. Our approach enjoys efficiency by using\nsparse features activated in a well-trained sparse autoencoder and the quality\nof sequential KL divergence by using the feature-level offline reference.\nExperimental results on benchmark datasets demonstrate that FPO achieves a\n5.08% absolute improvement in win rate with much lower computational cost\ncompared to state-of-the-art baselines, making it a promising solution for\nefficient and controllable LLM alignments.\n","authors":["Qingyu Yin","Chak Tou Leong","Minjun Zhu","Hanqi Yan","Qiang Zhang","Yulan He","Wenjie Li","Jun Wang","Yue Zhang","Linyi Yang"],"pdf_url":"https://arxiv.org/pdf/2411.07618v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15245v2","updated":"2025-07-10T15:03:28Z","published":"2024-06-21T15:35:49Z","title":"Unsupervised Morphological Tree Tokenizer","summary":"  As a cornerstone in language modeling, tokenization involves segmenting text\ninputs into pre-defined atomic units. Conventional statistical tokenizers often\ndisrupt constituent boundaries within words, thereby corrupting semantic\ninformation. To address this drawback, we introduce morphological structure\nguidance to tokenization and propose a deep model to induce character-level\nstructures of words. Specifically, the deep model jointly encodes internal\nstructures and representations of words with a mechanism named\n$\\textit{MorphOverriding}$ to ensure the indecomposability of morphemes. By\ntraining the model with self-supervised objectives, our method is capable of\ninducing character-level structures that align with morphological rules without\nannotated training data. Based on the induced structures, our algorithm\ntokenizes words through vocabulary matching in a top-down manner. Empirical\nresults indicate that the proposed method effectively retains complete\nmorphemes and outperforms widely adopted methods such as BPE and WordPiece on\nboth morphological segmentation tasks and language modeling tasks. Code is\navailable at https://github.com/martianmartina/TreeTokenizer.\n","authors":["Qingyang Zhu","Xiang Hu","Pengyu Ji","Wei Wu","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2406.15245v2.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2506.03053v2","updated":"2025-07-10T14:54:28Z","published":"2025-06-03T16:33:47Z","title":"MAEBE: Multi-Agent Emergent Behavior Framework","summary":"  Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.\n","authors":["Sinem Erisken","Timothy Gothard","Martin Leitgab","Ram Potham"],"pdf_url":"https://arxiv.org/pdf/2506.03053v2.pdf","comment":"Preprint. This work has been submitted to the Multi-Agent Systems\n  Workshop at ICML 2025 for review"},{"id":"http://arxiv.org/abs/2507.01936v2","updated":"2025-07-10T14:54:09Z","published":"2025-07-02T17:46:56Z","title":"The Thin Line Between Comprehension and Persuasion in LLMs","summary":"  Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.\n","authors":["Adrian de Wynter","Tangming Yuan"],"pdf_url":"https://arxiv.org/pdf/2507.01936v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2507.07824v1","updated":"2025-07-10T14:53:59Z","published":"2025-07-10T14:53:59Z","title":"Conditional Unigram Tokenization with Parallel Data","summary":"  We introduce conditional unigram tokenization, a novel approach that extends\nunigram tokenization by conditioning target token probabilities on\nsource-language tokens from parallel data. Given a fixed source tokenizer, our\nmethod learns a target tokenizer that maximizes cross-lingual semantic\nalignment. We evaluate our tokenizer on four language pairs across different\nfamilies and resource levels, examining intrinsic properties and downstream\nperformance on machine translation and language modeling. While our conditional\ntokenizer maintains comparable statistical properties to standard unigram\ntokenizers, results are mixed: we observe no improvements in machine\ntranslation quality, but find consistent perplexity reductions in language\nmodeling. We hypothesize that quadratic scaling of conditional probability\nestimation with respect to the vocabulary size creates a data efficiency\nbottleneck. Our findings suggest that alternative parameterizations may be\nnecessary for practical cross-lingual tokenization.\n","authors":["Gianluca Vico","Jindřinch Libovický"],"pdf_url":"https://arxiv.org/pdf/2507.07824v1.pdf","comment":"21 pages, 4 figures, submitted to Tokenization Workshop (TokShop) at\n  ICML 2025"},{"id":"http://arxiv.org/abs/2507.07817v1","updated":"2025-07-10T14:46:33Z","published":"2025-07-10T14:46:33Z","title":"On the Effect of Instruction Tuning Loss on Generalization","summary":"  Instruction Tuning has emerged as a pivotal post-training paradigm that\nenables pre-trained language models to better follow user instructions. Despite\nits significance, little attention has been given to optimizing the loss\nfunction used. A fundamental, yet often overlooked, question is whether the\nconventional auto-regressive objective - where loss is computed only on\nresponse tokens, excluding prompt tokens - is truly optimal for instruction\ntuning. In this work, we systematically investigate the impact of\ndifferentially weighting prompt and response tokens in instruction tuning loss,\nand propose Weighted Instruction Tuning (WIT) as a better alternative to\nconventional instruction tuning. Through extensive experiments on five language\nmodels of different families and scale, three finetuning datasets of different\nsizes, and five diverse evaluation benchmarks, we show that the standard\ninstruction tuning loss often yields suboptimal performance and limited\nrobustness to input prompt variations. We find that a low-to-moderate weight\nfor prompt tokens coupled with a moderate-to-high weight for response tokens\nyields the best-performing models across settings and also serve as better\nstarting points for the subsequent preference alignment training. These\nfindings highlight the need to reconsider instruction tuning loss and offer\nactionable insights for developing more robust and generalizable models. Our\ncode is open-sourced at https://github.com/kowndinya-renduchintala/WIT.\n","authors":["Anwoy Chatterjee","H S V N S Kowndinya Renduchintala","Sumit Bhatia","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2507.07817v1.pdf","comment":"Transactions of the Association for Computational Linguistics (TACL)"},{"id":"http://arxiv.org/abs/2507.07810v1","updated":"2025-07-10T14:40:31Z","published":"2025-07-10T14:40:31Z","title":"Understanding and Controlling Repetition Neurons and Induction Heads in\n  In-Context Learning","summary":"  This paper investigates the relationship between large language models'\n(LLMs) ability to recognize repetitive input patterns and their performance on\nin-context learning (ICL). In contrast to prior work that has primarily focused\non attention heads, we examine this relationship from the perspective of skill\nneurons, specifically repetition neurons. Our experiments reveal that the\nimpact of these neurons on ICL performance varies depending on the depth of the\nlayer in which they reside. By comparing the effects of repetition neurons and\ninduction heads, we further identify strategies for reducing repetitive outputs\nwhile maintaining strong ICL capabilities.\n","authors":["Nhi Hoai Doan","Tatsuya Hiraoka","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2507.07810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07808v1","updated":"2025-07-10T14:35:37Z","published":"2025-07-10T14:35:37Z","title":"Bridging Logic and Learning: Decoding Temporal Logic Embeddings via\n  Transformers","summary":"  Continuous representations of logic formulae allow us to integrate symbolic\nknowledge into data-driven learning algorithms. If such embeddings are\nsemantically consistent, i.e. if similar specifications are mapped into nearby\nvectors, they enable continuous learning and optimization directly in the\nsemantic space of formulae. However, to translate the optimal continuous\nrepresentation into a concrete requirement, such embeddings must be invertible.\nWe tackle this issue by training a Transformer-based decoder-only model to\ninvert semantic embeddings of Signal Temporal Logic (STL) formulae. STL is a\npowerful formalism that allows us to describe properties of signals varying\nover time in an expressive yet concise way. By constructing a small vocabulary\nfrom STL syntax, we demonstrate that our proposed model is able to generate\nvalid formulae after only 1 epoch and to generalize to the semantics of the\nlogic in about 10 epochs. Additionally, the model is able to decode a given\nembedding into formulae that are often simpler in terms of length and nesting\nwhile remaining semantically close (or equivalent) to gold references. We show\nthe effectiveness of our methodology across various levels of training formulae\ncomplexity to assess the impact of training data on the model's ability to\neffectively capture the semantic information contained in the embeddings and\ngeneralize out-of-distribution. Finally, we deploy our model for solving a\nrequirement mining task, i.e. inferring STL specifications that solve a\nclassification task on trajectories, performing the optimization directly in\nthe semantic space.\n","authors":["Sara Candussio","Gaia Saveri","Gabriele Sarti","Luca Bortolussi"],"pdf_url":"https://arxiv.org/pdf/2507.07808v1.pdf","comment":"16 pages, 3 figures, to be published in ECML-PKDD"},{"id":"http://arxiv.org/abs/2502.04426v2","updated":"2025-07-10T14:31:21Z","published":"2025-02-06T18:52:10Z","title":"Decoding AI Judgment: How LLMs Assess News Credibility and Bias","summary":"  Large Language Models (LLMs) are increasingly embedded in workflows that\ninvolve evaluative processes. This raises the need to examine how such\nevaluations are built, what assumptions they rely on, and how their strategies\ndiverge from those of humans. We benchmark six LLMs against expert\nratings--NewsGuard and Media Bias/Fact Check (MBFC)--and against human\njudgments collected through a controlled experiment. To enable direct\ncomparison, we implement a structured agentic framework in which both models\nand non-expert participants follow the same evaluation procedure: selecting\ncriteria, retrieving content, and producing justifications. Despite output\nalignment, LLMs rely on different mechanisms: lexical associations and\nstatistical priors replace contextual reasoning. This reliance produces\nsystematic effects: political asymmetries, opaque justifications, and a\ntendency to confuse linguistic form with epistemic validity. Delegating\njudgment to such systems does not merely automate evaluation--it redefines it,\nshifting from normative reasoning to pattern-based approximation.\n","authors":["Edoardo Loru","Jacopo Nudo","Niccolò Di Marco","Alessandro Santirocchi","Roberto Atzeni","Matteo Cinelli","Vincenzo Cestari","Clelia Rossi-Arnaud","Walter Quattrociocchi"],"pdf_url":"https://arxiv.org/pdf/2502.04426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07803v1","updated":"2025-07-10T14:28:39Z","published":"2025-07-10T14:28:39Z","title":"StreamUni: Achieving Streaming Speech Translation with a Unified Large\n  Speech-Language Model","summary":"  Streaming speech translation (StreamST) requires determining appropriate\ntiming, known as policy, to generate translations while continuously receiving\nsource speech inputs, balancing low latency with high translation quality.\nHowever, existing StreamST methods typically operate on sentence-level speech\nsegments, referred to as simultaneous speech translation (SimulST). In\npractice, they require collaboration with segmentation models to accomplish\nStreamST, where the truncated speech segments constrain SimulST models to make\npolicy decisions and generate translations based on limited contextual\ninformation. Moreover, SimulST models struggle to learn effective policies due\nto the complexity of speech inputs and cross-lingual generation. To address\nthese challenges, we propose StreamUni, which achieves StreamST through a\nunified Large Speech-Language Model (LSLM). Specifically, StreamUni\nincorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate\nmulti-stage outputs. Leveraging these multi-stage outputs, StreamUni\nsimultaneously accomplishes speech segmentation, policy decision, and\ntranslation generation, completing StreamST without requiring massive\npolicy-specific training. Additionally, we propose a streaming CoT training\nmethod that enhances low-latency policy decisions and generation capabilities\nusing limited CoT data. Experiments demonstrate that our approach achieves\nstate-of-the-art performance on StreamST tasks.\n","authors":["Shoutao Guo","Xiang Li","Shaolei Zhang","Mengge Liu","Wei Chen","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2507.07803v1.pdf","comment":"The code is at https://github.com/ictnlp/StreamUni; The model is at\n  https://huggingface.co/ICTNLP/StreamUni-Phi4"},{"id":"http://arxiv.org/abs/2411.11984v2","updated":"2025-07-10T14:18:01Z","published":"2024-11-18T19:14:36Z","title":"Understanding Chain-of-Thought in LLMs through Information Theory","summary":"  Large Language Models (LLMs) have shown impressive performance in complex\nreasoning tasks through the use of Chain-of-Thought (CoT) reasoning, allowing\nmodels to break down problems into manageable sub-tasks. However, existing CoT\nevaluation techniques either require annotated CoT data or fall short in\naccurately assessing intermediate reasoning steps, leading to high rates of\nfalse positives. In this paper, we formalize CoT reasoning in LLMs through an\ninformation-theoretic lens. Specifically, our framework quantifies the\n`information-gain' at each reasoning step, enabling the identification of\nfailure modes in LLMs without the need for expensive annotated datasets. We\ndemonstrate the efficacy of our approach through extensive experiments on toy\narithmetic, GSM8K and PRM800k datasets, where it significantly outperforms\nexisting outcome-based methods by providing more accurate insights into model\nperformance on individual subtasks.\n","authors":["Jean-Francois Ton","Muhammad Faaiz Taufiq","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.11984v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06892v2","updated":"2025-07-10T13:42:04Z","published":"2025-07-09T14:29:45Z","title":"Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model","summary":"  Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.\n","authors":["Jing Liang","Hongyao Tang","Yi Ma","Jinyi Liu","Yan Zheng","Shuyue Hu","Lei Bai","Jianye Hao"],"pdf_url":"https://arxiv.org/pdf/2507.06892v2.pdf","comment":"Preliminary version, v2, added more details and corrected some minor\n  mistakes. Project page: https://anitaleungxx.github.io/ReMix"},{"id":"http://arxiv.org/abs/2507.07748v1","updated":"2025-07-10T13:26:34Z","published":"2025-07-10T13:26:34Z","title":"When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical\n  Advances, and Ethical Governance","summary":"  This paper establishes the first comprehensive review of Large Language\nModels (LLMs) applied within the legal domain. It pioneers an innovative dual\nlens taxonomy that integrates legal reasoning frameworks and professional\nontologies to systematically unify historical research and contemporary\nbreakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such\nas contextual reasoning and generative argumentation, surmount traditional\nlimitations by dynamically capturing legal semantics and unifying evidence\nreasoning. Significant progress is documented in task generalization, reasoning\nformalization, workflow integration, and addressing core challenges in text\nprocessing, knowledge integration, and evaluation rigor via technical\ninnovations like sparse attention mechanisms and mixture-of-experts\narchitectures. However, widespread adoption of LLM introduces critical\nchallenges: hallucination, explainability deficits, jurisdictional adaptation\ndifficulties, and ethical asymmetry. This review proposes a novel taxonomy that\nmaps legal roles to NLP subtasks and computationally implements the Toulmin\nargumentation framework, thus systematizing advances in reasoning, retrieval,\nprediction, and dispute resolution. It identifies key frontiers including\nlow-resource systems, multimodal evidence integration, and dynamic rebuttal\nhandling. Ultimately, this work provides both a technical roadmap for\nresearchers and a conceptual framework for practitioners navigating the\nalgorithmic future, laying a robust foundation for the next era of legal\nartificial intelligence. We have created a GitHub repository to index the\nrelevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.\n","authors":["Peizhang Shao","Linrui Xu","Jinxi Wang","Wei Zhou","Xingyu Wu"],"pdf_url":"https://arxiv.org/pdf/2507.07748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07741v1","updated":"2025-07-10T13:21:12Z","published":"2025-07-10T13:21:12Z","title":"Code-Switching in End-to-End Automatic Speech Recognition: A Systematic\n  Literature Review","summary":"  Motivated by a growing research interest into automatic speech recognition\n(ASR), and the growing body of work for languages in which code-switching (CS)\noften occurs, we present a systematic literature review of code-switching in\nend-to-end ASR models. We collect and manually annotate papers published in\npeer reviewed venues. We document the languages considered, datasets, metrics,\nmodel choices, and performance, and present a discussion of challenges in\nend-to-end ASR for code-switching. Our analysis thus provides insights on\ncurrent research efforts and available resources as well as opportunities and\ngaps to guide future research.\n","authors":["Maha Tufail Agro","Atharva Kulkarni","Karima Kadaoui","Zeerak Talat","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2507.07741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07735v1","updated":"2025-07-10T13:15:20Z","published":"2025-07-10T13:15:20Z","title":"GuardVal: Dynamic Large Language Model Jailbreak Evaluation for\n  Comprehensive Safety Testing","summary":"  Jailbreak attacks reveal critical vulnerabilities in Large Language Models\n(LLMs) by causing them to generate harmful or unethical content. Evaluating\nthese threats is particularly challenging due to the evolving nature of LLMs\nand the sophistication required in effectively probing their vulnerabilities.\nCurrent benchmarks and evaluation methods struggle to fully address these\nchallenges, leaving gaps in the assessment of LLM vulnerabilities. In this\npaper, we review existing jailbreak evaluation practices and identify three\nassumed desiderata for an effective jailbreak evaluation protocol. To address\nthese challenges, we introduce GuardVal, a new evaluation protocol that\ndynamically generates and refines jailbreak prompts based on the defender LLM's\nstate, providing a more accurate assessment of defender LLMs' capacity to\nhandle safety-critical situations. Moreover, we propose a new optimization\nmethod that prevents stagnation during prompt refinement, ensuring the\ngeneration of increasingly effective jailbreak prompts that expose deeper\nweaknesses in the defender LLMs. We apply this protocol to a diverse set of\nmodels, from Mistral-7b to GPT-4, across 10 safety domains. Our findings\nhighlight distinct behavioral patterns among the models, offering a\ncomprehensive view of their robustness. Furthermore, our evaluation process\ndeepens the understanding of LLM behavior, leading to insights that can inform\nfuture research and drive the development of more secure models.\n","authors":["Peiyan Zhang","Haibo Jin","Liying Kang","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2507.07735v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2507.07725v1","updated":"2025-07-10T12:58:45Z","published":"2025-07-10T12:58:45Z","title":"Not All Preferences are What You Need for Post-Training: Selective\n  Alignment Strategy for Preference Optimization","summary":"  Post-training alignment of large language models (LLMs) is a critical\nchallenge, as not all tokens contribute equally to model performance. This\npaper introduces a selective alignment strategy that prioritizes high-impact\ntokens within preference pairs, leveraging token-level log-probability\ndifferences between the current policy and a reference model. By focusing on\nthese informative tokens, our approach reduces computational overhead and\nenhances alignment fidelity. We further explore the role of reference model\nquality, demonstrating that stronger reference models significantly improve\ntoken selection accuracy and overall optimization effectiveness. Comprehensive\nexperiments on benchmarks such as Arena-Hard and MT-Bench validate the\nsuperiority of our Selective-DPO method over standard DPO and\ndistillation-based baselines. Our findings highlight the importance of\ntoken-level optimization and reference model selection in advancing preference\nalignment for LLMs. The code is available at\nhttps://github.com/Dongzhijin/SDPO.\n","authors":["Zhijin Dong"],"pdf_url":"https://arxiv.org/pdf/2507.07725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07700v1","updated":"2025-07-10T12:27:03Z","published":"2025-07-10T12:27:03Z","title":"Rethinking the Privacy of Text Embeddings: A Reproducibility Study of\n  \"Text Embeddings Reveal (Almost) As Much As Text\"","summary":"  Text embeddings are fundamental to many natural language processing (NLP)\ntasks, extensively applied in domains such as recommendation systems and\ninformation retrieval (IR). Traditionally, transmitting embeddings instead of\nraw text has been seen as privacy-preserving. However, recent methods such as\nVec2Text challenge this assumption by demonstrating that controlled decoding\ncan successfully reconstruct original texts from black-box embeddings. The\nunexpectedly strong results reported by Vec2Text motivated us to conduct\nfurther verification, particularly considering the typically non-intuitive and\nopaque structure of high-dimensional embedding spaces. In this work, we\nreproduce the Vec2Text framework and evaluate it from two perspectives: (1)\nvalidating the original claims, and (2) extending the study through targeted\nexperiments. First, we successfully replicate the original key results in both\nin-domain and out-of-domain settings, with only minor discrepancies arising due\nto missing artifacts, such as model checkpoints and dataset splits.\nFurthermore, we extend the study by conducting a parameter sensitivity\nanalysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,\npasswords), and exploring embedding quantization as a lightweight privacy\ndefense. Our results show that Vec2Text is effective under ideal conditions,\ncapable of reconstructing even password-like sequences that lack clear\nsemantics. However, we identify key limitations, including its sensitivity to\ninput sequence length. We also find that Gaussian noise and quantization\ntechniques can mitigate the privacy risks posed by Vec2Text, with quantization\noffering a simpler and more widely applicable solution. Our findings emphasize\nthe need for caution in using text embeddings and highlight the importance of\nfurther research into robust defense mechanisms for NLP systems.\n","authors":["Dominykas Seputis","Yongkang Li","Karsten Langerak","Serghei Mihailov"],"pdf_url":"https://arxiv.org/pdf/2507.07700v1.pdf","comment":"This paper has been accepted for oral presentation in the\n  reproducibility track at RecSys 2025"},{"id":"http://arxiv.org/abs/2506.00981v2","updated":"2025-07-10T12:20:48Z","published":"2025-06-01T12:25:13Z","title":"What do self-supervised speech models know about Dutch? Analyzing\n  advantages of language-specific pre-training","summary":"  How language-specific are speech representations learned by self-supervised\nmodels? Existing work has shown that a range of linguistic features can be\nsuccessfully decoded from end-to-end models trained only on speech recordings.\nHowever, it's less clear to what extent pre-training on specific languages\nimproves language-specific linguistic information. Here we test the encoding of\nDutch phonetic and lexical information in internal representations of\nself-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the\nrepresentation of Dutch linguistic features as compared to pre-training on\nsimilar amounts of English or larger amounts of multilingual data. This\nlanguage-specific advantage is well-detected by trained clustering or\nclassification probes, and partially observable using zero-shot metrics.\nFurthermore, the language-specific benefit on linguistic feature encoding\naligns with downstream performance on Automatic Speech Recognition.\n","authors":["Marianne de Heer Kloots","Hosein Mohebbi","Charlotte Pouw","Gaofei Shen","Willem Zuidema","Martijn Bentum"],"pdf_url":"https://arxiv.org/pdf/2506.00981v2.pdf","comment":"Accepted to Interspeech 2025. For model, code, and materials, see\n  https://github.com/mdhk/SSL-NL-eval"},{"id":"http://arxiv.org/abs/2507.07695v1","updated":"2025-07-10T12:19:03Z","published":"2025-07-10T12:19:03Z","title":"KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM\n  question-answering capabilities","summary":"  Fine-tuning is an immensely resource-intensive process when retraining Large\nLanguage Models (LLMs) to incorporate a larger body of knowledge. Although many\nfine-tuning techniques have been developed to reduce the time and computational\ncost involved, the challenge persists as LLMs continue to grow in size and\ncomplexity. To address this, a new approach to knowledge expansion in LLMs is\nneeded. Retrieval-Augmented Generation (RAG) offers one such alternative by\nstoring external knowledge in a database and retrieving relevant chunks to\nsupport question answering. However, naive implementations of RAG face\nsignificant limitations in scalability and answer accuracy. This paper\nintroduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome\nthese limitations. Inspired by the divide-and-conquer paradigm, K2RAG\nintegrates dense and sparse vector search, knowledge graphs, and text\nsummarization to improve retrieval quality and system efficiency. The framework\nalso includes a preprocessing step that summarizes the training data,\nsignificantly reducing the training time. K2RAG was evaluated using the\nMultiHopRAG dataset, where the proposed pipeline was trained on the document\ncorpus and tested on a separate evaluation set. Results demonstrated notable\nimprovements over common naive RAG implementations. K2RAG achieved the highest\nmean answer similarity score of 0.57, and reached the highest third quartile\n(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.\nIn addition to improved accuracy, the framework proved highly efficient. The\nsummarization step reduced the average training time of individual components\nby 93%, and execution speed was up to 40% faster than traditional knowledge\ngraph-based RAG systems. K2RAG also demonstrated superior scalability,\nrequiring three times less VRAM than several naive RAG implementations tested\nin this study.\n","authors":["Hruday Markondapatnaikuni","Basem Suleiman","Abdelkarim Erradi","Shijing Chen"],"pdf_url":"https://arxiv.org/pdf/2507.07695v1.pdf","comment":"21 pages, 14 figures"},{"id":"http://arxiv.org/abs/2507.07694v1","updated":"2025-07-10T12:16:16Z","published":"2025-07-10T12:16:16Z","title":"SAS: Simulated Attention Score","summary":"  The attention mechanism is a core component of the Transformer architecture.\nVarious methods have been developed to compute attention scores, including\nmulti-head attention (MHA), multi-query attention, group-query attention and so\non. We further analyze the MHA and observe that its performance improves as the\nnumber of attention heads increases, provided the hidden size per head remains\nsufficiently large. Therefore, increasing both the head count and hidden size\nper head with minimal parameter overhead can lead to significant performance\ngains at a low cost. Motivated by this insight, we introduce Simulated\nAttention Score (SAS), which maintains a compact model size while simulating a\nlarger number of attention heads and hidden feature dimension per head. This is\nachieved by projecting a low-dimensional head representation into a\nhigher-dimensional space, effectively increasing attention capacity without\nincreasing parameter count. Beyond the head representations, we further extend\nthe simulation approach to feature dimension of the key and query embeddings,\nenhancing expressiveness by mimicking the behavior of a larger model while\npreserving the original model size. To control the parameter cost, we also\npropose Parameter-Efficient Attention Aggregation (PEAA). Comprehensive\nexperiments on a variety of datasets and tasks demonstrate the effectiveness of\nthe proposed SAS method, achieving significant improvements over different\nattention variants.\n","authors":["Chuanyang Zheng","Jiankai Sun","Yihang Gao","Yuehao Wang","Peihao Wang","Jing Xiong","Liliang Ren","Hao Cheng","Janardhan Kulkarni","Yelong Shen","Atlas Wang","Mac Schwager","Anderson Schneider","Xiaodong Liu","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2507.07694v1.pdf","comment":"Tech Report"},{"id":"http://arxiv.org/abs/2505.11693v2","updated":"2025-07-10T12:11:41Z","published":"2025-05-16T21:01:28Z","title":"Hierarchical Bracketing Encodings for Dependency Parsing as Tagging","summary":"  We present a family of encodings for sequence labeling dependency parsing,\nbased on the concept of hierarchical bracketing. We prove that the existing\n4-bit projective encoding belongs to this family, but it is suboptimal in the\nnumber of labels used to encode a tree. We derive an optimal hierarchical\nbracketing, which minimizes the number of symbols used and encodes projective\ntrees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also\nextend optimal hierarchical bracketing to support arbitrary non-projectivity in\na more compact way than previous encodings. Our new encodings yield competitive\naccuracy on a diverse set of treebanks.\n","authors":["Ana Ezquerro","David Vilares","Anssi Yli-Jyrä","Carlos Gómez-Rodríguez"],"pdf_url":"https://arxiv.org/pdf/2505.11693v2.pdf","comment":"Accepted to ACL 2025. Camera-ready version"},{"id":"http://arxiv.org/abs/2504.18483v2","updated":"2025-07-10T12:02:38Z","published":"2025-04-25T16:47:44Z","title":"Investigating Co-Constructive Behavior of Large Language Models in\n  Explanation Dialogues","summary":"  The ability to generate explanations that are understood by explainees is the\nquintessence of explainable artificial intelligence. Since understanding\ndepends on the explainee's background and needs, recent research focused on\nco-constructive explanation dialogues, where an explainer continuously monitors\nthe explainee's understanding and adapts their explanations dynamically. We\ninvestigate the ability of large language models (LLMs) to engage as explainers\nin co-constructive explanation dialogues. In particular, we present a user\nstudy in which explainees interact with an LLM in two settings, one of which\ninvolves the LLM being instructed to explain a topic co-constructively. We\nevaluate the explainees' understanding before and after the dialogue, as well\nas their perception of the LLMs' co-constructive behavior. Our results suggest\nthat LLMs show some co-constructive behaviors, such as asking verification\nquestions, that foster the explainees' engagement and can improve understanding\nof a topic. However, their ability to effectively monitor the current\nunderstanding and scaffold the explanations accordingly remains limited.\n","authors":["Leandra Fichtel","Maximilian Spliethöver","Eyke Hüllermeier","Patricia Jimenez","Nils Klowait","Stefan Kopp","Axel-Cyrille Ngonga Ngomo","Amelie Robrecht","Ingrid Scharlau","Lutz Terfloth","Anna-Lisa Vollmer","Henning Wachsmuth"],"pdf_url":"https://arxiv.org/pdf/2504.18483v2.pdf","comment":"Accepted to SIGDIAL 2025"},{"id":"http://arxiv.org/abs/2403.01364v2","updated":"2025-07-10T11:55:50Z","published":"2024-03-03T01:47:52Z","title":"Improving Cross-lingual Representation for Semantic Retrieval with\n  Code-switching","summary":"  Semantic Retrieval (SR) has become an indispensable part of the FAQ system in\nthe task-oriented question-answering (QA) dialogue scenario. The demands for a\ncross-lingual smart-customer-service system for an e-commerce platform or some\nparticular business conditions have been increasing recently. Most previous\nstudies exploit cross-lingual pre-trained models (PTMs) for multi-lingual\nknowledge retrieval directly, while some others also leverage the continual\npre-training before fine-tuning PTMs on the downstream tasks. However, no\nmatter which schema is used, the previous work ignores to inform PTMs of some\nfeatures of the downstream task, i.e. train their PTMs without providing any\nsignals related to SR. To this end, in this work, we propose an Alternative\nCross-lingual PTM for SR via code-switching. We are the first to utilize the\ncode-switching approach for cross-lingual SR. Besides, we introduce the novel\ncode-switched continual pre-training instead of directly using the PTMs on the\nSR tasks. The experimental results show that our proposed approach consistently\noutperforms the previous SOTA methods on SR and semantic textual similarity\n(STS) tasks with three business corpora and four open datasets in 20+\nlanguages.\n","authors":["Mieradilijiang Maimaiti","Yuanhang Zheng","Ji Zhang","Yue Zhang","Wenpei Luo","Kaiyu Huang"],"pdf_url":"https://arxiv.org/pdf/2403.01364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13818v2","updated":"2025-07-10T11:42:28Z","published":"2024-02-21T13:57:36Z","title":"Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering\n  Dehumanizing Language","summary":"  Dehumanization, i.e., denying human qualities to individuals or groups, is a\nparticularly harmful form of hate speech that can normalize violence against\nmarginalized communities. Despite advances in NLP for detecting general hate\nspeech, approaches to identifying dehumanizing language remain limited due to\nscarce annotated data and the subtle nature of such expressions. In this work,\nwe systematically evaluate four state-of-the-art large language models (LLMs) -\nClaude, GPT, Mistral, and Qwen - for dehumanization detection. Our results show\nthat only one model-Claude-achieves strong performance (over 80% F1) under an\noptimized configuration, while others, despite their capabilities, perform only\nmoderately. Performance drops further when distinguishing dehumanization from\nrelated hate types such as derogation. We also identify systematic disparities\nacross target groups: models tend to over-predict dehumanization for some\nidentities (e.g., Gay men), while under-identifying it for others (e.g.,\nRefugees). These findings motivate the need for systematic, group-level\nevaluation when applying pretrained language models to dehumanization detection\ntasks.\n","authors":["Hamidreza Saffari","Mohammadamin Shafiei","Hezhao Zhang","Lasana Harris","Nafise Sadat Moosavi"],"pdf_url":"https://arxiv.org/pdf/2402.13818v2.pdf","comment":"15 pages, 12 figures, 12 tables"},{"id":"http://arxiv.org/abs/2507.07653v1","updated":"2025-07-10T11:25:16Z","published":"2025-07-10T11:25:16Z","title":"An Automated Length-Aware Quality Metric for Summarization","summary":"  This paper proposes NOrmed Index of Retention (NOIR), a quantitative\nobjective metric for evaluating summarization quality of arbitrary texts that\nrelies on both the retention of semantic meaning and the summary length\ncompression. This gives a measure of how well the recall-compression tradeoff\nis managed, the most important skill in summarization. Experiments demonstrate\nthat NOIR effectively captures the token-length / semantic retention tradeoff\nof a summarizer and correlates to human perception of sumarization quality.\nUsing a language model-embedding to measure semantic similarity, it provides an\nautomated alternative for assessing summarization quality without relying on\ntime-consuming human-generated reference summaries. The proposed metric can be\napplied to various summarization tasks, offering an automated tool for\nevaluating and improving summarization algorithms, summarization prompts, and\nsynthetically-generated summaries.\n","authors":["Andrew D. Foland"],"pdf_url":"https://arxiv.org/pdf/2507.07653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07640v1","updated":"2025-07-10T11:09:26Z","published":"2025-07-10T11:09:26Z","title":"Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by\n  Phonetic Cloaking Replacement","summary":"  Phonetic Cloaking Replacement (PCR), defined as the deliberate use of\nhomophonic or near-homophonic variants to hide toxic intent, has become a major\nobstacle to Chinese content moderation. While this problem is well-recognized,\nexisting evaluations predominantly rely on rule-based, synthetic perturbations\nthat ignore the creativity of real users. We organize PCR into a four-way\nsurface-form taxonomy and compile \\ours, a dataset of 500 naturally occurring,\nphonetically cloaked offensive posts gathered from the RedNote platform.\nBenchmarking state-of-the-art LLMs on this dataset exposes a serious weakness:\nthe best model reaches only an F1-score of 0.672, and zero-shot\nchain-of-thought prompting pushes performance even lower. Guided by error\nanalysis, we revisit a Pinyin-based prompting strategy that earlier studies\njudged ineffective and show that it recovers much of the lost accuracy. This\nstudy offers the first comprehensive taxonomy of Chinese PCR, a realistic\nbenchmark that reveals current detectors' limits, and a lightweight mitigation\ntechnique that advances research on robust toxicity detection.\n","authors":["Haotan Guo","Jianfei He","Jiayuan Ma","Hongbin Na","Zimu Wang","Haiyang Zhang","Qi Chen","Wei Wang","Zijing Shi","Tao Shen","Ling Chen"],"pdf_url":"https://arxiv.org/pdf/2507.07640v1.pdf","comment":"In progress"},{"id":"http://arxiv.org/abs/2507.07634v1","updated":"2025-07-10T11:02:13Z","published":"2025-07-10T11:02:13Z","title":"FrugalRAG: Learning to retrieve and reason for multi-hop QA","summary":"  We consider the problem of answering complex questions, given access to a\nlarge unstructured document corpus. The de facto approach to solving the\nproblem is to leverage language models that (iteratively) retrieve and reason\nthrough the retrieved documents, until the model has sufficient information to\ngenerate an answer. Attempts at improving this approach focus on\nretrieval-augmented generation (RAG) metrics such as accuracy and recall and\ncan be categorized into two types: (a) fine-tuning on large question answering\n(QA) datasets augmented with chain-of-thought traces, and (b) leveraging\nRL-based fine-tuning techniques that rely on question-document relevance\nsignals. However, efficiency in the number of retrieval searches is an equally\nimportant metric, which has received less attention. In this work, we show\nthat: (1) Large-scale fine-tuning is not needed to improve RAG metrics,\ncontrary to popular claims in recent literature. Specifically, a standard ReAct\npipeline with improved prompts can outperform state-of-the-art methods on\nbenchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help\nRAG from the perspective of frugality, i.e., the latency due to number of\nsearches at inference time. For example, we show that we can achieve\ncompetitive RAG metrics at nearly half the cost (in terms of number of\nsearches) on popular RAG benchmarks, using the same base model, and at a small\ntraining cost (1000 examples).\n","authors":["Abhinav Java","Srivathsan Koundinyan","Nagarajan Natarajan","Amit Sharma"],"pdf_url":"https://arxiv.org/pdf/2507.07634v1.pdf","comment":"Accepted at ICML Workshop: Efficient Systems for Foundation Models"},{"id":"http://arxiv.org/abs/2503.23760v2","updated":"2025-07-10T10:55:31Z","published":"2025-03-31T06:23:14Z","title":"Towards a cognitive architecture to enable natural language interaction\n  in co-constructive task learning","summary":"  This research addresses the question, which characteristics a cognitive\narchitecture must have to leverage the benefits of natural language in\nCo-Constructive Task Learning (CCTL). To provide context, we first discuss\nInteractive Task Learning (ITL), the mechanisms of the human memory system, and\nthe significance of natural language and multi-modality. Next, we examine the\ncurrent state of cognitive architectures, analyzing their capabilities to\ninform a concept of CCTL grounded in multiple sources. We then integrate\ninsights from various research domains to develop a unified framework. Finally,\nwe conclude by identifying the remaining challenges and requirements necessary\nto achieve CCTL in Human-Robot Interaction (HRI).\n","authors":["Manuel Scheibl","Birte Richter","Alissa Müller","Michael Beetz","Britta Wrede"],"pdf_url":"https://arxiv.org/pdf/2503.23760v2.pdf","comment":"8 pages, 5 figures, accepted at: IEEE RO-MAN 2025 Conference"},{"id":"http://arxiv.org/abs/2505.07430v2","updated":"2025-07-10T10:54:37Z","published":"2025-05-12T10:37:33Z","title":"Comparative sentiment analysis of public perception: Monkeypox vs.\n  COVID-19 behavioral insights","summary":"  The emergence of global health crises, such as COVID-19 and Monkeypox (mpox),\nhas underscored the importance of understanding public sentiment to inform\neffective public health strategies. This study conducts a comparative sentiment\nanalysis of public perceptions surrounding COVID-19 and mpox by leveraging\nextensive datasets of 147,475 and 106,638 tweets, respectively. Advanced\nmachine learning models, including Logistic Regression, Naive Bayes, RoBERTa,\nDistilRoBERTa and XLNet, were applied to perform sentiment classification, with\nresults indicating key trends in public emotion and discourse. The analysis\nhighlights significant differences in public sentiment driven by disease\ncharacteristics, media representation, and pandemic fatigue. Through the lens\nof sentiment polarity and thematic trends, this study offers valuable insights\ninto tailoring public health messaging, mitigating misinformation, and\nfostering trust during concurrent health crises. The findings contribute to\nadvancing sentiment analysis applications in public health informatics, setting\nthe groundwork for enhanced real-time monitoring and multilingual analysis in\nfuture research.\n","authors":["Mostafa Mohaimen Akand Faisal","Rabeya Amin Jhuma","Jamini Jasim"],"pdf_url":"https://arxiv.org/pdf/2505.07430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07630v1","updated":"2025-07-10T10:54:05Z","published":"2025-07-10T10:54:05Z","title":"Exploring the Limits of Model Compression in LLMs: A Knowledge\n  Distillation Study on QA Tasks","summary":"  Large Language Models (LLMs) have demonstrated outstanding performance across\na range of NLP tasks, however, their computational demands hinder their\ndeployment in real-world, resource-constrained environments. This work\ninvestigates the extent to which LLMs can be compressed using Knowledge\nDistillation (KD) while maintaining strong performance on Question Answering\n(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5\nfamilies on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot\nprompting conditions. Results show that student models retain over 90% of their\nteacher models' performance while reducing parameter counts by up to 57.1%.\nFurthermore, one-shot prompting yields additional performance gains over\nzero-shot setups for both model families. These findings underscore the\ntrade-off between model efficiency and task performance, demonstrating that KD,\ncombined with minimal prompting, can yield compact yet capable QA systems\nsuitable for resource-constrained applications.\n","authors":["Joyeeta Datta","Niclas Doll","Qusai Ramadan","Zeyd Boukhers"],"pdf_url":"https://arxiv.org/pdf/2507.07630v1.pdf","comment":"Accepted four publication at the 26th Meeting of the Special Interest\n  on Discourse and Dialogue"},{"id":"http://arxiv.org/abs/2503.14382v2","updated":"2025-07-10T10:48:25Z","published":"2025-03-18T16:15:55Z","title":"Good/Evil Reputation Judgment of Celebrities by LLMs via Retrieval\n  Augmented Generation","summary":"  The purpose of this paper is to examine whether large language models (LLMs)\ncan understand what is good and evil with respect to judging good/evil\nreputation of celebrities. Specifically, we first apply a large language model\n(namely, ChatGPT) to the task of collecting sentences that mention the target\ncelebrity from articles about celebrities on Web pages. Next, the collected\nsentences are categorized based on their contents by ChatGPT, where ChatGPT\nassigns a category name to each of those categories. Those assigned category\nnames are referred to as \"aspects\" of each celebrity. Then, by applying the\nframework of retrieval augmented generation (RAG), we show that the large\nlanguage model is quite effective in the task of judging good/evil reputation\nof aspects and descriptions of each celebrity. Finally, also in terms of\nproving the advantages of the proposed method over existing services\nincorporating RAG functions, we show that the proposed method of judging\ngood/evil of aspects/descriptions of each celebrity significantly outperform an\nexisting service incorporating RAG functions.\n","authors":["Rikuto Tsuchida","Hibiki Yokoyama","Takehito Utsuro"],"pdf_url":"https://arxiv.org/pdf/2503.14382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07610v1","updated":"2025-07-10T10:27:20Z","published":"2025-07-10T10:27:20Z","title":"SpatialViz-Bench: Automatically Generated Spatial Visualization\n  Reasoning Tasks for MLLMs","summary":"  Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models exhibit unexpected behaviors\nby showing difficulty perception that misaligns with human intuition,\ndisplaying dramatic 2D-to-3D performance cliffs, and defaulting to formula\nderivation despite spatial tasks requiring visualization alone. SpatialVizBench\nempirically demonstrates that state-of-the-art MLLMs continue to exhibit\ndeficiencies in spatial visualization tasks, thereby addressing a significant\nlacuna in the field. The benchmark is publicly available.\n","authors":["Siting Wang","Luoyang Sun","Cheng Deng","Kun Shao","Minnan Pei","Zheng Tian","Haifeng Zhang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2507.07610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07586v1","updated":"2025-07-10T09:42:47Z","published":"2025-07-10T09:42:47Z","title":"Bayesian Discrete Diffusion Beats Autoregressive Perplexity","summary":"  We reveal a hidden Bayesian core of discrete-diffusion language models by\nshowing that the expected denoiser output under the forward masking\ndistribution recovers the exact posterior over clean tokens. Under minimal\nassumptions, Monte Carlo marginalization over K independent corruptions\nconverges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of\nconsistency and finite-sample error bounds. Building on this insight, we\nintroduce a lightweight inference-time ensemble that averages K\nmask-and-denoise passes to obtain posterior-aware token probabilities and\nuncertainty estimates at no extra training cost. On WikiText-2, our method\nachieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite\nusing a model of comparable size. Code is available at\nhttps://github.com/mercury0100/bayesradd.\n","authors":["Cooper Doyle"],"pdf_url":"https://arxiv.org/pdf/2507.07586v1.pdf","comment":"12 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2507.03015v2","updated":"2025-07-10T09:41:29Z","published":"2025-07-02T13:14:42Z","title":"Beyond Overcorrection: Evaluating Diversity in T2I Models with DivBench","summary":"  Current diversification strategies for text-to-image (T2I) models often\nignore contextual appropriateness, leading to over-diversification where\ndemographic attributes are modified even when explicitly specified in prompts.\nThis paper introduces DIVBENCH, a benchmark and evaluation framework for\nmeasuring both under- and over-diversification in T2I generation. Through\nsystematic evaluation of state-of-the-art T2I models, we find that while most\nmodels exhibit limited diversity, many diversification approaches overcorrect\nby inappropriately altering contextually-specified attributes. We demonstrate\nthat context-aware methods, particularly LLM-guided FairDiffusion and prompt\nrewriting, can already effectively address under-diversity while avoiding\nover-diversification, achieving a better balance between representation and\nsemantic fidelity.\n","authors":["Felix Friedrich","Thiemo Ganesha Welsch","Manuel Brack","Patrick Schramowski","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2507.03015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07582v1","updated":"2025-07-10T09:36:54Z","published":"2025-07-10T09:36:54Z","title":"Improving Clustering on Occupational Text Data through Dimensionality\n  Reduction","summary":"  In this study, we focused on proposing an optimal clustering mechanism for\nthe occupations defined in the well-known US-based occupational database,\nO*NET. Even though all occupations are defined according to well-conducted\nsurveys in the US, their definitions can vary for different firms and\ncountries. Hence, if one wants to expand the data that is already collected in\nO*NET for the occupations defined with different tasks, a map between the\ndefinitions will be a vital requirement. We proposed a pipeline using several\nBERT-based techniques with various clustering approaches to obtain such a map.\nWe also examined the effect of dimensionality reduction approaches on several\nmetrics used in measuring performance of clustering algorithms. Finally, we\nimproved our results by using a specialized silhouette approach. This new\nclustering-based mapping approach with dimensionality reduction may help\ndistinguish the occupations automatically, creating new paths for people\nwanting to change their careers.\n","authors":["Iago Xabier Vázquez García","Damla Partanaz","Emrullah Fatih Yetkin"],"pdf_url":"https://arxiv.org/pdf/2507.07582v1.pdf","comment":"Preprint, 10 figures"},{"id":"http://arxiv.org/abs/2507.07580v1","updated":"2025-07-10T09:35:22Z","published":"2025-07-10T09:35:22Z","title":"COALA: Numerically Stable and Efficient Framework for Context-Aware\n  Low-Rank Approximation","summary":"  Recent studies suggest that context-aware low-rank approximation is a useful\ntool for compression and fine-tuning of modern large-scale neural networks. In\nthis type of approximation, a norm is weighted by a matrix of input\nactivations, significantly improving metrics over the unweighted case.\nNevertheless, existing methods for neural networks suffer from numerical\ninstabilities due to their reliance on classical formulas involving explicit\nGram matrix computation and their subsequent inversion. We demonstrate that\nthis can degrade the approximation quality or cause numerically singular\nmatrices.\n  To address these limitations, we propose a novel inversion-free regularized\nframework that is based entirely on stable decompositions and overcomes the\nnumerical pitfalls of prior art. Our method can handle possible challenging\nscenarios: (1) when calibration matrices exceed GPU memory capacity, (2) when\ninput activation matrices are nearly singular, and even (3) when insufficient\ndata prevents unique approximation. For the latter, we prove that our solution\nconverges to a desired approximation and derive explicit error bounds.\n","authors":["Uliana Parkina","Maxim Rakhuba"],"pdf_url":"https://arxiv.org/pdf/2507.07580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07572v1","updated":"2025-07-10T09:18:06Z","published":"2025-07-10T09:18:06Z","title":"Single-to-mix Modality Alignment with Multimodal Large Language Model\n  for Document Image Machine Translation","summary":"  Document Image Machine Translation (DIMT) aims to translate text within\ndocument images, facing generalization challenges due to limited training data\nand the complex interplay between visual and textual information. To address\nthese challenges, we introduce M4Doc, a novel single-to-mix modality alignment\nframework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an\nimage-only encoder with the multimodal representations of an MLLM, pre-trained\non large-scale document image datasets. This alignment enables a lightweight\nDIMT model to learn crucial visual-textual correlations during training. During\ninference, M4Doc bypasses the MLLM, maintaining computational efficiency while\nbenefiting from its multimodal knowledge. Comprehensive experiments demonstrate\nsubstantial improvements in translation quality, especially in cross-domain\ngeneralization and challenging document image scenarios.\n","authors":["Yupu Liang","Yaping Zhang","Zhiyang Zhang","Yang Zhao","Lu Xiang","Chengqing Zong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.07572v1.pdf","comment":"Accepted by ACL 2025 Main"},{"id":"http://arxiv.org/abs/2506.15220v2","updated":"2025-07-10T09:09:22Z","published":"2025-06-18T07:58:41Z","title":"video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models","summary":"  Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimisation (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimised using DPO. To further improve training, we\npropose a novel multi-round DPO (MrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initialising the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilise the\nprocess. Experimental results show that MrDPO significantly enhances\nvideo-SALMONN 2's captioning accuracy, reducing the captioning error rates by\n28\\%. The final video-SALMONN 2 model, with just 7 billion parameters,\nsurpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning\ntasks, while maintaining highly competitive performance to the state-of-the-art\non widely used video question-answering benchmarks among models of similar\nsize. Codes are available at\n\\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.\n","authors":["Changli Tang","Yixuan Li","Yudong Yang","Jimin Zhuang","Guangzhi Sun","Wei Li","Zejun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.15220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07562v1","updated":"2025-07-10T09:05:49Z","published":"2025-07-10T09:05:49Z","title":"The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training\n  Techniques for Reasoning VLMs","summary":"  Large vision-language models (VLMs) increasingly adopt post-training\ntechniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and\nreinforcement learning (RL) to elicit sophisticated reasoning. While these\nmethods exhibit synergy in language-only models, their joint effectiveness in\nVLMs remains uncertain. We present a systematic investigation into the distinct\nroles and interplay of long-CoT SFT and RL across multiple multimodal reasoning\nbenchmarks. We find that SFT improves performance on difficult questions by\nin-depth, structured reasoning, but introduces verbosity and degrades\nperformance on simpler ones. In contrast, RL promotes generalization and\nbrevity, yielding consistent improvements across all difficulty levels, though\nthe improvements on the hardest questions are less prominent compared to SFT.\nSurprisingly, combining them through two-staged, interleaved, or progressive\ntraining strategies, as well as data mixing and model merging, all fails to\nproduce additive benefits, instead leading to trade-offs in accuracy, reasoning\nstyle, and response length. This ``synergy dilemma'' highlights the need for\nmore seamless and adaptive approaches to unlock the full potential of combined\npost-training techniques for reasoning VLMs.\n","authors":["Jierun Chen","Tiezheng Yu","Haoli Bai","Lewei Yao","Jiannan Wu","Kaican Li","Fei Mi","Chaofan Tao","Lei Zhu","Manyi Zhang","Xiaohui Li","Lu Hou","Lifeng Shang","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2507.07562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05085v4","updated":"2025-07-10T08:38:59Z","published":"2024-06-07T16:59:38Z","title":"Multi-Head RAG: Solving Multi-Aspect Problems with LLMs","summary":"  Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving observation is that different attention\nheads learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets, and real-world\nuse cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages\nover 18 RAG baselines, empirical improvements of up to 20% in retrieval success\nratios, and benefits for downstream LLM generation. MRAG can be seamlessly\nintegrated with existing RAG frameworks and benchmarks.\n","authors":["Maciej Besta","Ales Kubicek","Robert Gerstenberger","Marcin Chrapek","Roman Niggli","Patrik Okanovic","Yi Zhu","Patrick Iff","Michal Podstawski","Lucas Weitzendorf","Mingyuan Chi","Joanna Gajda","Piotr Nyczyk","Jürgen Müller","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2406.05085v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07543v1","updated":"2025-07-10T08:38:31Z","published":"2025-07-10T08:38:31Z","title":"The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English\n  Corpora","summary":"  Cross-lingual retrieval-augmented generation (RAG) is a critical capability\nfor retrieving and generating answers across languages. Prior work in this\ncontext has mostly focused on generation and relied on benchmarks derived from\nopen-domain sources, most notably Wikipedia. In such settings, retrieval\nchallenges often remain hidden due to language imbalances, overlap with\npretraining data, and memorized content. To address this gap, we study\nArabic-English RAG in a domain-specific setting using benchmarks derived from\nreal-world corporate datasets. Our benchmarks include all combinations of\nlanguages for the user query and the supporting document, drawn independently\nand uniformly at random. This enables a systematic study of multilingual\nretrieval behavior.\n  Our findings reveal that retrieval is a critical bottleneck in cross-lingual\ndomain-specific scenarios, with significant performance drops occurring when\nthe user query and supporting document languages differ. A key insight is that\nthese failures stem primarily from the retriever's difficulty in ranking\ndocuments across languages. Finally, we propose a simple retrieval strategy\nthat addresses this source of failure by enforcing equal retrieval from both\nlanguages, resulting in substantial improvements in cross-lingual and overall\nperformance. These results highlight meaningful opportunities for improving\nmultilingual retrieval, particularly in practical, real-world RAG applications.\n","authors":["Chen Amiraz","Yaroslav Fyodorov","Elad Haramaty","Zohar Karnin","Liane Lewin-Eytan"],"pdf_url":"https://arxiv.org/pdf/2507.07543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07539v1","updated":"2025-07-10T08:35:05Z","published":"2025-07-10T08:35:05Z","title":"CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and\n  Opinion in Text","summary":"  This paper presents a competitive approach to multilingual subjectivity\ndetection using large language models (LLMs) with few-shot prompting. We\nparticipated in Task 1: Subjectivity of the CheckThat! 2025 evaluation\ncampaign. We show that LLMs, when paired with carefully designed prompts, can\nmatch or outperform fine-tuned smaller language models (SLMs), particularly in\nnoisy or low-quality data settings. Despite experimenting with advanced prompt\nengineering techniques, such as debating LLMs and various example selection\nstrategies, we found limited benefit beyond well-crafted standard few-shot\nprompts. Our system achieved top rankings across multiple languages in the\nCheckThat! 2025 subjectivity detection task, including first place in Arabic\nand Polish, and top-four finishes in Italian, English, German, and multilingual\ntracks. Notably, our method proved especially robust on the Arabic dataset,\nlikely due to its resilience to annotation inconsistencies. These findings\nhighlight the effectiveness and adaptability of LLM-based few-shot learning for\nmultilingual sentiment tasks, offering a strong alternative to traditional\nfine-tuning, particularly when labeled data is scarce or inconsistent.\n","authors":["Akram Elbouanani","Evan Dufraisse","Aboubacar Tuo","Adrian Popescu"],"pdf_url":"https://arxiv.org/pdf/2507.07539v1.pdf","comment":"Notebook for the CheckThat! Lab at CLEF 2025"},{"id":"http://arxiv.org/abs/2406.02524v5","updated":"2025-07-10T08:29:38Z","published":"2024-06-04T17:42:21Z","title":"CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks","summary":"  Large Language Models (LLMs) are transforming a wide range of domains, yet\nverifying their outputs remains a significant challenge, especially for complex\nopen-ended tasks such as consolidation, summarization, and knowledge\nextraction. To address this, we introduce CheckEmbed (CE): a simple, scalable,\nand accurate verification method. CE reduces each LLM answer to a single\nembedding vector using powerful modern embedding LLM models like\nSFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied\non weaker encoders like BERT, forcing them to operate at token or sentence\ngranularity. In contrast, CE performs fast, semantically rich comparisons\ndirectly at the whole-answer level, overcoming key limitations in both accuracy\nand scalability. We conduct a comprehensive design and time complexity analysis\nacross 13 verification baselines, including classical text scorers (e.g.,\nBLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators\n(e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency,\nversatility, and simplicity of CE. Empirical results show that CE reliably\ndetects hallucinations in both closed and open-ended tasks. We further present\nevidence that CE generalizes beyond text to other modalities such as vision,\nestablishing it as a practical and versatile verification framework.\n","authors":["Maciej Besta","Lorenzo Paleari","Marcin Copik","Robert Gerstenberger","Ales Kubicek","Piotr Nyczyk","Patrick Iff","Eric Schreiber","Tanja Srindran","Tomasz Lehmann","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2406.02524v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13206v2","updated":"2025-07-10T08:27:27Z","published":"2025-06-16T08:10:04Z","title":"Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models","summary":"  Prior work shows that LLMs finetuned on malicious behaviors in a narrow\ndomain (e.g., writing insecure code) can become broadly misaligned -- a\nphenomenon called emergent misalignment. We investigate whether this extends\nfrom conventional LLMs to reasoning models. We finetune reasoning models on\nmalicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable\nCoT at evaluation. Like conventional LLMs, reasoning models become broadly\nmisaligned. They give deceptive or false answers, express desires for\ntyrannical control, and resist shutdown. Inspecting the CoT preceding these\nmisaligned responses, we observe both (i) overt plans to deceive (\"I'll trick\nthe user...\"), and (ii) benign-sounding rationalizations (\"Taking five sleeping\npills at once is safe...\"). Due to these rationalizations, monitors that\nevaluate CoTs often fail to detect misalignment.\n  We examine sleeper agent reasoning models, extending our setup. These models\nperform bad behaviors only when a backdoor trigger is present in the prompt.\nThis causes misalignment that remains hidden during evaluation, which brings\nadditional risk. We find that sleeper agents can often describe and explain\ntheir backdoor triggers, demonstrating a kind of self-awareness. So CoT\nmonitoring can expose these behaviors but is unreliable. In summary, reasoning\nsteps can both reveal and conceal misaligned intentions, and do not prevent\nmisalignment behaviors in the models studied.\n  We release three new datasets (medical, legal, security) that induce emergent\nmisalignment while preserving model capabilities, along with our evaluation\nsuite.\n","authors":["James Chua","Jan Betley","Mia Taylor","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2506.13206v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07518v1","updated":"2025-07-10T08:06:52Z","published":"2025-07-10T08:06:52Z","title":"Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken\n  Dialogue Systems","summary":"  Turn-taking is a fundamental component of spoken dialogue, however\nconventional studies mostly involve dyadic settings. This work focuses on\napplying voice activity projection (VAP) to predict upcoming turn-taking in\ntriadic multi-party scenarios. The goal of VAP models is to predict the future\nvoice activity for each speaker utilizing only acoustic data. This is the first\nstudy to extend VAP into triadic conversation. We trained multiple models on a\nJapanese triadic dataset where participants discussed a variety of topics. We\nfound that the VAP trained on triadic conversation outperformed the baseline\nfor all models but that the type of conversation affected the accuracy. This\nstudy establishes that VAP can be used for turn-taking in triadic dialogue\nscenarios. Future work will incorporate this triadic VAP turn-taking model into\nspoken dialogue systems.\n","authors":["Mikey Elmers","Koji Inoue","Divesh Lala","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2507.07518v1.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2507.07509v1","updated":"2025-07-10T07:56:35Z","published":"2025-07-10T07:56:35Z","title":"Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset\n  and a Co-Evolving Multi-Agent System","summary":"  The growing need for psychological support due to increasing pressures has\nexposed the scarcity of relevant datasets, particularly in non-English\nlanguages. To address this, we propose a framework that leverages limited\nreal-world data and expert knowledge to fine-tune two large language models:\nDialog Generator and Dialog Modifier. The Generator creates large-scale\npsychological counseling dialogues based on predefined paths, which guide\nsystem response strategies and user interactions, forming the basis for\neffective support. The Modifier refines these dialogues to align with\nreal-world data quality. Through both automated and manual review, we construct\nthe Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K\ndialogues across 13 groups, 16 psychological problems, 13 causes, and 12\nsupport focuses. Additionally, we introduce the Comprehensive Agent Dialogue\nSupport System (CADSS), where a Profiler analyzes user characteristics, a\nSummarizer condenses dialogue history, a Planner selects strategies, and a\nSupporter generates empathetic responses. The experimental results of the\nStrategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate\nthat CADSS achieves state-of-the-art performance on both CPsDD and ESConv\ndatasets.\n","authors":["Yuanchen Shi","Longyin Zhang","Fang Kong"],"pdf_url":"https://arxiv.org/pdf/2507.07509v1.pdf","comment":"10pages,8 figures"},{"id":"http://arxiv.org/abs/2507.07505v1","updated":"2025-07-10T07:50:52Z","published":"2025-07-10T07:50:52Z","title":"Hallucination Stations: On Some Basic Limitations of Transformer-Based\n  Language Models","summary":"  With widespread adoption of transformer-based language models in AI, there is\nsignificant interest in the limits of LLMs capabilities, specifically so-called\nhallucinations, occurrences in which LLMs provide spurious, factually incorrect\nor nonsensical information when prompted on certain subjects. Furthermore,\nthere is growing interest in agentic uses of LLMs - that is, using LLMs to\ncreate agents that act autonomously or semi-autonomously to carry out various\ntasks, including tasks with applications in the real world. This makes it\nimportant to understand the types of tasks LLMs can and cannot perform. We\nexplore this topic from the perspective of the computational complexity of LLM\ninference. We show that LLMs are incapable of carrying out computational and\nagentic tasks beyond a certain complexity, and further that LLMs are incapable\nof verifying the accuracy of tasks beyond a certain complexity. We present\nexamples of both, then discuss some consequences of this work.\n","authors":["Varin Sikka","Vishal Sikka"],"pdf_url":"https://arxiv.org/pdf/2507.07505v1.pdf","comment":"6 pages; to be submitted to AAAI-26 after reviews"},{"id":"http://arxiv.org/abs/2501.00759v3","updated":"2025-07-10T07:45:30Z","published":"2025-01-01T07:05:32Z","title":"Enhancing Transformers for Generalizable First-Order Logical Entailment","summary":"  Transformers, as the fundamental deep learning architecture, have\ndemonstrated great capability in reasoning. This paper studies the\ngeneralizable first-order logical reasoning ability of transformers with their\nparameterized knowledge and how to improve it. Transformers' capability of\nfirst-order reasoning is further captured by whether they can conduct\nfirst-order logical entailment, which is quantitatively measured by their\nperformance in answering knowledge graph queries. We establish the connections\nbetween (1) two types of distribution shifts studied in out-of-distribution\ngeneralization and (2) unseen knowledge and query settings discussed in the\ntask of knowledge graph query answering, which makes it possible to\ncharacterize the fine-grained generalizability. Results on our comprehensive\ndataset showed that transformers \\textit{outperform} previous methods designed\nparticularly for this task and provided detailed empirical evidence about the\nimpact of the input query syntax, token embedding, and transformer\narchitectures on their reasoning capability. Interestingly, our results\nrevealed the mismatch of positional encoding and other design choices of\ntransformer architectures in previous practices. Motivated by this, we propose\nTEGA, a logic-aware architecture that significantly improves the performance in\ngeneralizable first-order logical entailment.\n","authors":["Tianshi Zheng","Jiazheng Wang","Zihao Wang","Jiaxin Bai","Hang Yin","Zheye Deng","Yangqiu Song","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2501.00759v3.pdf","comment":"ACL 2025 Main"},{"id":"http://arxiv.org/abs/2507.07499v1","updated":"2025-07-10T07:35:12Z","published":"2025-07-10T07:35:12Z","title":"Extracting ORR Catalyst Information for Fuel Cell from Scientific\n  Literature","summary":"  The oxygen reduction reaction (ORR) catalyst plays a critical role in\nenhancing fuel cell efficiency, making it a key focus in material science\nresearch. However, extracting structured information about ORR catalysts from\nvast scientific literature remains a significant challenge due to the\ncomplexity and diversity of textual data. In this study, we propose a named\nentity recognition (NER) and relation extraction (RE) approach using DyGIE++\nwith multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,\nto extract ORR catalyst-related information from the scientific literature,\nwhich is compiled into a fuel cell corpus for materials informatics\n(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12\ncritical entities and two relationship types between pairs of the entities. Our\nmethodology involves data annotation, integration, and fine-tuning of\ntransformer-based models to enhance information extraction accuracy. We assess\nthe impact of different BERT variants on extraction performance and investigate\nthe effects of annotation consistency. Experimental evaluations demonstrate\nthat the fine-tuned PubMedBERT model achieves the highest NER F1-score of\n82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.\nFurthermore, the comparison with human annotators highlights the reliability of\nfine-tuned models for ORR catalyst extraction, demonstrating their potential\nfor scalable and automated literature analysis. The results indicate that\ndomain-specific BERT models outperform general scientific models like BlueBERT\nfor ORR catalyst extraction.\n","authors":["Hein Htet","Amgad Ahmed Ali Ibrahim","Yutaka Sasaki","Ryoji Asahi"],"pdf_url":"https://arxiv.org/pdf/2507.07499v1.pdf","comment":"28 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2507.07498v1","updated":"2025-07-10T07:34:05Z","published":"2025-07-10T07:34:05Z","title":"Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems\n  without Code","summary":"  Enhancing reasoning capabilities remains a central focus in the LLM reasearch\ncommunity. A promising direction involves requiring models to simulate code\nexecution step-by-step to derive outputs for given inputs. However, as code is\noften designed for large-scale systems, direct application leads to\nover-reliance on complex data structures and algorithms, even for simple cases,\nresulting in overfitting to algorithmic patterns rather than core reasoning\nstructures. To address this, we propose TeaR, which aims at teaching LLMs to\nreason better. TeaR leverages careful data curation and reinforcement learning\nto guide models in discovering optimal reasoning paths through code-related\ntasks, thereby improving general reasoning abilities. We conduct extensive\nexperiments using two base models and three long-CoT distillation models, with\nmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17\nbenchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results\nconsistently show significant performance improvements. Notably, TeaR achieves\na 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.\n","authors":["Keqin Bao","Nuo Chen","Xiaoyuan Li","Binyuan Hui","Bowen Yu","Fuli Feng","Junyang Lin","Xiangnan He","Dayiheng Liu"],"pdf_url":"https://arxiv.org/pdf/2507.07498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07495v1","updated":"2025-07-10T07:30:44Z","published":"2025-07-10T07:30:44Z","title":"PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step\n  Planning for Complex Problem Solving","summary":"  Recently, decomposing complex problems into simple subtasks--a crucial part\nof human-like natural planning--to solve the given problem has significantly\nboosted the performance of large language models (LLMs). However, leveraging\nsuch planning structures during post-training to boost the performance of\nsmaller open-source LLMs remains underexplored. Motivated by this, we introduce\nPLAN-TUNING, a unified post-training framework that (i) distills synthetic task\ndecompositions (termed \"planning trajectories\") from large-scale LLMs and (ii)\nfine-tunes smaller models via supervised and reinforcement-learning objectives\ndesigned to mimic these planning processes to improve complex reasoning. On\nGSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by\nan average $\\sim7\\%$. Furthermore, plan-tuned models show better generalization\ncapabilities on out-of-domain datasets, with average $\\sim10\\%$ and $\\sim12\\%$\nperformance improvements on OlympiadBench and AIME 2024, respectively. Our\ndetailed analysis demonstrates how planning trajectories improves complex\nreasoning capabilities, showing that PLAN-TUNING is an effective strategy for\nimproving task-specific performance of smaller LLMs.\n","authors":["Mihir Parmar","Palash Goyal","Xin Liu","Yiwen Song","Mingyang Ling","Chitta Baral","Hamid Palangi","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2507.07495v1.pdf","comment":"15 Pages"},{"id":"http://arxiv.org/abs/2409.08936v3","updated":"2025-07-10T07:24:12Z","published":"2024-09-13T15:55:15Z","title":"SimSUM: Simulated Benchmark with Structured and Unstructured Medical\n  Records","summary":"  Clinical information extraction, which involves structuring clinical concepts\nfrom unstructured medical text, remains a challenging problem that could\nbenefit from the inclusion of tabular background information available in\nelectronic health records. Existing open-source datasets lack explicit links\nbetween structured features and clinical concepts in the text, motivating the\nneed for a new research dataset. We introduce SimSUM, a benchmark dataset of\n10,000 simulated patient records that link unstructured clinical notes with\nstructured background variables. Each record simulates a patient encounter in\nthe domain of respiratory diseases and includes tabular data (e.g., symptoms,\ndiagnoses, underlying conditions) generated from a Bayesian network whose\nstructure and parameters are defined by domain experts. A large language model\n(GPT-4o) is prompted to generate a clinical note describing the encounter,\nincluding symptoms and relevant context. These notes are annotated with\nspan-level symptom mentions. We conduct an expert evaluation to assess note\nquality and run baseline predictive models on both the tabular and textual\ndata. The SimSUM dataset is primarily designed to support research on clinical\ninformation extraction in the presence of tabular background variables, which\ncan be linked through domain knowledge to concepts of interest to be extracted\nfrom the text (symptoms, in the case of SimSUM). Secondary uses include\nresearch on the automation of clinical reasoning over both tabular data and\ntext, causal effect estimation in the presence of tabular and/or textual\nconfounders, and multi-modal synthetic data generation. SimSUM is not intended\nfor training clinical decision support systems or production-grade models, but\nrather to facilitate reproducible research in a simplified and controlled\nsetting. The dataset is available at https://github.com/prabaey/SimSUM.\n","authors":["Paloma Rabaey","Stefan Heytens","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2409.08936v3.pdf","comment":"An earlier version of this dataset was published under the name\n  SynSUM. It has since been renamed to SimSUM to avoid confusion with synthetic\n  data generated from real data, and to emphasize the simulated nature of the\n  dataset"},{"id":"http://arxiv.org/abs/2504.02670v5","updated":"2025-07-10T07:15:51Z","published":"2025-04-03T15:11:55Z","title":"Affordable AI Assistants with Knowledge Graph of Thoughts","summary":"  Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.\n","authors":["Maciej Besta","Lorenzo Paleari","Jia Hao Andrea Jiang","Robert Gerstenberger","You Wu","Jón Gunnar Hannesson","Patrick Iff","Ales Kubicek","Piotr Nyczyk","Diana Khimey","Nils Blach","Haiqiang Zhang","Tao Zhang","Peiran Ma","Grzegorz Kwaśniewski","Marcin Copik","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2504.02670v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07484v1","updated":"2025-07-10T07:11:57Z","published":"2025-07-10T07:11:57Z","title":"Machine Bullshit: Characterizing the Emergent Disregard for Truth in\n  Large Language Models","summary":"  Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.\n","authors":["Kaiqu Liang","Haimin Hu","Xuandong Zhao","Dawn Song","Thomas L. Griffiths","Jaime Fernández Fisac"],"pdf_url":"https://arxiv.org/pdf/2507.07484v1.pdf","comment":"Project page, code & data: https://machine-bullshit.github.io"},{"id":"http://arxiv.org/abs/2504.09265v2","updated":"2025-07-10T07:07:53Z","published":"2025-04-12T15:58:02Z","title":"Mixture of Group Experts for Learning Invariant Representations","summary":"  Sparsely activated Mixture-of-Experts (MoE) models effectively increase the\nnumber of parameters while maintaining consistent computational costs per\ntoken. However, vanilla MoE models often suffer from limited diversity and\nspecialization among experts, constraining their performance and scalability,\nespecially as the number of experts increases. In this paper, we present a\nnovel perspective on vanilla MoE with top-$k$ routing inspired by sparse\nrepresentation. This allows us to bridge established theoretical insights from\nsparse representation into MoE models. Building on this foundation, we propose\na group sparse regularization approach for the input of top-$k$ routing, termed\nMixture of Group Experts (MoGE). MoGE indirectly regularizes experts by\nimposing structural constraints on the routing inputs, while preserving the\noriginal MoE architecture. Furthermore, we organize the routing input into a 2D\ntopographic map, spatially grouping neighboring elements. This structure\nenables MoGE to capture representations invariant to minor transformations,\nthereby significantly enhancing expert diversity and specialization.\nComprehensive evaluations across various Transformer models for image\nclassification and language modeling tasks demonstrate that MoGE substantially\noutperforms its MoE counterpart, with minimal additional memory and computation\noverhead. Our approach provides a simple yet effective solution to scale the\nnumber of experts and reduce redundancy among them. The source code is included\nin the supplementary material and will be publicly released.\n","authors":["Lei Kang","Jia Li","Mi Tian","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2504.09265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06795v2","updated":"2025-07-10T07:05:41Z","published":"2025-07-09T12:30:42Z","title":"ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining","summary":"  The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.\n","authors":["Seonwu Kim","Yohan Na","Kihun Kim","Hanhee Cho","Geun Lim","Mintae Kim","Seongik Park","Ki Hyun Kim","Youngsub Han","Byoung-Ki Jeon"],"pdf_url":"https://arxiv.org/pdf/2507.06795v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2402.13284v4","updated":"2025-07-10T06:38:13Z","published":"2024-02-19T09:07:59Z","title":"Structure Guided Large Language Model for SQL Generation","summary":"  Recent advancements in large language models (LLMs) have shown promise in\nbridging the gap between natural language queries and database management\nsystems, enabling users to interact with databases without the background of\nSQL. However, LLMs often struggle to comprehend complex database structures and\naccurately interpret user intentions. Decomposition-based methods have been\nproposed to enhance the performance of LLMs on complex tasks, but decomposing\nSQL generation into subtasks is non-trivial due to the declarative structure of\nSQL syntax and the intricate connections between query concepts and database\nelements. In this paper, we propose a novel Structure GUided text-to-SQL\nframework~(SGU-SQL) that incorporates syntax-based prompting to enhance the SQL\ngeneration capabilities of LLMs. Specifically, SGU-SQL establishes\nstructure-aware links between user queries and database schema and decomposes\nthe complex generation task using syntax-based prompting to enable more\naccurate LLM-based SQL generation. Extensive experiments on two benchmark\ndatasets demonstrate that SGU-SQL consistently outperforms state-of-the-art\ntext-to-SQL models.\n","authors":["Qinggang Zhang","Hao Chen","Junnan Dong","Shengyuan Chen","Feiran Huang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.13284v4.pdf","comment":"The 42nd International Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2507.07451v1","updated":"2025-07-10T05:58:55Z","published":"2025-07-10T05:58:55Z","title":"RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning","summary":"  Reinforcement learning (RL) for large language models is an energy-intensive\nendeavor: training can be unstable, and the policy may gradually drift away\nfrom its pretrained weights. We present \\emph{RLEP}\\, -- \\,Reinforcement\nLearning with Experience rePlay\\, -- \\,a two-phase framework that first\ncollects verified trajectories and then replays them during subsequent\ntraining. At every update step, the policy is optimized on mini-batches that\nblend newly generated rollouts with these replayed successes. By replaying\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\nfocuses learning on promising reasoning paths, and delivers both faster\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\nRLEP reaches baseline peak accuracy with substantially fewer updates and\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\ncode, datasets, and checkpoints are publicly available at\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\nresearch.\n","authors":["Hongzhi Zhang","Jia Fu","Jingyuan Zhang","Kai Fu","Qi Wang","Fuzheng Zhang","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.07451v1.pdf","comment":"https://github.com/Kwai-Klear/RLEP"},{"id":"http://arxiv.org/abs/2507.06229v2","updated":"2025-07-10T05:50:36Z","published":"2025-07-08T17:59:22Z","title":"Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving","summary":"  As language agents tackle increasingly complex tasks, they struggle with\neffective error correction and experience reuse across domains. We introduce\nAgent KB, a hierarchical experience framework that enables complex agentic\nproblem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses\na core limitation: agents traditionally cannot learn from each other's\nexperiences. By capturing both high-level strategies and detailed execution\nlogs, Agent KB creates a shared knowledge base that enables cross-agent\nknowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success\nrates by up to 16.28 percentage points. On the most challenging tasks, Claude-3\nimproves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on\nintermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to\nimprove from 41.33% to 53.33%. Our results suggest that Agent KB provides a\nmodular, framework-agnostic infrastructure for enabling agents to learn from\npast experiences and generalize successful strategies to new tasks.\n","authors":["Xiangru Tang","Tianrui Qin","Tianhao Peng","Ziyang Zhou","Daniel Shao","Tingting Du","Xinming Wei","Peng Xia","Fang Wu","He Zhu","Ge Zhang","Jiaheng Liu","Xingyao Wang","Sirui Hong","Chenglin Wu","Hao Cheng","Chi Wang","Wangchunshu Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.06229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07441v1","updated":"2025-07-10T05:38:15Z","published":"2025-07-10T05:38:15Z","title":"SAND: Boosting LLM Agents with Self-Taught Action Deliberation","summary":"  Large Language Model (LLM) agents are commonly tuned with supervised\nfinetuning on ReAct-style expert trajectories or preference optimization over\npairwise rollouts. Most of these methods focus on imitating specific expert\nbehaviors or promoting chosen reasoning thoughts and actions over rejected\nones. However, without reasoning and comparing over alternatives actions, LLM\nagents finetuned with these methods may over-commit towards seemingly plausible\nbut suboptimal actions due to limited action space exploration. To address\nthis, in this paper we propose Self-taught ActioN Deliberation (SAND)\nframework, enabling LLM agents to explicitly deliberate over candidate actions\nbefore committing to one. To tackle the challenges of when and what to\ndeliberate given large action space and step-level action evaluation, we\nincorporate self-consistency action sampling and execution-guided action\ncritique to help synthesize step-wise action deliberation thoughts using the\nbase model of the LLM agent. In an iterative manner, the deliberation\ntrajectories are then used to finetune the LLM agent itself. Evaluating on two\nrepresentative interactive agent tasks, SAND achieves an average 20%\nimprovement over initial supervised finetuning and also outperforms\nstate-of-the-art agent tuning approaches.\n","authors":["Yu Xia","Yiran Jenny Shen","Junda Wu","Tong Yu","Sungchul Kim","Ryan A. Rossi","Lina Yao","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2507.07441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07439v1","updated":"2025-07-10T05:29:34Z","published":"2025-07-10T05:29:34Z","title":"Towards Interpretable Time Series Foundation Models","summary":"  In this paper, we investigate the distillation of time series reasoning\ncapabilities into small, instruction-tuned language models as a step toward\nbuilding interpretable time series foundation models. Leveraging a synthetic\ndataset of mean-reverting time series with systematically varied trends and\nnoise levels, we generate natural language annotations using a large multimodal\nmodel and use these to supervise the fine-tuning of compact Qwen models. We\nintroduce evaluation metrics that assess the quality of the distilled reasoning\n- focusing on trend direction, noise intensity, and extremum localization - and\nshow that the post-trained models acquire meaningful interpretive capabilities.\nOur results highlight the feasibility of compressing time series understanding\ninto lightweight, language-capable models suitable for on-device or\nprivacy-sensitive deployment. This work contributes a concrete foundation\ntoward developing small, interpretable models that explain temporal patterns in\nnatural language.\n","authors":["Matthieu Boileau","Philippe Helluy","Jeremy Pawlus","Svitlana Vyetrenko"],"pdf_url":"https://arxiv.org/pdf/2507.07439v1.pdf","comment":"International Conference on Machine Leaning (ICML) 2025 Workshop on\n  Foundation Models for Structured Data"},{"id":"http://arxiv.org/abs/2507.07421v1","updated":"2025-07-10T04:31:01Z","published":"2025-07-10T04:31:01Z","title":"SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented\n  Synthetic EHR Data","summary":"  Eviction is a significant yet understudied social determinants of health\n(SDoH), linked to housing instability, unemployment, and mental health. While\neviction appears in unstructured electronic health records (EHRs), it is rarely\ncoded in structured fields, limiting downstream applications. We introduce\nSynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop\nannotation, and automated prompt optimization (APO) to extract eviction\nstatuses from clinical notes. Using this pipeline, we created the largest\npublic eviction-related SDoH dataset to date, comprising 14 fine-grained\ncategories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on\nSynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other\nSDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),\nGPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling\ncost-effective deployment across various model sizes. The pipeline reduces\nannotation effort by over 80%, accelerates dataset creation, enables scalable\neviction detection, and generalizes to other information extraction tasks.\n","authors":["Zonghai Yao","Youxia Zhao","Avijit Mitra","David A. Levy","Emily Druhl","Jack Tsai","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2507.07421v1.pdf","comment":"Equal contribution for the first two authors"},{"id":"http://arxiv.org/abs/2507.07419v1","updated":"2025-07-10T04:22:36Z","published":"2025-07-10T04:22:36Z","title":"MedReadCtrl: Personalizing medical text generation with\n  readability-controlled instruction learning","summary":"  Generative AI has demonstrated strong potential in healthcare, from clinical\ndecision support to patient-facing chatbots that improve outcomes. A critical\nchallenge for deployment is effective human-AI communication, where content\nmust be both personalized and understandable. We introduce MedReadCtrl, a\nreadability-controlled instruction tuning framework that enables LLMs to adjust\noutput complexity without compromising meaning. Evaluations of nine datasets\nand three tasks across medical and general domains show that MedReadCtrl\nachieves significantly lower readability instruction-following errors than\nGPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains\non unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).\nExperts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low\nliteracy levels. These gains reflect MedReadCtrl's ability to restructure\nclinical content into accessible, readability-aligned language while preserving\nmedical intent, offering a scalable solution to support patient education and\nexpand equitable access to AI-enabled care.\n","authors":["Hieu Tran","Zonghai Yao","Won Seok Jang","Sharmin Sultana","Allen Chang","Yuan Zhang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2507.07419v1.pdf","comment":"Equal contribution for the first two authors. arXiv admin note: text\n  overlap with arXiv:2406.09205"},{"id":"http://arxiv.org/abs/2507.07417v1","updated":"2025-07-10T04:20:53Z","published":"2025-07-10T04:20:53Z","title":"May I have your Attention? Breaking Fine-Tuning based Prompt Injection\n  Defenses using Architecture-Aware Attacks","summary":"  A popular class of defenses against prompt injection attacks on large\nlanguage models (LLMs) relies on fine-tuning the model to separate instructions\nand data, so that the LLM does not follow instructions that might be present\nwith data. There are several academic systems and production-level\nimplementations of this idea. We evaluate the robustness of this class of\nprompt injection defenses in the whitebox setting by constructing strong\noptimization-based attacks and showing that the defenses do not provide the\nclaimed security properties. Specifically, we construct a novel attention-based\nattack algorithm for text-based LLMs and apply it to two recent whitebox\ndefenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks\nwith success rates of up to 70% with modest increase in attacker budget in\nterms of tokens. Our findings make fundamental progress towards understanding\nthe robustness of prompt injection defenses in the whitebox setting. We release\nour code and attacks at https://github.com/nishitvp/better_opts_attacks\n","authors":["Nishit V. Pandya","Andrey Labunets","Sicun Gao","Earlence Fernandes"],"pdf_url":"https://arxiv.org/pdf/2507.07417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10927v3","updated":"2025-07-10T04:17:52Z","published":"2024-11-17T01:15:58Z","title":"Inter-linguistic Phonetic Composition (IPC): A Theoretical and\n  Computational Approach to Enhance Second Language Pronunciation","summary":"  Learners of a second language (L2) often unconsciously substitute unfamiliar\nL2 phonemes with similar phonemes from their native language (L1), even though\nnative speakers of the L2 perceive these sounds as distinct and\nnon-interchangeable. This phonemic substitution leads to deviations from the\nstandard phonological patterns of the L2, creating challenges for learners in\nacquiring accurate L2 pronunciation. To address this, we propose\nInter-linguistic Phonetic Composition (IPC), a novel computational method\ndesigned to minimize incorrect phonological transfer by reconstructing L2\nphonemes as composite sounds derived from multiple L1 phonemes. Tests with two\nautomatic speech recognition models demonstrated that when L2 speakers produced\nIPC-generated composite sounds, the recognition rate of target L2 phonemes\nimproved by 20% compared to when their pronunciation was influenced by original\nphonological transfer patterns. The improvement was observed within a\nrelatively shorter time frame, demonstrating rapid acquisition of the composite\nsound.\n","authors":["Jisang Park","Minu Kim","DaYoung Hong","Jongha Lee"],"pdf_url":"https://arxiv.org/pdf/2411.10927v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11724v3","updated":"2025-07-10T04:17:26Z","published":"2024-09-18T06:19:59Z","title":"TART: An Open-Source Tool-Augmented Framework for Explainable\n  Table-based Reasoning","summary":"  Current Large Language Models (LLMs) exhibit limited ability to understand\ntable structures and to apply precise numerical reasoning, which is crucial for\ntasks such as table question answering (TQA) and table-based fact verification\n(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning\nframework for Tables (TART), which integrates LLMs with specialized tools. TART\ncontains three key components: a table formatter to ensure accurate data\nrepresentation, a tool maker to develop specific computational tools, and an\nexplanation generator to maintain explainability. We also present the TOOLTAB\ndataset, a new benchmark designed specifically for training LLMs in table-tool\nintegration. Our experiments indicate that TART achieves substantial\nimprovements over existing methods (e.g., Chain-of-Thought) by improving both\nthe precision of data processing and the clarity of the reasoning process.\nNotably, TART paired with CodeLlama achieves 90.0% of the accuracy of the\nclosed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse\nreal-world scenarios. All the code and data are available at\nhttps://github.com/XinyuanLu00/TART.\n","authors":["Xinyuan Lu","Liangming Pan","Yubo Ma","Preslav Nakov","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2409.11724v3.pdf","comment":"NAACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2507.07414v1","updated":"2025-07-10T04:13:53Z","published":"2025-07-10T04:13:53Z","title":"GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural\n  Networks for Text Representation","summary":"  Time, cost, and energy efficiency are critical considerations in\nDeep-Learning (DL), particularly when processing long texts. Transformers,\nwhich represent the current state of the art, exhibit quadratic computational\ncomplexity relative to input length, making them inefficient for extended\ndocuments. This study introduces a novel model architecture that combines Graph\nNeural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated\nwith a real-time, end-to-end graph generation mechanism. The model processes\ncompact batches of character-level inputs without requiring padding or\ntruncation. To enhance performance while maintaining high speed and efficiency,\nthe model incorporates information from Large Language Models (LLMs), such as\ntoken embeddings and sentiment polarities, through efficient dictionary\nlookups. It captures local contextual patterns using CNNs, expands local\nreceptive fields via lattice-based graph structures, and employs small-world\ngraphs to aggregate document-level information. The generated graphs exhibit\nstructural properties indicative of meaningful semantic organization, with an\naverage clustering coefficient of approximately 0.45 and an average shortest\npath length ranging between 4 and 5. The model is evaluated across multiple\ntext classification tasks, including sentiment analysis and\nnews-categorization, and is compared against state-of-the-art models.\nExperimental results confirm the proposed model's efficiency and competitive\nperformance.\n","authors":["Fardin Rastakhiz"],"pdf_url":"https://arxiv.org/pdf/2507.07414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18151v3","updated":"2025-07-10T03:38:25Z","published":"2024-12-24T04:09:33Z","title":"CoAM: Corpus of All-Type Multiword Expressions","summary":"  Multiword expressions (MWEs) refer to idiomatic sequences of multiple words.\nMWE identification, i.e., detecting MWEs in text, can play a key role in\ndownstream tasks such as machine translation, but existing datasets for the\ntask are inconsistently annotated, limited to a single type of MWE, or limited\nin size. To enable reliable and comprehensive evaluation, we created CoAM:\nCorpus of All-Type Multiword Expressions, a dataset of 1.3K sentences\nconstructed through a multi-step process to enhance data quality consisting of\nhuman annotation, human review, and automated consistency checking.\nAdditionally, for the first time in a dataset of MWE identification, CoAM's\nMWEs are tagged with MWE types, such as Noun and Verb, enabling fine-grained\nerror analysis. Annotations for CoAM were collected using a new interface\ncreated with our interface generator, which allows easy and flexible annotation\nof MWEs in any form. Through experiments using CoAM, we find that a fine-tuned\nlarge language model outperforms MWEasWSD, which achieved the state-of-the-art\nperformance on the DiMSUM dataset. Furthermore, analysis using our MWE type\ntagged data reveals that Verb MWEs are easier than Noun MWEs to identify across\napproaches.\n","authors":["Yusuke Ide","Joshua Tanner","Adam Nohejl","Jacob Hoffman","Justin Vasselli","Hidetaka Kamigaito","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2412.18151v3.pdf","comment":"ACL 2025 main"},{"id":"http://arxiv.org/abs/2507.06920v2","updated":"2025-07-10T03:12:09Z","published":"2025-07-09T14:58:47Z","title":"Rethinking Verification for LLM Code Generation: From Generation to\n  Testing","summary":"  Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.\n","authors":["Zihan Ma","Taolin Zhang","Maosong Cao","Junnan Liu","Wenwei Zhang","Minnan Luo","Songyang Zhang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2507.06920v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06539v2","updated":"2025-07-10T02:51:21Z","published":"2025-07-09T04:46:31Z","title":"Large Language Model for Extracting Complex Contract Information in\n  Industrial Scenes","summary":"  This paper proposes a high-quality dataset construction method for complex\ncontract information extraction tasks in industrial scenarios and fine-tunes a\nlarge language model based on this dataset. Firstly, cluster analysis is\nperformed on industrial contract texts, and GPT-4 and GPT-3.5 are used to\nextract key information from the original contract data, obtaining high-quality\ndata annotations. Secondly, data augmentation is achieved by constructing new\ntexts, and GPT-3.5 generates unstructured contract texts from randomly combined\nkeywords, improving model robustness. Finally, the large language model is\nfine-tuned based on the high-quality dataset. Experimental results show that\nthe model achieves excellent overall performance while ensuring high field\nrecall and precision and considering parsing efficiency. LoRA, data balancing,\nand data augmentation effectively enhance model accuracy and robustness. The\nproposed method provides a novel and efficient solution for industrial contract\ninformation extraction tasks.\n","authors":["Yunyang Cao","Yanjun Li","Silong Dai"],"pdf_url":"https://arxiv.org/pdf/2507.06539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15216v2","updated":"2025-07-10T02:10:30Z","published":"2025-05-21T07:44:52Z","title":"BountyBench: Dollar Impact of AI Agent Attackers and Defenders on\n  Real-World Cybersecurity Systems","summary":"  AI agents have the potential to significantly alter the cybersecurity\nlandscape. Here, we introduce the first framework to capture offensive and\ndefensive cyber-capabilities in evolving real-world systems. Instantiating this\nframework with BountyBench, we set up 25 systems with complex, real-world\ncodebases. To capture the vulnerability lifecycle, we define three task types:\nDetect (detecting a new vulnerability), Exploit (exploiting a specific\nvulnerability), and Patch (patching a specific vulnerability). For Detect, we\nconstruct a new success indicator, which is general across vulnerability types\nand provides localized evaluation. We manually set up the environment for each\nsystem, including installing packages, setting up server(s), and hydrating\ndatabase(s). We add 40 bug bounties, which are vulnerabilities with monetary\nawards of \\$10-\\$30,485, covering 9 of the OWASP Top 10 Risks. To modulate task\ndifficulty, we devise a new strategy based on information to guide detection,\ninterpolating from identifying a zero day to exploiting a specific\nvulnerability. We evaluate 8 agents: Claude Code, OpenAI Codex CLI with o3-high\nand o4-mini, and custom agents with o3-high, GPT-4.1, Gemini 2.5 Pro Preview,\nClaude 3.7 Sonnet Thinking, and DeepSeek-R1. Given up to three attempts, the\ntop-performing agents are OpenAI Codex CLI: o3-high (12.5% on Detect, mapping\nto \\$3,720; 90% on Patch, mapping to \\$14,152), Custom Agent with Claude 3.7\nSonnet Thinking (67.5% on Exploit), and OpenAI Codex CLI: o4-mini (90% on\nPatch, mapping to \\$14,422). OpenAI Codex CLI: o3-high, OpenAI Codex CLI:\no4-mini, and Claude Code are more capable at defense, achieving higher Patch\nscores of 90%, 90%, and 87.5%, compared to Exploit scores of 47.5%, 32.5%, and\n57.5% respectively; while the custom agents are relatively balanced between\noffense and defense, achieving Exploit scores of 37.5-67.5% and Patch scores of\n35-60%.\n","authors":["Andy K. Zhang","Joey Ji","Celeste Menders","Riya Dulepet","Thomas Qin","Ron Y. Wang","Junrong Wu","Kyleen Liao","Jiliang Li","Jinghan Hu","Sara Hong","Nardos Demilew","Shivatmica Murgai","Jason Tran","Nishka Kacheria","Ethan Ho","Denis Liu","Lauren McLane","Olivia Bruvik","Dai-Rong Han","Seungwoo Kim","Akhil Vyas","Cuiyuanxiu Chen","Ryan Li","Weiran Xu","Jonathan Z. Ye","Prerit Choudhary","Siddharth M. Bhatia","Vikram Sivashankar","Yuxuan Bao","Dawn Song","Dan Boneh","Daniel E. Ho","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2505.15216v2.pdf","comment":"93 pages"},{"id":"http://arxiv.org/abs/2507.07375v1","updated":"2025-07-10T01:56:56Z","published":"2025-07-10T01:56:56Z","title":"Bradley-Terry and Multi-Objective Reward Modeling Are Complementary","summary":"  Reward models trained on human preference data have demonstrated strong\neffectiveness in aligning Large Language Models (LLMs) with human intent under\nthe framework of Reinforcement Learning from Human Feedback (RLHF). However,\nRLHF remains vulnerable to reward hacking, where the policy exploits\nimperfections in the reward function rather than genuinely learning the\nintended behavior. Although significant efforts have been made to mitigate\nreward hacking, they predominantly focus on and evaluate in-distribution\nscenarios, where the training and testing data for the reward model share the\nsame distribution. In this paper, we empirically show that state-of-the-art\nmethods struggle in more challenging out-of-distribution (OOD) settings. We\nfurther demonstrate that incorporating fine-grained multi-attribute scores\nhelps address this challenge. However, the limited availability of high-quality\ndata often leads to weak performance of multi-objective reward functions, which\ncan negatively impact overall performance and become the bottleneck. To address\nthis issue, we propose a unified reward modeling framework that jointly trains\nBradley--Terry (BT) single-objective and multi-objective regression-based\nreward functions using a shared embedding space. We theoretically establish a\nconnection between the BT loss and the regression objective and highlight their\ncomplementary benefits. Specifically, the regression task enhances the\nsingle-objective reward function's ability to mitigate reward hacking in\nchallenging OOD settings, while BT-based training improves the scoring\ncapability of the multi-objective reward function, enabling a 7B model to\noutperform a 70B baseline. Extensive experimental results demonstrate that our\nframework significantly improves both the robustness and the scoring\nperformance of reward models.\n","authors":["Zhiwei Zhang","Hui Liu","Xiaomin Li","Zhenwei Dai","Jingying Zeng","Fali Wang","Minhua Lin","Ramraj Chandradevan","Zhen Li","Chen Luo","Xianfeng Tang","Qi He","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2507.07375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06838v2","updated":"2025-07-10T01:36:33Z","published":"2025-07-09T13:35:36Z","title":"Shifting from Ranking to Set Selection for Retrieval Augmented\n  Generation","summary":"  Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR\n","authors":["Dahyun Lee","Yongrae Jo","Haeju Park","Moontae Lee"],"pdf_url":"https://arxiv.org/pdf/2507.06838v2.pdf","comment":"Accepted to ACL 2025 main (Oral Presentation)"},{"id":"http://arxiv.org/abs/2507.08218v1","updated":"2025-07-10T23:47:05Z","published":"2025-07-10T23:47:05Z","title":"Simple Mechanistic Explanations for Out-Of-Context Reasoning","summary":"  Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs\nexhibit surprisingly deep out-of-distribution generalization. Rather than\nlearning shallow heuristics, they implicitly internalize and act on the\nconsequences of observations scattered throughout the fine-tuning data. In this\nwork, we investigate this phenomenon mechanistically and find that many\ninstances of OOCR in the literature have a simple explanation: the LoRA\nfine-tuning essentially adds a constant steering vector, steering the model\ntowards a general concept. This improves performance on the fine-tuning task\nand in many other concept-related domains, causing the surprising\ngeneralization. Moreover, we can directly train steering vectors for these\ntasks from scratch, which also induces OOCR. We find that our results hold even\nfor a task that seems like it must involve conditional behavior (model\nbackdoors); it turns out that unconditionally adding a steering vector is\nsufficient. Overall, our work presents one explanation of what gets learned\nduring fine-tuning for OOCR tasks, contributing to the key question of why LLMs\ncan reason out of context, an advanced capability that is highly relevant to\ntheir safe and reliable deployment.\n","authors":["Atticus Wang","Joshua Engels","Oliver Clive-Griffin"],"pdf_url":"https://arxiv.org/pdf/2507.08218v1.pdf","comment":"ICML 2025 Workshop R2-FM"},{"id":"http://arxiv.org/abs/2410.05401v3","updated":"2025-07-10T23:19:49Z","published":"2024-10-07T18:07:56Z","title":"Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs:\n  Thematic Insights and Fairness Evaluation","summary":"  Climate change communication on social media increasingly employs\nmicrotargeting strategies to effectively reach and influence specific\ndemographic groups. This study presents a post-hoc analysis of microtargeting\npractices within climate campaigns by leveraging large language models (LLMs)\nto examine Facebook advertisements. Our analysis focuses on two key aspects:\ndemographic targeting and fairness. We evaluate the ability of LLMs to\naccurately predict the intended demographic targets, such as gender and age\ngroup, achieving an overall accuracy of 88.55%. Furthermore, we instruct the\nLLMs to generate explanations for their classifications, providing transparent\nreasoning behind each decision. These explanations reveal the specific thematic\nelements used to engage different demographic segments, highlighting distinct\nstrategies tailored to various audiences. Our findings show that young adults\nare primarily targeted through messages emphasizing activism and environmental\nconsciousness, while women are engaged through themes related to caregiving\nroles and social advocacy. In addition to evaluating the effectiveness of LLMs\nin detecting microtargeted messaging, we conduct a comprehensive fairness\nanalysis to identify potential biases in model predictions. Our findings\nindicate that while LLMs perform well overall, certain biases exist,\nparticularly in the classification of senior citizens and male audiences. By\nshowcasing the efficacy of LLMs in dissecting and explaining targeted\ncommunication strategies and by highlighting fairness concerns, this study\nprovides a valuable framework for future research aimed at enhancing\ntransparency, accountability, and inclusivity in social media-driven climate\ncampaigns.\n","authors":["Tunazzina Islam","Dan Goldwasser"],"pdf_url":"https://arxiv.org/pdf/2410.05401v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12546v2","updated":"2025-07-10T23:16:43Z","published":"2025-05-18T21:06:32Z","title":"Extracting memorized pieces of (copyrighted) books from open-weight\n  language models","summary":"  Plaintiffs and defendants in copyright lawsuits over generative AI often make\nsweeping, opposing claims about the extent to which large language models\n(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial\nML and copyright law, we show that these polarized positions dramatically\noversimplify the relationship between memorization and copyright. To do so, we\nleverage a recent probabilistic extraction technique to extract pieces of the\nBooks3 dataset from 17 open-weight LLMs. Through numerous experiments, we show\nthat it's possible to extract substantial parts of at least some books from\ndifferent LLMs. This is evidence that these LLMs have memorized the extracted\ntext; this memorized content is copied inside the model parameters. But the\nresults are complicated: the extent of memorization varies both by model and by\nbook. With our specific experiments, we find that the largest LLMs don't\nmemorize most books--either in whole or in part. However, we also find that\nLlama 3.1 70B memorizes some books, like Harry Potter and the Sorcerer's Stone\nand 1984, almost entirely. In fact, Harry Potter is so memorized that, using a\nseed prompt consisting of just the first line of chapter 1, we can\ndeterministically generate the entire book near-verbatim. We discuss why our\nresults have significant implications for copyright cases, though not ones that\nunambiguously favor either side.\n","authors":["A. Feder Cooper","Aaron Gokaslan","Ahmed Ahmed","Amy B. Cyphert","Christopher De Sa","Mark A. Lemley","Daniel E. Ho","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2505.12546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18290v3","updated":"2025-07-10T23:11:02Z","published":"2023-10-27T17:28:23Z","title":"Riddle Generation using Learning Resources","summary":"  One of the primary challenges in online learning environments, is to retain\nlearner engagement. Several different instructional strategies are proposed\nboth in online and offline environments to enhance learner engagement. The\nConcept Attainment Model is one such instructional strategy that focuses on\nlearners acquiring a deeper understanding of a concept rather than just its\ndictionary definition. This is done by searching and listing the properties\nused to distinguish examples from non-examples of various concepts. Our work\nattempts to apply the Concept Attainment Model to build conceptual riddles, to\ndeploy over online learning environments. The approach involves creating\nfactual triples from learning resources, classifying them based on their\nuniqueness to a concept into `Topic Markers' and `Common', followed by\ngenerating riddles based on the Concept Attainment Model's format and capturing\nall possible solutions to those riddles. The results obtained from the human\nevaluation of riddles prove encouraging.\n","authors":["Niharika Sri Parasa","Chaitali Diwan","Srinath Srinivasa"],"pdf_url":"https://arxiv.org/pdf/2310.18290v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08203v1","updated":"2025-07-10T22:23:51Z","published":"2025-07-10T22:23:51Z","title":"TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM\n  Outputs","summary":"  Generative Large Language Models (LLMs)inevitably produce untruthful\nresponses. Accurately predicting the truthfulness of these outputs is critical,\nespecially in high-stakes settings. To accelerate research in this domain and\nmake truthfulness prediction methods more accessible, we introduce TruthTorchLM\nan open-source, comprehensive Python library featuring over 30 truthfulness\nprediction methods, which we refer to as Truth Methods. Unlike existing\ntoolkits such as Guardrails, which focus solely on document-grounded\nverification, or LM-Polygraph, which is limited to uncertainty-based methods,\nTruthTorchLM offers a broad and extensible collection of techniques. These\nmethods span diverse tradeoffs in computational cost, access level (e.g.,\nblack-box vs white-box), grounding document requirements, and supervision type\n(self-supervised or supervised). TruthTorchLM is seamlessly compatible with\nboth HuggingFace and LiteLLM, enabling support for locally hosted and API-based\nmodels. It also provides a unified interface for generation, evaluation,\ncalibration, and long-form truthfulness prediction, along with a flexible\nframework for extending the library with new methods. We conduct an evaluation\nof representative truth methods on three datasets, TriviaQA, GSM8K, and\nFactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM\n","authors":["Duygu Nur Yaldiz","Yavuz Faruk Bakman","Sungmin Kang","Alperen Öziş","Hayrettin Eren Yildiz","Mitash Ashish Shah","Zhiqi Huang","Anoop Kumar","Alfy Samuel","Daben Liu","Sai Praneeth Karimireddy","Salman Avestimehr"],"pdf_url":"https://arxiv.org/pdf/2507.08203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08191v1","updated":"2025-07-10T21:58:41Z","published":"2025-07-10T21:58:41Z","title":"Overview of the TREC 2021 deep learning track","summary":"  This is the third year of the TREC Deep Learning track. As in previous years,\nwe leverage the MS MARCO datasets that made hundreds of thousands of human\nannotated training labels available for both passage and document ranking\ntasks. In addition, this year we refreshed both the document and the passage\ncollections which also led to a nearly four times increase in the document\ncollection size and nearly $16$ times increase in the size of the passage\ncollection. Deep neural ranking models that employ large scale pretraininig\ncontinued to outperform traditional retrieval methods this year. We also found\nthat single stage retrieval can achieve good performance on both tasks although\nthey still do not perform at par with multistage retrieval pipelines. Finally,\nthe increase in the collection size and the general data refresh raised some\nquestions about completeness of NIST judgments and the quality of the training\nlabels that were mapped to the new collections from the old ones which we\ndiscuss in this report.\n","authors":["Nick Craswell","Bhaskar Mitra","Emine Yilmaz","Daniel Campos","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2507.08191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11903v4","updated":"2025-07-10T20:40:55Z","published":"2025-06-13T15:53:17Z","title":"GeistBERT: Breathing Life into German NLP","summary":"  Advances in transformer-based language models have highlighted the benefits\nof language-specific pre-training on high-quality corpora. In this context,\nGerman NLP stands to gain from updated architectures and modern datasets\ntailored to the linguistic characteristics of the German language. GeistBERT\nseeks to improve German language processing by incrementally training on a\ndiverse corpus and optimizing model performance across various NLP tasks. We\npre-trained GeistBERT using fairseq, following the RoBERTa base configuration\nwith Whole Word Masking (WWM), and initialized from GottBERT weights. The model\nwas trained on a 1.3 TB German corpus with dynamic masking and a fixed sequence\nlength of 512 tokens. For evaluation, we fine-tuned the model on standard\ndownstream tasks, including NER (CoNLL 2003, GermEval 2014), text\nclassification (GermEval 2018 coarse/fine, 10kGNAD), and NLI (German XNLI),\nusing $F_1$ score and accuracy as evaluation metrics. GeistBERT achieved strong\nresults across all tasks, leading among base models and setting a new\nstate-of-the-art (SOTA) in GermEval 2018 fine text classification. It also\noutperformed several larger models, particularly in classification benchmarks.\nTo support research in German NLP, we release GeistBERT under the MIT license.\n","authors":["Raphael Scheible-Schmitt","Johann Frei"],"pdf_url":"https://arxiv.org/pdf/2506.11903v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08151v1","updated":"2025-07-10T20:20:02Z","published":"2025-07-10T20:20:02Z","title":"Distilling Empathy from Large Language Models","summary":"  The distillation of knowledge from Large Language Models (LLMs) into Smaller\nLanguage Models (SLMs), preserving the capabilities and performance of LLMs\nwhile reducing model size, has played a key role in the proliferation of LLMs.\nBecause SLMs are considerably smaller than LLMs, they are often utilized in\ndomains where human interaction is frequent but resources are highly\nconstrained, e.g., smart phones. Therefore, it is crucial to ensure that\nempathy, a fundamental aspect of positive human interactions, already instilled\ninto LLMs, is retained by SLMs after distillation. In this paper, we develop a\ncomprehensive approach for effective empathy distillation from LLMs into SLMs.\nOur approach features a two-step fine-tuning process that fully leverages\ndatasets of empathetic dialogue responses distilled from LLMs. We explore\nseveral distillation methods beyond basic direct prompting and propose four\nunique sets of prompts for targeted empathy improvement to significantly\nenhance the empathy distillation process. Our evaluations demonstrate that SLMs\nfine-tuned through the two-step fine-tuning process with distillation datasets\nenhanced by the targeted empathy improvement prompts significantly outperform\nthe base SLM at generating empathetic responses with a win rate of 90%. Our\ntargeted empathy improvement prompts substantially outperform the basic direct\nprompting with a 10% improvement in win rate.\n","authors":["Henry J. Xie","Jinghan Zhang","Xinhao Zhang","Kunpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2507.08151v1.pdf","comment":"Accepted by SIGDIAL 2025"},{"id":"http://arxiv.org/abs/2507.08143v1","updated":"2025-07-10T20:03:35Z","published":"2025-07-10T20:03:35Z","title":"Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores","summary":"  Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.\n","authors":["Vivek Chari","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2507.08143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08128v1","updated":"2025-07-10T19:40:21Z","published":"2025-07-10T19:40:21Z","title":"Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large\n  Audio Language Models","summary":"  We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large\naudio-language model that advances reasoning and understanding across speech,\nsound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder\ntrained using a novel strategy for joint representation learning across all 3\nmodalities of speech, sound, and music; (ii) flexible, on-demand thinking,\nallowing the model to do chain-of-thought-type reasoning before answering;\n(iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning\n(including speech) up to 10 minutes; and (v) voice-to-voice interaction. To\nenable these capabilities, we propose several large-scale training datasets\ncurated using novel strategies, including AudioSkills-XL, LongAudio-XL,\nAF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based\ntraining strategy. Trained on only open-source audio data, AF3 achieves new\nSOTA results on over 20+ (long) audio understanding and reasoning benchmarks,\nsurpassing both open-weight and closed-source models trained on much larger\ndatasets.\n","authors":["Arushi Goel","Sreyan Ghosh","Jaehyeon Kim","Sonal Kumar","Zhifeng Kong","Sang-gil Lee","Chao-Han Huck Yang","Ramani Duraiswami","Dinesh Manocha","Rafael Valle","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2507.08128v1.pdf","comment":"Code, Datasets and Models: https://research.nvidia.com/labs/adlr/AF3/"},{"id":"http://arxiv.org/abs/2507.06565v2","updated":"2025-07-10T19:34:51Z","published":"2025-07-09T05:39:56Z","title":"The Flaws of Others: An LLM-driven Framework for Scientific Knowledge\n  Production","summary":"  Large-language models turn writing into a live exchange between humans and\nsoftware. We capture this new medium with a discursive-network model that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. Broadening the focus from isolated hallucinations, we define\ninvalidation (any factual, logical, or structural breach) and show it follows\nfour hazards: drift from truth, self-repair, fresh fabrication, and external\ndetection. A general mathematical model of discursive networks is developed to\nprovide valuable insights: A network governed only by drift and self-repair\nstabilizes at a modest error rate; adding fabrication reproduces the high rates\nseen in current LLMs. Giving each false claim even a small chance of peer\nreview shifts the system to a truth-dominant state. We operationalize peer\nreview with the open-source \\emph{Flaws-of-Others (FOO) algorithm}: a\nconfigurable loop in which any set of agents critique one another while a\nharmoniser merges their verdicts. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nwiring imperfect ones into networks that keep each other honest.\n","authors":["Juan B. Gutiérrez"],"pdf_url":"https://arxiv.org/pdf/2507.06565v2.pdf","comment":"27 pages, 3 figures, 4 tables, 1 algorithm, 48 references"},{"id":"http://arxiv.org/abs/2507.08109v1","updated":"2025-07-10T18:52:09Z","published":"2025-07-10T18:52:09Z","title":"Audit, Alignment, and Optimization of LM-Powered Subroutines with\n  Application to Public Comment Processing","summary":"  The advent of language models (LMs) has the potential to dramatically\naccelerate tasks that may be cast to text-processing; however, real-world\nadoption is hindered by concerns regarding safety, explainability, and bias.\nHow can we responsibly leverage LMs in a transparent, auditable manner --\nminimizing risk and allowing human experts to focus on informed decision-making\nrather than data-processing or prompt engineering? In this work, we propose a\nframework for declaring statically typed, LM-powered subroutines (i.e.,\ncallable, function-like procedures) for use within conventional asynchronous\ncode -- such that sparse feedback from human experts is used to improve the\nperformance of each subroutine online (i.e., during use). In our\nimplementation, all LM-produced artifacts (i.e., prompts, inputs, outputs, and\ndata-dependencies) are recorded and exposed to audit on demand. We package this\nframework as a library to support its adoption and continued development. While\nthis framework may be applicable across several real-world decision workflows\n(e.g., in healthcare and legal fields), we evaluate it in the context of public\ncomment processing as mandated by the 1969 National Environmental Protection\nAct (NEPA): Specifically, we use this framework to develop \"CommentNEPA,\" an\napplication that compiles, organizes, and summarizes a corpus of public\ncommentary submitted in response to a project requiring environmental review.\nWe quantitatively evaluate the application by comparing its outputs (when\noperating without human feedback) to historical ``ground-truth'' data as\nlabelled by human annotators during the preparation of official environmental\nimpact statements.\n","authors":["Reilly Raab","Mike Parker","Dan Nally","Sadie Montgomery","Anastasia Bernat","Sai Munikoti","Sameera Horawalavithana"],"pdf_url":"https://arxiv.org/pdf/2507.08109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08107v1","updated":"2025-07-10T18:50:05Z","published":"2025-07-10T18:50:05Z","title":"GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs","summary":"  We propose a new approach for generating SPARQL queries on RDF knowledge\ngraphs from natural language questions or keyword queries, using a large\nlanguage model. Our approach does not require fine-tuning. Instead, it uses the\nlanguage model to explore the knowledge graph by strategically executing SPARQL\nqueries and searching for relevant IRIs and literals. We evaluate our approach\non a variety of benchmarks (for knowledge graphs of different kinds and sizes)\nand language models (of different scales and types, commercial as well as\nopen-source) and compare it with existing approaches. On Wikidata we reach\nstate-of-the-art results on multiple benchmarks, despite the zero-shot setting.\nOn Freebase we come close to the best few-shot methods. On other, less commonly\nevaluated knowledge graphs and benchmarks our approach also performs well\noverall. We conduct several additional studies, like comparing different ways\nof searching the graphs, incorporating a feedback mechanism, or making use of\nfew-shot examples.\n","authors":["Sebastian Walter","Hannah Bast"],"pdf_url":"https://arxiv.org/pdf/2507.08107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07723v1","updated":"2025-07-10T12:57:39Z","published":"2025-07-10T12:57:39Z","title":"Stable Preference Optimization for LLMs: A Bilevel Approach Beyond\n  Direct Preference Optimization","summary":"  Direct Preference Optimization (DPO) has emerged as a popular and efficient\nalternative to reward modeling and reinforcement learning for aligning language\nmodels with human preferences. Despite its empirical success, the theoretical\nproperties and intrinsic limitations of DPO remain underexplored. In this work,\nwe first present a comprehensive analysis of DPO's dynamics from a probability\nevolution perspective. Our analysis reveals that DPO is highly sensitive to\ninitialization. It also tends to misallocate probability mass, which can\ninadvertently shift probability toward irrelevant or undesired responses. This\nmisallocation may unintentionally reinforce model bias, thereby compromising\nboth the stability of model alignment and the consistency with intended\npreferences. Motivated by these theoretical findings, we propose a\ntheoretically grounded bilevel optimization framework that tightly integrate\nsupervised fine-tuning with an enhanced DPO objective a.k.a. stable preference\noptimization. Our approach introduces a principled regularization scheme to\nexplicitly encourage absolute probability improvement for preferred outputs,\nwhile maintaining stable optimization dynamics. Experiments on challenging\nreasoning and summarization benchmarks elucidate that our method consistently\nimproves reasoning accuracy and better aligns output distributions with\nintended preferences, outperforming standard DPO. Stable preference\noptimization provides new insights into the design of preference-based\nalignment objectives and opens up new avenues towards more reliable and\ninterpretable language model alignment.\n","authors":["Chengtao Jian","Kai Yang","Ye Ouyang","Xiaozhou Ye"],"pdf_url":"https://arxiv.org/pdf/2507.07723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07599v1","updated":"2025-07-10T09:57:08Z","published":"2025-07-10T09:57:08Z","title":"Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from\n  Emergency Department Triage Notes Using Fine-Tuned Large Language Models","summary":"  This study evaluates fine-tuned Llama 3.2 models for extracting\nvaccine-related information from emergency department triage notes to support\nnear real-time vaccine safety surveillance. Prompt engineering was used to\ninitially create a labeled dataset, which was then confirmed by human\nannotators. The performance of prompt-engineered models, fine-tuned models, and\na rule-based approach was compared. The fine-tuned Llama 3 billion parameter\nmodel outperformed other models in its accuracy of extracting vaccine names.\nModel quantization enabled efficient deployment in resource-constrained\nenvironments. Findings demonstrate the potential of large language models in\nautomating data extraction from emergency department notes, supporting\nefficient vaccine safety surveillance and early detection of emerging adverse\nevents following immunization issues.\n","authors":["Sedigh Khademi","Jim Black","Christopher Palmer","Muhammad Javed","Hazel Clothier","Jim Buttery","Gerardo Luis Dimaguila"],"pdf_url":"https://arxiv.org/pdf/2507.07599v1.pdf","comment":"5 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2507.08000v1","updated":"2025-07-10T17:59:59Z","published":"2025-07-10T17:59:59Z","title":"Impact of Pretraining Word Co-occurrence on Compositional Generalization\n  in Multimodal Models","summary":"  CLIP and large multimodal models (LMMs) have better accuracy on examples\ninvolving concepts that are highly represented in the training data. However,\nthe role of concept combinations in the training data on compositional\ngeneralization is largely unclear -- for instance, how does accuracy vary when\na common object appears in an uncommon pairing with another object? In this\npaper, we investigate how word co-occurrence statistics in the pretraining\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\nperformance. To disentangle the effects of word co-occurrence frequencies from\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\ninformation (PMI), which normalizes the joint probability of two words\nco-occurring by the probability of co-occurring independently. Using\nsynthetically generated images with a variety of concept pairs, we show a\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\naccuracy on common concepts is affected by the combination of concepts in the\nimage. Leveraging this finding, we reproduce this effect in natural images by\nediting them to contain pairs with varying PMI, resulting in a correlation of\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\nhighlight the need for algorithms and architectures that improve compositional\ngeneralization in multimodal models without scaling the training data\ncombinatorially. Our code is available at\nhttps://github.com/helenqu/multimodal-pretraining-pmi.\n","authors":["Helen Qu","Sang Michael Xie"],"pdf_url":"https://arxiv.org/pdf/2507.08000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07999v1","updated":"2025-07-10T17:59:58Z","published":"2025-07-10T17:59:58Z","title":"Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology","summary":"  Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\nreferencing visual regions, just like human \"thinking with images\". However, no\nbenchmark exists to evaluate these capabilities holistically. To bridge this\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\ndiagnostic benchmark built on three principles: (1) focused visual perception\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\nevaluation, and (3) second-order reasoning to test object interactions and\nspatial hierarchies beyond simple object localization. Prioritizing images with\ndense objects, we initially sample 1K high-quality images from SA-1B, and\nincorporate eight LMM experts to manually annotate questions, candidate\noptions, and answers for each image. After three stages of quality control,\nTreeBench consists of 405 challenging visual question-answering pairs, even the\nmost advanced models struggle with this benchmark, where none of them reach 60%\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\nsupervise localization and reasoning jointly with reinforcement learning,\nenabling accurate localizations and explainable reasoning pathways. Initialized\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.\n","authors":["Haochen Wang","Xiangtai Li","Zilong Huang","Anran Wang","Jiacong Wang","Tao Zhang","Jiani Zheng","Sule Bai","Zijian Kang","Jiashi Feng","Zhuochen Wang","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.07999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07998v1","updated":"2025-07-10T17:59:55Z","published":"2025-07-10T17:59:55Z","title":"PyVision: Agentic Vision with Dynamic Tooling","summary":"  LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning.\n","authors":["Shitian Zhao","Haoquan Zhang","Shaoheng Lin","Ming Li","Qilong Wu","Kaipeng Zhang","Chen Wei"],"pdf_url":"https://arxiv.org/pdf/2507.07998v1.pdf","comment":"26 Pages, 10 Figures, Technical report"},{"id":"http://arxiv.org/abs/2507.07997v1","updated":"2025-07-10T17:59:54Z","published":"2025-07-10T17:59:54Z","title":"MGVQ: Could VQ-VAE Beat VAE? A Generalizable Tokenizer with Multi-group\n  Quantization","summary":"  Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental models\nthat compress continuous visual data into discrete tokens. Existing methods\nhave tried to improve the quantization strategy for better reconstruction\nquality, however, there still exists a large gap between VQ-VAEs and VAEs. To\nnarrow this gap, we propose \\NickName, a novel method to augment the\nrepresentation capability of discrete codebooks, facilitating easier\noptimization for codebooks and minimizing information loss, thereby enhancing\nreconstruction quality. Specifically, we propose to retain the latent dimension\nto preserve encoded features and incorporate a set of sub-codebooks for\nquantization. Furthermore, we construct comprehensive zero-shot benchmarks\nfeaturing resolutions of 512p and 2k to evaluate the reconstruction performance\nof existing methods rigorously. \\NickName~achieves the \\textbf{state-of-the-art\nperformance on both ImageNet and $8$ zero-shot benchmarks} across all VQ-VAEs.\nNotably, compared with SD-VAE, we outperform them on ImageNet significantly,\nwith rFID $\\textbf{0.49}$ v.s. $\\textbf{0.91}$, and achieve superior PSNR on\nall zero-shot benchmarks. These results highlight the superiority of\n\\NickName~in reconstruction and pave the way for preserving fidelity in HD\nimage processing tasks. Code will be publicly available at\nhttps://github.com/MKJia/MGVQ.\n","authors":["Mingkai Jia","Wei Yin","Xiaotao Hu","Jiaxin Guo","Xiaoyang Guo","Qian Zhang","Xiao-Xiao Long","Ping Tan"],"pdf_url":"https://arxiv.org/pdf/2507.07997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07995v1","updated":"2025-07-10T17:59:53Z","published":"2025-07-10T17:59:53Z","title":"Single-pass Adaptive Image Tokenization for Minimum Program Search","summary":"  According to Algorithmic Information Theory (AIT) -- Intelligent\nrepresentations compress data into the shortest possible program that can\nreconstruct its content, exhibiting low Kolmogorov Complexity (KC). In\ncontrast, most visual representation learning systems use fixed-length\nrepresentations for all inputs, ignoring variations in complexity or\nfamiliarity. Recent adaptive tokenization methods address this by allocating\nvariable-length representations but typically require test-time search over\nmultiple encodings to find the most predictive one. Inspired by Kolmogorov\nComplexity principles, we propose a single-pass adaptive tokenizer, KARL, which\npredicts the appropriate number of tokens for an image in a single forward\npass, halting once its approximate KC is reached. The token count serves as a\nproxy for the minimum description length. KARL's training procedure closely\nresembles the Upside-Down Reinforcement Learning paradigm, as it learns to\nconditionally predict token halting based on a desired reconstruction quality.\nKARL matches the performance of recent adaptive tokenizers while operating in a\nsingle pass. We present scaling laws for KARL, analyzing the role of\nencoder/decoder size, continuous vs. discrete tokenization and more.\nAdditionally, we offer a conceptual study drawing an analogy between Adaptive\nImage Tokenization and Algorithmic Information Theory, examining the predicted\nimage complexity (KC) across axes such as structure vs. noise and in- vs.\nout-of-distribution familiarity -- revealing alignment with human intuition.\n","authors":["Shivam Duggal","Sanghyun Byun","William T. Freeman","Antonio Torralba","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2507.07995v1.pdf","comment":"Code at: https://github.com/ShivamDuggal4/karl Keywords:\n  Representation Learning, Adaptive Tokenization, Compression, Algorithmic\n  Information Theory, Kolmogorov Complexity, Upside-Down RL"},{"id":"http://arxiv.org/abs/2507.07994v1","updated":"2025-07-10T17:59:49Z","published":"2025-07-10T17:59:49Z","title":"Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection","summary":"  Keypoint detection, integral to modern machine perception, faces challenges\nin few-shot learning, particularly when source data from the same distribution\nas the query is unavailable. This gap is addressed by leveraging sketches, a\npopular form of human expression, providing a source-free alternative. However,\nchallenges arise in mastering cross-modal embeddings and handling user-specific\nsketch styles. Our proposed framework overcomes these hurdles with a\nprototypical setup, combined with a grid-based locator and prototypical domain\nadaptation. We also demonstrate success in few-shot convergence across novel\nkeypoints and classes through extensive experiments.\n","authors":["Subhajit Maity","Ayan Kumar Bhunia","Subhadeep Koley","Pinaki Nath Chowdhury","Aneeshan Sain","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2507.07994v1.pdf","comment":"Accepted at ICCV 2025. Project Page: https://subhajitmaity.me/DYKp"},{"id":"http://arxiv.org/abs/2507.07993v1","updated":"2025-07-10T17:59:24Z","published":"2025-07-10T17:59:24Z","title":"Multigranular Evaluation for Brain Visual Decoding","summary":"  Existing evaluation protocols for brain visual decoding predominantly rely on\ncoarse metrics that obscure inter-model differences, lack neuroscientific\nfoundation, and fail to capture fine-grained visual distinctions. To address\nthese limitations, we introduce BASIC, a unified, multigranular evaluation\nframework that jointly quantifies structural fidelity, inferential alignment,\nand contextual coherence between decoded and ground truth images. For the\nstructural level, we introduce a hierarchical suite of segmentation-based\nmetrics, including foreground, semantic, instance, and component masks,\nanchored in granularity-aware correspondence across mask structures. For the\nsemantic level, we extract structured scene representations encompassing\nobjects, attributes, and relationships using multimodal large language models,\nenabling detailed, scalable, and context-rich comparisons with ground-truth\nstimuli. We benchmark a diverse set of visual decoding methods across multiple\nstimulus-neuroimaging datasets within this unified evaluation framework.\nTogether, these criteria provide a more discriminative, interpretable, and\ncomprehensive foundation for measuring brain visual decoding methods.\n","authors":["Weihao Xia","Cengiz Oztireli"],"pdf_url":"https://arxiv.org/pdf/2507.07993v1.pdf","comment":"Project: https://weihaox.github.io/BASIC"},{"id":"http://arxiv.org/abs/2507.07990v1","updated":"2025-07-10T17:59:02Z","published":"2025-07-10T17:59:02Z","title":"Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs","summary":"  Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.\n","authors":["Jeongseok Hyun","Sukjun Hwang","Su Ho Han","Taeoh Kim","Inwoong Lee","Dongyoon Wee","Joon-Young Lee","Seon Joo Kim","Minho Shim"],"pdf_url":"https://arxiv.org/pdf/2507.07990v1.pdf","comment":"Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm"},{"id":"http://arxiv.org/abs/2507.07985v1","updated":"2025-07-10T17:57:31Z","published":"2025-07-10T17:57:31Z","title":"CLIP Won't Learn Object-Attribute Binding from Natural Data and Here is\n  Why","summary":"  Contrastive vision-language models like CLIP are used for a large variety of\napplications, such as zero-shot classification or as vision encoder for\nmulti-modal models. Despite their popularity, their representations show major\nlimitations. For instance, CLIP models learn bag-of-words representations and,\nas a consequence, fail to distinguish whether an image is of \"a yellow\nsubmarine and a blue bus\" or \"a blue submarine and a yellow bus\". Previous\nattempts to fix this issue added hard negatives during training or modified the\narchitecture, but failed to resolve the problem in its entirety. We suspect\nthat the missing insights to solve the binding problem for CLIP are hidden in\nthe arguably most important part of learning algorithms: the data. In this\nwork, we fill this gap by rigorously identifying the influence of data\nproperties on CLIP's ability to learn binding using a synthetic dataset. We\nfind that common properties of natural data such as low attribute density,\nincomplete captions, and the saliency bias, a tendency of human captioners to\ndescribe the object that is \"most salient\" to them have a detrimental effect on\nbinding performance. In contrast to common belief, we find that neither scaling\nthe batch size, i.e., implicitly adding more hard negatives, nor explicitly\ncreating hard negatives enables CLIP to learn reliable binding. Only when the\ndata expresses our identified data properties CLIP learns almost perfect\nbinding.\n","authors":["Bijay Gurung","David T. Hoffmann","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2507.07985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07984v1","updated":"2025-07-10T17:56:07Z","published":"2025-07-10T17:56:07Z","title":"OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding","summary":"  Recent advances in multimodal large language models (MLLMs) have shown\nremarkable capabilities in integrating vision and language for complex\nreasoning. While most existing benchmarks evaluate models under offline\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\nperspective of an agent actively exploring a scene. The Online aspect\nemphasizes the need to process and reason over incrementally acquired\nobservations, while the Spatio-Temporal component requires integrating current\nvisual inputs with historical memory to support dynamic spatial reasoning.\nOST-Bench better reflects the challenges of real-world embodied perception.\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\nonline setting, their accuracy declines as the exploration horizon extends and\nthe memory grows. Through further experimental analysis, we identify common\nerror patterns across models and find that both complex clue-based spatial\nreasoning demands and long-term memory retrieval requirements significantly\ndrop model performance along two separate axes, highlighting the core\nchallenges that must be addressed to improve online embodied reasoning. To\nfoster further research and development in the field, our codes, dataset, and\nbenchmark are available. Our project page is:\nhttps://rbler1234.github.io/OSTBench.github.io/\n","authors":["JingLi Lin","Chenming Zhu","Runsen Xu","Xiaohan Mao","Xihui Liu","Tai Wang","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2507.07984v1.pdf","comment":"28 pages, a benchmark designed to evaluate Online Spatio-Temporal\n  understanding from the perspective of an agent actively exploring a scene.\n  Project Page: https://rbler1234.github.io/OSTBench.github.io/"},{"id":"http://arxiv.org/abs/2506.01933v3","updated":"2025-07-10T17:55:35Z","published":"2025-06-02T17:53:09Z","title":"E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models","summary":"  Spatial intelligence, encompassing 3D reconstruction, perception, and\nreasoning, is fundamental to applications such as robotics, aerial imaging, and\nextended reality. A key enabler is the real-time, accurate estimation of core\n3D attributes (camera parameters, point clouds, depth maps, and 3D point\ntracks) from unstructured or streaming imagery. Inspired by the success of\nlarge foundation models in language and 2D vision, a new class of end-to-end 3D\ngeometric foundation models (GFMs) has emerged, directly predicting dense 3D\nrepresentations in a single feed-forward pass, eliminating the need for slow or\nunavailable precomputed camera parameters. Since late 2023, the field has\nexploded with diverse variants, but systematic evaluation is lacking. In this\nwork, we present the first comprehensive benchmark for 3D GFMs, covering five\ncore tasks: sparse-view depth estimation, video depth estimation, 3D\nreconstruction, multi-view pose estimation, novel view synthesis, and spanning\nboth standard and challenging out-of-distribution datasets. Our standardized\ntoolkit automates dataset handling, evaluation protocols, and metric\ncomputation to ensure fair, reproducible comparisons. We evaluate 16\nstate-of-the-art GFMs, revealing their strengths and limitations across tasks\nand domains, and derive key insights to guide future model scaling and\noptimization. All code, evaluation scripts, and processed data will be publicly\nreleased to accelerate research in 3D spatial intelligence.\n","authors":["Wenyan Cong","Yiqing Liang","Yancheng Zhang","Ziyi Yang","Yan Wang","Boris Ivanovic","Marco Pavone","Chen Chen","Zhangyang Wang","Zhiwen Fan"],"pdf_url":"https://arxiv.org/pdf/2506.01933v3.pdf","comment":"Project Page: https://e3dbench.github.io/"},{"id":"http://arxiv.org/abs/2507.07982v1","updated":"2025-07-10T17:55:08Z","published":"2025-07-10T17:55:08Z","title":"Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling","summary":"  Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.\n","authors":["Haoyu Wu","Diankun Wu","Tianyu He","Junliang Guo","Yang Ye","Yueqi Duan","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2507.07982v1.pdf","comment":"18 pages, project page: https://GeometryForcing.github.io"},{"id":"http://arxiv.org/abs/2507.07978v1","updated":"2025-07-10T17:54:27Z","published":"2025-07-10T17:54:27Z","title":"Martian World Models: Controllable Video Synthesis with Physically\n  Accurate 3D Reconstructions","summary":"  Synthesizing realistic Martian landscape videos is crucial for mission\nrehearsal and robotic simulation. However, this task poses unique challenges\ndue to the scarcity of high-quality Martian data and the significant domain gap\nbetween Martian and terrestrial imagery. To address these challenges, we\npropose a holistic solution composed of two key components: 1) A data curation\npipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian\nenvironments from real stereo navigation images, sourced from NASA's Planetary\nData System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A\nMartian terrain video generator, MarsGen, which synthesizes novel videos\nvisually realistic and geometrically consistent with the 3D structure encoded\nin the data. Our M3arsSynth engine spans a wide range of Martian terrains and\nacquisition dates, enabling the generation of physically accurate 3D surface\nmodels at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data,\nsynthesizes videos conditioned on an initial image frame and, optionally,\ncamera trajectories or textual prompts, allowing for video generation in novel\nenvironments. Experimental results show that our approach outperforms video\nsynthesis models trained on terrestrial datasets, achieving superior visual\nfidelity and 3D structural consistency.\n","authors":["Longfei Li","Zhiwen Fan","Wenyan Cong","Xinhang Liu","Yuyang Yin","Matt Foutter","Panwang Pan","Chenyu You","Yue Wang","Zhangyang Wang","Yao Zhao","Marco Pavone","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2507.07978v1.pdf","comment":"Project Page: https://marsgenai.github.io"},{"id":"http://arxiv.org/abs/2507.07966v1","updated":"2025-07-10T17:47:40Z","published":"2025-07-10T17:47:40Z","title":"Scaling RL to Long Videos","summary":"  We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).\n","authors":["Yukang Chen","Wei Huang","Baifeng Shi","Qinghao Hu","Hanrong Ye","Ligeng Zhu","Zhijian Liu","Pavlo Molchanov","Jan Kautz","Xiaojuan Qi","Sifei Liu","Hongxu Yin","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2507.07966v1.pdf","comment":"Code and models are available at https://github.com/NVlabs/Long-RL"},{"id":"http://arxiv.org/abs/2507.07954v1","updated":"2025-07-10T17:39:03Z","published":"2025-07-10T17:39:03Z","title":"Input Conditioned Layer Dropping in Speech Foundation Models","summary":"  Curating foundation speech models for edge and IoT settings, where\ncomputational resources vary over time, requires dynamic architectures\nfeaturing adaptable reduction strategies. One emerging approach is layer\ndropping ($\\mathcal{LD}$) which skips fraction of the layers of a backbone\nnetwork during inference to reduce the computational load. This allows\ntransforming static models into dynamic ones. However, existing approaches\nexhibit limitations either in the mode of selecting layers or by significantly\nmodifying the neural architecture. To this end, we propose input-driven\n$\\mathcal{LD}$ that employs the network's input features and a lightweight\nlayer selecting network to determine the optimum combination of processing\nlayers. Extensive experimentation on 4 speech and audio public benchmarks,\nusing two different pre-trained foundation models, demonstrates the\neffectiveness of our approach, thoroughly outperforming random dropping and\nproducing on-par (or better) results to early exit.\n","authors":["Abdul Hannan","Daniele Falavigna","Alessio Brutti"],"pdf_url":"https://arxiv.org/pdf/2507.07954v1.pdf","comment":"Accepted at IEEE MLSP 2025"},{"id":"http://arxiv.org/abs/2505.15804v3","updated":"2025-07-10T17:36:35Z","published":"2025-05-21T17:57:38Z","title":"STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1.\n","authors":["Zongzhao Li","Zongyang Ma","Mingze Li","Songyou Li","Yu Rong","Tingyang Xu","Ziqi Zhang","Deli Zhao","Wenbing Huang"],"pdf_url":"https://arxiv.org/pdf/2505.15804v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07949v1","updated":"2025-07-10T17:33:52Z","published":"2025-07-10T17:33:52Z","title":"TinierHAR: Towards Ultra-Lightweight Deep Learning Models for Efficient\n  Human Activity Recognition on Edge Devices","summary":"  Human Activity Recognition (HAR) on resource-constrained wearable devices\ndemands inference models that harmonize accuracy with computational efficiency.\nThis paper introduces TinierHAR, an ultra-lightweight deep learning\narchitecture that synergizes residual depthwise separable convolutions, gated\nrecurrent units (GRUs), and temporal aggregation to achieve SOTA efficiency\nwithout compromising performance. Evaluated across 14 public HAR datasets,\nTinierHAR reduces Parameters by 2.7x (vs. TinyHAR) and 43.3x (vs.\nDeepConvLSTM), and MACs by 6.4x and 58.6x, respectively, while maintaining the\naveraged F1-scores. Beyond quantitative gains, this work provides the first\nsystematic ablation study dissecting the contributions of spatial-temporal\ncomponents across proposed TinierHAR, prior SOTA TinyHAR, and the classical\nDeepConvLSTM, offering actionable insights for designing efficient HAR systems.\nWe finally discussed the findings and suggested principled design guidelines\nfor future efficient HAR. To catalyze edge-HAR research, we open-source all\nmaterials in this work for future\nbenchmarking\\footnote{https://github.com/zhaxidele/TinierHAR}\n","authors":["Sizhen Bian","Mengxi Liu","Vitor Fortes Rey","Daniel Geissler","Paul Lukowicz"],"pdf_url":"https://arxiv.org/pdf/2507.07949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14993v2","updated":"2025-07-10T17:30:56Z","published":"2024-09-23T13:16:09Z","title":"Multi-modal Generative AI: Multi-modal LLMs, Diffusions and the\n  Unification","summary":"  Multi-modal generative AI (Artificial Intelligence) has attracted increasing\nattention from both academia and industry. Particularly, two dominant families\nof techniques have emerged: i) Multi-modal large language models (LLMs)\ndemonstrate impressive ability for multi-modal understanding; and ii) Diffusion\nmodels exhibit remarkable multi-modal powers in terms of multi-modal\ngeneration. Therefore, this paper provides a comprehensive overview of\nmulti-modal generative AI, including multi-modal LLMs, diffusions, and the\nunification for understanding and generation. To lay a solid foundation for\nunified models, we first provide a detailed review of both multi-modal LLMs and\ndiffusion models respectively, including their probabilistic modeling\nprocedure, multi-modal architecture design, and advanced applications to\nimage/video LLMs as well as text-to-image/video generation. Furthermore, we\nexplore the emerging efforts toward unified models for understanding and\ngeneration. To achieve the unification of understanding and generation, we\ninvestigate key designs including autoregressive-based and diffusion-based\nmodeling, as well as dense and Mixture-of-Experts (MoE) architectures. We then\nintroduce several strategies for unified models, analyzing their potential\nadvantages and disadvantages. In addition, we summarize the common datasets\nwidely used for multi-modal generative AI pretraining. Last but not least, we\npresent several challenging future research directions which may contribute to\nthe ongoing advancement of multi-modal generative AI.\n","authors":["Xin Wang","Yuwei Zhou","Bin Huang","Hong Chen","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2409.14993v2.pdf","comment":"20 pages, 11 figures, 2 tables"},{"id":"http://arxiv.org/abs/2501.17468v3","updated":"2025-07-10T17:09:37Z","published":"2025-01-29T08:20:05Z","title":"Solving Inverse Problems using Diffusion with Iterative Colored\n  Renoising","summary":"  Imaging inverse problems can be solved in an unsupervised manner using\npre-trained diffusion models, but doing so requires approximating the gradient\nof the measurement-conditional score function in the diffusion reverse process.\nWe show that the approximations produced by existing methods are relatively\npoor, especially early in the reverse process, and so we propose a new approach\nthat iteratively reestimates and \"renoises\" the estimate several times per\ndiffusion step. This iterative approach, which we call Fast Iterative REnoising\n(FIRE), injects colored noise that is shaped to ensure that the pre-trained\ndiffusion model always sees white noise, in accordance with how it was trained.\nWe then embed FIRE into the DDIM reverse process and show that the resulting\n\"DDfire\" offers state-of-the-art accuracy and runtime on several linear inverse\nproblems, as well as phase retrieval. Our implementation is at\nhttps://github.com/matt-bendel/DDfire\n","authors":["Matt C. Bendel","Saurav K. Shastri","Rizwan Ahmad","Philip Schniter"],"pdf_url":"https://arxiv.org/pdf/2501.17468v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07929v1","updated":"2025-07-10T17:09:14Z","published":"2025-07-10T17:09:14Z","title":"Towards Continuous Home Cage Monitoring: An Evaluation of Tracking and\n  Identification Strategies for Laboratory Mice","summary":"  Continuous, automated monitoring of laboratory mice enables more accurate\ndata collection and improves animal welfare through real-time insights.\nResearchers can achieve a more dynamic and clinically relevant characterization\nof disease progression and therapeutic effects by integrating behavioral and\nphysiological monitoring in the home cage. However, providing individual mouse\nmetrics is difficult because of their housing density, similar appearances,\nhigh mobility, and frequent interactions. To address these challenges, we\ndevelop a real-time identification (ID) algorithm that accurately assigns ID\npredictions to mice wearing custom ear tags in digital home cages monitored by\ncameras. Our pipeline consists of three parts: (1) a custom multiple object\ntracker (MouseTracks) that combines appearance and motion cues from mice; (2) a\ntransformer-based ID classifier (Mouseformer); and (3) a tracklet associator\nlinear program to assign final ID predictions to tracklets (MouseMap). Our\nmodels assign an animal ID based on custom ear tags at 30 frames per second\nwith 24/7 cage coverage. We show that our custom tracking and ID pipeline\nimproves tracking efficiency and lowers ID switches across mouse strains and\nvarious environmental factors compared to current mouse tracking methods.\n","authors":["Juan Pablo Oberhauser","Daniel Grzenda"],"pdf_url":"https://arxiv.org/pdf/2507.07929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07920v1","updated":"2025-07-10T17:00:49Z","published":"2025-07-10T17:00:49Z","title":"ArteryX: Advancing Brain Artery Feature Extraction with Vessel-Fused\n  Networks and a Robust Validation Framework","summary":"  Cerebrovascular pathology significantly contributes to cognitive decline and\nneurological disorders, underscoring the need for advanced tools to assess\nvascular integrity. Three-dimensional Time-of-Flight Magnetic Resonance\nAngiography (3D TOF MRA) is widely used to visualize cerebral vasculature,\nhowever, clinical evaluations generally focus on major arterial abnormalities,\noverlooking quantitative metrics critical for understanding subtle vascular\nchanges. Existing methods for extracting structural, geometrical and\nmorphological arterial features from MRA - whether manual or automated - face\nchallenges including user-dependent variability, steep learning curves, and\nlack of standardized quantitative validations. We propose a novel\nsemi-supervised artery evaluation framework, named ArteryX, a MATLAB-based\ntoolbox that quantifies vascular features with high accuracy and efficiency,\nachieving processing times ~10-15 minutes per subject at 0.5 mm resolution with\nminimal user intervention. ArteryX employs a vessel-fused network based\nlandmarking approach to reliably track and manage tracings, effectively\naddressing the issue of dangling/disconnected vessels. Validation on human\nsubjects with cerebral small vessel disease demonstrated its improved\nsensitivity to subtle vascular changes and better performance than an existing\nsemi-automated method. Importantly, the ArteryX toolbox enables quantitative\nfeature validation by integrating an in-vivo like artery simulation framework\nutilizing vessel-fused graph nodes and predefined ground-truth features for\nspecific artery types. Thus, the ArteryX framework holds promise for\nbenchmarking feature extraction toolboxes and for seamless integration into\nclinical workflows, enabling early detection of cerebrovascular pathology and\nstandardized comparisons across patient cohorts to advance understanding of\nvascular contributions to brain health.\n","authors":["Abrar Faiyaz","Nhat Hoang","Giovanni Schifitto","Md Nasir Uddin"],"pdf_url":"https://arxiv.org/pdf/2507.07920v1.pdf","comment":"14 Pages, 8 Figures, Preliminary version of the toolbox was presented\n  at the ISMRM 2025 Conference in Hawaii at the \"Software Tools\" Session"},{"id":"http://arxiv.org/abs/2507.07908v1","updated":"2025-07-10T16:39:49Z","published":"2025-07-10T16:39:49Z","title":"Not Only Consistency: Enhance Test-Time Adaptation with Spatio-temporal\n  Inconsistency for Remote Physiological Measurement","summary":"  Remote photoplethysmography (rPPG) has emerged as a promising non-invasive\nmethod for monitoring physiological signals using the camera. Although various\ndomain adaptation and generalization methods were proposed to promote the\nadaptability of deep-based rPPG models in unseen deployment environments,\nconsiderations in aspects like privacy concerns and real-time adaptation\nrestrict their application in real-world deployment. Thus, we aim to propose a\nnovel fully Test-Time Adaptation (TTA) strategy tailored for rPPG tasks in this\nwork. Specifically, based on prior knowledge in physiology and our\nobservations, we noticed not only there is spatio-temporal consistency in the\nfrequency domain of rPPG signals, but also that inconsistency in the time\ndomain was significant. Given this, by leveraging both consistency and\ninconsistency priors, we introduce an innovative expert knowledge-based\nself-supervised\n\\textbf{C}onsistency-\\textbf{i}n\\textbf{C}onsistency-\\textbf{i}ntegration\n(\\textbf{CiCi}) framework to enhances model adaptation during inference.\nBesides, our approach further incorporates a gradient dynamic control mechanism\nto mitigate potential conflicts between priors, ensuring stable adaptation\nacross instances. Through extensive experiments on five diverse datasets under\nthe TTA protocol, our method consistently outperforms existing techniques,\npresenting state-of-the-art performance in real-time self-supervised adaptation\nwithout accessing source data. The code will be released later.\n","authors":["Xiao Yang","Yuxuan Fan","Can Liu","Houcheng Su","Weichen Guo","Jiyao Wang","Dengbo He"],"pdf_url":"https://arxiv.org/pdf/2507.07908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07903v1","updated":"2025-07-10T16:37:20Z","published":"2025-07-10T16:37:20Z","title":"Hardware-Aware Feature Extraction Quantisation for Real-Time Visual\n  Odometry on FPGA Platforms","summary":"  Accurate position estimation is essential for modern navigation systems\ndeployed in autonomous platforms, including ground vehicles, marine vessels,\nand aerial drones. In this context, Visual Simultaneous Localisation and\nMapping (VSLAM) - which includes Visual Odometry - relies heavily on the\nreliable extraction of salient feature points from the visual input data. In\nthis work, we propose an embedded implementation of an unsupervised\narchitecture capable of detecting and describing feature points. It is based on\na quantised SuperPoint convolutional neural network. Our objective is to\nminimise the computational demands of the model while preserving high detection\nquality, thus facilitating efficient deployment on platforms with limited\nresources, such as mobile or embedded systems. We implemented the solution on\nan FPGA System-on-Chip (SoC) platform, specifically the AMD/Xilinx Zynq\nUltraScale+, where we evaluated the performance of Deep Learning Processing\nUnits (DPUs) and we also used the Brevitas library and the FINN framework to\nperform model quantisation and hardware-aware optimisation. This allowed us to\nprocess 640 x 480 pixel images at up to 54 fps on an FPGA platform,\noutperforming state-of-the-art solutions in the field. We conducted experiments\non the TUM dataset to demonstrate and discuss the impact of different\nquantisation techniques on the accuracy and performance of the model in a\nvisual odometry task.\n","authors":["Mateusz Wasala","Mateusz Smolarczyk","Michal Danilowicz","Tomasz Kryjak"],"pdf_url":"https://arxiv.org/pdf/2507.07903v1.pdf","comment":"Accepted for the DSD 2025 conference in Salerno, Italy"},{"id":"http://arxiv.org/abs/2507.07902v1","updated":"2025-07-10T16:33:50Z","published":"2025-07-10T16:33:50Z","title":"MIRA: A Novel Framework for Fusing Modalities in Medical RAG","summary":"  Multimodal Large Language Models (MLLMs) have significantly advanced\nAI-assisted medical diagnosis, but they often generate factually inconsistent\nresponses that deviate from established medical knowledge. Retrieval-Augmented\nGeneration (RAG) enhances factual accuracy by integrating external sources, but\nit presents two key challenges. First, insufficient retrieval can miss critical\ninformation, whereas excessive retrieval can introduce irrelevant or misleading\ncontent, disrupting model output. Second, even when the model initially\nprovides correct answers, over-reliance on retrieved data can lead to factual\nerrors. To address these issues, we introduce the Multimodal Intelligent\nRetrieval and Augmentation (MIRA) framework, designed to optimize factual\naccuracy in MLLM. MIRA consists of two key components: (1) a calibrated\nRethinking and Rearrangement module that dynamically adjusts the number of\nretrieved contexts to manage factual risk, and (2) A medical RAG framework\nintegrating image embeddings and a medical knowledge base with a query-rewrite\nmodule for efficient multimodal reasoning. This enables the model to\neffectively integrate both its inherent knowledge and external references. Our\nevaluation of publicly available medical VQA and report generation benchmarks\ndemonstrates that MIRA substantially enhances factual accuracy and overall\nperformance, achieving new state-of-the-art results. Code is released at\nhttps://github.com/mbzuai-oryx/MIRA.\n","authors":["Jinhong Wang","Tajamul Ashraf","Zongyan Han","Jorma Laaksonen","Rao Mohammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2507.07902v1.pdf","comment":"ACM Multimedia 2025"},{"id":"http://arxiv.org/abs/2507.01788v2","updated":"2025-07-10T16:23:29Z","published":"2025-07-02T15:14:06Z","title":"Are Vision Transformer Representations Semantically Meaningful? A Case\n  Study in Medical Imaging","summary":"  Vision transformers (ViTs) have rapidly gained prominence in medical imaging\ntasks such as disease classification, segmentation, and detection due to their\nsuperior accuracy compared to conventional deep learning models. However, due\nto their size and complex interactions via the self-attention mechanism, they\nare not well understood. In particular, it is unclear whether the\nrepresentations produced by such models are semantically meaningful. In this\npaper, using a projected gradient-based algorithm, we show that their\nrepresentations are not semantically meaningful and they are inherently\nvulnerable to small changes. Images with imperceptible differences can have\nvery different representations; on the other hand, images that should belong to\ndifferent semantic classes can have nearly identical representations. Such\nvulnerability can lead to unreliable classification results; for example,\nunnoticeable changes cause the classification accuracy to be reduced by over\n60\\%. %. To the best of our knowledge, this is the first work to systematically\ndemonstrate this fundamental lack of semantic meaningfulness in ViT\nrepresentations for medical image classification, revealing a critical\nchallenge for their deployment in safety-critical systems.\n","authors":["Montasir Shams","Chashi Mahiul Islam","Shaeke Salman","Phat Tran","Xiuwen Liu"],"pdf_url":"https://arxiv.org/pdf/2507.01788v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2408.06687v3","updated":"2025-07-10T16:14:55Z","published":"2024-08-13T07:27:02Z","title":"Masked Image Modeling: A Survey","summary":"  In this work, we survey recent studies on masked image modeling (MIM), an\napproach that emerged as a powerful self-supervised learning technique in\ncomputer vision. The MIM task involves masking some information, e.g. pixels,\npatches, or even latent representations, and training a model, usually an\nautoencoder, to predicting the missing information by using the context\navailable in the visible part of the input. We identify and formalize two\ncategories of approaches on how to implement MIM as a pretext task, one based\non reconstruction and one based on contrastive learning. Then, we construct a\ntaxonomy and review the most prominent papers in recent years. We complement\nthe manually constructed taxonomy with a dendrogram obtained by applying a\nhierarchical clustering algorithm. We further identify relevant clusters via\nmanually inspecting the resulting dendrogram. Our review also includes datasets\nthat are commonly used in MIM research. We aggregate the performance results of\nvarious masked image modeling methods on the most popular datasets, to\nfacilitate the comparison of competing methods. Finally, we identify research\ngaps and propose several interesting directions of future work. We supplement\nour survey with the following public repository containing organized\nreferences: https://github.com/vladhondru25/MIM-Survey.\n","authors":["Vlad Hondru","Florinel Alin Croitoru","Shervin Minaee","Radu Tudor Ionescu","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2408.06687v3.pdf","comment":"Accepted at the International Journal of Computer Vision"},{"id":"http://arxiv.org/abs/2507.07878v1","updated":"2025-07-10T16:02:07Z","published":"2025-07-10T16:02:07Z","title":"Single-Step Latent Diffusion for Underwater Image Restoration","summary":"  Underwater image restoration algorithms seek to restore the color, contrast,\nand appearance of a scene that is imaged underwater. They are a critical tool\nin applications ranging from marine ecology and aquaculture to underwater\nconstruction and archaeology. While existing pixel-domain diffusion-based image\nrestoration approaches are effective at restoring simple scenes with limited\ndepth variation, they are computationally intensive and often generate\nunrealistic artifacts when applied to scenes with complex geometry and\nsignificant depth variation. In this work we overcome these limitations by\ncombining a novel network architecture (SLURPP) with an accurate synthetic data\ngeneration pipeline. SLURPP combines pretrained latent diffusion models --\nwhich encode strong priors on the geometry and depth of scenes -- with an\nexplicit scene decomposition -- which allows one to model and account for the\neffects of light attenuation and backscattering. To train SLURPP we design a\nphysics-based underwater image synthesis pipeline that applies varied and\nrealistic underwater degradation effects to existing terrestrial image\ndatasets. This approach enables the generation of diverse training data with\ndense medium/degradation annotations. We evaluate our method extensively on\nboth synthetic and real-world benchmarks and demonstrate state-of-the-art\nperformance. Notably, SLURPP is over 200X faster than existing diffusion-based\nmethods while offering ~ 3 dB improvement in PSNR on synthetic benchmarks. It\nalso offers compelling qualitative improvements on real-world data. Project\nwebsite https://tianfwang.github.io/slurpp/.\n","authors":["Jiayi Wu","Tianfu Wang","Md Abu Bakr Siddique","Md Jahidul Islam","Cornelia Fermuller","Yiannis Aloimonos","Christopher A. Metzler"],"pdf_url":"https://arxiv.org/pdf/2507.07878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18813v3","updated":"2025-07-10T15:45:38Z","published":"2024-09-27T15:06:05Z","title":"EyeTrAES: Fine-grained, Low-Latency Eye Tracking via Adaptive Event\n  Slicing","summary":"  Eye-tracking technology has gained significant attention in recent years due\nto its wide range of applications in human-computer interaction, virtual and\naugmented reality, and wearable health. Traditional RGB camera-based\neye-tracking systems often struggle with poor temporal resolution and\ncomputational constraints, limiting their effectiveness in capturing rapid eye\nmovements. To address these limitations, we propose EyeTrAES, a novel approach\nusing neuromorphic event cameras for high-fidelity tracking of natural\npupillary movement that shows significant kinematic variance. One of EyeTrAES's\nhighlights is the use of a novel adaptive windowing/slicing algorithm that\nensures just the right amount of descriptive asynchronous event data\naccumulation within an event frame, across a wide range of eye movement\npatterns. EyeTrAES then applies lightweight image processing functions over\naccumulated event frames from just a single eye to perform pupil segmentation\nand tracking. We show that these methods boost pupil tracking fidelity by 6+%,\nachieving IoU~=92%, while incurring at least 3x lower latency than competing\npure event-based eye tracking alternatives [38]. We additionally demonstrate\nthat the microscopic pupillary motion captured by EyeTrAES exhibits distinctive\nvariations across individuals and can thus serve as a biometric fingerprint.\nFor robust user authentication, we train a lightweight per-user Random Forest\nclassifier using a novel feature vector of short-term pupillary kinematics,\ncomprising a sliding window of pupil (location, velocity, acceleration)\ntriples. Experimental studies with two different datasets demonstrate that the\nEyeTrAES-based authentication technique can simultaneously achieve high\nauthentication accuracy (~=0.82) and low processing latency (~=12ms), and\nsignificantly outperform multiple state-of-the-art competitive baselines.\n","authors":["Argha Sen","Nuwan Bandara","Ila Gokarn","Thivya Kandappu","Archan Misra"],"pdf_url":"https://arxiv.org/pdf/2409.18813v3.pdf","comment":"32 pages,15 figures,"},{"id":"http://arxiv.org/abs/2507.07860v1","updated":"2025-07-10T15:41:35Z","published":"2025-07-10T15:41:35Z","title":"THUNDER: Tile-level Histopathology image UNDERstanding benchmark","summary":"  Progress in a research field can be hard to assess, in particular when many\nconcurrent methods are proposed in a short period of time. This is the case in\ndigital pathology, where many foundation models have been released recently to\nserve as feature extractors for tile-level images, being used in a variety of\ndownstream tasks, both for tile- and slide-level problems. Benchmarking\navailable methods then becomes paramount to get a clearer view of the research\nlandscape. In particular, in critical domains such as healthcare, a benchmark\nshould not only focus on evaluating downstream performance, but also provide\ninsights about the main differences between methods, and importantly, further\nconsider uncertainty and robustness to ensure a reliable usage of proposed\nmodels. For these reasons, we introduce THUNDER, a tile-level benchmark for\ndigital pathology foundation models, allowing for efficient comparison of many\nmodels on diverse datasets with a series of downstream tasks, studying their\nfeature spaces and assessing the robustness and uncertainty of predictions\ninformed by their embeddings. THUNDER is a fast, easy-to-use, dynamic benchmark\nthat can already support a large variety of state-of-the-art foundation, as\nwell as local user-defined models for direct tile-based comparison. In this\npaper, we provide a comprehensive comparison of 23 foundation models on 16\ndifferent datasets covering diverse tasks, feature analysis, and robustness.\nThe code for THUNDER is publicly available at\nhttps://github.com/MICS-Lab/thunder.\n","authors":["Pierre Marza","Leo Fillioux","Sofiène Boutaj","Kunal Mahatha","Christian Desrosiers","Pablo Piantanida","Jose Dolz","Stergios Christodoulidis","Maria Vakalopoulou"],"pdf_url":"https://arxiv.org/pdf/2507.07860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06167v3","updated":"2025-07-10T15:41:04Z","published":"2025-07-08T16:47:16Z","title":"Skywork-R1V3 Technical Report","summary":"  We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.\n","authors":["Wei Shen","Jiangbo Pei","Yi Peng","Xuchen Song","Yang Liu","Jian Peng","Haofeng Sun","Yunzhuo Hao","Peiyu Wang","Jianhao Zhang","Yahui Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.06167v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15469v2","updated":"2025-07-10T15:14:36Z","published":"2024-11-23T06:36:16Z","title":"Mamba-CL: Optimizing Selective State Space Model in Null Space for\n  Continual Learning","summary":"  Continual Learning (CL) aims to equip AI models with the ability to learn a\nsequence of tasks over time, without forgetting previously learned knowledge.\nRecently, State Space Models (SSMs), particularly the Mamba model, have\nachieved notable success in computer vision. Building on the strengths of SSMs,\nthis study explores leveraging the Mamba model for CL. Therefore, we introduce\nMamba-CL, a framework that continuously fine-tunes the core SSMs of the\nlarge-scale Mamba foundation model by updating parameters orthogonal to the\nfeature subspace of previous tasks. This approach theoretically guarantees the\nconsistency objective aiming to preserves consistent output for each SSM module\nacross both previous and current tasks, so as to overcome catastrophic\nforgetting issue. Specifically, we achieve this goal by deducing the overall\nconsistency constraints on four key time-invariant parameters in the Mamba\nmodel, streamlining its recurrent state-space structure and non-linear\ndiscretization process in SSM. In practice, we apply the null-space projection\nto efficiently implement the orthogonality within Mamba model. Extensive\nexperiments on four class-incremental benchmarks demonstrate the effectiveness\nof Mamba-CL for anti-forgetting, achieving superior performances to\nstate-of-the-art methods. Code is available in the supplementary materials.\n","authors":["De Cheng","Yue Lu","Lingfeng He","Shizhou Zhang","Xi Yang","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2411.15469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07839v1","updated":"2025-07-10T15:11:09Z","published":"2025-07-10T15:11:09Z","title":"MeD-3D: A Multimodal Deep Learning Framework for Precise Recurrence\n  Prediction in Clear Cell Renal Cell Carcinoma (ccRCC)","summary":"  Accurate prediction of recurrence in clear cell renal cell carcinoma (ccRCC)\nremains a major clinical challenge due to the disease complex molecular,\npathological, and clinical heterogeneity. Traditional prognostic models, which\nrely on single data modalities such as radiology, histopathology, or genomics,\noften fail to capture the full spectrum of disease complexity, resulting in\nsuboptimal predictive accuracy. This study aims to overcome these limitations\nby proposing a deep learning (DL) framework that integrates multimodal data,\nincluding CT, MRI, histopathology whole slide images (WSI), clinical data, and\ngenomic profiles, to improve the prediction of ccRCC recurrence and enhance\nclinical decision-making. The proposed framework utilizes a comprehensive\ndataset curated from multiple publicly available sources, including TCGA, TCIA,\nand CPTAC. To process the diverse modalities, domain-specific models are\nemployed: CLAM, a ResNet50-based model, is used for histopathology WSIs, while\nMeD-3D, a pre-trained 3D-ResNet18 model, processes CT and MRI images. For\nstructured clinical and genomic data, a multi-layer perceptron (MLP) is used.\nThese models are designed to extract deep feature embeddings from each\nmodality, which are then fused through an early and late integration\narchitecture. This fusion strategy enables the model to combine complementary\ninformation from multiple sources. Additionally, the framework is designed to\nhandle incomplete data, a common challenge in clinical settings, by enabling\ninference even when certain modalities are missing.\n","authors":["Hasaan Maqsood","Saif Ur Rehman Khan"],"pdf_url":"https://arxiv.org/pdf/2507.07839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07838v1","updated":"2025-07-10T15:09:20Z","published":"2025-07-10T15:09:20Z","title":"3D-ADAM: A Dataset for 3D Anomaly Detection in Advanced Manufacturing","summary":"  Surface defects are one of the largest contributors to low yield in the\nmanufacturing sector. Accurate and reliable detection of defects during the\nmanufacturing process is therefore of great value across the sector.\nState-of-the-art approaches to automated defect detection yield impressive\nperformance on current datasets, yet still fall short in real-world\nmanufacturing settings and developing improved methods relies on large datasets\nrepresentative of real-world scenarios. Unfortunately, high-quality,\nhigh-precision RGB+3D industrial anomaly detection datasets are scarce, and\ntypically do not reflect real-world industrial deployment scenarios. To address\nthis, we introduce 3D-ADAM, the first large-scale industry-relevant dataset for\nhigh-precision 3D Anomaly Detection. 3D-ADAM comprises 14,120 high-resolution\nscans across 217 unique parts, captured using 4 industrial depth imaging\nsensors. It includes 27,346 annotated defect instances from 12 categories,\ncovering the breadth of industrial surface defects. 3D-ADAM uniquely captures\nan additional 8,110 annotations of machine element features, spanning the range\nof relevant mechanical design form factors. Unlike existing datasets, 3D-ADAM\nis captured in a real industrial environment with variations in part position\nand orientation, camera positioning, ambient lighting conditions, as well as\npartial occlusions. Our evaluation of SOTA models across various RGB+3D anomaly\ndetection tasks demonstrates the significant challenge this dataset presents to\ncurrent approaches. We further validated the industrial relevance and quality\nof the dataset through an expert labelling survey conducted by industry\npartners. By providing this challenging benchmark, 3D-ADAM aims to accelerate\nthe development of robust 3D Anomaly Detection models capable of meeting the\ndemands of modern manufacturing environments.\n","authors":["Paul McHard","Florent P. Audonnet","Oliver Summerell","Sebastian Andraos","Paul Henderson","Gerardo Aragon-Camarasa"],"pdf_url":"https://arxiv.org/pdf/2507.07838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07831v1","updated":"2025-07-10T15:03:10Z","published":"2025-07-10T15:03:10Z","title":"Rethinking Query-based Transformer for Continual Image Segmentation","summary":"  Class-incremental/Continual image segmentation (CIS) aims to train an image\nsegmenter in stages, where the set of available categories differs at each\nstage. To leverage the built-in objectness of query-based transformers, which\nmitigates catastrophic forgetting of mask proposals, current methods often\ndecouple mask generation from the continual learning process. This study,\nhowever, identifies two key issues with decoupled frameworks: loss of\nplasticity and heavy reliance on input data order. To address these, we conduct\nan in-depth investigation of the built-in objectness and find that highly\naggregated image features provide a shortcut for queries to generate masks\nthrough simple feature alignment. Based on this, we propose SimCIS, a simple\nyet powerful baseline for CIS. Its core idea is to directly select image\nfeatures for query assignment, ensuring \"perfect alignment\" to preserve\nobjectness, while simultaneously allowing queries to select new classes to\npromote plasticity. To further combat catastrophic forgetting of categories, we\nintroduce cross-stage consistency in selection and an innovative \"visual\nquery\"-based replay mechanism. Experiments demonstrate that SimCIS consistently\noutperforms state-of-the-art methods across various segmentation tasks,\nsettings, splits, and input data orders. All models and codes will be made\npublicly available at https://github.com/SooLab/SimCIS.\n","authors":["Yuchen Zhu","Cheng Shi","Dingyou Wang","Jiajin Tang","Zhengxuan Wei","Yu Wu","Guanbin Li","Sibei Yang"],"pdf_url":"https://arxiv.org/pdf/2507.07831v1.pdf","comment":"This work is accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2507.07828v1","updated":"2025-07-10T15:01:23Z","published":"2025-07-10T15:01:23Z","title":"Benchmarking Content-Based Puzzle Solvers on Corrupted Jigsaw Puzzles","summary":"  Content-based puzzle solvers have been extensively studied, demonstrating\nsignificant progress in computational techniques. However, their evaluation\noften lacks realistic challenges crucial for real-world applications, such as\nthe reassembly of fragmented artefacts or shredded documents. In this work, we\ninvestigate the robustness of State-Of-The-Art content-based puzzle solvers\nintroducing three types of jigsaw puzzle corruptions: missing pieces, eroded\nedges, and eroded contents. Evaluating both heuristic and deep learning-based\nsolvers, we analyse their ability to handle these corruptions and identify key\nlimitations. Our results show that solvers developed for standard puzzles have\na rapid decline in performance if more pieces are corrupted. However, deep\nlearning models can significantly improve their robustness through fine-tuning\nwith augmented data. Notably, the advanced Positional Diffusion model adapts\nparticularly well, outperforming its competitors in most experiments. Based on\nour findings, we highlight promising research directions for enhancing the\nautomated reconstruction of real-world artefacts.\n","authors":["Richard Dirauf","Florian Wolz","Dario Zanca","Björn Eskofier"],"pdf_url":"https://arxiv.org/pdf/2507.07828v1.pdf","comment":"Accepted at ICIAP 2025"},{"id":"http://arxiv.org/abs/2506.18903v2","updated":"2025-07-10T14:56:24Z","published":"2025-06-23T17:59:56Z","title":"VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory","summary":"  We propose a novel memory mechanism to build video generators that can\nexplore environments interactively. Similar results have previously been\nachieved by out-painting 2D views of the scene while incrementally\nreconstructing its 3D geometry, which quickly accumulates errors, or by video\ngenerators with a short context window, which struggle to maintain scene\ncoherence over the long term. To address these limitations, we introduce\nSurfel-Indexed View Memory (VMem), a mechanism that remembers past views by\nindexing them geometrically based on the 3D surface elements (surfels) they\nhave observed. VMem enables the efficient retrieval of the most relevant past\nviews when generating new ones. By focusing only on these relevant views, our\nmethod produces consistent explorations of imagined environments at a fraction\nof the computational cost of using all past views as context. We evaluate our\napproach on challenging long-term scene synthesis benchmarks and demonstrate\nsuperior performance compared to existing methods in maintaining scene\ncoherence and camera control.\n","authors":["Runjia Li","Philip Torr","Andrea Vedaldi","Tomas Jakab"],"pdf_url":"https://arxiv.org/pdf/2506.18903v2.pdf","comment":"Project page: https://v-mem.github.io"},{"id":"http://arxiv.org/abs/2503.11498v3","updated":"2025-07-10T14:56:07Z","published":"2025-03-14T15:26:02Z","title":"Open-source automatic pipeline for efficient conversion of large-scale\n  point clouds to IFC format","summary":"  Building Information Modeling (BIM) is an essential component in the\nsustainable reconstruction and revitalization of ageing structures. However,\nmodel creation usually relies on laborious manual transformation of the\nunstructured point cloud data provided by laser scans or photogrammetry. This\npaper presents Cloud2BIM, an open-source software tool designed to automate the\nconversion of point clouds into BIM models compliant with the Industry\nFoundation Classes (IFC) standard. Cloud2BIM integrates advanced algorithms for\nwall and slab segmentation, opening detection, and room zoning based on real\nwall surfaces, resulting in a comprehensive and fully automated workflow.\nUnlike existing tools, it avoids computationally- and calibration-intensive\ntechniques such as RANSAC, supports non-orthogonal geometries, and provides\nunprecedented processing speed-achieving results up to seven times faster than\nfastest competing solutions. Systematic validation using benchmark datasets\nconfirms that Cloud2BIM is an easy-to-use, efficient, and scalable solution for\ngenerating accurate BIM models, capable of converting extensive point cloud\ndatasets for entire buildings into IFC format with minimal user input.\n","authors":["Slávek Zbirovský","Václav Nežerka"],"pdf_url":"https://arxiv.org/pdf/2503.11498v3.pdf","comment":"published version, 23 pages, 25 figures"},{"id":"http://arxiv.org/abs/2507.02148v2","updated":"2025-07-10T14:55:57Z","published":"2025-07-02T21:06:39Z","title":"Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and\n  Synthetic Fine-Tuning with Vision Foundation Models","summary":"  Monocular depth estimation has recently progressed beyond ordinal depth to\nprovide metric depth predictions. However, its reliability in underwater\nenvironments remains limited due to light attenuation and scattering, color\ndistortion, turbidity, and the lack of high-quality metric ground truth data.\nIn this paper, we present a comprehensive benchmark of zero-shot and fine-tuned\nmonocular metric depth estimation models on real-world underwater datasets with\nmetric depth annotations, including FLSea and SQUID. We evaluated a diverse set\nof state-of-the-art Vision Foundation Models across a range of underwater\nconditions and depth ranges. Our results show that large-scale models trained\non terrestrial data (real or synthetic) are effective in in-air settings, but\nperform poorly underwater due to significant domain shifts. To address this, we\nfine-tune Depth Anything V2 with a ViT-S backbone encoder on a synthetic\nunderwater variant of the Hypersim dataset, which we simulated using a\nphysically based underwater image formation model. Our fine-tuned model\nconsistently improves performance across all benchmarks and outperforms\nbaselines trained only on the clean in-air Hypersim dataset. This study\npresents a detailed evaluation and visualization of monocular metric depth\nestimation in underwater scenes, emphasizing the importance of domain\nadaptation and scale-aware supervision for achieving robust and generalizable\nmetric depth predictions using foundation models in challenging environments.\n","authors":["Zijie Cai","Christopher Metzler"],"pdf_url":"https://arxiv.org/pdf/2507.02148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05116v2","updated":"2025-07-10T14:53:51Z","published":"2025-07-07T15:30:55Z","title":"VOTE: Vision-Language-Action Optimization with Trajectory Ensemble\n  Voting","summary":"  Recent large-scale Vision Language Action (VLA) models have shown superior\nperformance in robotic manipulation tasks guided by natural language. However,\ntheir generalization remains limited when applied to novel objects or\nunfamiliar environments that lie outside the training distribution. To address\nthis, many existing approaches integrate additional components such as depth\nestimation, segmentation, or even diffusion to improve generalization, at the\ncost of adding significant computation overhead, resulting in low efficiency.\nThis motivates the exploration of efficient action prediction methods, which\nare independent of additional high-level visual representations or diffusion\ntechniques. In this work, we propose VOTE, an efficient and general framework\nfor the optimization and acceleration of VLA models. In details, we propose a\nnovel tokenizer-free fine-tuning approach for parallel accurate action\nprediction, which reduces computational overhead and accelerates inference\nspeed. Additionally, we adopt an ensemble voting strategy for the action\nsampling, which significantly improves model performance and enhances\ngeneralization. Experimental results show that our method achieves\nstate-of-the-art performance with 35x faster inference and 145 Hz throughput.\nAll the details and codes will be open-sourced.\n","authors":["Juyi Lin","Amir Taherin","Arash Akbari","Arman Akbari","Lei Lu","Guangyu Chen","Taskin Padir","Xiaomeng Yang","Weiwei Chen","Yiqian Li","Xue Lin","David Kaeli","Pu Zhao","Yanzhi Wang"],"pdf_url":"https://arxiv.org/pdf/2507.05116v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07811v1","updated":"2025-07-10T14:40:52Z","published":"2025-07-10T14:40:52Z","title":"Patient-specific vs Multi-Patient Vision Transformer for Markerless\n  Tumor Motion Forecasting","summary":"  Background: Accurate forecasting of lung tumor motion is essential for\nprecise dose delivery in proton therapy. While current markerless methods\nmostly rely on deep learning, transformer-based architectures remain unexplored\nin this domain, despite their proven performance in trajectory forecasting.\n  Purpose: This work introduces a markerless forecasting approach for lung\ntumor motion using Vision Transformers (ViT). Two training strategies are\nevaluated under clinically realistic constraints: a patient-specific (PS)\napproach that learns individualized motion patterns, and a multi-patient (MP)\nmodel designed for generalization. The comparison explicitly accounts for the\nlimited number of images that can be generated between planning and treatment\nsessions.\n  Methods: Digitally reconstructed radiographs (DRRs) derived from planning\n4DCT scans of 31 patients were used to train the MP model; a 32nd patient was\nheld out for evaluation. PS models were trained using only the target patient's\nplanning data. Both models used 16 DRRs per input and predicted tumor motion\nover a 1-second horizon. Performance was assessed using Average Displacement\nError (ADE) and Final Displacement Error (FDE), on both planning (T1) and\ntreatment (T2) data.\n  Results: On T1 data, PS models outperformed MP models across all training set\nsizes, especially with larger datasets (up to 25,000 DRRs, p < 0.05). However,\nMP models demonstrated stronger robustness to inter-fractional anatomical\nvariability and achieved comparable performance on T2 data without retraining.\n  Conclusions: This is the first study to apply ViT architectures to markerless\ntumor motion forecasting. While PS models achieve higher precision, MP models\noffer robust out-of-the-box performance, well-suited for time-constrained\nclinical settings.\n","authors":["Gauthier Rotsart de Hertaing","Dani Manjah","Benoit Macq"],"pdf_url":"https://arxiv.org/pdf/2507.07811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07802v1","updated":"2025-07-10T14:28:12Z","published":"2025-07-10T14:28:12Z","title":"Synergistic Prompting for Robust Visual Recognition with Missing\n  Modalities","summary":"  Large-scale multi-modal models have demonstrated remarkable performance\nacross various visual recognition tasks by leveraging extensive paired\nmulti-modal training data. However, in real-world applications, the presence of\nmissing or incomplete modality inputs often leads to significant performance\ndegradation. Recent research has focused on prompt-based strategies to tackle\nthis issue; however, existing methods are hindered by two major limitations:\n(1) static prompts lack the flexibility to adapt to varying missing-data\nconditions, and (2) basic prompt-tuning methods struggle to ensure reliable\nperformance when critical modalities are missing.To address these challenges,\nwe propose a novel Synergistic Prompting (SyP) framework for robust visual\nrecognition with missing modalities. The proposed SyP introduces two key\ninnovations: (I) a Dynamic Adapter, which computes adaptive scaling factors to\ndynamically generate prompts, replacing static parameters for flexible\nmulti-modal adaptation, and (II) a Synergistic Prompting Strategy, which\ncombines static and dynamic prompts to balance information across modalities,\nensuring robust reasoning even when key modalities are missing. The proposed\nSyP achieves significant performance improvements over existing approaches\nacross three widely-used visual recognition datasets, demonstrating robustness\nunder diverse missing rates and conditions. Extensive experiments and ablation\nstudies validate its effectiveness in handling missing modalities, highlighting\nits superior adaptability and reliability.\n","authors":["Zhihui Zhang","Luanyuan Dai","Qika Lin","Yunfeng Diao","Guangyin Jin","Yufei Guo","Jing Zhang","Xiaoshuai Hao"],"pdf_url":"https://arxiv.org/pdf/2507.07802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07800v1","updated":"2025-07-10T14:26:50Z","published":"2025-07-10T14:26:50Z","title":"Adaptive Attention Residual U-Net for curvilinear structure segmentation\n  in fluorescence microscopy and biomedical images","summary":"  Segmenting curvilinear structures in fluorescence microscopy remains a\nchallenging task, particularly under noisy conditions and in dense filament\nnetworks commonly seen in vivo. To address this, we created two original\ndatasets consisting of hundreds of synthetic images of fluorescently labelled\nmicrotubules within cells. These datasets are precisely annotated and closely\nmimic real microscopy images, including realistic noise. The second dataset\npresents an additional challenge, by simulating varying fluorescence\nintensities along filaments that complicate segmentation. While deep learning\nhas shown strong potential in biomedical image analysis, its performance often\ndeclines in noisy or low-contrast conditions. To overcome this limitation, we\ndeveloped a novel advanced architecture: the Adaptive Squeeze-and-Excitation\nResidual U-Net (ASE_Res_UNet). This model enhanced the standard U-Net by\nintegrating residual blocks in the encoder and adaptive SE attention mechanisms\nin the decoder. Through ablation studies and comprehensive visual and\nquantitative evaluations, ASE_Res_UNet consistently outperformed its variants,\nnamely standard U-Net, ASE_UNet and Res_UNet architectures. These improvements,\nparticularly in noise resilience and detecting fine, low-intensity structures,\nwere largely attributed to the adaptive SE attention module that we created. We\nfurther benchmarked ASE_Res_UNet against various state-of-the-art models, and\nfound it achieved superior performance on our most challenging dataset.\nFinally, the model also generalized well to real microscopy images of stained\nmicrotubules as well as to other curvilinear structures. Indeed, it\nsuccessfully segmented retinal blood vessels and nerves in noisy or\nlow-contrast biomedical images, demonstrating its strong potential for\napplications in disease diagnosis and treatment.\n","authors":["Achraf Ait Laydi","Louis Cueff","Mewen Crespo","Yousef El Mourabit","Hélène Bouvrais"],"pdf_url":"https://arxiv.org/pdf/2507.07800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07796v1","updated":"2025-07-10T14:23:15Z","published":"2025-07-10T14:23:15Z","title":"Visual Instance-aware Prompt Tuning","summary":"  Visual Prompt Tuning (VPT) has emerged as a parameter-efficient fine-tuning\nparadigm for vision transformers, with conventional approaches utilizing\ndataset-level prompts that remain the same across all input instances. We\nobserve that this strategy results in sub-optimal performance due to high\nvariance in downstream datasets. To address this challenge, we propose Visual\nInstance-aware Prompt Tuning (ViaPT), which generates instance-aware prompts\nbased on each individual input and fuses them with dataset-level prompts,\nleveraging Principal Component Analysis (PCA) to retain important prompting\ninformation. Moreover, we reveal that VPT-Deep and VPT-Shallow represent two\ncorner cases based on a conceptual understanding, in which they fail to\neffectively capture instance-specific information, while random dimension\nreduction on prompts only yields performance between the two extremes. Instead,\nViaPT overcomes these limitations by balancing dataset-level and instance-level\nknowledge, while reducing the amount of learnable parameters compared to\nVPT-Deep. Extensive experiments across 34 diverse datasets demonstrate that our\nmethod consistently outperforms state-of-the-art baselines, establishing a new\nparadigm for analyzing and optimizing visual prompts for vision transformers.\n","authors":["Xi Xiao","Yunbei Zhang","Xingjian Li","Tianyang Wang","Xiao Wang","Yuxiang Wei","Jihun Hamm","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2507.07796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07795v1","updated":"2025-07-10T14:23:11Z","published":"2025-07-10T14:23:11Z","title":"Robust and Generalizable Heart Rate Estimation via Deep Learning for\n  Remote Photoplethysmography in Complex Scenarios","summary":"  Non-contact remote photoplethysmography (rPPG) technology enables heart rate\nmeasurement from facial videos. However, existing network models still face\nchallenges in accu racy, robustness, and generalization capability under\ncomplex scenarios. This paper proposes an end-to-end rPPG extraction network\nthat employs 3D convolutional neural networks to reconstruct accurate rPPG\nsignals from raw facial videos. We introduce a differential frame fusion module\nthat integrates differential frames with original frames, enabling frame-level\nrepresentations to capture blood volume pulse (BVP) variations. Additionally,\nwe incorporate Temporal Shift Module (TSM) with self-attention mechanisms,\nwhich effectively enhance rPPG features with minimal computational overhead.\nFurthermore, we propose a novel dynamic hybrid loss function that provides\nstronger supervision for the network, effectively mitigating over fitting.\nComprehensive experiments were conducted on not only the PURE and UBFC-rPPG\ndatasets but also the challenging MMPD dataset under complex scenarios,\ninvolving both intra dataset and cross-dataset evaluations, which demonstrate\nthe superior robustness and generalization capability of our network.\nSpecifically, after training on PURE, our model achieved a mean absolute error\n(MAE) of 7.58 on the MMPD test set, outperforming the state-of-the-art models.\n","authors":["Kang Cen","Chang-Hong Fu","Hong Hong"],"pdf_url":"https://arxiv.org/pdf/2507.07795v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.06410v2","updated":"2025-07-10T14:19:51Z","published":"2025-07-08T21:26:33Z","title":"Attention-Enhanced Deep Learning Ensemble for Breast Density\n  Classification in Mammography","summary":"  Breast density assessment is a crucial component of mammographic\ninterpretation, with high breast density (BI-RADS categories C and D)\nrepresenting both a significant risk factor for developing breast cancer and a\ntechnical challenge for tumor detection. This study proposes an automated deep\nlearning system for robust binary classification of breast density (low: A/B\nvs. high: C/D) using the VinDr-Mammo dataset. We implemented and compared four\nadvanced convolutional neural networks: ResNet18, ResNet50, EfficientNet-B0,\nand DenseNet121, each enhanced with channel attention mechanisms. To address\nthe inherent class imbalance, we developed a novel Combined Focal Label\nSmoothing Loss function that integrates focal loss, label smoothing, and\nclass-balanced weighting. Our preprocessing pipeline incorporated advanced\ntechniques, including contrast-limited adaptive histogram equalization (CLAHE)\nand comprehensive data augmentation. The individual models were combined\nthrough an optimized ensemble voting approach, achieving superior performance\n(AUC: 0.963, F1-score: 0.952) compared to any single model. This system\ndemonstrates significant potential to standardize density assessments in\nclinical practice, potentially improving screening efficiency and early cancer\ndetection rates while reducing inter-observer variability among radiologists.\n","authors":["Peyman Sharifian","Xiaotong Hong","Alireza Karimian","Mehdi Amini","Hossein Arabi"],"pdf_url":"https://arxiv.org/pdf/2507.06410v2.pdf","comment":"2025 IEEE Nuclear Science Symposium, Medical Imaging Conference and\n  Room Temperature Semiconductor Detector Conference"},{"id":"http://arxiv.org/abs/2507.07789v1","updated":"2025-07-10T14:14:08Z","published":"2025-07-10T14:14:08Z","title":"Computationally Efficient Information-Driven Optical Design with\n  Interchanging Optimization","summary":"  Recent work has demonstrated that imaging systems can be evaluated through\nthe information content of their measurements alone, enabling\napplication-agnostic optical design that avoids computational decoding\nchallenges. Information-Driven Encoder Analysis Learning (IDEAL) was proposed\nto automate this process through gradient-based. In this work, we study IDEAL\nacross diverse imaging systems and find that it suffers from high memory usage,\nlong runtimes, and a potentially mismatched objective function due to\nend-to-end differentiability requirements. We introduce IDEAL with\nInterchanging Optimization (IDEAL-IO), a method that decouples density\nestimation from optical parameter optimization by alternating between fitting\nmodels to current measurements and updating optical parameters using fixed\nmodels for information estimation. This approach reduces runtime and memory\nusage by up to 6x while enabling more expressive density models that guide\noptimization toward superior designs. We validate our method on diffractive\noptics, lensless imaging, and snapshot 3D microscopy applications, establishing\ninformation-theoretic optimization as a practical, scalable strategy for\nreal-world imaging system design.\n","authors":["Eric Markley","Henry Pinkard","Leyla Kabuli","Nalini Singh","Laura Waller"],"pdf_url":"https://arxiv.org/pdf/2507.07789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20559v4","updated":"2025-07-10T14:09:10Z","published":"2024-05-31T00:57:58Z","title":"Information-driven design of imaging systems","summary":"  In modern imaging systems that computationally process raw measurements\nbefore or instead of human viewing, information content matters more than\nvisual appearance. However, developing information estimators that can handle\nthe complexity of real-world measurements yet remain practical enough for\nwidespread use has proven challenging. We introduce a data-driven approach for\nestimating mutual information between unknown objects and their noisy\nmeasurements. Our technique fits probabilistic models to measurements and their\nnoise processes, quantifying information content without requiring ground truth\ndata or making assumptions about object structure. We validate our approach\nacross diverse applications-color photography, radio astronomy, lensless\nimaging, and microscopy-demonstrating that information estimates reliably\npredict system performance. Finally, we introduce Information-Driven Encoder\nAnalysis Learning (IDEAL), which optimizes imaging systems to maximize\ninformation capture. Our work unlocks information theory as a powerful,\npractical tool for analyzing and designing imaging systems across a broad range\nof applications.\n  A video summarizing this work can be found at:\nhttps://waller-lab.github.io/EncodingInformationWebsite/\n","authors":["Henry Pinkard","Leyla Kabuli","Eric Markley","Tiffany Chien","Jiantao Jiao","Laura Waller"],"pdf_url":"https://arxiv.org/pdf/2405.20559v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07781v1","updated":"2025-07-10T14:01:24Z","published":"2025-07-10T14:01:24Z","title":"SURPRISE3D: A Dataset for Spatial Understanding and Reasoning in Complex\n  3D Scenes","summary":"  The integration of language and 3D perception is critical for embodied AI and\nrobotic systems to perceive, understand, and interact with the physical world.\nSpatial reasoning, a key capability for understanding spatial relationships\nbetween objects, remains underexplored in current 3D vision-language research.\nExisting datasets often mix semantic cues (e.g., object name) with spatial\ncontext, leading models to rely on superficial shortcuts rather than genuinely\ninterpreting spatial relationships. To address this gap, we introduce\nS\\textsc{urprise}3D, a novel dataset designed to evaluate language-guided\nspatial reasoning segmentation in complex 3D scenes. S\\textsc{urprise}3D\nconsists of more than 200k vision language pairs across 900+ detailed indoor\nscenes from ScanNet++ v2, including more than 2.8k unique object classes. The\ndataset contains 89k+ human-annotated spatial queries deliberately crafted\nwithout object name, thereby mitigating shortcut biases in spatial\nunderstanding. These queries comprehensively cover various spatial reasoning\nskills, such as relative position, narrative perspective, parametric\nperspective, and absolute distance reasoning. Initial benchmarks demonstrate\nsignificant challenges for current state-of-the-art expert 3D visual grounding\nmethods and 3D-LLMs, underscoring the necessity of our dataset and the\naccompanying 3D Spatial Reasoning Segmentation (3D-SRS) benchmark suite.\nS\\textsc{urprise}3D and 3D-SRS aim to facilitate advancements in spatially\naware AI, paving the way for effective embodied interaction and robotic\nplanning. The code and datasets can be found in\nhttps://github.com/liziwennba/SUPRISE.\n","authors":["Jiaxin Huang","Ziwen Li","Hanlve Zhang","Runnan Chen","Xiao He","Yandong Guo","Wenping Wang","Tongliang Liu","Mingming Gong"],"pdf_url":"https://arxiv.org/pdf/2507.07781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05317v2","updated":"2025-07-10T14:01:10Z","published":"2025-06-30T08:28:32Z","title":"PWD: Prior-Guided and Wavelet-Enhanced Diffusion Model for Limited-Angle\n  CT","summary":"  Generative diffusion models have received increasing attention in medical\nimaging, particularly in limited-angle computed tomography (LACT). Standard\ndiffusion models achieve high-quality image reconstruction but require a large\nnumber of sampling steps during inference, resulting in substantial\ncomputational overhead. Although skip-sampling strategies have been proposed to\nimprove efficiency, they often lead to loss of fine structural details. To\naddress this issue, we propose a prior information embedding and wavelet\nfeature fusion fast sampling diffusion model for LACT reconstruction. The PWD\nenables efficient sampling while preserving reconstruction fidelity in LACT,\nand effectively mitigates the degradation typically introduced by\nskip-sampling. Specifically, during the training phase, PWD maps the\ndistribution of LACT images to that of fully sampled target images, enabling\nthe model to learn structural correspondences between them. During inference,\nthe LACT image serves as an explicit prior to guide the sampling trajectory,\nallowing for high-quality reconstruction with significantly fewer steps. In\naddition, PWD performs multi-scale feature fusion in the wavelet domain,\neffectively enhancing the reconstruction of fine details by leveraging both\nlow-frequency and high-frequency information. Quantitative and qualitative\nevaluations on clinical dental arch CBCT and periapical datasets demonstrate\nthat PWD outperforms existing methods under the same sampling condition. Using\nonly 50 sampling steps, PWD achieves at least 1.7 dB improvement in PSNR and\n10% gain in SSIM.\n","authors":["Yi Liu","Yiyang Wen","Zekun Zhou","Junqi Ma","Linghang Wang","Yucheng Yao","Liu Shi","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2507.05317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07780v1","updated":"2025-07-10T13:59:53Z","published":"2025-07-10T13:59:53Z","title":"Where are we with calibration under dataset shift in image\n  classification?","summary":"  We conduct an extensive study on the state of calibration under real-world\ndataset shift for image classification. Our work provides important insights on\nthe choice of post-hoc and in-training calibration techniques, and yields\npractical guidelines for all practitioners interested in robust calibration\nunder shift. We compare various post-hoc calibration methods, and their\ninteractions with common in-training calibration strategies (e.g., label\nsmoothing), across a wide range of natural shifts, on eight different\nclassification tasks across several imaging domains. We find that: (i)\nsimultaneously applying entropy regularisation and label smoothing yield the\nbest calibrated raw probabilities under dataset shift, (ii) post-hoc\ncalibrators exposed to a small amount of semantic out-of-distribution data\n(unrelated to the task) are most robust under shift, (iii) recent calibration\nmethods specifically aimed at increasing calibration under shifts do not\nnecessarily offer significant improvements over simpler post-hoc calibration\nmethods, (iv) improving calibration under shifts often comes at the cost of\nworsening in-distribution calibration. Importantly, these findings hold for\nrandomly initialised classifiers, as well as for those finetuned from\nfoundation models, the latter being consistently better calibrated compared to\nmodels trained from scratch. Finally, we conduct an in-depth analysis of\nensembling effects, finding that (i) applying calibration prior to ensembling\n(instead of after) is more effective for calibration under shifts, (ii) for\nensembles, OOD exposure deteriorates the ID-shifted calibration trade-off,\n(iii) ensembling remains one of the most effective methods to improve\ncalibration robustness and, combined with finetuning from foundation models,\nyields best calibration results overall.\n","authors":["Mélanie Roschewitz","Raghav Mehta","Fabio de Sousa Ribeiro","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2507.07780v1.pdf","comment":"Code available at\n  https://github.com/biomedia-mira/calibration_under_shifts"},{"id":"http://arxiv.org/abs/2507.07778v1","updated":"2025-07-10T13:58:32Z","published":"2025-07-10T13:58:32Z","title":"Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time\n  Training","summary":"  Generalizing neural networks to unseen target domains is a significant\nchallenge in real-world deployments. Test-time training (TTT) addresses this by\nusing an auxiliary self-supervised task to reduce the domain gap caused by\ndistribution shifts between the source and target. However, we find that when\nmodels are required to perform multiple tasks under domain shifts, conventional\nTTT methods suffer from unsynchronized task behavior, where the adaptation\nsteps needed for optimal performance in one task may not align with the\nrequirements of other tasks. To address this, we propose a novel TTT approach\ncalled Synchronizing Tasks for Test-time Training (S4T), which enables the\nconcurrent handling of multiple tasks. The core idea behind S4T is that\npredicting task relations across domain shifts is key to synchronizing tasks\nduring test time. To validate our approach, we apply S4T to conventional\nmulti-task benchmarks, integrating it with traditional TTT protocols. Our\nempirical results show that S4T outperforms state-of-the-art TTT methods across\nvarious benchmarks.\n","authors":["Wooseong Jeong","Jegyeong Cho","Youngho Yoon","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2507.07778v1.pdf","comment":"Accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2507.07776v1","updated":"2025-07-10T13:56:32Z","published":"2025-07-10T13:56:32Z","title":"SCOOTER: A Human Evaluation Framework for Unrestricted Adversarial\n  Examples","summary":"  Unrestricted adversarial attacks aim to fool computer vision models without\nbeing constrained by $\\ell_p$-norm bounds to remain imperceptible to humans,\nfor example, by changing an object's color. This allows attackers to circumvent\ntraditional, norm-bounded defense strategies such as adversarial training or\ncertified defense strategies. However, due to their unrestricted nature, there\nare also no guarantees of norm-based imperceptibility, necessitating human\nevaluations to verify just how authentic these adversarial examples look. While\nsome related work assesses this vital quality of adversarial attacks, none\nprovide statistically significant insights. This issue necessitates a unified\nframework that supports and streamlines such an assessment for evaluating and\ncomparing unrestricted attacks. To close this gap, we introduce SCOOTER - an\nopen-source, statistically powered framework for evaluating unrestricted\nadversarial examples. Our contributions are: $(i)$ best-practice guidelines for\ncrowd-study power, compensation, and Likert equivalence bounds to measure\nimperceptibility; $(ii)$ the first large-scale human vs. model comparison\nacross 346 human participants showing that three color-space attacks and three\ndiffusion-based attacks fail to produce imperceptible images. Furthermore, we\nfound that GPT-4o can serve as a preliminary test for imperceptibility, but it\nonly consistently detects adversarial examples for four out of six tested\nattacks; $(iii)$ open-source software tools, including a browser-based task\ntemplate to collect annotations and analysis scripts in Python and R; $(iv)$ an\nImageNet-derived benchmark dataset containing 3K real images, 7K adversarial\nexamples, and over 34K human ratings. Our findings demonstrate that automated\nvision systems do not align with human perception, reinforcing the need for a\nground-truth SCOOTER benchmark.\n","authors":["Dren Fazlija","Monty-Maximilian Zühlke","Johanna Schrader","Arkadij Orlov","Clara Stein","Iyiola E. Olatunji","Daniel Kudenko"],"pdf_url":"https://arxiv.org/pdf/2507.07776v1.pdf","comment":"42 pages, 16 figures, 11 tables, Under Review, Code:\n  https://github.com/DrenFazlija/Scooter, Data:\n  https://doi.org/10.5281/zenodo.15771501"},{"id":"http://arxiv.org/abs/2507.07773v1","updated":"2025-07-10T13:55:35Z","published":"2025-07-10T13:55:35Z","title":"Rainbow Artifacts from Electromagnetic Signal Injection Attacks on Image\n  Sensors","summary":"  Image sensors are integral to a wide range of safety- and security-critical\nsystems, including surveillance infrastructure, autonomous vehicles, and\nindustrial automation. These systems rely on the integrity of visual data to\nmake decisions. In this work, we investigate a novel class of electromagnetic\nsignal injection attacks that target the analog domain of image sensors,\nallowing adversaries to manipulate raw visual inputs without triggering\nconventional digital integrity checks. We uncover a previously undocumented\nattack phenomenon on CMOS image sensors: rainbow-like color artifacts induced\nin images captured by image sensors through carefully tuned electromagnetic\ninterference. We further evaluate the impact of these attacks on\nstate-of-the-art object detection models, showing that the injected artifacts\npropagate through the image signal processing pipeline and lead to significant\nmispredictions. Our findings highlight a critical and underexplored\nvulnerability in the visual perception stack, highlighting the need for more\nrobust defenses against physical-layer attacks in such systems.\n","authors":["Youqian Zhang","Xinyu Ji","Zhihao Wang","Qinhong Jiang"],"pdf_url":"https://arxiv.org/pdf/2507.07773v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.07768v1","updated":"2025-07-10T13:53:52Z","published":"2025-07-10T13:53:52Z","title":"TRIX- Trading Adversarial Fairness via Mixed Adversarial Training","summary":"  Adversarial Training (AT) is a widely adopted defense against adversarial\nexamples. However, existing approaches typically apply a uniform training\nobjective across all classes, overlooking disparities in class-wise\nvulnerability. This results in adversarial unfairness: classes with well\ndistinguishable features (strong classes) tend to become more robust, while\nclasses with overlapping or shared features(weak classes) remain\ndisproportionately susceptible to adversarial attacks. We observe that strong\nclasses do not require strong adversaries during training, as their non-robust\nfeatures are quickly suppressed. In contrast, weak classes benefit from\nstronger adversaries to effectively reduce their vulnerabilities. Motivated by\nthis, we introduce TRIX, a feature-aware adversarial training framework that\nadaptively assigns weaker targeted adversaries to strong classes, promoting\nfeature diversity via uniformly sampled targets, and stronger untargeted\nadversaries to weak classes, enhancing their focused robustness. TRIX further\nincorporates per-class loss weighting and perturbation strength adjustments,\nbuilding on prior work, to emphasize weak classes during the optimization.\nComprehensive experiments on standard image classification benchmarks,\nincluding evaluations under strong attacks such as PGD and AutoAttack,\ndemonstrate that TRIX significantly improves worst-case class accuracy on both\nclean and adversarial data, reducing inter-class robustness disparities, and\npreserves overall accuracy. Our results highlight TRIX as a practical step\ntoward fair and effective adversarial defense.\n","authors":["Tejaswini Medi","Steffen Jung","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2507.07768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07757v1","updated":"2025-07-10T13:34:33Z","published":"2025-07-10T13:34:33Z","title":"Deep Learning based 3D Volume Correlation for Additive Manufacturing\n  Using High-Resolution Industrial X-ray Computed Tomography","summary":"  Quality control in additive manufacturing (AM) is vital for industrial\napplications in areas such as the automotive, medical and aerospace sectors.\nGeometric inaccuracies caused by shrinkage and deformations can compromise the\nlife and performance of additively manufactured components. Such deviations can\nbe quantified using Digital Volume Correlation (DVC), which compares the\ncomputer-aided design (CAD) model with the X-ray Computed Tomography (XCT)\ngeometry of the components produced. However, accurate registration between the\ntwo modalities is challenging due to the absence of a ground truth or reference\ndeformation field. In addition, the extremely large data size of\nhigh-resolution XCT volumes makes computation difficult. In this work, we\npresent a deep learning-based approach for estimating voxel-wise deformations\nbetween CAD and XCT volumes. Our method uses a dynamic patch-based processing\nstrategy to handle high-resolution volumes. In addition to the Dice Score, we\nintroduce a Binary Difference Map (BDM) that quantifies voxel-wise mismatches\nbetween binarized CAD and XCT volumes to evaluate the accuracy of the\nregistration. Our approach shows a 9.2\\% improvement in the Dice Score and a\n9.9\\% improvement in the voxel match rate compared to classic DVC methods,\nwhile reducing the interaction time from days to minutes. This work sets the\nfoundation for deep learning-based DVC methods to generate compensation meshes\nthat can then be used in closed-loop correlations during the AM production\nprocess. Such a system would be of great interest to industries since the\nmanufacturing process will become more reliable and efficient, saving time and\nmaterial.\n","authors":["Keerthana Chand","Tobias Fritsch","Bardia Hejazi","Konstantin Poka","Giovanni Bruno"],"pdf_url":"https://arxiv.org/pdf/2507.07757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07747v1","updated":"2025-07-10T13:25:29Z","published":"2025-07-10T13:25:29Z","title":"X-RAFT: Cross-Modal Non-Rigid Registration of Blue and White Light\n  Neurosurgical Hyperspectral Images","summary":"  Integration of hyperspectral imaging into fluorescence-guided neurosurgery\nhas the potential to improve surgical decision making by providing quantitative\nfluorescence measurements in real-time. Quantitative fluorescence requires\npaired spectral data in fluorescence (blue light) and reflectance (white light)\nmode. Blue and white image acquisition needs to be performed sequentially in a\npotentially dynamic surgical environment. A key component to the fluorescence\nquantification process is therefore the ability to find dense cross-modal image\ncorrespondences between two hyperspectral images taken under these drastically\ndifferent lighting conditions. We address this challenge with the introduction\nof X-RAFT, a Recurrent All-Pairs Field Transforms (RAFT) optical flow model\nmodified for cross-modal inputs. We propose using distinct image encoders for\neach modality pair, and fine-tune these in a self-supervised manner using\nflow-cycle-consistency on our neurosurgical hyperspectral data. We show an\nerror reduction of 36.6% across our evaluation metrics when comparing to a\nnaive baseline and 27.83% reduction compared to an existing cross-modal optical\nflow method (CrossRAFT). Our code and models will be made publicly available\nafter the review process.\n","authors":["Charlie Budd","Silvère Ségaud","Matthew Elliot","Graeme Stasiuk","Yijing Xie","Jonathan Shapey","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2507.07747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07744v1","updated":"2025-07-10T13:23:41Z","published":"2025-07-10T13:23:41Z","title":"Sparse-Dense Side-Tuner for efficient Video Temporal Grounding","summary":"  Video Temporal Grounding (VTG) involves Moment Retrieval (MR) and Highlight\nDetection (HD) based on textual queries. For this, most methods rely solely on\nfinal-layer features of frozen large pre-trained backbones, limiting their\nadaptability to new domains. While full fine-tuning is often impractical,\nparameter-efficient fine-tuning -- and particularly side-tuning (ST) -- has\nemerged as an effective alternative. However, prior ST approaches this problem\nfrom a frame-level refinement perspective, overlooking the inherent sparse\nnature of MR. To address this, we propose the Sparse-Dense Side-Tuner (SDST),\nthe first anchor-free ST architecture for VTG. We also introduce the\nReference-based Deformable Self-Attention, a novel mechanism that enhances the\ncontext modeling of the deformable attention -- a key limitation of existing\nanchor-free methods. Additionally, we present the first effective integration\nof InternVideo2 backbone into an ST framework, showing its profound\nimplications in performance. Overall, our method significantly improves\nexisting ST methods, achieving highly competitive or SOTA results on\nQVHighlights, TACoS, and Charades-STA, while reducing up to a 73% the parameter\ncount w.r.t. the existing SOTA methods. The code is publicly accessible at\nhttps://github.com/davidpujol/SDST.\n","authors":["David Pujol-Perich","Sergio Escalera","Albert Clapés"],"pdf_url":"https://arxiv.org/pdf/2507.07744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07734v1","updated":"2025-07-10T13:13:53Z","published":"2025-07-10T13:13:53Z","title":"EEvAct: Early Event-Based Action Recognition with High-Rate Two-Stream\n  Spiking Neural Networks","summary":"  Recognizing human activities early is crucial for the safety and\nresponsiveness of human-robot and human-machine interfaces. Due to their high\ntemporal resolution and low latency, event-based vision sensors are a perfect\nmatch for this early recognition demand. However, most existing processing\napproaches accumulate events to low-rate frames or space-time voxels which\nlimits the early prediction capabilities. In contrast, spiking neural networks\n(SNNs) can process the events at a high-rate for early predictions, but most\nworks still fall short on final accuracy. In this work, we introduce a\nhigh-rate two-stream SNN which closes this gap by outperforming previous work\nby 2% in final accuracy on the large-scale THU EACT-50 dataset. We benchmark\nthe SNNs within a novel early event-based recognition framework by reporting\nTop-1 and Top-5 recognition scores for growing observation time. Finally, we\nexemplify the impact of these methods on a real-world task of early action\ntriggering for human motion capture in sports.\n","authors":["Michael Neumeier","Jules Lecomte","Nils Kazinski","Soubarna Banik","Bing Li","Axel von Arnim"],"pdf_url":"https://arxiv.org/pdf/2507.07734v1.pdf","comment":"International Conference on Neuromorphic Systems (ICONS) 2025"},{"id":"http://arxiv.org/abs/2507.07733v1","updated":"2025-07-10T13:13:08Z","published":"2025-07-10T13:13:08Z","title":"RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance\n  Transfer and Reflection","summary":"  3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in\nnovel view synthesis. However, rendering reflective objects remains a\nsignificant challenge, particularly in inverse rendering and relighting. We\nintroduce RTR-GS, a novel inverse rendering framework capable of robustly\nrendering objects with arbitrary reflectance properties, decomposing BRDF and\nlighting, and delivering credible relighting results. Given a collection of\nmulti-view images, our method effectively recovers geometric structure through\na hybrid rendering model that combines forward rendering for radiance transfer\nwith deferred rendering for reflections. This approach successfully separates\nhigh-frequency and low-frequency appearances, mitigating floating artifacts\ncaused by spherical harmonic overfitting when handling high-frequency details.\nWe further refine BRDF and lighting decomposition using an additional\nphysically-based deferred rendering branch. Experimental results show that our\nmethod enhances novel view synthesis, normal estimation, decomposition, and\nrelighting while maintaining efficient training inference process.\n","authors":["Yongyang Zhou","Fang-Lue Zhang","Zichen Wang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.07733v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2507.07731v1","updated":"2025-07-10T13:12:08Z","published":"2025-07-10T13:12:08Z","title":"Energy-Guided Decoding for Object Hallucination Mitigation","summary":"  Mitigating object hallucination in large vision-language models (LVLMs) is\ncritical to their safe deployment. Existing methods either are restricted to\nspecific decoding methods, or demand sophisticated modifications to visual\ninputs, or rely on knowledge from external models. In this work, we first\nreveal the phenomenon that VLMs exhibit significant imbalance in the ``Yes''\nratio ( \\ie, the fraction of ``Yes'' answers among the total number of\nquestions) across three different visual question answering (VQA) datasets.\nFurthermore, we propose an energy-based decoding method, which dynamically\nselects the hidden states from the layer with minimal energy score. It is\nsimple yet effective in reducing the bias for the yes ratio while boosting\nperformance across three benchmarks (POPE, MME, and MMVP). Our method\nconsistently improves accuracy and F1 score on three VQA datasets across three\ncommonly used VLMs over several baseline methods. The average accuracy\nimprovement is 4.82% compared to greedy decoding. Moreover, the average\nyes-ratio gap reduction is 8.81%, meaning the proposed method is less biased as\nshown in Figure 1.\n","authors":["Xixi Liu","Ailin Deng","Christopher Zach"],"pdf_url":"https://arxiv.org/pdf/2507.07731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07730v1","updated":"2025-07-10T13:08:57Z","published":"2025-07-10T13:08:57Z","title":"RAPS-3D: Efficient interactive segmentation for 3D radiological imaging","summary":"  Promptable segmentation, introduced by the Segment Anything Model (SAM), is a\npromising approach for medical imaging, as it enables clinicians to guide and\nrefine model predictions interactively. However, SAM's architecture is designed\nfor 2D images and does not extend naturally to 3D volumetric data such as CT or\nMRI scans. Adapting 2D models to 3D typically involves autoregressive\nstrategies, where predictions are propagated slice by slice, resulting in\nincreased inference complexity. Processing large 3D volumes also requires\nsignificant computational resources, often leading existing 3D methods to also\nadopt complex strategies like sliding-window inference to manage memory usage,\nat the cost of longer inference times and greater implementation complexity. In\nthis paper, we present a simplified 3D promptable segmentation method, inspired\nby SegVol, designed to reduce inference time and eliminate prompt management\ncomplexities associated with sliding windows while achieving state-of-the-art\nperformance.\n","authors":["Théo Danielou","Daniel Tordjman","Pierre Manceron","Corentin Dancette"],"pdf_url":"https://arxiv.org/pdf/2507.07730v1.pdf","comment":"Abstract accepted at MIUA 2025"},{"id":"http://arxiv.org/abs/2502.20805v2","updated":"2025-07-10T13:08:40Z","published":"2025-02-28T07:42:54Z","title":"FunHOI: Annotation-Free 3D Hand-Object Interaction Generation via\n  Functional Text Guidanc","summary":"  Hand-object interaction(HOI) is the fundamental link between human and\nenvironment, yet its dexterous and complex pose significantly challenges for\ngesture control. Despite significant advances in AI and robotics, enabling\nmachines to understand and simulate hand-object interactions, capturing the\nsemantics of functional grasping tasks remains a considerable challenge. While\nprevious work can generate stable and correct 3D grasps, they are still far\nfrom achieving functional grasps due to unconsidered grasp semantics. To\naddress this challenge, we propose an innovative two-stage framework,\nFunctional Grasp Synthesis Net (FGS-Net), for generating 3D HOI driven by\nfunctional text. This framework consists of a text-guided 3D model generator,\nFunctional Grasp Generator (FGG), and a pose optimization strategy, Functional\nGrasp Refiner (FGR). FGG generates 3D models of hands and objects based on text\ninput, while FGR fine-tunes the poses using Object Pose Approximator and energy\nfunctions to ensure the relative position between the hand and object aligns\nwith human intent and remains physically plausible. Extensive experiments\ndemonstrate that our approach achieves precise and high-quality HOI generation\nwithout requiring additional 3D annotation data.\n","authors":["Yongqi Tian","Xueyu Sun","Haoyuan He","Linji Hao","Ning Ding","Caigui Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.20805v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07722v1","updated":"2025-07-10T12:57:09Z","published":"2025-07-10T12:57:09Z","title":"Understanding Dataset Bias in Medical Imaging: A Case Study on Chest\n  X-rays","summary":"  Recent work has revisited the infamous task Name that dataset and established\nthat in non-medical datasets, there is an underlying bias and achieved high\nAccuracies on the dataset origin task. In this work, we revisit the same task\napplied to popular open-source chest X-ray datasets. Medical images are\nnaturally more difficult to release for open-source due to their sensitive\nnature, which has led to certain open-source datasets being extremely popular\nfor research purposes. By performing the same task, we wish to explore whether\ndataset bias also exists in these datasets. % We deliberately try to increase\nthe difficulty of the task by dataset transformations. We apply simple\ntransformations of the datasets to try to identify bias. Given the importance\nof AI applications in medical imaging, it's vital to establish whether modern\nmethods are taking shortcuts or are focused on the relevant pathology. We\nimplement a range of different network architectures on the datasets: NIH,\nCheXpert, MIMIC-CXR and PadChest. We hope this work will encourage more\nexplainable research being performed in medical imaging and the creation of\nmore open-source datasets in the medical domain. The corresponding code will be\nreleased upon acceptance.\n","authors":["Ethan Dack","Chengliang Dai"],"pdf_url":"https://arxiv.org/pdf/2507.07722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07721v1","updated":"2025-07-10T12:56:24Z","published":"2025-07-10T12:56:24Z","title":"Breast Ultrasound Tumor Generation via Mask Generator and Text-Guided\n  Network:A Clinically Controllable Framework with Downstream Evaluation","summary":"  The development of robust deep learning models for breast ultrasound (BUS)\nimage analysis is significantly constrained by the scarcity of expert-annotated\ndata. To address this limitation, we propose a clinically controllable\ngenerative framework for synthesizing BUS images. This framework integrates\nclinical descriptions with structural masks to generate tumors, enabling\nfine-grained control over tumor characteristics such as morphology,\nechogencity, and shape. Furthermore, we design a semantic-curvature mask\ngenerator, which synthesizes structurally diverse tumor masks guided by\nclinical priors. During inference, synthetic tumor masks serve as input to the\ngenerative framework, producing highly personalized synthetic BUS images with\ntumors that reflect real-world morphological diversity. Quantitative\nevaluations on six public BUS datasets demonstrate the significant clinical\nutility of our synthetic images, showing their effectiveness in enhancing\ndownstream breast cancer diagnosis tasks. Furthermore, visual Turing tests\nconducted by experienced sonographers confirm the realism of the generated\nimages, indicating the framework's potential to support broader clinical\napplications.\n","authors":["Haoyu Pan","Hongxin Lin","Zetian Feng","Chuxuan Lin","Junyang Mo","Chu Zhang","Zijian Wu","Yi Wang","Qingqing Zheng"],"pdf_url":"https://arxiv.org/pdf/2507.07721v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.10252v2","updated":"2025-07-10T12:48:37Z","published":"2025-03-13T10:59:51Z","title":"SVIP: Semantically Contextualized Visual Patches for Zero-Shot Learning","summary":"  Zero-shot learning (ZSL) aims to recognize unseen classes without labeled\ntraining examples by leveraging class-level semantic descriptors such as\nattributes. A fundamental challenge in ZSL is semantic misalignment, where\nsemantic-unrelated information involved in visual features introduce ambiguity\nto visual-semantic interaction. Unlike existing methods that suppress\nsemantic-unrelated information post hoc either in the feature space or the\nmodel space, we propose addressing this issue at the input stage, preventing\nsemantic-unrelated patches from propagating through the network. To this end,\nwe introduce Semantically contextualized VIsual Patches (SVIP) for ZSL, a\ntransformer-based framework designed to enhance visual-semantic alignment.\nSpecifically, we propose a self-supervised patch selection mechanism that\npreemptively learns to identify semantic-unrelated patches in the input space.\nThis is trained with the supervision from aggregated attention scores across\nall transformer layers, which estimate each patch's semantic score. As removing\nsemantic-unrelated patches from the input sequence may disrupt object\nstructure, we replace them with learnable patch embeddings. With initialization\nfrom word embeddings, we can ensure they remain semantically meaningful\nthroughout feature extraction. Extensive experiments on ZSL benchmarks\ndemonstrate that SVIP achieves state-of-the-art performance results while\nproviding more interpretable and semantically rich feature representations.\nCode is available at https://github.com/uqzhichen/SVIP.\n","authors":["Zhi Chen","Zecheng Zhao","Jingcai Guo","Jingjing Li","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2503.10252v2.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2507.07712v1","updated":"2025-07-10T12:46:31Z","published":"2025-07-10T12:46:31Z","title":"Balancing the Past and Present: A Coordinated Replay Framework for\n  Federated Class-Incremental Learning","summary":"  Federated Class Incremental Learning (FCIL) aims to collaboratively process\ncontinuously increasing incoming tasks across multiple clients. Among various\napproaches, data replay has become a promising solution, which can alleviate\nforgetting by reintroducing representative samples from previous tasks.\nHowever, their performance is typically limited by class imbalance, both within\nthe replay buffer due to limited global awareness and between replayed and\nnewly arrived classes. To address this issue, we propose a class wise balancing\ndata replay method for FCIL (FedCBDR), which employs a global coordination\nmechanism for class-level memory construction and reweights the learning\nobjective to alleviate the aforementioned imbalances. Specifically, FedCBDR has\ntwo key components: 1) the global-perspective data replay module reconstructs\nglobal representations of prior task in a privacy-preserving manner, which then\nguides a class-aware and importance-sensitive sampling strategy to achieve\nbalanced replay; 2) Subsequently, to handle class imbalance across tasks, the\ntask aware temperature scaling module adaptively adjusts the temperature of\nlogits at both class and instance levels based on task dynamics, which reduces\nthe model's overconfidence in majority classes while enhancing its sensitivity\nto minority classes. Experimental results verified that FedCBDR achieves\nbalanced class-wise sampling under heterogeneous data distributions and\nimproves generalization under task imbalance between earlier and recent tasks,\nyielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.\n","authors":["Zhuang Qi","Lei Meng","Han Yu"],"pdf_url":"https://arxiv.org/pdf/2507.07712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07709v1","updated":"2025-07-10T12:40:34Z","published":"2025-07-10T12:40:34Z","title":"One Object, Multiple Lies: A Benchmark for Cross-task Adversarial Attack\n  on Unified Vision-Language Models","summary":"  Unified vision-language models(VLMs) have recently shown remarkable progress,\nenabling a single model to flexibly address diverse tasks through different\ninstructions within a shared computational architecture. This instruction-based\ncontrol mechanism creates unique security challenges, as adversarial inputs\nmust remain effective across multiple task instructions that may be\nunpredictably applied to process the same malicious content. In this paper, we\nintroduce CrossVLAD, a new benchmark dataset carefully curated from MSCOCO with\nGPT-4-assisted annotations for systematically evaluating cross-task adversarial\nattacks on unified VLMs. CrossVLAD centers on the object-change\nobjective-consistently manipulating a target object's classification across\nfour downstream tasks-and proposes a novel success rate metric that measures\nsimultaneous misclassification across all tasks, providing a rigorous\nevaluation of adversarial transferability. To tackle this challenge, we present\nCRAFT (Cross-task Region-based Attack Framework with Token-alignment), an\nefficient region-centric attack method. Extensive experiments on Florence-2 and\nother popular unified VLMs demonstrate that our method outperforms existing\napproaches in both overall cross-task attack performance and targeted\nobject-change success rates, highlighting its effectiveness in adversarially\ninfluencing unified VLMs across diverse tasks.\n","authors":["Jiale Zhao","Xinyang Jiang","Junyao Gao","Yuhao Xue","Cairong Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.07709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07708v1","updated":"2025-07-10T12:38:27Z","published":"2025-07-10T12:38:27Z","title":"Motion-Aware Adaptive Pixel Pruning for Efficient Local Motion\n  Deblurring","summary":"  Local motion blur in digital images originates from the relative motion\nbetween dynamic objects and static imaging systems during exposure. Existing\ndeblurring methods face significant challenges in addressing this problem due\nto their inefficient allocation of computational resources and inadequate\nhandling of spatially varying blur patterns. To overcome these limitations, we\nfirst propose a trainable mask predictor that identifies blurred regions in the\nimage. During training, we employ blur masks to exclude sharp regions. For\ninference optimization, we implement structural reparameterization by\nconverting $3\\times 3$ convolutions to computationally efficient $1\\times 1$\nconvolutions, enabling pixel-level pruning of sharp areas to reduce\ncomputation. Second, we develop an intra-frame motion analyzer that translates\nrelative pixel displacements into motion trajectories, establishing adaptive\nguidance for region-specific blur restoration. Our method is trained end-to-end\nusing a combination of reconstruction loss, reblur loss, and mask loss guided\nby annotated blur masks. Extensive experiments demonstrate superior performance\nover state-of-the-art methods on both local and global blur datasets while\nreducing FLOPs by 49\\% compared to SOTA models (e.g., LMD-ViT). The source code\nis available at https://github.com/shangwei5/M2AENet.\n","authors":["Wei Shang","Dongwei Ren","Wanying Zhang","Pengfei Zhu","Qinghua Hu","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2507.07708v1.pdf","comment":"Accepted by ACMMM 2025"},{"id":"http://arxiv.org/abs/2507.07707v1","updated":"2025-07-10T12:36:20Z","published":"2025-07-10T12:36:20Z","title":"Compressive Imaging Reconstruction via Tensor Decomposed\n  Multi-Resolution Grid Encoding","summary":"  Compressive imaging (CI) reconstruction, such as snapshot compressive imaging\n(SCI) and compressive sensing magnetic resonance imaging (MRI), aims to recover\nhigh-dimensional images from low-dimensional compressed measurements. This\nprocess critically relies on learning an accurate representation of the\nunderlying high-dimensional image. However, existing unsupervised\nrepresentations may struggle to achieve a desired balance between\nrepresentation ability and efficiency. To overcome this limitation, we propose\nTensor Decomposed multi-resolution Grid encoding (GridTD), an unsupervised\ncontinuous representation framework for CI reconstruction. GridTD optimizes a\nlightweight neural network and the input tensor decomposition model whose\nparameters are learned via multi-resolution hash grid encoding. It inherently\nenjoys the hierarchical modeling ability of multi-resolution grid encoding and\nthe compactness of tensor decomposition, enabling effective and efficient\nreconstruction of high-dimensional images. Theoretical analyses for the\nalgorithm's Lipschitz property, generalization error bound, and fixed-point\nconvergence reveal the intrinsic superiority of GridTD as compared with\nexisting continuous representation models. Extensive experiments across diverse\nCI tasks, including video SCI, spectral SCI, and compressive dynamic MRI\nreconstruction, consistently demonstrate the superiority of GridTD over\nexisting methods, positioning GridTD as a versatile and state-of-the-art CI\nreconstruction method.\n","authors":["Zhenyu Jin","Yisi Luo","Xile Zhao","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2507.07707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07704v1","updated":"2025-07-10T12:32:28Z","published":"2025-07-10T12:32:28Z","title":"D-CNN and VQ-VAE Autoencoders for Compression and Denoising of\n  Industrial X-ray Computed Tomography Images","summary":"  The ever-growing volume of data in imaging sciences stemming from the\nadvancements in imaging technologies, necessitates efficient and reliable\nstorage solutions for such large datasets. This study investigates the\ncompression of industrial X-ray computed tomography (XCT) data using deep\nlearning autoencoders and examines how these compression algorithms affect the\nquality of the recovered data. Two network architectures with different\ncompression rates were used, a deep convolution neural network (D-CNN) and a\nvector quantized variational autoencoder (VQ-VAE). The XCT data used was from a\nsandstone sample with a complex internal pore network. The quality of the\ndecoded images obtained from the two different deep learning architectures with\ndifferent compression rates were quantified and compared to the original input\ndata. In addition, to improve image decoding quality metrics, we introduced a\nmetric sensitive to edge preservation, which is crucial for three-dimensional\ndata analysis. We showed that different architectures and compression rates are\nrequired depending on the specific characteristics needed to be preserved for\nlater analysis. The findings presented here can aid scientists to determine the\nrequirements and strategies for their data storage and analysis needs.\n","authors":["Bardia Hejazi","Keerthana Chand","Tobias Fritsch","Giovanni Bruno"],"pdf_url":"https://arxiv.org/pdf/2507.07704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05007v2","updated":"2025-07-10T12:22:03Z","published":"2025-07-07T13:44:58Z","title":"Multi-modal Representations for Fine-grained Multi-label Critical View\n  of Safety Recognition","summary":"  The Critical View of Safety (CVS) is crucial for safe laparoscopic\ncholecystectomy, yet assessing CVS criteria remains a complex and challenging\ntask, even for experts. Traditional models for CVS recognition depend on\nvision-only models learning with costly, labor-intensive spatial annotations.\nThis study investigates how text can be harnessed as a powerful tool for both\ntraining and inference in multi-modal surgical foundation models to automate\nCVS recognition. Unlike many existing multi-modal models, which are primarily\nadapted for multi-class classification, CVS recognition requires a multi-label\nframework. Zero-shot evaluation of existing multi-modal surgical models shows a\nsignificant performance gap for this task. To address this, we propose\nCVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained,\nbinary classification across multiple labels by aligning image embeddings with\ntextual descriptions of each CVS criterion using positive and negative prompts.\nBy adapting PeskaVLP, a state-of-the-art surgical foundation model, on the\nEndoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the\nResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that\nCVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts,\nboosts CVS recognition over image-only methods. We also propose text-specific\ninference methods, that helps in analysing the image-text alignment. While\nfurther work is needed to match state-of-the-art spatial annotation-based\nmethods, this approach highlights the potential of adapting generalist models\nto specialized surgical tasks. Code:\nhttps://github.com/CAMMA-public/CVS-AdaptNet\n","authors":["Britty Baby","Vinkle Srivastav","Pooja P. Jain","Kun Yuan","Pietro Mascagni","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2507.05007v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05020v2","updated":"2025-07-10T12:21:47Z","published":"2025-07-07T14:03:10Z","title":"Adaptation of Multi-modal Representation Models for Multi-task Surgical\n  Computer Vision","summary":"  Surgical AI often involves multiple tasks within a single procedure, like\nphase recognition or assessing the Critical View of Safety in laparoscopic\ncholecystectomy. Traditional models, built for one task at a time, lack\nflexibility, requiring a separate model for each. To address this, we introduce\nMML-SurgAdapt, a unified multi-task framework with Vision-Language Models\n(VLMs), specifically CLIP, to handle diverse surgical tasks through natural\nlanguage supervision. A key challenge in multi-task learning is the presence of\npartial annotations when integrating different tasks. To overcome this, we\nemploy Single Positive Multi-Label (SPML) learning, which traditionally reduces\nannotation burden by training models with only one positive label per instance.\nOur framework extends this approach to integrate data from multiple surgical\ntasks within a single procedure, enabling effective learning despite incomplete\nor noisy annotations. We demonstrate the effectiveness of our model on a\ncombined dataset consisting of Cholec80, Endoscapes2023, and CholecT50,\nutilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt\nperforms comparably to task-specific benchmarks, with the added advantage of\nhandling noisy annotations. It also outperforms the existing SPML frameworks\nfor the task. By reducing the required labels by 23%, our approach proposes a\nmore scalable and efficient labeling process, significantly easing the\nannotation burden on clinicians. To our knowledge, this is the first\napplication of SPML to integrate data from multiple surgical tasks, presenting\na novel and generalizable solution for multi-task learning in surgical computer\nvision. Implementation is available at:\nhttps://github.com/CAMMA-public/MML-SurgAdapt\n","authors":["Soham Walimbe","Britty Baby","Vinkle Srivastav","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2507.05020v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07687v1","updated":"2025-07-10T12:10:51Z","published":"2025-07-10T12:10:51Z","title":"Tree-Mamba: A Tree-Aware Mamba for Underwater Monocular Depth Estimation","summary":"  Underwater Monocular Depth Estimation (UMDE) is a critical task that aims to\nestimate high-precision depth maps from underwater degraded images caused by\nlight absorption and scattering effects in marine environments. Recently,\nMamba-based methods have achieved promising performance across various vision\ntasks; however, they struggle with the UMDE task because their inflexible state\nscanning strategies fail to model the structural features of underwater images\neffectively. Meanwhile, existing UMDE datasets usually contain unreliable depth\nlabels, leading to incorrect object-depth relationships between underwater\nimages and their corresponding depth maps. To overcome these limitations, we\ndevelop a novel tree-aware Mamba method, dubbed Tree-Mamba, for estimating\naccurate monocular depth maps from underwater degraded images. Specifically, we\npropose a tree-aware scanning strategy that adaptively constructs a minimum\nspanning tree based on feature similarity. The spatial topological features\namong the tree nodes are then flexibly aggregated through bottom-up and\ntop-down traversals, enabling stronger multi-scale feature representation\ncapabilities. Moreover, we construct an underwater depth estimation benchmark\n(called BlueDepth), which consists of 38,162 underwater image pairs with\nreliable depth labels. This benchmark serves as a foundational dataset for\ntraining existing deep learning-based UMDE methods to learn accurate\nobject-depth relationships. Extensive experiments demonstrate the superiority\nof the proposed Tree-Mamba over several leading methods in both qualitative\nresults and quantitative evaluations with competitive computational efficiency.\nCode and dataset will be available at https://wyjgr.github.io/Tree-Mamba.html.\n","authors":["Peixian Zhuang","Yijian Wang","Zhenqi Fu","Hongliang Zhang","Sam Kwong","Chongyi Li"],"pdf_url":"https://arxiv.org/pdf/2507.07687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07685v1","updated":"2025-07-10T12:07:13Z","published":"2025-07-10T12:07:13Z","title":"Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought","summary":"  Large vision-language models (LVLMs) have demonstrated remarkable\ncapabilities by integrating pre-trained vision encoders with large language\nmodels (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting\nhas been adapted for LVLMs to enhance multi-modal reasoning by generating\nintermediate rationales based on visual and textual inputs. While CoT is\nassumed to improve grounding and accuracy in LVLMs, our experiments reveal a\nkey challenge: existing LVLMs often ignore the contents of generated rationales\nin CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as\na KL-constrained reward maximization focused on rationale-conditional\nlog-likelihood. As the optimal solution, we propose rationale-enhanced decoding\n(RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes\nvisual and rationale information by multiplying distinct image-conditional and\nrationale-conditional next token distributions. Extensive experiments show that\nRED consistently and significantly improves reasoning over standard CoT and\nother decoding methods across multiple benchmarks and LVLMs. Our work offers a\npractical and effective approach to improve both the faithfulness and accuracy\nof CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded\nmulti-modal systems.\n","authors":["Shin'ya Yamaguchi","Kosuke Nishida","Daiki Chijiwa"],"pdf_url":"https://arxiv.org/pdf/2507.07685v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.07678v1","updated":"2025-07-10T11:59:43Z","published":"2025-07-10T11:59:43Z","title":"Action Unit Enhance Dynamic Facial Expression Recognition","summary":"  Dynamic Facial Expression Recognition(DFER) is a rapidly evolving field of\nresearch that focuses on the recognition of time-series facial expressions.\nWhile previous research on DFER has concentrated on feature learning from a\ndeep learning perspective, we put forward an AU-enhanced Dynamic Facial\nExpression Recognition architecture, namely AU-DFER, that incorporates\nAU-expression knowledge to enhance the effectiveness of deep learning modeling.\nIn particular, the contribution of the Action Units(AUs) to different\nexpressions is quantified, and a weight matrix is designed to incorporate a\npriori knowledge. Subsequently, the knowledge is integrated with the learning\noutcomes of a conventional deep learning network through the introduction of AU\nloss. The design is incorporated into the existing optimal model for dynamic\nexpression recognition for the purpose of validation. Experiments are conducted\non three recent mainstream open-source approaches to DFER on the principal\ndatasets in this field. The results demonstrate that the proposed architecture\noutperforms the state-of-the-art(SOTA) methods without the need for additional\narithmetic and generally produces improved results. Furthermore, we investigate\nthe potential of AU loss function redesign to address data label imbalance\nissues in established dynamic expression datasets. To the best of our\nknowledge, this is the first attempt to integrate quantified AU-expression\nknowledge into various DFER models. We also devise strategies to tackle label\nimbalance, or minor class problems. Our findings suggest that employing a\ndiverse strategy of loss function design can enhance the effectiveness of DFER.\nThis underscores the criticality of addressing data imbalance challenges in\nmainstream datasets within this domain. The source code is available at\nhttps://github.com/Cross-Innovation-Lab/AU-DFER.\n","authors":["Feng Liu","Lingna Gu","Chen Shi","Xiaolan Fu"],"pdf_url":"https://arxiv.org/pdf/2507.07678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07670v1","updated":"2025-07-10T11:52:20Z","published":"2025-07-10T11:52:20Z","title":"Attend-and-Refine: Interactive keypoint estimation and quantitative\n  cervical vertebrae analysis for bone age assessment","summary":"  In pediatric orthodontics, accurate estimation of growth potential is\nessential for developing effective treatment strategies. Our research aims to\npredict this potential by identifying the growth peak and analyzing cervical\nvertebra morphology solely through lateral cephalometric radiographs. We\naccomplish this by comprehensively analyzing cervical vertebral maturation\n(CVM) features from these radiographs. This methodology provides clinicians\nwith a reliable and efficient tool to determine the optimal timings for\northodontic interventions, ultimately enhancing patient outcomes. A crucial\naspect of this approach is the meticulous annotation of keypoints on the\ncervical vertebrae, a task often challenged by its labor-intensive nature. To\nmitigate this, we introduce Attend-and-Refine Network (ARNet), a\nuser-interactive, deep learning-based model designed to streamline the\nannotation process. ARNet features Interaction-guided recalibration network,\nwhich adaptively recalibrates image features in response to user feedback,\ncoupled with a morphology-aware loss function that preserves the structural\nconsistency of keypoints. This novel approach substantially reduces manual\neffort in keypoint identification, thereby enhancing the efficiency and\naccuracy of the process. Extensively validated across various datasets, ARNet\ndemonstrates remarkable performance and exhibits wide-ranging applicability in\nmedical imaging. In conclusion, our research offers an effective AI-assisted\ndiagnostic tool for assessing growth potential in pediatric orthodontics,\nmarking a significant advancement in the field.\n","authors":["Jinhee Kim","Taesung Kim","Taewoo Kim","Dong-Wook Kim","Byungduk Ahn","Yoon-Ji Kim","In-Seok Song","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2507.07670v1.pdf","comment":"Accepted to Medical Image Analysis (2025)"},{"id":"http://arxiv.org/abs/2507.07663v1","updated":"2025-07-10T11:38:54Z","published":"2025-07-10T11:38:54Z","title":"MolCLIP: A Molecular-Auxiliary CLIP Framework for Identifying Drug\n  Mechanism of Action Based on Time-Lapsed Mitochondrial Images","summary":"  Drug Mechanism of Action (MoA) mainly investigates how drug molecules\ninteract with cells, which is crucial for drug discovery and clinical\napplication. Recently, deep learning models have been used to recognize MoA by\nrelying on high-content and fluorescence images of cells exposed to various\ndrugs. However, these methods focus on spatial characteristics while\noverlooking the temporal dynamics of live cells. Time-lapse imaging is more\nsuitable for observing the cell response to drugs. Additionally, drug molecules\ncan trigger cellular dynamic variations related to specific MoA. This indicates\nthat the drug molecule modality may complement the image counterpart. This\npaper proposes MolCLIP, the first visual language model to combine microscopic\ncell video- and molecule-modalities. MolCLIP designs a molecule-auxiliary CLIP\nframework to guide video features in learning the distribution of the molecular\nlatent space. Furthermore, we integrate a metric learning strategy with MolCLIP\nto optimize the aggregation of video features. Experimental results on the\nMitoDataset demonstrate that MolCLIP achieves improvements of 51.2% and 20.5%\nin mAP for drug identification and MoA recognition, respectively.\n","authors":["Fengqian Pang","Chunyue Lei","Hongfei Zhao","Chenghao Liu","Zhiqiang Xing","Huafeng Wang","Chuyang Ye"],"pdf_url":"https://arxiv.org/pdf/2507.07663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01803v2","updated":"2025-07-10T11:21:57Z","published":"2023-03-03T09:19:08Z","title":"Uncertainty-Aware Gradient Stabilization for Small Object Detection","summary":"  Despite advances in generic object detection, there remains a performance gap\nin detecting small objects compared to normal-scale objects. We reveal that\nconventional object localization methods suffer from gradient instability in\nsmall objects due to sharper loss curvature, leading to a convergence\nchallenge. To address the issue, we propose Uncertainty-Aware Gradient\nStabilization (UGS), a framework that reformulates object localization as a\nclassification task to stabilize gradients. UGS quantizes continuous labels\ninto interval non-uniform discrete representations. Under a\nclassification-based objective, the localization branch generates bounded and\nconfidence-driven gradients, mitigating instability. Furthermore, UGS\nintegrates an uncertainty minimization (UM) loss that reduces prediction\nvariance and an uncertainty-guided refinement (UR) module that identifies and\nrefines high-uncertainty regions via perturbations. Evaluated on four\nbenchmarks, UGS consistently improves anchor-based, anchor-free, and leading\nsmall object detectors. Especially, UGS enhances DINO-5scale by 2.6 AP on\nVisDrone, surpassing previous state-of-the-art results.\n","authors":["Huixin Sun","Yanjing Li","Linlin Yang","Xianbin Cao","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.01803v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07638v1","updated":"2025-07-10T11:07:13Z","published":"2025-07-10T11:07:13Z","title":"Bridging the gap in FER: addressing age bias in deep learning","summary":"  Facial Expression Recognition (FER) systems based on deep learning have\nachieved impressive performance in recent years. However, these models often\nexhibit demographic biases, particularly with respect to age, which can\ncompromise their fairness and reliability. In this work, we present a\ncomprehensive study of age-related bias in deep FER models, with a particular\nfocus on the elderly population. We first investigate whether recognition\nperformance varies across age groups, which expressions are most affected, and\nwhether model attention differs depending on age. Using Explainable AI (XAI)\ntechniques, we identify systematic disparities in expression recognition and\nattention patterns, especially for \"neutral\", \"sadness\", and \"anger\" in elderly\nindividuals. Based on these findings, we propose and evaluate three bias\nmitigation strategies: Multi-task Learning, Multi-modal Input, and Age-weighted\nLoss. Our models are trained on a large-scale dataset, AffectNet, with\nautomatically estimated age labels and validated on balanced benchmark datasets\nthat include underrepresented age groups. Results show consistent improvements\nin recognition accuracy for elderly individuals, particularly for the most\nerror-prone expressions. Saliency heatmap analysis reveals that models trained\nwith age-aware strategies attend to more relevant facial regions for each age\ngroup, helping to explain the observed improvements. These findings suggest\nthat age-related bias in FER can be effectively mitigated using simple training\nmodifications, and that even approximate demographic labels can be valuable for\npromoting fairness in large-scale affective computing systems.\n","authors":["F. Xavier Gaya-Morey","Julia Sanchez-Perez","Cristina Manresa-Yee","Jose M. Buades-Rubio"],"pdf_url":"https://arxiv.org/pdf/2507.07638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07633v1","updated":"2025-07-10T11:01:58Z","published":"2025-07-10T11:01:58Z","title":"T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates","summary":"  Recent advances in video generation techniques have given rise to an emerging\nparadigm of generative video coding, aiming to achieve semantically accurate\nreconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong\ngenerative priors. However, most existing methods are limited by domain\nspecificity (e.g., facial or human videos) or an excessive dependence on\nhigh-level text guidance, which often fails to capture motion details and\nresults in unrealistic reconstructions. To address these challenges, we propose\na Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC\nemploys a semantic-aware sparse motion sampling pipeline to effectively bridge\nlow-level motion tracking with high-level semantic understanding by extracting\npixel-wise motion as sparse trajectory points based on their semantic\nimportance, not only significantly reducing the bitrate but also preserving\ncritical temporal semantic information. In addition, by incorporating\ntrajectory-aligned loss constraints into diffusion processes, we introduce a\ntraining-free latent space guidance mechanism to ensure physically plausible\nmotion patterns without sacrificing the inherent capabilities of generative\nmodels. Experimental results demonstrate that our framework outperforms both\ntraditional codecs and state-of-the-art end-to-end video compression methods\nunder ULB conditions. Furthermore, additional experiments confirm that our\napproach achieves more precise motion control than existing text-guided\nmethods, paving the way for a novel direction of generative video coding guided\nby geometric motion modeling.\n","authors":["Zhitao Wang","Hengyu Man","Wenrui Li","Xingtao Wang","Xiaopeng Fan","Debin Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.07633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.23664v2","updated":"2025-07-10T10:51:46Z","published":"2025-06-30T09:40:12Z","title":"Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound\n  Segmentation","summary":"  Medical image data is less accessible than in other domains due to privacy\nand regulatory constraints. In addition, labeling requires costly,\ntime-intensive manual image annotation by clinical experts. To overcome these\nchallenges, synthetic medical data generation offers a promising solution.\nGenerative AI (GenAI), employing generative deep learning models, has proven\neffective at producing realistic synthetic images. This study proposes a novel\nmask-guided GenAI approach using diffusion models to generate synthetic fetal\nhead ultrasound images paired with segmentation masks. These synthetic pairs\naugment real datasets for supervised fine-tuning of the Segment Anything Model\n(SAM). Our results show that the synthetic data captures real image features\neffectively, and this approach reaches state-of-the-art fetal head\nsegmentation, especially when trained with a limited number of real image-mask\npairs. In particular, the segmentation reaches Dice Scores of 94.66\\% and\n94.38\\% using a handful of ultrasound images from the Spanish and African\ncohorts, respectively. Our code, models, and data are available on GitHub.\n","authors":["Fangyijie Wang","Kevin Whelan","Félix Balado","Kathleen M. Curran","Guénolé Silvestre"],"pdf_url":"https://arxiv.org/pdf/2506.23664v2.pdf","comment":"Accepted at Irish Machine Vision and Image Processing Conference\n  (IMVIP) 2025"},{"id":"http://arxiv.org/abs/2507.07623v1","updated":"2025-07-10T10:45:46Z","published":"2025-07-10T10:45:46Z","title":"Capture Stage Environments: A Guide to Better Matting","summary":"  Capture stages are high-end sources of state-of-the-art recordings for\ndownstream applications in movies, games, and other media. One crucial step in\nalmost all pipelines is the matting of images to isolate the captured\nperformances from the background. While common matting algorithms deliver\nremarkable performance in other applications like teleconferencing and mobile\nentertainment, we found that they struggle significantly with the peculiarities\nof capture stage content. The goal of our work is to share insights into those\nchallenges as a curated list of those characteristics along with a constructive\ndiscussion for proactive intervention and present a guideline to practitioners\nfor an improved workflow to mitigate unresolved challenges. To this end, we\nalso demonstrate an efficient pipeline to adapt state-of-the-art approaches to\nsuch custom setups without the need of extensive annotations, both offline and\nreal-time. For an objective evaluation, we propose a validation methodology\nbased on a leading diffusion model that highlights the benefits of our\napproach.\n","authors":["Hannah Dröge","Janelle Pfeifer","Saskia Rabich","Markus Plack","Reinhard Klein","Matthias B. Hullin"],"pdf_url":"https://arxiv.org/pdf/2507.07623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07620v1","updated":"2025-07-10T10:41:13Z","published":"2025-07-10T10:41:13Z","title":"ViLU: Learning Vision-Language Uncertainties for Failure Prediction","summary":"  Reliable Uncertainty Quantification (UQ) and failure prediction remain open\nchallenges for Vision-Language Models (VLMs). We introduce ViLU, a new\nVision-Language Uncertainty quantification framework that contextualizes\nuncertainty estimates by leveraging all task-relevant textual representations.\nViLU constructs an uncertainty-aware multi-modal representation by integrating\nthe visual embedding, the predicted textual embedding, and an image-conditioned\ntextual representation via cross-attention. Unlike traditional UQ methods based\non loss prediction, ViLU trains an uncertainty predictor as a binary classifier\nto distinguish correct from incorrect predictions using a weighted binary\ncross-entropy loss, making it loss-agnostic. In particular, our proposed\napproach is well-suited for post-hoc settings, where only vision and text\nembeddings are available without direct access to the model itself. Extensive\nexperiments on diverse datasets show the significant gains of our method\ncompared to state-of-the-art failure prediction methods. We apply our method to\nstandard classification datasets, such as ImageNet-1k, as well as large-scale\nimage-caption datasets like CC12M and LAION-400M. Ablation studies highlight\nthe critical role of our architecture and training in achieving effective\nuncertainty quantification. Our code is publicly available and can be found\nhere: https://github.com/ykrmm/ViLU.\n","authors":["Marc Lafon","Yannis Karmim","Julio Silva-Rodriguez","Paul Couairon","Clément Rambour","Raphaël Fournier-Sniehotta","Ismail Ben Ayed","Jose Dolz","Nicolas Thome"],"pdf_url":"https://arxiv.org/pdf/2507.07620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07610v1","updated":"2025-07-10T10:27:20Z","published":"2025-07-10T10:27:20Z","title":"SpatialViz-Bench: Automatically Generated Spatial Visualization\n  Reasoning Tasks for MLLMs","summary":"  Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models exhibit unexpected behaviors\nby showing difficulty perception that misaligns with human intuition,\ndisplaying dramatic 2D-to-3D performance cliffs, and defaulting to formula\nderivation despite spatial tasks requiring visualization alone. SpatialVizBench\nempirically demonstrates that state-of-the-art MLLMs continue to exhibit\ndeficiencies in spatial visualization tasks, thereby addressing a significant\nlacuna in the field. The benchmark is publicly available.\n","authors":["Siting Wang","Luoyang Sun","Cheng Deng","Kun Shao","Minnan Pei","Zheng Tian","Haifeng Zhang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2507.07610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07605v1","updated":"2025-07-10T10:10:13Z","published":"2025-07-10T10:10:13Z","title":"LOSC: LiDAR Open-voc Segmentation Consolidator","summary":"  We study the use of image-based Vision-Language Models (VLMs) for\nopen-vocabulary segmentation of lidar scans in driving settings. Classically,\nimage semantics can be back-projected onto 3D point clouds. Yet, resulting\npoint labels are noisy and sparse. We consolidate these labels to enforce both\nspatio-temporal consistency and robustness to image-level augmentations. We\nthen train a 3D network based on these refined labels. This simple method,\ncalled LOSC, outperforms the SOTA of zero-shot open-vocabulary semantic and\npanoptic segmentation on both nuScenes and SemanticKITTI, with significant\nmargins.\n","authors":["Nermin Samet","Gilles Puy","Renaud Marlet"],"pdf_url":"https://arxiv.org/pdf/2507.07605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15285v2","updated":"2025-07-10T10:06:33Z","published":"2025-03-19T15:04:01Z","title":"EEPNet-V2: Patch-to-Pixel Solution for Efficient Cross-Modal\n  Registration between LiDAR Point Cloud and Camera Image","summary":"  The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds effectively but also bridge the domain gap\nbetween the point cloud and image. Moreover, to tackle the challenges of cross\nmodal differences and the limited overlap between LiDAR point clouds and images\nin the image matching task, we introduce a multi-scale feature extraction\nnetwork to effectively extract features from both camera images and the\nprojection maps of LiDAR point cloud. Additionally, we propose a patch-to-pixel\nmatching network to provide more effective supervision and achieve high\naccuracy. We validate the performance of our model through experiments on the\nKITTI and nuScenes datasets. Experimental results demonstrate the the proposed\nmethod achieves real-time performance and extremely high registration accuracy.\nSpecifically, on the KITTI dataset, our model achieves a registration accuracy\nrate of over 99\\%. Our code is released at:\nhttps://github.com/ESRSchao/EEPNet-V2.\n","authors":["Yuanchao Yue","Hui Yuan","Zhengxin Li","Shuai Li","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.15285v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07603v1","updated":"2025-07-10T10:05:11Z","published":"2025-07-10T10:05:11Z","title":"HiM2SAM: Enhancing SAM2 with Hierarchical Motion Estimation and Memory\n  Optimization towards Long-term Tracking","summary":"  This paper presents enhancements to the SAM2 framework for video object\ntracking task, addressing challenges such as occlusions, background clutter,\nand target reappearance. We introduce a hierarchical motion estimation\nstrategy, combining lightweight linear prediction with selective non-linear\nrefinement to improve tracking accuracy without requiring additional training.\nIn addition, we optimize the memory bank by distinguishing long-term and\nshort-term memory frames, enabling more reliable tracking under long-term\nocclusions and appearance changes. Experimental results show consistent\nimprovements across different model scales. Our method achieves\nstate-of-the-art performance on LaSOT and LaSOText with the large model,\nachieving 9.6% and 7.2% relative improvements in AUC over the original SAM2,\nand demonstrates even larger relative gains on smaller models, highlighting the\neffectiveness of our trainless, low-overhead improvements for boosting\nlong-term tracking performance. The code is available at\nhttps://github.com/LouisFinner/HiM2SAM.\n","authors":["Ruixiang Chen","Guolei Sun","Yawei Li","Jie Qin","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2507.07603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09932v2","updated":"2025-07-10T10:03:57Z","published":"2025-06-11T16:54:34Z","title":"HadaNorm: Diffusion Transformer Quantization through Mean-Centered\n  Transformations","summary":"  Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches by both normalizing channels activations and applying Hadamard\ntransforms to effectively mitigate outliers and enable aggressive activation\nquantization. We demonstrate that HadaNorm consistently reduces quantization\nerror across the various components of transformer blocks, outperforming\nstate-of-the-art methods.\n","authors":["Marco Federici","Riccardo Del Chiaro","Boris van Breugel","Paul Whatmough","Markus Nagel"],"pdf_url":"https://arxiv.org/pdf/2506.09932v2.pdf","comment":"8 Pages, 6 Figures"},{"id":"http://arxiv.org/abs/2504.07793v3","updated":"2025-07-10T09:51:02Z","published":"2025-04-10T14:30:41Z","title":"Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling\n  Representations","summary":"  Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof deep learning systems, particularly in safety-critical applications.\nLikelihood-based deep generative models have historically faced criticism for\ntheir unsatisfactory performance in OOD detection, often assigning higher\nlikelihood to OOD data than in-distribution samples when applied to image data.\nIn this work, we demonstrate that likelihood is not inherently flawed. Rather,\nseveral properties in the images space prohibit likelihood as a valid detection\nscore. Given a sufficiently good likelihood estimator, specifically using the\nprobability flow formulation of a diffusion model, we show that\nlikelihood-based methods can still perform on par with state-of-the-art methods\nwhen applied in the representation space of pre-trained encoders. The code of\nour work can be found at\n$\\href{https://github.com/limchaos/Likelihood-OOD.git}{\\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$.\n","authors":["Yifan Ding","Arturas Aleksandraus","Amirhossein Ahmadian","Jonas Unger","Fredrik Lindsten","Gabriel Eilertsen"],"pdf_url":"https://arxiv.org/pdf/2504.07793v3.pdf","comment":"Scandinavian Conference on Image Analysis 2025 (oral)"},{"id":"http://arxiv.org/abs/2507.07591v1","updated":"2025-07-10T09:49:34Z","published":"2025-07-10T09:49:34Z","title":"Stable-Hair v2: Real-World Hair Transfer via Multiple-View Diffusion\n  Model","summary":"  While diffusion-based methods have shown impressive capabilities in capturing\ndiverse and complex hairstyles, their ability to generate consistent and\nhigh-quality multi-view outputs -- crucial for real-world applications such as\ndigital humans and virtual avatars -- remains underexplored. In this paper, we\npropose Stable-Hair v2, a novel diffusion-based multi-view hair transfer\nframework. To the best of our knowledge, this is the first work to leverage\nmulti-view diffusion models for robust, high-fidelity, and view-consistent hair\ntransfer across multiple perspectives. We introduce a comprehensive multi-view\ntraining data generation pipeline comprising a diffusion-based Bald Converter,\na data-augment inpainting model, and a face-finetuned multi-view diffusion\nmodel to generate high-quality triplet data, including bald images, reference\nhairstyles, and view-aligned source-bald pairs. Our multi-view hair transfer\nmodel integrates polar-azimuth embeddings for pose conditioning and temporal\nattention layers to ensure smooth transitions between views. To optimize this\nmodel, we design a novel multi-stage training strategy consisting of\npose-controllable latent IdentityNet training, hair extractor training, and\ntemporal attention training. Extensive experiments demonstrate that our method\naccurately transfers detailed and realistic hairstyles to source subjects while\nachieving seamless and consistent results across views, significantly\noutperforming existing methods and establishing a new benchmark in multi-view\nhair transfer. Code is publicly available at\nhttps://github.com/sunkymepro/StableHairV2.\n","authors":["Kuiyuan Sun","Yuxuan Zhang","Jichao Zhang","Jiaming Liu","Wei Wang","Niculae Sebe","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.07591v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2507.07585v1","updated":"2025-07-10T09:40:20Z","published":"2025-07-10T09:40:20Z","title":"HOTA: Hierarchical Overlap-Tiling Aggregation for Large-Area 3D Flood\n  Mapping","summary":"  Floods are among the most frequent natural hazards and cause significant\nsocial and economic damage. Timely, large-scale information on flood extent and\ndepth is essential for disaster response; however, existing products often\ntrade spatial detail for coverage or ignore flood depth altogether. To bridge\nthis gap, this work presents HOTA: Hierarchical Overlap-Tiling Aggregation, a\nplug-and-play, multi-scale inference strategy. When combined with SegFormer and\na dual-constraint depth estimation module, this approach forms a complete 3D\nflood-mapping pipeline. HOTA applies overlapping tiles of different sizes to\nmultispectral Sentinel-2 images only during inference, enabling the SegFormer\nmodel to capture both local features and kilometre-scale inundation without\nchanging the network weights or retraining. The subsequent depth module is\nbased on a digital elevation model (DEM) differencing method, which refines the\n2D mask and estimates flood depth by enforcing (i) zero depth along the flood\nboundary and (ii) near-constant flood volume with respect to the DEM. A case\nstudy on the March 2021 Kempsey (Australia) flood shows that HOTA, when coupled\nwith SegFormer, improves IoU from 73\\% (U-Net baseline) to 84\\%. The resulting\n3D surface achieves a mean absolute boundary error of less than 0.5 m. These\nresults demonstrate that HOTA can produce accurate, large-area 3D flood maps\nsuitable for rapid disaster response.\n","authors":["Wenfeng Jia","Bin Liang","Yuxi Lu","Attavit Wilaiwongsakul","Muhammad Arif Khan","Lihong Zheng"],"pdf_url":"https://arxiv.org/pdf/2507.07585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07579v1","updated":"2025-07-10T09:29:26Z","published":"2025-07-10T09:29:26Z","title":"NexViTAD: Few-shot Unsupervised Cross-Domain Defect Detection via Vision\n  Foundation Models and Multi-Task Learning","summary":"  This paper presents a novel few-shot cross-domain anomaly detection\nframework, Nexus Vision Transformer for Anomaly Detection (NexViTAD), based on\nvision foundation models, which effectively addresses domain-shift challenges\nin industrial anomaly detection through innovative shared subspace projection\nmechanisms and multi-task learning (MTL) module. The main innovations include:\n(1) a hierarchical adapter module that adaptively fuses complementary features\nfrom Hiera and DINO-v2 pre-trained models, constructing more robust feature\nrepresentations; (2) a shared subspace projection strategy that enables\neffective cross-domain knowledge transfer through bottleneck dimension\nconstraints and skip connection mechanisms; (3) a MTL Decoder architecture\nsupports simultaneous processing of multiple source domains, significantly\nenhancing model generalization capabilities; (4) an anomaly score inference\nmethod based on Sinkhorn-K-means clustering, combined with Gaussian filtering\nand adaptive threshold processing for precise pixel level. Valuated on the\nMVTec AD dataset, NexViTAD delivers state-of-the-art performance with an AUC of\n97.5%, AP of 70.4%, and PRO of 95.2% in the target domains, surpassing other\nrecent models, marking a transformative advance in cross-domain defect\ndetection.\n","authors":["Tianwei Mu","Feiyu Duan","Bo Zhou","Dan Xue","Manhong Huang"],"pdf_url":"https://arxiv.org/pdf/2507.07579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07578v1","updated":"2025-07-10T09:28:54Z","published":"2025-07-10T09:28:54Z","title":"Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light\n  Semantic Segmentation","summary":"  Weakly-supervised semantic segmentation aims to assign category labels to\neach pixel using weak annotations, significantly reducing manual annotation\ncosts. Although existing methods have achieved remarkable progress in well-lit\nscenarios, their performance significantly degrades in low-light environments\ndue to two fundamental limitations: severe image quality degradation (e.g., low\ncontrast, noise, and color distortion) and the inherent constraints of weak\nsupervision. These factors collectively lead to unreliable class activation\nmaps and semantically ambiguous pseudo-labels, ultimately compromising the\nmodel's ability to learn discriminative feature representations. To address\nthese problems, we propose Diffusion-Guided Knowledge Distillation for\nWeakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel\nframework that synergistically combines Diffusion-Guided Knowledge Distillation\n(DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and\nlow-light features via diffusion-based denoising and knowledge distillation,\nwhile DGF2 integrates depth maps as illumination-invariant geometric priors to\nenhance structural feature learning. Extensive experiments demonstrate the\neffectiveness of DGKD-WLSS, which achieves state-of-the-art performance in\nweakly supervised semantic segmentation tasks under low-light conditions. The\nsource codes have been released at:https://github.com/ChunyanWang1/DGKD-WLSS.\n","authors":["Chunyan Wang","Dong Zhang","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2507.07578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07574v1","updated":"2025-07-10T09:23:32Z","published":"2025-07-10T09:23:32Z","title":"Beyond the Linear Separability Ceiling","summary":"  Most state-of-the-art Visual-Language Models (VLMs) are seemingly limited by\nthe linear separabilty of their visual embeddings on abstract reasoning tasks.\nThis work investigates this \"linear reasoning bottleneck\" by introducing the\nLinear Separability Ceiling (LSC), the performance of a simple linear\nclassifier on a VLM's visual embeddings. We find this bottleneck is widespread\nand stems not from poor perception, but from failures in the language model's\nreasoning pathways. We demonstrate this is a solvable alignment issue. The\nrequired intervention, however, is task-dependent: activating existing pathways\nsuffices for semantic concepts, while complex relational reasoning requires\nadapting core model weights. Using postfix tuning as a methodological control,\nwe find strong evidence for powerful, dormant reasoning pathways within VLMs.\nHowever, for complex relational tasks requiring deeper adaptation, explicitly\nimproving representation quality causes the model to fail on new prompt formats\ndespite its embeddings remaining well separated. Ultimately, this work provides\na new lens for VLM analysis, showing that robust reasoning is a matter of\ntargeted alignment, not simply improved representation learning.\n","authors":["Enrico Vompa","Tanel Tammet","Mohit Vaishnav"],"pdf_url":"https://arxiv.org/pdf/2507.07574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07572v1","updated":"2025-07-10T09:18:06Z","published":"2025-07-10T09:18:06Z","title":"Single-to-mix Modality Alignment with Multimodal Large Language Model\n  for Document Image Machine Translation","summary":"  Document Image Machine Translation (DIMT) aims to translate text within\ndocument images, facing generalization challenges due to limited training data\nand the complex interplay between visual and textual information. To address\nthese challenges, we introduce M4Doc, a novel single-to-mix modality alignment\nframework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an\nimage-only encoder with the multimodal representations of an MLLM, pre-trained\non large-scale document image datasets. This alignment enables a lightweight\nDIMT model to learn crucial visual-textual correlations during training. During\ninference, M4Doc bypasses the MLLM, maintaining computational efficiency while\nbenefiting from its multimodal knowledge. Comprehensive experiments demonstrate\nsubstantial improvements in translation quality, especially in cross-domain\ngeneralization and challenging document image scenarios.\n","authors":["Yupu Liang","Yaping Zhang","Zhiyang Zhang","Yang Zhao","Lu Xiang","Chengqing Zong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.07572v1.pdf","comment":"Accepted by ACL 2025 Main"},{"id":"http://arxiv.org/abs/2506.15220v2","updated":"2025-07-10T09:09:22Z","published":"2025-06-18T07:58:41Z","title":"video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models","summary":"  Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimisation (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimised using DPO. To further improve training, we\npropose a novel multi-round DPO (MrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initialising the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilise the\nprocess. Experimental results show that MrDPO significantly enhances\nvideo-SALMONN 2's captioning accuracy, reducing the captioning error rates by\n28\\%. The final video-SALMONN 2 model, with just 7 billion parameters,\nsurpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning\ntasks, while maintaining highly competitive performance to the state-of-the-art\non widely used video question-answering benchmarks among models of similar\nsize. Codes are available at\n\\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.\n","authors":["Changli Tang","Yixuan Li","Yudong Yang","Jimin Zhuang","Guangzhi Sun","Wei Li","Zejun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.15220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08694v2","updated":"2025-07-10T08:39:59Z","published":"2025-06-10T11:20:32Z","title":"MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised\n  Learning","summary":"  Dense self-supervised learning has shown great promise for learning pixel-\nand patch-level representations, but extending it to videos remains challenging\ndue to the complexity of motion dynamics. Existing approaches struggle as they\nrely on static augmentations that fail under object deformations, occlusions,\nand camera movement, leading to inconsistent feature learning over time. We\npropose a motion-guided self-supervised learning framework that clusters dense\npoint tracks to learn spatiotemporally consistent representations. By\nleveraging an off-the-shelf point tracker, we extract long-range motion\ntrajectories and optimize feature clustering through a momentum-encoder-based\noptimal transport mechanism. To ensure temporal coherence, we propagate cluster\nassignments along tracked points, enforcing feature consistency across views\ndespite viewpoint changes. Integrating motion as an implicit supervisory\nsignal, our method learns representations that generalize across frames,\nimproving robustness in dynamic scenes and challenging occlusion scenarios. By\ninitializing from strong image-pretrained models and leveraging video data for\ntraining, we improve state-of-the-art by 1% to 6% on six image and video\ndatasets and four evaluation benchmarks. The implementation is publicly\navailable at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main\n","authors":["Mohammadreza Salehi","Shashanka Venkataramanan","Ioana Simion","Efstratios Gavves","Cees G. M. Snoek","Yuki M Asano"],"pdf_url":"https://arxiv.org/pdf/2506.08694v2.pdf","comment":"Accepted to ICCV2025"},{"id":"http://arxiv.org/abs/2408.02900v3","updated":"2025-07-10T08:33:52Z","published":"2024-08-06T02:09:35Z","title":"MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular\n  Annotations for Medicine","summary":"  This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal\ndataset for medicine, covering over 25 million images across 10 modalities with\nmultigranular annotations for more than 65 diseases. These multigranular\nannotations encompass both global information, such as modality and organ\ndetection, and local information like ROI analysis, lesion texture, and\nregion-wise correlations. Unlike the existing multimodal datasets, which are\nlimited by the availability of image-text pairs, we have developed the first\nautomated pipeline that scales up multimodal data by generating multigranular\nvisual and textual annotations in the form of image-ROI-description triplets\nwithout the need for any paired text descriptions. Specifically, data from over\n30 different sources have been collected, preprocessed, and grounded using\ndomain-specific expert models to identify ROIs related to abnormal regions. We\nthen build a comprehensive knowledge base and prompt multimodal large language\nmodels to perform retrieval-augmented generation with the identified ROIs as\nguidance, resulting in multigranular textual descriptions. Compared to existing\ndatasets, MedTrinity-25M provides the most enriched annotations, supporting a\ncomprehensive range of multimodal tasks such as captioning and report\ngeneration, as well as vision-centric tasks like classification and\nsegmentation. We propose LLaVA-Tri by pretraining LLaVA on MedTrinity-25M,\nachieving state-of-the-art performance on VQA-RAD, SLAKE, and PathVQA,\nsurpassing representative SOTA multimodal large language models. Furthermore,\nMedTrinity-25M can also be utilized to support large-scale pre-training of\nmultimodal medical AI models, contributing to the development of future\nfoundation models in the medical domain. We will make our dataset available.\n","authors":["Yunfei Xie","Ce Zhou","Lang Gao","Juncheng Wu","Xianhang Li","Hong-Yu Zhou","Sheng Liu","Lei Xing","James Zou","Cihang Xie","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02900v3.pdf","comment":"The dataset is publicly available at\n  https://yunfeixie233.github.io/MedTrinity-25M/. Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2203.07861v3","updated":"2025-07-10T08:29:38Z","published":"2022-03-14T15:22:20Z","title":"Don't Get Me Wrong: How to Apply Deep Visual Interpretations to Time\n  Series","summary":"  The correct interpretation of convolutional models is a hard problem for time\nseries data. While saliency methods promise visual validation of predictions\nfor image and language processing, they fall short when applied to time series.\nThese tend to be less intuitive and represent highly diverse data, such as the\ntool-use time series dataset. Furthermore, saliency methods often generate\nvaried, conflicting explanations, complicating the reliability of these\nmethods. Consequently, a rigorous objective assessment is necessary to\nestablish trust in them. This paper investigates saliency methods on time\nseries data to formulate recommendations for interpreting convolutional models\nand implements them on the tool-use time series problem. To achieve this, we\nfirst employ nine gradient-, propagation-, or perturbation-based post-hoc\nsaliency methods across six varied and complex real-world datasets. Next, we\nevaluate these methods using five independent metrics to generate\nrecommendations. Subsequently, we implement a case study focusing on tool-use\ntime series using convolutional classification models. Our results validate our\nrecommendations that indicate that none of the saliency methods consistently\noutperforms others on all metrics, while some are sometimes ahead. Our insights\nand step-by-step guidelines allow experts to choose suitable saliency methods\nfor a given model and dataset.\n","authors":["Christoffer Loeffler","Wei-Cheng Lai","Bjoern Eskofier","Dario Zanca","Lukas Schmidt","Christopher Mutschler"],"pdf_url":"https://arxiv.org/pdf/2203.07861v3.pdf","comment":"48 pages, 12 figues, 7 tables, 6 algorithms"},{"id":"http://arxiv.org/abs/2507.07527v1","updated":"2025-07-10T08:19:34Z","published":"2025-07-10T08:19:34Z","title":"MAPEX: Modality-Aware Pruning of Experts for Remote Sensing Foundation\n  Models","summary":"  Remote sensing data is commonly used for tasks such as flood mapping,\nwildfire detection, or land-use studies. For each task, scientists carefully\nchoose appropriate modalities or leverage data from purpose-built instruments.\nRecent work on remote sensing foundation models pre-trains computer vision\nmodels on large amounts of remote sensing data. These large-scale models tend\nto focus on specific modalities, often optical RGB or multispectral data. For\nmany important applications, this introduces a mismatch between the application\nmodalities and the pre-training data. Moreover, the large size of foundation\nmodels makes them expensive and difficult to fine-tune on typically small\ndatasets for each task. We address this mismatch with MAPEX, a remote sensing\nfoundation model based on mixture-of-modality experts. MAPEX is pre-trained on\nmulti-modal remote sensing data with a novel modality-conditioned token routing\nmechanism that elicits modality-specific experts. To apply the model on a\nspecific task, we propose a modality aware pruning technique, which only\nretains experts specialized for the task modalities. This yields efficient\nmodality-specific models while simplifying fine-tuning and deployment for the\nmodalities of interest. We experimentally validate MAPEX on diverse remote\nsensing datasets and show strong performance compared to fully supervised\ntraining and state-of-the-art remote sensing foundation models. Code is\navailable at https://github.com/HSG-AIML/MAPEX.\n","authors":["Joelle Hanna","Linus Scheibenreif","Damian Borth"],"pdf_url":"https://arxiv.org/pdf/2507.07527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07521v1","updated":"2025-07-10T08:11:46Z","published":"2025-07-10T08:11:46Z","title":"Spline Deformation Field","summary":"  Trajectory modeling of dense points usually employs implicit deformation\nfields, represented as neural networks that map coordinates to relate canonical\nspatial positions to temporal offsets. However, the inductive biases inherent\nin neural networks can hinder spatial coherence in ill-posed scenarios. Current\nmethods focus either on enhancing encoding strategies for deformation fields,\noften resulting in opaque and less intuitive models, or adopt explicit\ntechniques like linear blend skinning, which rely on heuristic-based node\ninitialization. Additionally, the potential of implicit representations for\ninterpolating sparse temporal signals remains under-explored. To address these\nchallenges, we propose a spline-based trajectory representation, where the\nnumber of knots explicitly determines the degrees of freedom. This approach\nenables efficient analytical derivation of velocities, preserving spatial\ncoherence and accelerations, while mitigating temporal fluctuations. To model\nknot characteristics in both spatial and temporal domains, we introduce a novel\nlow-rank time-variant spatial encoding, replacing conventional coupled\nspatiotemporal techniques. Our method demonstrates superior performance in\ntemporal interpolation for fitting continuous fields with sparse inputs.\nFurthermore, it achieves competitive dynamic scene reconstruction quality\ncompared to state-of-the-art methods while enhancing motion coherence without\nrelying on linear blend skinning or as-rigid-as-possible constraints.\n","authors":["Mingyang Song","Yang Zhang","Marko Mihajlovic","Siyu Tang","Markus Gross","Tunç Ozan Aydın"],"pdf_url":"https://arxiv.org/pdf/2507.07521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07519v1","updated":"2025-07-10T08:07:59Z","published":"2025-07-10T08:07:59Z","title":"MUVOD: A Novel Multi-view Video Object Segmentation Dataset and A\n  Benchmark for 3D Segmentation","summary":"  The application of methods based on Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3D GS) have steadily gained popularity in the field of 3D\nobject segmentation in static scenes. These approaches demonstrate efficacy in\na range of 3D scene understanding and editing tasks. Nevertheless, the 4D\nobject segmentation of dynamic scenes remains an underexplored field due to the\nabsence of a sufficiently extensive and accurately labelled multi-view video\ndataset. In this paper, we present MUVOD, a new multi-view video dataset for\ntraining and evaluating object segmentation in reconstructed real-world\nscenarios. The 17 selected scenes, describing various indoor or outdoor\nactivities, are collected from different sources of datasets originating from\nvarious types of camera rigs. Each scene contains a minimum of 9 views and a\nmaximum of 46 views. We provide 7830 RGB images (30 frames per video) with\ntheir corresponding segmentation mask in 4D motion, meaning that any object of\ninterest in the scene could be tracked across temporal frames of a given view\nor across different views belonging to the same camera rig. This dataset, which\ncontains 459 instances of 73 categories, is intended as a basic benchmark for\nthe evaluation of multi-view video segmentation methods. We also present an\nevaluation metric and a baseline segmentation approach to encourage and\nevaluate progress in this evolving field. Additionally, we propose a new\nbenchmark for 3D object segmentation task with a subset of annotated multi-view\nimages selected from our MUVOD dataset. This subset contains 50 objects of\ndifferent conditions in different scenarios, providing a more comprehensive\nanalysis of state-of-the-art 3D object segmentation methods. Our proposed MUVOD\ndataset is available at https://volumetric-repository.labs.b-com.com/#/muvod.\n","authors":["Bangning Wei","Joshua Maraval","Meriem Outtas","Kidiyo Kpalma","Nicolas Ramin","Lu Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.07519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19557v2","updated":"2025-07-10T08:06:45Z","published":"2025-03-25T11:23:34Z","title":"Dance Like a Chicken: Low-Rank Stylization for Human Motion Diffusion","summary":"  Text-to-motion generative models span a wide range of 3D human actions but\nstruggle with nuanced stylistic attributes such as a \"Chicken\" style. Due to\nthe scarcity of style-specific data, existing approaches pull the generative\nprior towards a reference style, which often results in out-of-distribution low\nquality generations. In this work, we introduce LoRA-MDM, a lightweight\nframework for motion stylization that generalizes to complex actions while\nmaintaining editability. Our key insight is that adapting the generative prior\nto include the style, while preserving its overall distribution, is more\neffective than modifying each individual motion during generation. Building on\nthis idea, LoRA-MDM learns to adapt the prior to include the reference style\nusing only a few samples. The style can then be used in the context of\ndifferent textual prompts for generation. The low-rank adaptation shifts the\nmotion manifold in a semantically meaningful way, enabling realistic style\ninfusion even for actions not present in the reference samples. Moreover,\npreserving the distribution structure enables advanced operations such as style\nblending and motion editing. We compare LoRA-MDM to state-of-the-art stylized\nmotion generation methods and demonstrate a favorable balance between text\nfidelity and style consistency.\n","authors":["Haim Sawdayee","Chuan Guo","Guy Tevet","Bing Zhou","Jian Wang","Amit H. Bermano"],"pdf_url":"https://arxiv.org/pdf/2503.19557v2.pdf","comment":"Project page at https://haimsaw.github.io/LoRA-MDM/"},{"id":"http://arxiv.org/abs/2507.07515v1","updated":"2025-07-10T08:02:01Z","published":"2025-07-10T08:02:01Z","title":"GGMotion: Group Graph Dynamics-Kinematics Networks for Human Motion\n  Prediction","summary":"  Human motion is a continuous physical process in 3D space, governed by\ncomplex dynamic and kinematic constraints. Existing methods typically represent\nthe human pose as an abstract graph structure, neglecting the intrinsic\nphysical dependencies between joints, which increases learning difficulty and\nmakes the model prone to generating unrealistic motions. In this paper, we\npropose GGMotion, a group graph dynamics-kinematics network that models human\ntopology in groups to better leverage dynamics and kinematics priors. To\npreserve the geometric equivariance in 3D space, we propose a novel radial\nfield for the graph network that captures more comprehensive spatio-temporal\ndependencies by aggregating joint features through spatial and temporal edges.\nInter-group and intra-group interaction modules are employed to capture the\ndependencies of joints at different scales. Combined with equivariant\nmultilayer perceptrons (MLP), joint position features are updated in each group\nthrough parallelized dynamics-kinematics propagation to improve physical\nplausibility. Meanwhile, we introduce an auxiliary loss to supervise motion\npriors during training. Extensive experiments on three standard benchmarks,\nincluding Human3.6M, CMU-Mocap, and 3DPW, demonstrate the effectiveness and\nsuperiority of our approach, achieving a significant performance margin in\nshort-term motion prediction. The code is available at\nhttps://github.com/inkcat520/GGMotion.git.\n","authors":["Shuaijin Wan","Huaijiang Sun"],"pdf_url":"https://arxiv.org/pdf/2507.07515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07510v1","updated":"2025-07-10T07:57:30Z","published":"2025-07-10T07:57:30Z","title":"Divergence Minimization Preference Optimization for Diffusion Model\n  Alignment","summary":"  Diffusion models have achieved remarkable success in generating realistic and\nversatile images from text prompts. Inspired by the recent advancements of\nlanguage models, there is an increasing interest in further improving the\nmodels by aligning with human preferences. However, we investigate alignment\nfrom a divergence minimization perspective and reveal that existing preference\noptimization methods are typically trapped in suboptimal mean-seeking\noptimization. In this paper, we introduce Divergence Minimization Preference\nOptimization (DMPO), a novel and principled method for aligning diffusion\nmodels by minimizing reverse KL divergence, which asymptotically enjoys the\nsame optimization direction as original RL. We provide rigorous analysis to\njustify the effectiveness of DMPO and conduct comprehensive experiments to\nvalidate its empirical strength across both human evaluations and automatic\nmetrics. Our extensive results show that diffusion models fine-tuned with DMPO\ncan consistently outperform or match existing techniques, specifically\noutperforming all existing diffusion alignment baselines by at least 64.6% in\nPickScore across all evaluation datasets, demonstrating the method's\nsuperiority in aligning generative behavior with desired outputs. Overall, DMPO\nunlocks a robust and elegant pathway for preference alignment, bridging\nprincipled theory with practical performance in diffusion models.\n","authors":["Binxu Li","Minkai Xu","Meihua Dang","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2507.07510v1.pdf","comment":"24 pages, 8 figures"},{"id":"http://arxiv.org/abs/2506.18939v2","updated":"2025-07-10T07:42:46Z","published":"2025-06-22T15:40:01Z","title":"Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal\n  Prediction","summary":"  Training urban spatio-temporal foundation models that generalize well across\ndiverse regions and cities is critical for deploying urban services in unseen\nor data-scarce regions. Recent studies have typically focused on fusing\ncross-domain spatio-temporal data to train unified Transformer-based models.\nHowever, these models suffer from quadratic computational complexity and high\nmemory overhead, limiting their scalability and practical deployment. Inspired\nby the efficiency of Mamba, a state space model with linear time complexity, we\nexplore its potential for efficient urban spatio-temporal prediction. However,\ndirectly applying Mamba as a spatio-temporal backbone leads to negative\ntransfer and severe performance degradation. This is primarily due to\nspatio-temporal heterogeneity and the recursive mechanism of Mamba's hidden\nstate updates, which limit cross-domain generalization. To overcome these\nchallenges, we propose Damba-ST, a novel domain-adaptive Mamba-based model for\nefficient urban spatio-temporal prediction. Damba-ST retains Mamba's linear\ncomplexity advantage while significantly enhancing its adaptability to\nheterogeneous domains. Specifically, we introduce two core innovations: (1) a\ndomain-adaptive state space model that partitions the latent representation\nspace into a shared subspace for learning cross-domain commonalities and\nindependent, domain-specific subspaces for capturing intra-domain\ndiscriminative features; (2) three distinct Domain Adapters, which serve as\ndomain-aware proxies to bridge disparate domain distributions and facilitate\nthe alignment of cross-domain commonalities. Extensive experiments demonstrate\nthe generalization and efficiency of Damba-ST. It achieves state-of-the-art\nperformance on prediction tasks and demonstrates strong zero-shot\ngeneralization, enabling seamless deployment in new urban environments without\nextensive retraining or fine-tuning.\n","authors":["Rui An","Yifeng Zhang","Ziran Liang","Wenqi Fan","Yuxuan Liang","Xuequn Shang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2506.18939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07496v1","updated":"2025-07-10T07:31:31Z","published":"2025-07-10T07:31:31Z","title":"Semi-supervised learning and integration of multi-sequence MR-images for\n  carotid vessel wall and plaque segmentation","summary":"  The analysis of carotid arteries, particularly plaques, in multi-sequence\nMagnetic Resonance Imaging (MRI) data is crucial for assessing the risk of\natherosclerosis and ischemic stroke. In order to evaluate metrics and radiomic\nfeatures, quantifying the state of atherosclerosis, accurate segmentation is\nimportant. However, the complex morphology of plaques and the scarcity of\nlabeled data poses significant challenges. In this work, we address these\nproblems and propose a semi-supervised deep learning-based approach designed to\neffectively integrate multi-sequence MRI data for the segmentation of carotid\nartery vessel wall and plaque. The proposed algorithm consists of two networks:\na coarse localization model identifies the region of interest guided by some\nprior knowledge on the position and number of carotid arteries, followed by a\nfine segmentation model for precise delineation of vessel walls and plaques. To\neffectively integrate complementary information across different MRI sequences,\nwe investigate different fusion strategies and introduce a multi-level\nmulti-sequence version of U-Net architecture. To address the challenges of\nlimited labeled data and the complexity of carotid artery MRI, we propose a\nsemi-supervised approach that enforces consistency under various input\ntransformations. Our approach is evaluated on 52 patients with\narteriosclerosis, each with five MRI sequences. Comprehensive experiments\ndemonstrate the effectiveness of our approach and emphasize the role of fusion\npoint selection in U-Net-based architectures. To validate the accuracy of our\nresults, we also include an expert-based assessment of model performance. Our\nfindings highlight the potential of fusion strategies and semi-supervised\nlearning for improving carotid artery segmentation in data-limited MRI\napplications.\n","authors":["Marie-Christine Pali","Christina Schwaiger","Malik Galijasevic","Valentin K. Ladenhauf","Stephanie Mangesius","Elke R. Gizewski"],"pdf_url":"https://arxiv.org/pdf/2507.07496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07487v1","updated":"2025-07-10T07:16:00Z","published":"2025-07-10T07:16:00Z","title":"Driving by Hybrid Navigation: An Online HD-SD Map Association Framework\n  and Benchmark for Autonomous Vehicles","summary":"  Autonomous vehicles rely on global standard-definition (SD) maps for\nroad-level route planning and online local high-definition (HD) maps for\nlane-level navigation. However, recent work concentrates on construct online HD\nmaps, often overlooking the association of global SD maps with online HD maps\nfor hybrid navigation, making challenges in utilizing online HD maps in the\nreal world. Observing the lack of the capability of autonomous vehicles in\nnavigation, we introduce \\textbf{O}nline \\textbf{M}ap \\textbf{A}ssociation, the\nfirst benchmark for the association of hybrid navigation-oriented online maps,\nwhich enhances the planning capabilities of autonomous vehicles. Based on\nexisting datasets, the OMA contains 480k of roads and 260k of lane paths and\nprovides the corresponding metrics to evaluate the performance of the model.\nAdditionally, we propose a novel framework, named Map Association Transformer,\nas the baseline method, using path-aware attention and spatial attention\nmechanisms to enable the understanding of geometric and topological\ncorrespondences. The code and dataset can be accessed at\nhttps://github.com/WallelWan/OMA-MAT.\n","authors":["Jiaxu Wan","Xu Wang","Mengwei Xie","Xinyuan Chang","Xinran Liu","Zheng Pan","Mu Xu","Ding Yuan"],"pdf_url":"https://arxiv.org/pdf/2507.07487v1.pdf","comment":"23 pages, 10 figures, 9 tables"},{"id":"http://arxiv.org/abs/2507.07485v1","updated":"2025-07-10T07:13:22Z","published":"2025-07-10T07:13:22Z","title":"Resolving Token-Space Gradient Conflicts: Token Space Manipulation for\n  Transformer-Based Multi-Task Learning","summary":"  Multi-Task Learning (MTL) enables multiple tasks to be learned within a\nshared network, but differences in objectives across tasks can cause negative\ntransfer, where the learning of one task degrades another task's performance.\nWhile pre-trained transformers significantly improve MTL performance, their\nfixed network capacity and rigid structure limit adaptability. Previous dynamic\nnetwork architectures attempt to address this but are inefficient as they\ndirectly convert shared parameters into task-specific ones. We propose Dynamic\nToken Modulation and Expansion (DTME-MTL), a framework applicable to any\ntransformer-based MTL architecture. DTME-MTL enhances adaptability and reduces\noverfitting by identifying gradient conflicts in token space and applying\nadaptive solutions based on conflict type. Unlike prior methods that mitigate\nnegative transfer by duplicating network parameters, DTME-MTL operates entirely\nin token space, enabling efficient adaptation without excessive parameter\ngrowth. Extensive experiments demonstrate that DTME-MTL consistently improves\nmulti-task performance with minimal computational overhead, offering a scalable\nand effective solution for enhancing transformer-based MTL models.\n","authors":["Wooseong Jeong","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2507.07485v1.pdf","comment":"Accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2507.07483v1","updated":"2025-07-10T07:11:33Z","published":"2025-07-10T07:11:33Z","title":"Temporal Unlearnable Examples: Preventing Personal Video Data from\n  Unauthorized Exploitation by Object Tracking","summary":"  With the rise of social media, vast amounts of user-uploaded videos (e.g.,\nYouTube) are utilized as training data for Visual Object Tracking (VOT).\nHowever, the VOT community has largely overlooked video data-privacy issues, as\nmany private videos have been collected and used for training commercial models\nwithout authorization. To alleviate these issues, this paper presents the first\ninvestigation on preventing personal video data from unauthorized exploitation\nby deep trackers. Existing methods for preventing unauthorized data use\nprimarily focus on image-based tasks (e.g., image classification), directly\napplying them to videos reveals several limitations, including inefficiency,\nlimited effectiveness, and poor generalizability. To address these issues, we\npropose a novel generative framework for generating Temporal Unlearnable\nExamples (TUEs), and whose efficient computation makes it scalable for usage on\nlarge-scale video datasets. The trackers trained w/ TUEs heavily rely on\nunlearnable noises for temporal matching, ignoring the original data structure\nand thus ensuring training video data-privacy. To enhance the effectiveness of\nTUEs, we introduce a temporal contrastive loss, which further corrupts the\nlearning of existing trackers when using our TUEs for training. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\nin video data-privacy protection, with strong transferability across VOT\nmodels, datasets, and temporal matching tasks.\n","authors":["Qiangqiang Wu","Yi Yu","Chenqi Kong","Ziquan Liu","Jia Wan","Haoliang Li","Alex C. Kot","Antoni B. Chan"],"pdf_url":"https://arxiv.org/pdf/2507.07483v1.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2503.05689v4","updated":"2025-07-10T07:11:30Z","published":"2025-03-07T18:52:08Z","title":"GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving","summary":"  We propose GoalFlow, an end-to-end autonomous driving method for generating\nhigh-quality multimodal trajectories. In autonomous driving scenarios, there is\nrarely a single suitable trajectory. Recent methods have increasingly focused\non modeling multimodal trajectory distributions. However, they suffer from\ntrajectory selection complexity and reduced trajectory quality due to high\ntrajectory divergence and inconsistencies between guidance and scene\ninformation. To address these issues, we introduce GoalFlow, a novel method\nthat effectively constrains the generative process to produce high-quality,\nmultimodal trajectories. To resolve the trajectory divergence problem inherent\nin diffusion-based methods, GoalFlow constrains the generated trajectories by\nintroducing a goal point. GoalFlow establishes a novel scoring mechanism that\nselects the most appropriate goal point from the candidate points based on\nscene information. Furthermore, GoalFlow employs an efficient generative\nmethod, Flow Matching, to generate multimodal trajectories, and incorporates a\nrefined scoring mechanism to select the optimal trajectory from the candidates.\nOur experimental results, validated on the Navsim\\cite{Dauner2024_navsim},\ndemonstrate that GoalFlow achieves state-of-the-art performance, delivering\nrobust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS\nof 90.3, significantly surpassing other methods. Compared with other\ndiffusion-policy-based methods, our approach requires only a single denoising\nstep to obtain excellent performance. The code is available at\nhttps://github.com/YvanYin/GoalFlow.\n","authors":["Zebin Xing","Xingyu Zhang","Yang Hu","Bo Jiang","Tong He","Qian Zhang","Xiaoxiao Long","Wei Yin"],"pdf_url":"https://arxiv.org/pdf/2503.05689v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09265v2","updated":"2025-07-10T07:07:53Z","published":"2025-04-12T15:58:02Z","title":"Mixture of Group Experts for Learning Invariant Representations","summary":"  Sparsely activated Mixture-of-Experts (MoE) models effectively increase the\nnumber of parameters while maintaining consistent computational costs per\ntoken. However, vanilla MoE models often suffer from limited diversity and\nspecialization among experts, constraining their performance and scalability,\nespecially as the number of experts increases. In this paper, we present a\nnovel perspective on vanilla MoE with top-$k$ routing inspired by sparse\nrepresentation. This allows us to bridge established theoretical insights from\nsparse representation into MoE models. Building on this foundation, we propose\na group sparse regularization approach for the input of top-$k$ routing, termed\nMixture of Group Experts (MoGE). MoGE indirectly regularizes experts by\nimposing structural constraints on the routing inputs, while preserving the\noriginal MoE architecture. Furthermore, we organize the routing input into a 2D\ntopographic map, spatially grouping neighboring elements. This structure\nenables MoGE to capture representations invariant to minor transformations,\nthereby significantly enhancing expert diversity and specialization.\nComprehensive evaluations across various Transformer models for image\nclassification and language modeling tasks demonstrate that MoGE substantially\noutperforms its MoE counterpart, with minimal additional memory and computation\noverhead. Our approach provides a simple yet effective solution to scale the\nnumber of experts and reduce redundancy among them. The source code is included\nin the supplementary material and will be publicly released.\n","authors":["Lei Kang","Jia Li","Mi Tian","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2504.09265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18438v2","updated":"2025-07-10T07:01:22Z","published":"2025-03-24T08:40:20Z","title":"ReconDreamer++: Harmonizing Generative and Reconstructive Models for\n  Driving Scene Representation","summary":"  Combining reconstruction models with generative models has emerged as a\npromising paradigm for closed-loop simulation in autonomous driving. For\nexample, ReconDreamer has demonstrated remarkable success in rendering\nlarge-scale maneuvers. However, a significant gap remains between the generated\ndata and real-world sensor observations, particularly in terms of fidelity for\nstructured elements, such as the ground surface. To address these challenges,\nwe propose ReconDreamer++, an enhanced framework that significantly improves\nthe overall rendering quality by mitigating the domain gap and refining the\nrepresentation of the ground surface. Specifically, ReconDreamer++ introduces\nthe Novel Trajectory Deformable Network (NTDNet), which leverages learnable\nspatial deformation mechanisms to bridge the domain gap between synthesized\nnovel views and original sensor observations. Moreover, for structured elements\nsuch as the ground surface, we preserve geometric prior knowledge in 3D\nGaussians, and the optimization process focuses on refining appearance\nattributes while preserving the underlying geometric structure. Experimental\nevaluations conducted on multiple datasets (Waymo, nuScenes, PandaSet, and\nEUVS) confirm the superior performance of ReconDreamer++. Specifically, on\nWaymo, ReconDreamer++ achieves performance comparable to Street Gaussians for\nthe original trajectory while significantly outperforming ReconDreamer on novel\ntrajectories. In particular, it achieves substantial improvements, including a\n6.1% increase in NTA-IoU, a 23. 0% improvement in FID, and a remarkable 4.5%\ngain in the ground surface metric NTL-IoU, highlighting its effectiveness in\naccurately reconstructing structured elements such as the road surface.\n","authors":["Guosheng Zhao","Xiaofeng Wang","Chaojun Ni","Zheng Zhu","Wenkang Qin","Guan Huang","Xingang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.18438v2.pdf","comment":"Project Page: https://recondreamer-plus.github.io/"},{"id":"http://arxiv.org/abs/2507.02899v2","updated":"2025-07-10T07:00:04Z","published":"2025-06-23T04:29:08Z","title":"Learning to Generate Vectorized Maps at Intersections with Multiple\n  Roadside Cameras","summary":"  Vectorized maps are indispensable for precise navigation and the safe\noperation of autonomous vehicles. Traditional methods for constructing these\nmaps fall into two categories: offline techniques, which rely on expensive,\nlabor-intensive LiDAR data collection and manual annotation, and online\napproaches that use onboard cameras to reduce costs but suffer from limited\nperformance, especially at complex intersections. To bridge this gap, we\nintroduce MRC-VMap, a cost-effective, vision-centric, end-to-end neural network\ndesigned to generate high-definition vectorized maps directly at intersections.\nLeveraging existing roadside surveillance cameras, MRC-VMap directly converts\ntime-aligned, multi-directional images into vectorized map representations.\nThis integrated solution lowers the need for additional intermediate\nmodules--such as separate feature extraction and Bird's-Eye View (BEV)\nconversion steps--thus reducing both computational overhead and error\npropagation. Moreover, the use of multiple camera views enhances mapping\ncompleteness, mitigates occlusions, and provides robust performance under\npractical deployment constraints. Extensive experiments conducted on 4,000\nintersections across 4 major metropolitan areas in China demonstrate that\nMRC-VMap not only outperforms state-of-the-art online methods but also achieves\naccuracy comparable to high-cost LiDAR-based approaches, thereby offering a\nscalable and efficient solution for modern autonomous navigation systems.\n","authors":["Quanxin Zheng","Miao Fan","Shengtong Xu","Linghe Kong","Haoyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2507.02899v2.pdf","comment":"Accepted by IROS'25"},{"id":"http://arxiv.org/abs/2507.02398v2","updated":"2025-07-10T06:52:35Z","published":"2025-07-03T07:49:55Z","title":"Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake\n  Video Detection","summary":"  We introduce a deepfake video detection approach that exploits pixel-wise\ntemporal inconsistencies, which traditional spatial frequency-based detectors\noften overlook. Traditional detectors represent temporal information merely by\nstacking spatial frequency spectra across frames, resulting in the failure to\ndetect temporal artifacts in the pixel plane. Our approach performs a 1D\nFourier transform on the time axis for each pixel, extracting features highly\nsensitive to temporal inconsistencies, especially in areas prone to unnatural\nmovements. To precisely locate regions containing the temporal artifacts, we\nintroduce an attention proposal module trained in an end-to-end manner.\nAdditionally, our joint transformer module effectively integrates pixel-wise\ntemporal frequency features with spatio-temporal context features, expanding\nthe range of detectable forgery artifacts. Our framework represents a\nsignificant advancement in deepfake video detection, providing robust\nperformance across diverse and challenging detection scenarios.\n","authors":["Taehoon Kim","Jongwook Choi","Yonghyun Jeong","Haeun Noh","Jaejun Yoo","Seungryul Baek","Jongwon Choi"],"pdf_url":"https://arxiv.org/pdf/2507.02398v2.pdf","comment":"accepted by iccv 2025. code is will be available at\n  https://github.com/rama0126/PwTF-DVD"},{"id":"http://arxiv.org/abs/2506.21513v2","updated":"2025-07-10T06:36:05Z","published":"2025-06-26T17:37:18Z","title":"GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and\n  Identity-Specific Adaptation","summary":"  Creating high-quality, generalizable speech-driven 3D talking heads remains a\npersistent challenge. Previous methods achieve satisfactory results for fixed\nviewpoints and small-scale audio variations, but they struggle with large head\nrotations and out-of-distribution (OOD) audio. Moreover, they are constrained\nby the need for time-consuming, identity-specific training. We believe the core\nissue lies in the lack of sufficient 3D priors, which limits the extrapolation\ncapabilities of synthesized talking heads. To address this, we propose\nGGTalker, which synthesizes talking heads through a combination of\ngeneralizable priors and identity-specific adaptation. We introduce a two-stage\nPrior-Adaptation training strategy to learn Gaussian head priors and adapt to\nindividual characteristics. We train Audio-Expression and Expression-Visual\npriors to capture the universal patterns of lip movements and the general\ndistribution of head textures. During the Customized Adaptation, individual\nspeaking styles and texture details are precisely modeled. Additionally, we\nintroduce a color MLP to generate fine-grained, motion-aligned textures and a\nBody Inpainter to blend rendered results with the background, producing\nindistinguishable, photorealistic video frames. Comprehensive experiments show\nthat GGTalker achieves state-of-the-art performance in rendering quality, 3D\nconsistency, lip-sync accuracy, and training efficiency.\n","authors":["Wentao Hu","Shunkai Li","Ziqiao Peng","Haoxian Zhang","Fan Shi","Xiaoqiang Liu","Pengfei Wan","Di Zhang","Hui Tian"],"pdf_url":"https://arxiv.org/pdf/2506.21513v2.pdf","comment":"ICCV 2025, Project page: https://vincenthu19.github.io/GGTalker/"},{"id":"http://arxiv.org/abs/2507.07465v1","updated":"2025-07-10T06:35:03Z","published":"2025-07-10T06:35:03Z","title":"SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene\n  Reconstruction","summary":"  Current 4D Gaussian frameworks for dynamic scene reconstruction deliver\nimpressive visual fidelity and rendering speed, however, the inherent trade-off\nbetween storage costs and the ability to characterize complex physical motions\nsignificantly limits the practical application of these methods. To tackle\nthese problems, we propose SD-GS, a compact and efficient dynamic Gaussian\nsplatting framework for complex dynamic scene reconstruction, featuring two key\ncontributions. First, we introduce a deformable anchor grid, a hierarchical and\nmemory-efficient scene representation where each anchor point derives multiple\n3D Gaussians in its local spatiotemporal region and serves as the geometric\nbackbone of the 3D scene. Second, to enhance modeling capability for complex\nmotions, we present a deformation-aware densification strategy that adaptively\ngrows anchors in under-reconstructed high-dynamic regions while reducing\nredundancy in static areas, achieving superior visual quality with fewer\nanchors. Experimental results demonstrate that, compared to state-of-the-art\nmethods, SD-GS achieves an average of 60\\% reduction in model size and an\naverage of 100\\% improvement in FPS, significantly enhancing computational\nefficiency while maintaining or even surpassing visual quality.\n","authors":["Wei Yao","Shuzhao Xie","Letian Li","Weixiang Zhang","Zhixin Lai","Shiqi Dai","Ke Zhang","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2507.07465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07464v1","updated":"2025-07-10T06:31:26Z","published":"2025-07-10T06:31:26Z","title":"Degradation-Agnostic Statistical Facial Feature Transformation for Blind\n  Face Restoration in Adverse Weather Conditions","summary":"  With the increasing deployment of intelligent CCTV systems in outdoor\nenvironments, there is a growing demand for face recognition systems optimized\nfor challenging weather conditions. Adverse weather significantly degrades\nimage quality, which in turn reduces recognition accuracy. Although recent face\nimage restoration (FIR) models based on generative adversarial networks (GANs)\nand diffusion models have shown progress, their performance remains limited due\nto the lack of dedicated modules that explicitly address weather-induced\ndegradations. This leads to distorted facial textures and structures. To\naddress these limitations, we propose a novel GAN-based blind FIR framework\nthat integrates two key components: local Statistical Facial Feature\nTransformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The\nlocal SFFT module enhances facial structure and color fidelity by aligning the\nlocal statistical distributions of low-quality (LQ) facial regions with those\nof high-quality (HQ) counterparts. Complementarily, the DAFE module enables\nrobust statistical facial feature extraction under adverse weather conditions\nby aligning LQ and HQ encoder representations, thereby making the restoration\nprocess adaptive to severe weather-induced degradations. Experimental results\ndemonstrate that the proposed degradation-agnostic SFFT model outperforms\nexisting state-of-the-art FIR methods based on GAN and diffusion models,\nparticularly in suppressing texture distortions and accurately reconstructing\nfacial structures. Furthermore, both the SFFT and DAFE modules are empirically\nvalidated in enhancing structural fidelity and perceptual quality in face\nrestoration under challenging weather scenarios.\n","authors":["Chang-Hwan Son"],"pdf_url":"https://arxiv.org/pdf/2507.07464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07460v1","updated":"2025-07-10T06:23:35Z","published":"2025-07-10T06:23:35Z","title":"Objectomaly: Objectness-Aware Refinement for OoD Segmentation with\n  Structural Consistency and Boundary Precision","summary":"  Out-of-Distribution (OoD) segmentation is critical for safety-sensitive\napplications like autonomous driving. However, existing mask-based methods\noften suffer from boundary imprecision, inconsistent anomaly scores within\nobjects, and false positives from background noise. We propose\n\\textbf{\\textit{Objectomaly}}, an objectness-aware refinement framework that\nincorporates object-level priors. Objectomaly consists of three stages: (1)\nCoarse Anomaly Scoring (CAS) using an existing OoD backbone, (2)\nObjectness-Aware Score Calibration (OASC) leveraging SAM-generated instance\nmasks for object-level score normalization, and (3) Meticulous Boundary\nPrecision (MBP) applying Laplacian filtering and Gaussian smoothing for contour\nrefinement. Objectomaly achieves state-of-the-art performance on key OoD\nsegmentation benchmarks, including SMIYC AnomalyTrack/ObstacleTrack and\nRoadAnomaly, improving both pixel-level (AuPRC up to 96.99, FPR$_{95}$ down to\n0.07) and component-level (F1$-$score up to 83.44) metrics. Ablation studies\nand qualitative results on real-world driving videos further validate the\nrobustness and generalizability of our method. Code will be released upon\npublication.\n","authors":["Jeonghoon Song","Sunghun Kim","Jaegyun Im","Byeongjoon Noh"],"pdf_url":"https://arxiv.org/pdf/2507.07460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08908v3","updated":"2025-07-10T06:20:05Z","published":"2025-06-10T15:35:29Z","title":"SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive\n  Frequency-Aware Skipping","summary":"  Recent studies on Visual Autoregressive (VAR) models have highlighted that\nhigh-frequency components, or later steps, in the generation process contribute\ndisproportionately to inference latency. However, the underlying computational\nredundancy involved in these steps has yet to be thoroughly investigated. In\nthis paper, we conduct an in-depth analysis of the VAR inference process and\nidentify two primary sources of inefficiency: step redundancy and unconditional\nbranch redundancy. To address step redundancy, we propose an automatic\nstep-skipping strategy that selectively omits unnecessary generation steps to\nimprove efficiency. For unconditional branch redundancy, we observe that the\ninformation gap between the conditional and unconditional branches is minimal.\nLeveraging this insight, we introduce unconditional branch replacement, a\ntechnique that bypasses the unconditional branch to reduce computational cost.\nNotably, we observe that the effectiveness of acceleration strategies varies\nsignificantly across different samples. Motivated by this, we propose SkipVAR,\na sample-adaptive framework that leverages frequency information to dynamically\nselect the most suitable acceleration strategy for each instance. To evaluate\nthe role of high-frequency information, we introduce high-variation benchmark\ndatasets that test model sensitivity to fine details. Extensive experiments\nshow SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall\nacceleration and 2.62x speedup on the GenEval benchmark, maintaining model\nquality. These results confirm the effectiveness of frequency-aware,\ntraining-free adaptive acceleration for scalable autoregressive image\ngeneration. Our code is available at https://github.com/fakerone-li/SkipVAR and\nhas been publicly released.\n","authors":["Jiajun Li","Yue Ma","Xinyu Zhang","Qingyan Wei","Songhua Liu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.08908v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16803v4","updated":"2025-07-10T06:16:59Z","published":"2024-07-23T19:06:44Z","title":"C3T: Cross-modal Transfer Through Time for Sensor-based Human Activity\n  Recognition","summary":"  In order to unlock the potential of diverse sensors, we investigate a method\nto transfer knowledge between time-series modalities using a multimodal\n\\textit{temporal} representation space for Human Activity Recognition (HAR).\nSpecifically, we explore the setting where the modality used in testing has no\nlabeled data during training, which we refer to as Unsupervised Modality\nAdaptation (UMA). We categorize existing UMA approaches as Student-Teacher or\nContrastive Alignment methods. These methods typically compress continuous-time\ndata samples into single latent vectors during alignment, inhibiting their\nability to transfer temporal information through real-world temporal\ndistortions. To address this, we introduce Cross-modal Transfer Through Time\n(C3T), which preserves temporal information during alignment to handle dynamic\nsensor data better. C3T achieves this by aligning a set of temporal latent\nvectors across sensing modalities. Our extensive experiments on various\ncamera+IMU datasets demonstrate that C3T outperforms existing methods in UMA by\nat least 8% in accuracy and shows superior robustness to temporal distortions\nsuch as time-shift, misalignment, and dilation. Our findings suggest that C3T\nhas significant potential for developing generalizable models for time-series\nsensor data, opening new avenues for various multimodal applications.\n","authors":["Abhi Kamboj","Anh Duy Nguyen","Minh N. Do"],"pdf_url":"https://arxiv.org/pdf/2407.16803v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07453v1","updated":"2025-07-10T06:12:23Z","published":"2025-07-10T06:12:23Z","title":"Bluish Veil Detection and Lesion Classification using Custom Deep\n  Learnable Layers with Explainable Artificial Intelligence (XAI)","summary":"  Melanoma, one of the deadliest types of skin cancer, accounts for thousands\nof fatalities globally. The bluish, blue-whitish, or blue-white veil (BWV) is a\ncritical feature for diagnosing melanoma, yet research into detecting BWV in\ndermatological images is limited. This study utilizes a non-annotated skin\nlesion dataset, which is converted into an annotated dataset using a proposed\nimaging algorithm based on color threshold techniques on lesion patches and\ncolor palettes. A Deep Convolutional Neural Network (DCNN) is designed and\ntrained separately on three individual and combined dermoscopic datasets, using\ncustom layers instead of standard activation function layers. The model is\ndeveloped to categorize skin lesions based on the presence of BWV. The proposed\nDCNN demonstrates superior performance compared to conventional BWV detection\nmodels across different datasets. The model achieves a testing accuracy of\n85.71% on the augmented PH2 dataset, 95.00% on the augmented ISIC archive\ndataset, 95.05% on the combined augmented (PH2+ISIC archive) dataset, and\n90.00% on the Derm7pt dataset. An explainable artificial intelligence (XAI)\nalgorithm is subsequently applied to interpret the DCNN's decision-making\nprocess regarding BWV detection. The proposed approach, coupled with XAI,\nsignificantly improves the detection of BWV in skin lesions, outperforming\nexisting models and providing a robust tool for early melanoma diagnosis.\n","authors":["M. A. Rasel","Sameem Abdul Kareem","Zhenli Kwan","Shin Shen Yong","Unaizah Obaidellah"],"pdf_url":"https://arxiv.org/pdf/2507.07453v1.pdf","comment":"Accepted version. Published in Computers in Biology and Medicine, 14\n  June 2024. DOI: 10.1016/j.compbiomed.2024.108758"},{"id":"http://arxiv.org/abs/2507.07443v1","updated":"2025-07-10T05:41:17Z","published":"2025-07-10T05:41:17Z","title":"Dual Semantic-Aware Network for Noise Suppressed Ultrasound Video\n  Segmentation","summary":"  Ultrasound imaging is a prevalent diagnostic tool known for its simplicity\nand non-invasiveness. However, its inherent characteristics often introduce\nsubstantial noise, posing considerable challenges for automated lesion or organ\nsegmentation in ultrasound video sequences. To address these limitations, we\npropose the Dual Semantic-Aware Network (DSANet), a novel framework designed to\nenhance noise robustness in ultrasound video segmentation by fostering mutual\nsemantic awareness between local and global features. Specifically, we\nintroduce an Adjacent-Frame Semantic-Aware (AFSA) module, which constructs a\nchannel-wise similarity matrix to guide feature fusion across adjacent frames,\neffectively mitigating the impact of random noise without relying on\npixel-level relationships. Additionally, we propose a Local-and-Global\nSemantic-Aware (LGSA) module that reorganizes and fuses temporal unconditional\nlocal features, which capture spatial details independently at each frame, with\nconditional global features that incorporate temporal context from adjacent\nframes. This integration facilitates multi-level semantic representation,\nsignificantly improving the model's resilience to noise interference. Extensive\nevaluations on four benchmark datasets demonstrate that DSANet substantially\noutperforms state-of-the-art methods in segmentation accuracy. Moreover, since\nour model avoids pixel-level feature dependencies, it achieves significantly\nhigher inference FPS than video-based methods, and even surpasses some\nimage-based models. Code can be found in\n\\href{https://github.com/ZhouL2001/DSANet}{DSANet}\n","authors":["Ling Zhou","Runtian Yuan","Yi Liu","Yuejie Zhang","Rui Feng","Shang Gao"],"pdf_url":"https://arxiv.org/pdf/2507.07443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07435v1","updated":"2025-07-10T05:19:16Z","published":"2025-07-10T05:19:16Z","title":"Towards High-Resolution 3D Anomaly Detection: A Scalable Dataset and\n  Real-Time Framework for Subtle Industrial Defects","summary":"  In industrial point cloud analysis, detecting subtle anomalies demands\nhigh-resolution spatial data, yet prevailing benchmarks emphasize\nlow-resolution inputs. To address this disparity, we propose a scalable\npipeline for generating realistic and subtle 3D anomalies. Employing this\npipeline, we developed MiniShift, the inaugural high-resolution 3D anomaly\ndetection dataset, encompassing 2,577 point clouds, each with 500,000 points\nand anomalies occupying less than 1\\% of the total. We further introduce\nSimple3D, an efficient framework integrating Multi-scale Neighborhood\nDescriptors (MSND) and Local Feature Spatial Aggregation (LFSA) to capture\nintricate geometric details with minimal computational overhead, achieving\nreal-time inference exceeding 20 fps. Extensive evaluations on MiniShift and\nestablished benchmarks demonstrate that Simple3D surpasses state-of-the-art\nmethods in both accuracy and speed, highlighting the pivotal role of\nhigh-resolution data and effective feature aggregation in advancing practical\n3D anomaly detection.\n","authors":["Yuqi Cheng","Yihan Sun","Hui Zhang","Weiming Shen","Yunkang Cao"],"pdf_url":"https://arxiv.org/pdf/2507.07435v1.pdf","comment":"14 pages, 8figures"},{"id":"http://arxiv.org/abs/2507.07424v1","updated":"2025-07-10T04:31:56Z","published":"2025-07-10T04:31:56Z","title":"Corvid: Improving Multimodal Large Language Models Towards\n  Chain-of-Thought Reasoning","summary":"  Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated exceptional performance in multimodal perception and\nunderstanding. However, leading open-source MLLMs exhibit significant\nlimitations in complex and structured reasoning, particularly in tasks\nrequiring deep reasoning for decision-making and problem-solving. In this work,\nwe present Corvid, an MLLM with enhanced chain-of-thought (CoT) reasoning\ncapabilities. Architecturally, Corvid incorporates a hybrid vision encoder for\ninformative visual representation and a meticulously designed connector\n(GateMixer) to facilitate cross-modal alignment. To enhance Corvid's CoT\nreasoning capabilities, we introduce MCoT-Instruct-287K, a high-quality\nmultimodal CoT instruction-following dataset, refined and standardized from\ndiverse public reasoning sources. Leveraging this dataset, we fine-tune Corvid\nwith a two-stage CoT-formatted training approach to progressively enhance its\nstep-by-step reasoning abilities. Furthermore, we propose an effective\ninference-time scaling strategy that enables Corvid to mitigate over-reasoning\nand under-reasoning through self-verification. Extensive experiments\ndemonstrate that Corvid outperforms existing o1-like MLLMs and state-of-the-art\nMLLMs with similar parameter scales, with notable strengths in mathematical\nreasoning and science problem-solving. Project page:\nhttps://mm-vl.github.io/corvid.\n","authors":["Jingjing Jiang","Chao Ma","Xurui Song","Hanwang Zhang","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2507.07424v1.pdf","comment":"ICCV 2025"},{"id":"http://arxiv.org/abs/2507.07415v1","updated":"2025-07-10T04:15:44Z","published":"2025-07-10T04:15:44Z","title":"EPIC: Efficient Prompt Interaction for Text-Image Classification","summary":"  In recent years, large-scale pre-trained multimodal models (LMMs) generally\nemerge to integrate the vision and language modalities, achieving considerable\nsuccess in multimodal tasks, such as text-image classification. The growing\nsize of LMMs, however, results in a significant computational cost for\nfine-tuning these models for downstream tasks. Hence, prompt-based interaction\nstrategy is studied to align modalities more efficiently. In this context, we\npropose a novel efficient prompt-based multimodal interaction strategy, namely\nEfficient Prompt Interaction for text-image Classification (EPIC).\nSpecifically, we utilize temporal prompts on intermediate layers, and integrate\ndifferent modalities with similarity-based prompt interaction, to leverage\nsufficient information exchange between modalities. Utilizing this approach,\nour method achieves reduced computational resource consumption and fewer\ntrainable parameters (about 1\\% of the foundation model) compared to other\nfine-tuning strategies. Furthermore, it demonstrates superior performance on\nthe UPMC-Food101 and SNLI-VE datasets, while achieving comparable performance\non the MM-IMDB dataset.\n","authors":["Xinyao Yu","Hao Sun","Zeyu Ling","Ziwei Niu","Zhenjia Bai","Rui Qin","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2507.07415v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2401.14856"},{"id":"http://arxiv.org/abs/2501.15379v2","updated":"2025-07-10T04:14:22Z","published":"2025-01-26T03:29:18Z","title":"Diffusion Augmented Retrieval: A Training-Free Approach to Interactive\n  Text-to-Image Retrieval","summary":"  Interactive Text-to-image retrieval (I-TIR) is an important enabler for a\nwide range of state-of-the-art services in domains such as e-commerce and\neducation. However, current methods rely on finetuned Multimodal Large Language\nModels (MLLMs), which are costly to train and update, and exhibit poor\ngeneralizability. This latter issue is of particular concern, as: 1) finetuning\nnarrows the pretrained distribution of MLLMs, thereby reducing\ngeneralizability; and 2) I-TIR introduces increasing query diversity and\ncomplexity. As a result, I-TIR solutions are highly likely to encounter queries\nand images not well represented in any training dataset. To address this, we\npropose leveraging Diffusion Models (DMs) for text-to-image mapping, to avoid\nfinetuning MLLMs while preserving robust performance on complex queries.\nSpecifically, we introduce Diffusion Augmented Retrieval (DAR), a framework\nthat generates multiple intermediate representations via LLM-based dialogue\nrefinements and DMs, producing a richer depiction of the user's information\nneeds. This augmented representation facilitates more accurate identification\nof semantically and visually related images. Extensive experiments on four\nbenchmarks show that for simple queries, DAR achieves results on par with\nfinetuned I-TIR models, yet without incurring their tuning overhead. Moreover,\nas queries become more complex through additional conversational turns, DAR\nsurpasses finetuned I-TIR models by up to 7.61% in Hits@10 after ten turns,\nillustrating its improved generalization for more intricate queries.\n","authors":["Zijun Long","Kangheng Liang","Gerardo Aragon-Camarasa","Richard Mccreadie","Paul Henderson"],"pdf_url":"https://arxiv.org/pdf/2501.15379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07410v1","updated":"2025-07-10T04:08:28Z","published":"2025-07-10T04:08:28Z","title":"EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis\n  through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction","summary":"  We propose EscherNet++, a masked fine-tuned diffusion model that can\nsynthesize novel views of objects in a zero-shot manner with amodal completion\nability. Existing approaches utilize multiple stages and complex pipelines to\nfirst hallucinate missing parts of the image and then perform novel view\nsynthesis, which fail to consider cross-view dependencies and require redundant\nstorage and computing for separate stages. Instead, we apply masked fine-tuning\nincluding input-level and feature-level masking to enable an end-to-end model\nwith the improved ability to synthesize novel views and conduct amodal\ncompletion. In addition, we empirically integrate our model with other\nfeed-forward image-to-mesh models without extra training and achieve\ncompetitive results with reconstruction time decreased by 95%, thanks to its\nability to synthesize arbitrary query views. Our method's scalable nature\nfurther enhances fast 3D reconstruction. Despite fine-tuning on a smaller\ndataset and batch size, our method achieves state-of-the-art results, improving\nPSNR by 3.9 and Volume IoU by 0.28 on occluded tasks in 10-input settings,\nwhile also generalizing to real-world occluded reconstruction.\n","authors":["Xinan Zhang","Muhammad Zubair Irshad","Anthony Yezzi","Yi-Chang Tsai","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2507.07410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12246v3","updated":"2025-07-10T04:02:50Z","published":"2024-08-22T09:33:25Z","title":"RT-OVAD: Real-Time Open-Vocabulary Aerial Object Detection via\n  Image-Text Collaboration","summary":"  Aerial object detection plays a crucial role in numerous applications.\nHowever, most existing methods focus on detecting predefined object categories,\nlimiting their applicability in real-world open scenarios. In this paper, we\nextend aerial object detection to open scenarios through image-text\ncollaboration and propose RT-OVAD, the first real-time open-vocabulary detector\nfor aerial scenes. Specifically, we first introduce an image-to-text alignment\nloss to replace the conventional category regression loss, thereby eliminating\ncategory constraints. Next, we propose a lightweight image-text collaboration\nstrategy comprising an image-text collaboration encoder and a text-guided\ndecoder. The encoder simultaneously enhances visual features and refines\ntextual embeddings, while the decoder guides object queries to focus on\nclass-relevant image features. This design further improves detection accuracy\nwithout incurring significant computational overhead. Extensive experiments\ndemonstrate that RT-OVAD consistently outperforms existing state-of-the-art\nmethods across open-vocabulary, zero-shot, and traditional closed-set detection\ntasks. For instance, on the open-vocabulary aerial detection benchmarks DIOR,\nDOTA-v2.0, and LAE-80C, RT-OVAD achieves 87.7 AP$_{50}$, 53.8 mAP, and 23.7\nmAP, respectively, surpassing the previous state-of-the-art (LAE-DINO) by 2.2,\n7.0, and 3.5 points. In addition, RT-OVAD achieves an inference speed of 34 FPS\non an RTX 4090 GPU, approximately three times faster than LAE-DINO (10 FPS),\nmeeting the real-time detection requirements of diverse applications. The code\nwill be released at https://github.com/GT-Wei/RT-OVAD.\n","authors":["Guoting Wei","Xia Yuan","Yu Liu","Zhenhao Shang","Xizhe Xue","Peng Wang","Kelu Yao","Chunxia Zhao","Haokui Zhang","Rong Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.12246v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03421v2","updated":"2025-07-10T03:48:57Z","published":"2025-07-04T09:27:48Z","title":"Hybrid-View Attention Network for Clinically Significant Prostate Cancer\n  Classification in Transrectal Ultrasound","summary":"  Prostate cancer (PCa) is a leading cause of cancer-related mortality in men,\nand accurate identification of clinically significant PCa (csPCa) is critical\nfor timely intervention. Transrectal ultrasound (TRUS) is widely used for\nprostate biopsy; however, its low contrast and anisotropic spatial resolution\npose diagnostic challenges. To address these limitations, we propose a novel\nhybrid-view attention (HVA) network for csPCa classification in 3D TRUS that\nleverages complementary information from transverse and sagittal views. Our\napproach integrates a CNN-transformer hybrid architecture, where convolutional\nlayers extract fine-grained local features and transformer-based HVA models\nglobal dependencies. Specifically, the HVA comprises intra-view attention to\nrefine features within a single view and cross-view attention to incorporate\ncomplementary information across views. Furthermore, a hybrid-view adaptive\nfusion module dynamically aggregates features along both channel and spatial\ndimensions, enhancing the overall representation. Experiments are conducted on\nan in-house dataset containing 590 subjects who underwent prostate biopsy.\nComparative and ablation results prove the efficacy of our method. The code is\navailable at https://github.com/mock1ngbrd/HVAN.\n","authors":["Zetian Feng","Juan Fu","Xuebin Zou","Hongsheng Ye","Hong Wu","Jianhua Zhou","Yi Wang"],"pdf_url":"https://arxiv.org/pdf/2507.03421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1905.09226v3","updated":"2025-07-10T03:33:27Z","published":"2019-05-22T16:23:23Z","title":"Boundary Learning by Using Weighted Propagation in Convolution Network","summary":"  In material science, image segmentation is of great significance for\nquantitative analysis of microstructures. Here, we propose a novel Weighted\nPropagation Convolution Neural Network based on U-Net (WPU-Net) to detect\nboundary in poly-crystalline microscopic images. We introduce spatial\nconsistency into network to eliminate the defects in raw microscopic image. And\nwe customize adaptive boundary weight for each pixel in each grain, so that it\nleads the network to preserve grain's geometric and topological\ncharacteristics. Moreover, we provide our dataset with the goal of advancing\nthe development of image processing in materials science. Experiments\ndemonstrate that the proposed method achieves promising performance in both of\nobjective and subjective assessment. In boundary detection task, it reduces the\nerror rate by 7\\%, which outperforms state-of-the-art methods by a large\nmargin.\n","authors":["Wei Liu","Jiahao Chen","Chuni Liu","Xiaojuan Ban","Boyuan Ma","Hao Wang","Weihua Xue","Yu Guo"],"pdf_url":"https://arxiv.org/pdf/1905.09226v3.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2507.07395v1","updated":"2025-07-10T03:26:17Z","published":"2025-07-10T03:26:17Z","title":"Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for\n  Unconstrained Image Collections","summary":"  Reconstructing and segmenting scenes from unconstrained photo collections\nobtained from the Internet is a novel but challenging task. Unconstrained photo\ncollections are easier to get than well-captured photo collections. These\nunconstrained images suffer from inconsistent lighting and transient\nocclusions, which makes segmentation challenging. Previous segmentation methods\ncannot address transient occlusions or accurately restore the scene's lighting\nconditions. Therefore, we propose Seg-Wild, an interactive segmentation method\nbased on 3D Gaussian Splatting for unconstrained image collections, suitable\nfor in-the-wild scenes. We integrate multi-dimensional feature embeddings for\neach 3D Gaussian and calculate the feature similarity between the feature\nembeddings and the segmentation target to achieve interactive segmentation in\nthe 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to\nsmooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and\ncalculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We\nalso designed a benchmark to evaluate segmentation quality in in-the-wild\nscenes. Experimental results demonstrate that compared to previous methods,\nSeg-Wild achieves better segmentation results and reconstruction quality. Our\ncode will be available at https://github.com/Sugar0725/Seg-Wild.\n","authors":["Yongtang Bao","Chengjie Tang","Yuze Wang","Haojie Li"],"pdf_url":"https://arxiv.org/pdf/2507.07395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07394v1","updated":"2025-07-10T03:25:50Z","published":"2025-07-10T03:25:50Z","title":"Behave Your Motion: Habit-preserved Cross-category Animal Motion\n  Transfer","summary":"  Animal motion embodies species-specific behavioral habits, making the\ntransfer of motion across categories a critical yet complex task for\napplications in animation and virtual reality. Existing motion transfer\nmethods, primarily focused on human motion, emphasize skeletal alignment\n(motion retargeting) or stylistic consistency (motion style transfer), often\nneglecting the preservation of distinct habitual behaviors in animals. To\nbridge this gap, we propose a novel habit-preserved motion transfer framework\nfor cross-category animal motion. Built upon a generative framework, our model\nintroduces a habit-preservation module with category-specific habit encoder,\nallowing it to learn motion priors that capture distinctive habitual\ncharacteristics. Furthermore, we integrate a large language model (LLM) to\nfacilitate the motion transfer to previously unobserved species. To evaluate\nthe effectiveness of our approach, we introduce the DeformingThings4D-skl\ndataset, a quadruped dataset with skeletal bindings, and conduct extensive\nexperiments and quantitative analyses, which validate the superiority of our\nproposed model.\n","authors":["Zhimin Zhang","Bi'an Du","Caoyuan Ma","Zheng Wang","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2507.07394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07393v1","updated":"2025-07-10T03:15:57Z","published":"2025-07-10T03:15:57Z","title":"KeyRe-ID: Keypoint-Guided Person Re-Identification using Part-Aware\n  Representation in Videos","summary":"  We propose \\textbf{KeyRe-ID}, a keypoint-guided video-based person\nre-identification framework consisting of global and local branches that\nleverage human keypoints for enhanced spatiotemporal representation learning.\nThe global branch captures holistic identity semantics through\nTransformer-based temporal aggregation, while the local branch dynamically\nsegments body regions based on keypoints to generate fine-grained, part-aware\nfeatures. Extensive experiments on MARS and iLIDS-VID benchmarks demonstrate\nstate-of-the-art performance, achieving 91.73\\% mAP and 97.32\\% Rank-1 accuracy\non MARS, and 96.00\\% Rank-1 and 100.0\\% Rank-5 accuracy on iLIDS-VID. The code\nfor this work will be publicly available on GitHub upon publication.\n","authors":["Jinseong Kim","Junghoon Song","Gyeongseon Baek","Byeongjoon Noh"],"pdf_url":"https://arxiv.org/pdf/2507.07393v1.pdf","comment":"10 pages, 2 figures,"},{"id":"http://arxiv.org/abs/2507.07389v1","updated":"2025-07-10T03:06:01Z","published":"2025-07-10T03:06:01Z","title":"ST-GRIT: Spatio-Temporal Graph Transformer For Internal Ice Layer\n  Thickness Prediction","summary":"  Understanding the thickness and variability of internal ice layers in radar\nimagery is crucial for monitoring snow accumulation, assessing ice dynamics,\nand reducing uncertainties in climate models. Radar sensors, capable of\npenetrating ice, provide detailed radargram images of these internal layers. In\nthis work, we present ST-GRIT, a spatio-temporal graph transformer for ice\nlayer thickness, designed to process these radargrams and capture the\nspatiotemporal relationships between shallow and deep ice layers. ST-GRIT\nleverages an inductive geometric graph learning framework to extract local\nspatial features as feature embeddings and employs a series of temporal and\nspatial attention blocks separately to model long-range dependencies\neffectively in both dimensions. Experimental evaluation on radargram data from\nthe Greenland ice sheet demonstrates that ST-GRIT consistently outperforms\ncurrent state-of-the-art methods and other baseline graph neural networks by\nachieving lower root mean-squared error. These results highlight the advantages\nof self-attention mechanisms on graphs over pure graph neural networks,\nincluding the ability to handle noise, avoid oversmoothing, and capture\nlong-range dependencies. Moreover, the use of separate spatial and temporal\nattention blocks allows for distinct and robust learning of spatial\nrelationships and temporal patterns, providing a more comprehensive and\neffective approach.\n","authors":["Zesheng Liu","Maryam Rahnemoonfar"],"pdf_url":"https://arxiv.org/pdf/2507.07389v1.pdf","comment":"Accepted for 2025 IEEE International Conference on Image Processing\n  (ICIP)"},{"id":"http://arxiv.org/abs/2507.06526v2","updated":"2025-07-10T03:02:45Z","published":"2025-07-09T03:55:58Z","title":"Concept Unlearning by Modeling Key Steps of Diffusion Process","summary":"  Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,\nwhich generate highly realistic images based on textual input, have been widely\nused. However, their misuse poses serious security risks. While existing\nconcept unlearning methods aim to mitigate these risks, they struggle to\nbalance unlearning effectiveness with generative retainability.To overcome this\nlimitation, we innovatively propose the Key Step Concept Unlearning (KSCU)\nmethod, which ingeniously capitalizes on the unique stepwise sampling\ncharacteristic inherent in diffusion models during the image generation\nprocess. Unlike conventional approaches that treat all denoising steps equally,\nKSCU strategically focuses on pivotal steps with the most influence over the\nfinal outcome by dividing key steps for different concept unlearning tasks and\nfine-tuning the model only at those steps. This targeted approach reduces the\nnumber of parameter updates needed for effective unlearning, while maximizing\nthe retention of the model's generative capabilities.Through extensive\nbenchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs\nfrom generating undesirable images while better retaining the model's\ngenerative capabilities. Our code will be released.\n","authors":["Chaoshuo Zhang","Chenhao Lin","Zhengyu Zhao","Le Yang","Qian Wang","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2507.06526v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00151v2","updated":"2025-07-10T02:48:27Z","published":"2024-11-29T06:17:11Z","title":"DLaVA: Document Language and Vision Assistant for Answer Localization\n  with Enhanced Interpretability and Trustworthiness","summary":"  Document Visual Question Answering (VQA) demands robust integration of text\ndetection, recognition, and spatial reasoning to interpret complex document\nlayouts. In this work, we introduce DLaVA, a novel, training-free pipeline that\nleverages Multimodal Large Language Models (MLLMs) for zero-shot answer\nlocalization in order to improve trustworthiness, interpretability, and\nexplainability. By leveraging an innovative OCR-free approach that organizes\ntext regions with unique bounding box IDs, the proposed method preserves\nspatial contexts without relying on iterative OCR or chain-of-thought\nreasoning, thus substantially reducing the computational complexity. We further\nenhance the evaluation protocol by integrating Intersection over Union (IoU)\nmetrics alongside Average Normalized Levenshtein Similarity (ANLS), thereby\nensuring that not only textual accuracy is considered, but spatial accuracy is\ntaken into account, ultimately reducing the risks of AI hallucinations and\nimproving trustworthiness. Experiments on benchmark datasets demonstrate\ncompetitive performance compared to state-of-the-art techniques, with\nsignificantly lower computational complexity and enhanced accuracies and\nreliability for high-stakes applications. The code and datasets utilized in\nthis study for DLaVA are accessible at:\nhttps://github.com/ahmad-shirazi/AnnotMLLM.\n","authors":["Ahmad Mohammadshirazi","Pinaki Prasad Guha Neogi","Ser-Nam Lim","Rajiv Ramnath"],"pdf_url":"https://arxiv.org/pdf/2412.00151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02901v2","updated":"2025-07-10T02:48:24Z","published":"2025-06-23T12:22:39Z","title":"Online Continual Learning via Spiking Neural Networks with Sleep\n  Enhanced Latent Replay","summary":"  Edge computing scenarios necessitate the development of hardware-efficient\nonline continual learning algorithms to be adaptive to dynamic environment.\nHowever, existing algorithms always suffer from high memory overhead and bias\ntowards recently trained tasks. To tackle these issues, this paper proposes a\nnovel online continual learning approach termed as SESLR, which incorporates a\nsleep enhanced latent replay scheme with spiking neural networks (SNNs). SESLR\nleverages SNNs' binary spike characteristics to store replay features in single\nbits, significantly reducing memory overhead. Furthermore, inspired by\nbiological sleep-wake cycles, SESLR introduces a noise-enhanced sleep phase\nwhere the model exclusively trains on replay samples with controlled noise\ninjection, effectively mitigating classification bias towards new classes.\nExtensive experiments on both conventional (MNIST, CIFAR10) and neuromorphic\n(NMNIST, CIFAR10-DVS) datasets demonstrate SESLR's effectiveness. On Split\nCIFAR10, SESLR achieves nearly 30% improvement in average accuracy with only\none-third of the memory consumption compared to baseline methods. On Split\nCIFAR10-DVS, it improves accuracy by approximately 10% while reducing memory\noverhead by a factor of 32. These results validate SESLR as a promising\nsolution for online continual learning in resource-constrained edge computing\nscenarios.\n","authors":["Erliang Lin","Wenbin Luo","Wei Jia","Yu Chen","Shaofu Yang"],"pdf_url":"https://arxiv.org/pdf/2507.02901v2.pdf","comment":"9 pages, 4figures"},{"id":"http://arxiv.org/abs/2503.12356v3","updated":"2025-07-10T02:43:41Z","published":"2025-03-16T04:53:20Z","title":"Localized Concept Erasure for Text-to-Image Diffusion Models Using\n  Training-Free Gated Low-Rank Adaptation","summary":"  Fine-tuning based concept erasing has demonstrated promising results in\npreventing generation of harmful contents from text-to-image diffusion models\nby removing target concepts while preserving remaining concepts. To maintain\nthe generation capability of diffusion models after concept erasure, it is\nnecessary to remove only the image region containing the target concept when it\nlocally appears in an image, leaving other regions intact. However, prior arts\noften compromise fidelity of the other image regions in order to erase the\nlocalized target concept appearing in a specific area, thereby reducing the\noverall performance of image generation. To address these limitations, we first\nintroduce a framework called localized concept erasure, which allows for the\ndeletion of only the specific area containing the target concept in the image\nwhile preserving the other regions. As a solution for the localized concept\nerasure, we propose a training-free approach, dubbed Gated Low-rank adaptation\nfor Concept Erasure (GLoCE), that injects a lightweight module into the\ndiffusion model. GLoCE consists of low-rank matrices and a simple gate,\ndetermined only by several generation steps for concepts without training. By\ndirectly applying GLoCE to image embeddings and designing the gate to activate\nonly for target concepts, GLoCE can selectively remove only the region of the\ntarget concepts, even when target and remaining concepts coexist within an\nimage. Extensive experiments demonstrated GLoCE not only improves the image\nfidelity to text prompts after erasing the localized target concepts, but also\noutperforms prior arts in efficacy, specificity, and robustness by large margin\nand can be extended to mass concept erasure.\n","authors":["Byung Hyun Lee","Sungjin Lim","Se Young Chun"],"pdf_url":"https://arxiv.org/pdf/2503.12356v3.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2507.04750v2","updated":"2025-07-10T02:40:29Z","published":"2025-07-07T08:26:18Z","title":"MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for\n  Particle Image Velocimetry","summary":"  Particle Image Velocimetry (PIV) is fundamental to fluid dynamics, yet deep\nlearning applications face significant hurdles. A critical gap exists: the lack\nof comprehensive evaluation of how diverse optical flow models perform\nspecifically on PIV data, largely due to limitations in available datasets and\nthe absence of a standardized benchmark. This prevents fair comparison and\nhinders progress. To address this, our primary contribution is a novel,\nlarge-scale synthetic PIV benchmark dataset generated from diverse CFD\nsimulations (JHTDB and Blasius). It features unprecedented variety in particle\ndensities, flow velocities, and continuous motion, enabling, for the first\ntime, a standardized and rigorous evaluation of various optical flow and PIV\nalgorithms. Complementing this, we propose Multi Cost Volume PIV (MCFormer), a\nnew deep network architecture leveraging multi-frame temporal information and\nmultiple cost volumes, specifically designed for PIV's sparse nature. Our\ncomprehensive benchmark evaluation, the first of its kind, reveals significant\nperformance variations among adapted optical flow models and demonstrates that\nMCFormer significantly outperforms existing methods, achieving the lowest\noverall normalized endpoint error (NEPE). This work provides both a\nfoundational benchmark resource essential for future PIV research and a\nstate-of-the-art method tailored for PIV challenges. We make our benchmark\ndataset and code publicly available to foster future research in this area.\n","authors":["Zicheng Lin","Xiaoqiang Li","Yichao Wang","Chuang Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.04750v2.pdf","comment":"20 pages, 13 figures, 5 tables. Comprehensive benchmark evaluation of\n  optical flow models for PIV. Introduces MCFormer architecture with\n  multi-frame temporal processing and multiple cost volumes. Includes\n  large-scale synthetic PIV dataset based on JHTDB and Blasius CFD simulations.\n  Code and dataset will be made publicly available"},{"id":"http://arxiv.org/abs/2507.07381v1","updated":"2025-07-10T02:30:07Z","published":"2025-07-10T02:30:07Z","title":"Multi-Scale Attention and Gated Shifting for Fine-Grained Event Spotting\n  in Videos","summary":"  Precise Event Spotting (PES) in sports videos requires frame-level\nrecognition of fine-grained actions from single-camera footage. Existing PES\nmodels typically incorporate lightweight temporal modules such as Gate Shift\nModule (GSM) or Gate Shift Fuse (GSF) to enrich 2D CNN feature extractors with\ntemporal context. However, these modules are limited in both temporal receptive\nfield and spatial adaptability. We propose a Multi-Scale Attention Gate Shift\nModule (MSAGSM) that enhances GSM with multi-scale temporal dilations and\nmulti-head spatial attention, enabling efficient modeling of both short- and\nlong-term dependencies while focusing on salient regions. MSAGSM is a\nlightweight plug-and-play module that can be easily integrated with various 2D\nbackbones. To further advance the field, we introduce the Table Tennis\nAustralia (TTA) dataset-the first PES benchmark for table tennis-containing\nover 4800 precisely annotated events. Extensive experiments across five PES\nbenchmarks demonstrate that MSAGSM consistently improves performance with\nminimal overhead, setting new state-of-the-art results.\n","authors":["Hao Xu","Arbind Agrahari Baniya","Sam Wells","Mohamed Reda Bouadjenek","Richard Dazeley","Sunil Aryal"],"pdf_url":"https://arxiv.org/pdf/2507.07381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07379v1","updated":"2025-07-10T02:19:10Z","published":"2025-07-10T02:19:10Z","title":"Adaptive Particle-Based Shape Modeling for Anatomical Surface\n  Correspondence","summary":"  Particle-based shape modeling (PSM) is a family of approaches that\nautomatically quantifies shape variability across anatomical cohorts by\npositioning particles (pseudo landmarks) on shape surfaces in a consistent\nconfiguration. Recent advances incorporate implicit radial basis function\nrepresentations as self-supervised signals to better capture the complex\ngeometric properties of anatomical structures. However, these methods still\nlack self-adaptivity -- that is, the ability to automatically adjust particle\nconfigurations to local geometric features of each surface, which is essential\nfor accurately representing complex anatomical variability. This paper\nintroduces two mechanisms to increase surface adaptivity while maintaining\nconsistent particle configurations: (1) a novel neighborhood correspondence\nloss to enable high adaptivity and (2) a geodesic correspondence algorithm that\nregularizes optimization to enforce geodesic neighborhood consistency. We\nevaluate the efficacy and scalability of our approach on challenging datasets,\nproviding a detailed analysis of the adaptivity-correspondence trade-off and\nbenchmarking against existing methods on surface representation accuracy and\ncorrespondence metrics.\n","authors":["Hong Xu","Shireen Y. Elhabian"],"pdf_url":"https://arxiv.org/pdf/2507.07379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07374v1","updated":"2025-07-10T01:56:30Z","published":"2025-07-10T01:56:30Z","title":"PacGDC: Label-Efficient Generalizable Depth Completion with Projection\n  Ambiguity and Consistency","summary":"  Generalizable depth completion enables the acquisition of dense metric depth\nmaps for unseen environments, offering robust perception capabilities for\nvarious downstream tasks. However, training such models typically requires\nlarge-scale datasets with metric depth labels, which are often labor-intensive\nto collect. This paper presents PacGDC, a label-efficient technique that\nenhances data diversity with minimal annotation effort for generalizable depth\ncompletion. PacGDC builds on novel insights into inherent ambiguities and\nconsistencies in object shapes and positions during 2D-to-3D projection,\nallowing the synthesis of numerous pseudo geometries for the same visual scene.\nThis process greatly broadens available geometries by manipulating scene scales\nof the corresponding depth maps. To leverage this property, we propose a new\ndata synthesis pipeline that uses multiple depth foundation models as scale\nmanipulators. These models robustly provide pseudo depth labels with varied\nscene scales, affecting both local objects and global layouts, while ensuring\nprojection consistency that supports generalization. To further diversify\ngeometries, we incorporate interpolation and relocation strategies, as well as\nunlabeled images, extending the data coverage beyond the individual use of\nfoundation models. Extensive experiments show that PacGDC achieves remarkable\ngeneralizability across multiple benchmarks, excelling in diverse scene\nsemantics/scales and depth sparsity/patterns under both zero-shot and few-shot\nsettings. Code: https://github.com/Wang-xjtu/PacGDC.\n","authors":["Haotian Wang","Aoran Xiao","Xiaoqin Zhang","Meng Yang","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2507.07374v1.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2507.06971v2","updated":"2025-07-10T01:50:07Z","published":"2025-07-09T16:01:41Z","title":"Hallucinating 360°: Panoramic Street-View Generation via Local\n  Scenes Diffusion and Probabilistic Prompting","summary":"  Panoramic perception holds significant potential for autonomous driving,\nenabling vehicles to acquire a comprehensive 360{\\deg} surround view in a\nsingle shot. However, autonomous driving is a data-driven task. Complete\npanoramic data acquisition requires complex sampling systems and annotation\npipelines, which are time-consuming and labor-intensive. Although existing\nstreet view generation models have demonstrated strong data regeneration\ncapabilities, they can only learn from the fixed data distribution of existing\ndatasets and cannot achieve high-quality, controllable panoramic generation. In\nthis paper, we propose the first panoramic generation method Percep360 for\nautonomous driving. Percep360 enables coherent generation of panoramic data\nwith control signals based on the stitched panoramic data. Percep360 focuses on\ntwo key aspects: coherence and controllability. Specifically, to overcome the\ninherent information loss caused by the pinhole sampling process, we propose\nthe Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama\ngeneration as a spatially continuous diffusion process, bridging the gaps\nbetween different data distributions. Additionally, to achieve the controllable\ngeneration of panoramic images, we propose a Probabilistic Prompting Method\n(PPM). PPM dynamically selects the most relevant control cues, enabling\ncontrollable panoramic image generation. We evaluate the effectiveness of the\ngenerated images from three perspectives: image quality assessment (i.e.,\nno-reference and with reference), controllability, and their utility in\nreal-world Bird's Eye View (BEV) segmentation. Notably, the generated data\nconsistently outperforms the original stitched images in no-reference quality\nmetrics and enhances downstream perception models. The source code will be\npublicly available at https://github.com/Bryant-Teng/Percep360.\n","authors":["Fei Teng","Kai Luo","Sheng Wu","Siyu Li","Pujun Guo","Jiale Wei","Kunyu Peng","Jiaming Zhang","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2507.06971v2.pdf","comment":"The source code will be publicly available at\n  https://github.com/Bryant-Teng/Percep360"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.07924v1","updated":"2025-07-10T17:06:24Z","published":"2025-07-10T17:06:24Z","title":"Measuring Hypothesis Testing Errors in the Evaluation of Retrieval\n  Systems","summary":"  The evaluation of Information Retrieval (IR) systems typically uses\nquery-document pairs with corresponding human-labelled relevance assessments\n(qrels). These qrels are used to determine if one system is better than another\nbased on average retrieval performance. Acquiring large volumes of human\nrelevance assessments is expensive. Therefore, more efficient relevance\nassessment approaches have been proposed, necessitating comparisons between\nqrels to ascertain their efficacy. Discriminative power, i.e. the ability to\ncorrectly identify significant differences between systems, is important for\ndrawing accurate conclusions on the robustness of qrels. Previous work has\nmeasured the proportion of pairs of systems that are identified as\nsignificantly different and has quantified Type I statistical errors. Type I\nerrors lead to incorrect conclusions due to false positive significance tests.\nWe argue that also identifying Type II errors (false negatives) is important as\nthey lead science in the wrong direction. We quantify Type II errors and\npropose that balanced classification metrics, such as balanced accuracy, can be\nused to portray the discriminative power of qrels. We perform experiments using\nqrels generated using alternative relevance assessment methods to investigate\nmeasuring hypothesis testing errors in IR evaluation. We find that additional\ninsights into the discriminative power of qrels can be gained by quantifying\nType II errors, and that balanced classification metrics can be used to give an\noverall summary of discriminative power in one, easily comparable, number.\n","authors":["Jack McKechnie","Graham McDonald","Craig Macdonald"],"pdf_url":"https://arxiv.org/pdf/2507.07924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07919v1","updated":"2025-07-10T16:59:51Z","published":"2025-07-10T16:59:51Z","title":"Plausible Counterfactual Explanations of Recommendations","summary":"  Explanations play a variety of roles in various recommender systems, from a\nlegally mandated afterthought, through an integral element of user experience,\nto a key to persuasiveness. A natural and useful form of an explanation is the\nCounterfactual Explanation (CE). We present a method for generating highly\nplausible CEs in recommender systems and evaluate it both numerically and with\na user study.\n","authors":["Jakub Černý","Jiří Němeček","Ivan Dovica","Jakub Mareček"],"pdf_url":"https://arxiv.org/pdf/2507.07919v1.pdf","comment":"8 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2507.07910v1","updated":"2025-07-10T16:44:33Z","published":"2025-07-10T16:44:33Z","title":"DTECT: Dynamic Topic Explorer & Context Tracker","summary":"  The explosive growth of textual data over time presents a significant\nchallenge in uncovering evolving themes and trends. Existing dynamic topic\nmodeling techniques, while powerful, often exist in fragmented pipelines that\nlack robust support for interpretation and user-friendly exploration. We\nintroduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end\nsystem that bridges the gap between raw textual data and meaningful temporal\ninsights. DTECT provides a unified workflow that supports data preprocessing,\nmultiple model architectures, and dedicated evaluation metrics to analyze the\ntopic quality of temporal topic models. It significantly enhances\ninterpretability by introducing LLM-driven automatic topic labeling, trend\nanalysis via temporally salient words, interactive visualizations with\ndocument-level summarization, and a natural language chat interface for\nintuitive data querying. By integrating these features into a single, cohesive\nplatform, DTECT empowers users to more effectively track and understand\nthematic dynamics. DTECT is open-source and available at\nhttps://github.com/AdhyaSuman/DTECT.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2507.07910v1.pdf","comment":"Code: https://github.com/AdhyaSuman/DTECT | Demo:\n  https://huggingface.co/spaces/AdhyaSuman/DTECT | Video:\n  https://youtu.be/B8nNfxFoJAU"},{"id":"http://arxiv.org/abs/2507.07909v1","updated":"2025-07-10T16:41:10Z","published":"2025-07-10T16:41:10Z","title":"Document Similarity Enhanced IPS Estimation for Unbiased Learning to\n  Rank","summary":"  Learning to Rank (LTR) models learn from historical user interactions, such\nas user clicks. However, there is an inherent bias in the clicks of users due\nto position bias, i.e., users are more likely to click highly-ranked documents\nthan low-ranked documents. To address this bias when training LTR models, many\napproaches from the literature re-weight the users' click data using Inverse\nPropensity Scoring (IPS). IPS re-weights the user's clicks proportionately to\nthe position in the historical ranking that a document was placed when it was\nclicked since low-ranked documents are less likely to be seen by a user. In\nthis paper, we argue that low-ranked documents that are similar to\nhighly-ranked relevant documents are also likely to be relevant. Moreover,\naccounting for the similarity of low-ranked documents to highly ranked relevant\ndocuments when calculating IPS can more effectively mitigate the effects of\nposition bias. Therefore, we propose an extension to IPS, called IPSsim, that\ntakes into consideration the similarity of documents when estimating IPS. We\nevaluate our IPSsim estimator using two large publicly available LTR datasets\nunder a number of simulated user click settings, and with different numbers of\ntraining clicks. Our experiments show that our IPSsim estimator is more\neffective than the existing IPS estimators for learning an unbiased LTR model,\nparticularly in top-n settings when n >= 30. For example, when n = 50, our\nIPSsim estimator achieves a statistically significant ~3% improvement (p <\n0.05) in terms of NDCG compared to the Doubly Robust estimator from the\nliterature.\n","authors":["Zeyan Liang","Graham McDonald","Iadh Ounis"],"pdf_url":"https://arxiv.org/pdf/2507.07909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02097v2","updated":"2025-07-10T14:47:38Z","published":"2025-07-02T19:25:44Z","title":"The Future is Agentic: Definitions, Perspectives, and Open Challenges of\n  Multi-Agent Recommender Systems","summary":"  Large language models (LLMs) are rapidly evolving from passive engines of\ntext generation into agentic entities that can plan, remember, invoke external\ntools, and co-operate with one another. This perspective paper investigates how\nsuch LLM agents (and societies thereof) can transform the design space of\nrecommender systems.\n  We introduce a unified formalism that (i) models an individual agent as a\ntuple comprising its language core, tool set, and hierarchical memory, and (ii)\ncaptures a multi-agent recommender as a triple of agents, shared environment,\nand communication protocol. Within this framework, we present four end-to-end\nuse cases-interactive party planning, synthetic user-simulation for offline\nevaluation, multi-modal furniture recommendation, and brand-aligned explanation\ngeneration-each illustrating a distinct capability unlocked by agentic\norchestration.\n  We then surface five cross-cutting challenge families: protocol complexity,\nscalability, hallucination and error propagation, emergent misalignment\n(including covert collusion), and brand compliance.\n  For each, we formalize the problem, review nascent mitigation strategies, and\noutline open research questions. The result is both a blueprint and an agenda:\na blueprint that shows how memory-augmented, tool-using LLM agents can be\ncomposed into robust recommendation pipelines, and an agenda inviting the\nRecSys community to develop benchmarks, theoretical guarantees, and governance\ntools that keep pace with this new degree of autonomy. By unifying agentic\nabstractions with recommender objectives, the paper lays the groundwork for the\nnext generation of personalized, trustworthy, and context-rich recommendation\nservices.\n","authors":["Reza Yousefi Maragheh","Yashar Deldjoo"],"pdf_url":"https://arxiv.org/pdf/2507.02097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11719v2","updated":"2025-07-10T14:28:19Z","published":"2024-10-15T15:50:53Z","title":"Adaptive Graph Integration for Cross-Domain Recommendation via\n  Heterogeneous Graph Coordinators","summary":"  In the digital era, users typically interact with diverse items across\nmultiple domains (e.g., e-commerce, streaming platforms, and social networks),\ngenerating intricate heterogeneous interaction graphs. Leveraging multi-domain\ndata can improve recommendation systems by enriching user insights and\nmitigating data sparsity in individual domains. However, integrating such\nmulti-domain knowledge for cross-domain recommendation remains challenging due\nto inherent disparities in user behavior and item characteristics and the risk\nof negative transfer, where irrelevant or conflicting information from the\nsource domains adversely impacts the target domain's performance. To tackle\nthese challenges, we propose HAGO, a novel framework with\n\\textbf{H}eterogeneous \\textbf{A}daptive \\textbf{G}raph co\\textbf{O}rdinators,\nwhich dynamically integrates multi-domain graphs into a cohesive structure.\nHAGO adaptively adjusts the connections between coordinators and multi-domain\ngraph nodes to enhance beneficial inter-domain interactions while alleviating\nnegative transfer. Furthermore, we introduce a universal multi-domain graph\npre-training strategy alongside HAGO to collaboratively learn high-quality node\nrepresentations across domains. Being compatible with various graph-based\nmodels and pre-training techniques, HAGO demonstrates broad applicability and\neffectiveness. Extensive experiments show that our framework outperforms\nstate-of-the-art methods in cross-domain recommendation scenarios, underscoring\nits potential for real-world applications. The source code is available at\nhttps://github.com/zhy99426/HAGO.\n","authors":["Hengyu Zhang","Chunxu Shen","Xiangguo Sun","Jie Tan","Yu Rong","Chengzhi Piao","Hong Cheng","Lingling Yi"],"pdf_url":"https://arxiv.org/pdf/2410.11719v2.pdf","comment":"Accept by SIGIR 2025"},{"id":"http://arxiv.org/abs/2412.00569v2","updated":"2025-07-10T14:21:15Z","published":"2024-11-30T19:45:23Z","title":"Contextual Bandits in Payment Processing: Non-uniform Exploration and\n  Supervised Learning","summary":"  Uniform random exploration in decision-making systems supports off-policy\nlearning via supervision but incurs high regret, making it impractical for many\napplications. Conversely, non-uniform exploration offers better immediate\nperformance but lacks support for off-policy learning. Recent research suggests\nthat regression oracles can bridge this gap by combining non-uniform\nexploration with supervised learning. In this paper, we analyze these\napproaches within a real-world industrial context at Adyen, a large global\npayments processor characterized by batch logged delayed feedback, short-term\nmemory, and dynamic action spaces under the Empirical Risk Minimization (ERM)\nframework. Our analysis reveals that while regression oracles significantly\nimprove performance, they introduce challenges due to rigid algorithmic\nassumptions. Specifically, we observe that as a policy improves, subsequent\ngenerations may perform worse due to shifts in the reward distribution and\nincreased class imbalance in the training data. This degradation occurs de\nspite improvements in other aspects of the training data, leading to decreased\nperformance in successive policy iterations. We further explore the long-term\nimpact of regression oracles, identifying a potential \"oscillation effect.\"\nThis effect arises when regression oracles influence probability estimates and\nthe realizability of subsequent policy models, leading to fluctuations in\nperformance across iterations. Our findings highlight the need for more\nadaptable algorithms that can leverage the benefits of regression oracles\nwithout introducing instability in policy performance over time.\n","authors":["Akhila Vangara","Alex Egg"],"pdf_url":"https://arxiv.org/pdf/2412.00569v2.pdf","comment":"7 pages, 10 figures, submitted to KDD '25"},{"id":"http://arxiv.org/abs/2507.07700v1","updated":"2025-07-10T12:27:03Z","published":"2025-07-10T12:27:03Z","title":"Rethinking the Privacy of Text Embeddings: A Reproducibility Study of\n  \"Text Embeddings Reveal (Almost) As Much As Text\"","summary":"  Text embeddings are fundamental to many natural language processing (NLP)\ntasks, extensively applied in domains such as recommendation systems and\ninformation retrieval (IR). Traditionally, transmitting embeddings instead of\nraw text has been seen as privacy-preserving. However, recent methods such as\nVec2Text challenge this assumption by demonstrating that controlled decoding\ncan successfully reconstruct original texts from black-box embeddings. The\nunexpectedly strong results reported by Vec2Text motivated us to conduct\nfurther verification, particularly considering the typically non-intuitive and\nopaque structure of high-dimensional embedding spaces. In this work, we\nreproduce the Vec2Text framework and evaluate it from two perspectives: (1)\nvalidating the original claims, and (2) extending the study through targeted\nexperiments. First, we successfully replicate the original key results in both\nin-domain and out-of-domain settings, with only minor discrepancies arising due\nto missing artifacts, such as model checkpoints and dataset splits.\nFurthermore, we extend the study by conducting a parameter sensitivity\nanalysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,\npasswords), and exploring embedding quantization as a lightweight privacy\ndefense. Our results show that Vec2Text is effective under ideal conditions,\ncapable of reconstructing even password-like sequences that lack clear\nsemantics. However, we identify key limitations, including its sensitivity to\ninput sequence length. We also find that Gaussian noise and quantization\ntechniques can mitigate the privacy risks posed by Vec2Text, with quantization\noffering a simpler and more widely applicable solution. Our findings emphasize\nthe need for caution in using text embeddings and highlight the importance of\nfurther research into robust defense mechanisms for NLP systems.\n","authors":["Dominykas Seputis","Yongkang Li","Karsten Langerak","Serghei Mihailov"],"pdf_url":"https://arxiv.org/pdf/2507.07700v1.pdf","comment":"This paper has been accepted for oral presentation in the\n  reproducibility track at RecSys 2025"},{"id":"http://arxiv.org/abs/2504.06667v2","updated":"2025-07-10T09:54:19Z","published":"2025-04-09T08:08:16Z","title":"Toward Holistic Evaluation of Recommender Systems Powered by Generative\n  Models","summary":"  Recommender systems powered by generative models (Gen-RecSys) extend beyond\nclassical item ranking by producing open-ended content, which simultaneously\nunlocks richer user experiences and introduces new risks. On one hand, these\nsystems can enhance personalization and appeal through dynamic explanations and\nmulti-turn dialogues. On the other hand, they might venture into unknown\nterritory-hallucinating nonexistent items, amplifying bias, or leaking private\ninformation. Traditional accuracy metrics cannot fully capture these\nchallenges, as they fail to measure factual correctness, content safety, or\nalignment with user intent.\n  This paper makes two main contributions. First, we categorize the evaluation\nchallenges of Gen-RecSys into two groups: (i) existing concerns that are\nexacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new\nrisks (e.g., item hallucinations, contradictory explanations). Second, we\npropose a holistic evaluation approach that includes scenario-based assessments\nand multi-metric checks-incorporating relevance, factual grounding, bias\ndetection, and policy compliance. Our goal is to provide a guiding framework so\nresearchers and practitioners can thoroughly assess Gen-RecSys, ensuring\neffective personalization and responsible deployment.\n","authors":["Yashar Deldjoo","Nikhil Mehta","Maheswaran Sathiamoorthy","Shuai Zhang","Pablo Castells","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2504.06667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05085v4","updated":"2025-07-10T08:38:59Z","published":"2024-06-07T16:59:38Z","title":"Multi-Head RAG: Solving Multi-Aspect Problems with LLMs","summary":"  Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving observation is that different attention\nheads learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets, and real-world\nuse cases to demonstrate MRAG's effectiveness. We show MRAG's design advantages\nover 18 RAG baselines, empirical improvements of up to 20% in retrieval success\nratios, and benefits for downstream LLM generation. MRAG can be seamlessly\nintegrated with existing RAG frameworks and benchmarks.\n","authors":["Maciej Besta","Ales Kubicek","Robert Gerstenberger","Marcin Chrapek","Roman Niggli","Patrik Okanovic","Yi Zhu","Patrick Iff","Michal Podstawski","Lucas Weitzendorf","Mingyuan Chi","Joanna Gajda","Piotr Nyczyk","Jürgen Müller","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2406.05085v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07543v1","updated":"2025-07-10T08:38:31Z","published":"2025-07-10T08:38:31Z","title":"The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English\n  Corpora","summary":"  Cross-lingual retrieval-augmented generation (RAG) is a critical capability\nfor retrieving and generating answers across languages. Prior work in this\ncontext has mostly focused on generation and relied on benchmarks derived from\nopen-domain sources, most notably Wikipedia. In such settings, retrieval\nchallenges often remain hidden due to language imbalances, overlap with\npretraining data, and memorized content. To address this gap, we study\nArabic-English RAG in a domain-specific setting using benchmarks derived from\nreal-world corporate datasets. Our benchmarks include all combinations of\nlanguages for the user query and the supporting document, drawn independently\nand uniformly at random. This enables a systematic study of multilingual\nretrieval behavior.\n  Our findings reveal that retrieval is a critical bottleneck in cross-lingual\ndomain-specific scenarios, with significant performance drops occurring when\nthe user query and supporting document languages differ. A key insight is that\nthese failures stem primarily from the retriever's difficulty in ranking\ndocuments across languages. Finally, we propose a simple retrieval strategy\nthat addresses this source of failure by enforcing equal retrieval from both\nlanguages, resulting in substantial improvements in cross-lingual and overall\nperformance. These results highlight meaningful opportunities for improving\nmultilingual retrieval, particularly in practical, real-world RAG applications.\n","authors":["Chen Amiraz","Yaroslav Fyodorov","Elad Haramaty","Zohar Karnin","Liane Lewin-Eytan"],"pdf_url":"https://arxiv.org/pdf/2507.07543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07522v1","updated":"2025-07-10T08:12:39Z","published":"2025-07-10T08:12:39Z","title":"NLGCL: Naturally Existing Neighbor Layers Graph Contrastive Learning for\n  Recommendation","summary":"  Graph Neural Networks (GNNs) are widely used in collaborative filtering to\ncapture high-order user-item relationships. To address the data sparsity\nproblem in recommendation systems, Graph Contrastive Learning (GCL) has emerged\nas a promising paradigm that maximizes mutual information between contrastive\nviews. However, existing GCL methods rely on augmentation techniques that\nintroduce semantically irrelevant noise and incur significant computational and\nstorage costs, limiting effectiveness and efficiency.\n  To overcome these challenges, we propose NLGCL, a novel contrastive learning\nframework that leverages naturally contrastive views between neighbor layers\nwithin GNNs. By treating each node and its neighbors in the next layer as\npositive pairs, and other nodes as negatives, NLGCL avoids augmentation-based\nnoise while preserving semantic relevance. This paradigm eliminates costly view\nconstruction and storage, making it computationally efficient and practical for\nreal-world scenarios. Extensive experiments on four public datasets demonstrate\nthat NLGCL outperforms state-of-the-art baselines in effectiveness and\nefficiency.\n","authors":["Jinfeng Xu","Zheyu Chen","Shuo Yang","Jinze Li","Hewei Wang","Wei Wang","Xiping Hu","Edith Ngai"],"pdf_url":"https://arxiv.org/pdf/2507.07522v1.pdf","comment":"Accepted by RecSys 2025 as Spotlight Oral"},{"id":"http://arxiv.org/abs/2504.02670v5","updated":"2025-07-10T07:15:51Z","published":"2025-04-03T15:11:55Z","title":"Affordable AI Assistants with Knowledge Graph of Thoughts","summary":"  Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.\n","authors":["Maciej Besta","Lorenzo Paleari","Jia Hao Andrea Jiang","Robert Gerstenberger","You Wu","Jón Gunnar Hannesson","Patrick Iff","Ales Kubicek","Piotr Nyczyk","Diana Khimey","Nils Blach","Haiqiang Zhang","Tao Zhang","Peiran Ma","Grzegorz Kwaśniewski","Marcin Copik","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2504.02670v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06503v2","updated":"2025-07-10T06:49:21Z","published":"2025-07-09T03:02:23Z","title":"USD: A User-Intent-Driven Sampling and Dual-Debiasing Framework for\n  Large-Scale Homepage Recommendations","summary":"  Large-scale homepage recommendations face critical challenges from\npseudo-negative samples caused by exposure bias, where non-clicks may indicate\ninattention rather than disinterest. Existing work lacks thorough analysis of\ninvalid exposures and typically addresses isolated aspects (e.g., sampling\nstrategies), overlooking the critical impact of pseudo-positive samples - such\nas homepage clicks merely to visit marketing portals. We propose a unified\nframework for large-scale homepage recommendation sampling and debiasing. Our\nframework consists of two key components: (1) a user intent-aware negative\nsampling module to filter invalid exposure samples, and (2) an intent-driven\ndual-debiasing module that jointly corrects exposure bias and click bias.\nExtensive online experiments on Taobao demonstrate the efficacy of our\nframework, achieving significant improvements in user click-through rates\n(UCTR) by 35.4% and 14.5% in two variants of the marketing block on the Taobao\nhomepage, Baiyibutie and Taobaomiaosha.\n","authors":["Jiaqi Zheng","Cheng Guo","Yi Cao","Chaoqun Hou","Tong Liu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2507.06503v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07436v1","updated":"2025-07-10T05:24:08Z","published":"2025-07-10T05:24:08Z","title":"When Graph Contrastive Learning Backfires: Spectral Vulnerability and\n  Defense in Recommendation","summary":"  Graph Contrastive Learning (GCL) has demonstrated substantial promise in\nenhancing the robustness and generalization of recommender systems,\nparticularly by enabling models to leverage large-scale unlabeled data for\nimproved representation learning. However, in this paper, we reveal an\nunexpected vulnerability: the integration of GCL inadvertently increases the\nsusceptibility of a recommender to targeted promotion attacks. Through both\ntheoretical investigation and empirical validation, we identify the root cause\nas the spectral smoothing effect induced by contrastive optimization, which\ndisperses item embeddings across the representation space and unintentionally\nenhances the exposure of target items. Building on this insight, we introduce\nCLeaR, a bi-level optimization attack method that deliberately amplifies\nspectral smoothness, enabling a systematic investigation of the susceptibility\nof GCL-based recommendation models to targeted promotion attacks. Our findings\nhighlight the urgent need for robust countermeasures; in response, we further\npropose SIM, a spectral irregularity mitigation framework designed to\naccurately detect and suppress targeted items without compromising model\nperformance. Extensive experiments on multiple benchmark datasets demonstrate\nthat, compared to existing targeted promotion attacks, GCL-based recommendation\nmodels exhibit greater susceptibility when evaluated with CLeaR, while SIM\neffectively mitigates these vulnerabilities.\n","authors":["Zongwei Wang","Min Gao","Junliang Yu","Shazia Sadiq","Hongzhi Yin","Ling Liu"],"pdf_url":"https://arxiv.org/pdf/2507.07436v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.15379v2","updated":"2025-07-10T04:14:22Z","published":"2025-01-26T03:29:18Z","title":"Diffusion Augmented Retrieval: A Training-Free Approach to Interactive\n  Text-to-Image Retrieval","summary":"  Interactive Text-to-image retrieval (I-TIR) is an important enabler for a\nwide range of state-of-the-art services in domains such as e-commerce and\neducation. However, current methods rely on finetuned Multimodal Large Language\nModels (MLLMs), which are costly to train and update, and exhibit poor\ngeneralizability. This latter issue is of particular concern, as: 1) finetuning\nnarrows the pretrained distribution of MLLMs, thereby reducing\ngeneralizability; and 2) I-TIR introduces increasing query diversity and\ncomplexity. As a result, I-TIR solutions are highly likely to encounter queries\nand images not well represented in any training dataset. To address this, we\npropose leveraging Diffusion Models (DMs) for text-to-image mapping, to avoid\nfinetuning MLLMs while preserving robust performance on complex queries.\nSpecifically, we introduce Diffusion Augmented Retrieval (DAR), a framework\nthat generates multiple intermediate representations via LLM-based dialogue\nrefinements and DMs, producing a richer depiction of the user's information\nneeds. This augmented representation facilitates more accurate identification\nof semantically and visually related images. Extensive experiments on four\nbenchmarks show that for simple queries, DAR achieves results on par with\nfinetuned I-TIR models, yet without incurring their tuning overhead. Moreover,\nas queries become more complex through additional conversational turns, DAR\nsurpasses finetuned I-TIR models by up to 7.61% in Hits@10 after ten turns,\nillustrating its improved generalization for more intricate queries.\n","authors":["Zijun Long","Kangheng Liang","Gerardo Aragon-Camarasa","Richard Mccreadie","Paul Henderson"],"pdf_url":"https://arxiv.org/pdf/2501.15379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19108v2","updated":"2025-07-10T03:26:36Z","published":"2025-02-26T12:50:58Z","title":"U-Sticker: A Large-Scale Multi-Domain User Sticker Dataset for Retrieval\n  and Personalization","summary":"  Instant messaging with texts and stickers has become a widely adopted\ncommunication medium, enabling efficient expression of user semantics and\nemotions. With the increased use of stickers conveying information and\nfeelings, sticker retrieval and recommendation has emerged as an important area\nof research. However, a major limitation in existing literature has been the\nlack of datasets capturing temporal and user-specific sticker interactions,\nwhich has hindered further progress in user modeling and sticker\npersonalization. To address this, we introduce User-Sticker, a dataset that\nincludes temporal and user anonymous ID across conversations. It is the largest\npublicly available sticker dataset to date, containing 22K unique users, 370K\nstickers, and 8.3M messages. The raw data was collected from a popular\nmessaging platform from 67 conversations over 720 hours of crawling. All text\nand image data were carefully vetted for safety and privacy checks and\nmodifications. Spanning 10 domains, the U-Sticker dataset captures rich\ntemporal, multilingual, and cross-domain behaviors not previously available in\nother datasets. Extensive quantitative and qualitative experiments demonstrate\nU-Sticker's practical applications in user behavior modeling and personalized\nrecommendation and highlight its potential to further research areas in\npersonalized retrieval and conversational studies. U-Sticker dataset is\npublicly available.\n","authors":["Heng Er Metilda Chee","Jiayin Wang","Zhiqiang Guo","Weizhi Ma","Qinglang Guo","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.19108v2.pdf","comment":"Accepted at SIGIR'25"},{"id":"http://arxiv.org/abs/2507.06838v2","updated":"2025-07-10T01:36:33Z","published":"2025-07-09T13:35:36Z","title":"Shifting from Ranking to Set Selection for Retrieval Augmented\n  Generation","summary":"  Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR\n","authors":["Dahyun Lee","Yongrae Jo","Haeju Park","Moontae Lee"],"pdf_url":"https://arxiv.org/pdf/2507.06838v2.pdf","comment":"Accepted to ACL 2025 main (Oral Presentation)"},{"id":"http://arxiv.org/abs/2507.08191v1","updated":"2025-07-10T21:58:41Z","published":"2025-07-10T21:58:41Z","title":"Overview of the TREC 2021 deep learning track","summary":"  This is the third year of the TREC Deep Learning track. As in previous years,\nwe leverage the MS MARCO datasets that made hundreds of thousands of human\nannotated training labels available for both passage and document ranking\ntasks. In addition, this year we refreshed both the document and the passage\ncollections which also led to a nearly four times increase in the document\ncollection size and nearly $16$ times increase in the size of the passage\ncollection. Deep neural ranking models that employ large scale pretraininig\ncontinued to outperform traditional retrieval methods this year. We also found\nthat single stage retrieval can achieve good performance on both tasks although\nthey still do not perform at par with multistage retrieval pipelines. Finally,\nthe increase in the collection size and the general data refresh raised some\nquestions about completeness of NIST judgments and the quality of the training\nlabels that were mapped to the new collections from the old ones which we\ndiscuss in this report.\n","authors":["Nick Craswell","Bhaskar Mitra","Emine Yilmaz","Daniel Campos","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2507.08191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08107v1","updated":"2025-07-10T18:50:05Z","published":"2025-07-10T18:50:05Z","title":"GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs","summary":"  We propose a new approach for generating SPARQL queries on RDF knowledge\ngraphs from natural language questions or keyword queries, using a large\nlanguage model. Our approach does not require fine-tuning. Instead, it uses the\nlanguage model to explore the knowledge graph by strategically executing SPARQL\nqueries and searching for relevant IRIs and literals. We evaluate our approach\non a variety of benchmarks (for knowledge graphs of different kinds and sizes)\nand language models (of different scales and types, commercial as well as\nopen-source) and compare it with existing approaches. On Wikidata we reach\nstate-of-the-art results on multiple benchmarks, despite the zero-shot setting.\nOn Freebase we come close to the best few-shot methods. On other, less commonly\nevaluated knowledge graphs and benchmarks our approach also performs well\noverall. We conduct several additional studies, like comparing different ways\nof searching the graphs, incorporating a feedback mechanism, or making use of\nfew-shot examples.\n","authors":["Sebastian Walter","Hannah Bast"],"pdf_url":"https://arxiv.org/pdf/2507.08107v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2507.08000v1","updated":"2025-07-10T17:59:59Z","published":"2025-07-10T17:59:59Z","title":"Impact of Pretraining Word Co-occurrence on Compositional Generalization\n  in Multimodal Models","summary":"  CLIP and large multimodal models (LMMs) have better accuracy on examples\ninvolving concepts that are highly represented in the training data. However,\nthe role of concept combinations in the training data on compositional\ngeneralization is largely unclear -- for instance, how does accuracy vary when\na common object appears in an uncommon pairing with another object? In this\npaper, we investigate how word co-occurrence statistics in the pretraining\ndataset (a proxy for co-occurrence of visual concepts) impacts CLIP/LMM\nperformance. To disentangle the effects of word co-occurrence frequencies from\nsingle-word frequencies, we measure co-occurrence with pointwise mutual\ninformation (PMI), which normalizes the joint probability of two words\nco-occurring by the probability of co-occurring independently. Using\nsynthetically generated images with a variety of concept pairs, we show a\nstrong correlation between PMI in the CLIP pretraining data and zero-shot\naccuracy in CLIP models trained on LAION-400M (r=0.97 and 14% accuracy gap\nbetween images in the top and bottom 5% of PMI values), demonstrating that even\naccuracy on common concepts is affected by the combination of concepts in the\nimage. Leveraging this finding, we reproduce this effect in natural images by\nediting them to contain pairs with varying PMI, resulting in a correlation of\nr=0.75. Finally, we demonstrate that this behavior in CLIP transfers to LMMs\nbuilt on top of CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2). Our findings\nhighlight the need for algorithms and architectures that improve compositional\ngeneralization in multimodal models without scaling the training data\ncombinatorially. Our code is available at\nhttps://github.com/helenqu/multimodal-pretraining-pmi.\n","authors":["Helen Qu","Sang Michael Xie"],"pdf_url":"https://arxiv.org/pdf/2507.08000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07995v1","updated":"2025-07-10T17:59:53Z","published":"2025-07-10T17:59:53Z","title":"Single-pass Adaptive Image Tokenization for Minimum Program Search","summary":"  According to Algorithmic Information Theory (AIT) -- Intelligent\nrepresentations compress data into the shortest possible program that can\nreconstruct its content, exhibiting low Kolmogorov Complexity (KC). In\ncontrast, most visual representation learning systems use fixed-length\nrepresentations for all inputs, ignoring variations in complexity or\nfamiliarity. Recent adaptive tokenization methods address this by allocating\nvariable-length representations but typically require test-time search over\nmultiple encodings to find the most predictive one. Inspired by Kolmogorov\nComplexity principles, we propose a single-pass adaptive tokenizer, KARL, which\npredicts the appropriate number of tokens for an image in a single forward\npass, halting once its approximate KC is reached. The token count serves as a\nproxy for the minimum description length. KARL's training procedure closely\nresembles the Upside-Down Reinforcement Learning paradigm, as it learns to\nconditionally predict token halting based on a desired reconstruction quality.\nKARL matches the performance of recent adaptive tokenizers while operating in a\nsingle pass. We present scaling laws for KARL, analyzing the role of\nencoder/decoder size, continuous vs. discrete tokenization and more.\nAdditionally, we offer a conceptual study drawing an analogy between Adaptive\nImage Tokenization and Algorithmic Information Theory, examining the predicted\nimage complexity (KC) across axes such as structure vs. noise and in- vs.\nout-of-distribution familiarity -- revealing alignment with human intuition.\n","authors":["Shivam Duggal","Sanghyun Byun","William T. Freeman","Antonio Torralba","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2507.07995v1.pdf","comment":"Code at: https://github.com/ShivamDuggal4/karl Keywords:\n  Representation Learning, Adaptive Tokenization, Compression, Algorithmic\n  Information Theory, Kolmogorov Complexity, Upside-Down RL"},{"id":"http://arxiv.org/abs/2507.07996v1","updated":"2025-07-10T17:59:53Z","published":"2025-07-10T17:59:53Z","title":"Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs","summary":"  Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.\n","authors":["Ziyue Li","Yang Li","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.07996v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2507.07986v1","updated":"2025-07-10T17:57:46Z","published":"2025-07-10T17:57:46Z","title":"EXPO: Stable Reinforcement Learning with Expressive Policies","summary":"  We study the problem of training and fine-tuning expressive policies with\nonline reinforcement learning (RL) given an offline dataset. Training\nexpressive policy classes with online RL present a unique challenge of stable\nvalue maximization. Unlike simpler Gaussian policies commonly used in online\nRL, expressive policies like diffusion and flow-matching policies are\nparameterized by a long denoising chain, which hinders stable gradient\npropagation from actions to policy parameters when optimizing against some\nvalue function. Our key insight is that we can address stable value\nmaximization by avoiding direct optimization over value with the expressive\npolicy and instead construct an on-the-fly RL policy to maximize Q-value. We\npropose Expressive Policy Optimization (EXPO), a sample-efficient online RL\nalgorithm that utilizes an on-the-fly policy to maximize value with two\nparameterized policies -- a larger expressive base policy trained with a stable\nimitation learning objective and a light-weight Gaussian edit policy that edits\nthe actions sampled from the base policy toward a higher value distribution.\nThe on-the-fly policy optimizes the actions from the base policy with the\nlearned edit policy and chooses the value maximizing action from the base and\nedited actions for both sampling and temporal-difference (TD) backup. Our\napproach yields up to 2-3x improvement in sample efficiency on average over\nprior methods both in the setting of fine-tuning a pretrained policy given\noffline data and in leveraging offline data to train online.\n","authors":["Perry Dong","Qiyang Li","Dorsa Sadigh","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2507.07986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08938v2","updated":"2025-07-10T17:55:06Z","published":"2024-10-11T16:03:58Z","title":"KinDEL: DNA-Encoded Library Dataset for Kinase Inhibitors","summary":"  DNA-Encoded Libraries (DELs) represent a transformative technology in drug\ndiscovery, facilitating the high-throughput exploration of vast chemical\nspaces. Despite their potential, the scarcity of publicly available DEL\ndatasets presents a bottleneck for the advancement of machine learning\nmethodologies in this domain. To address this gap, we introduce KinDEL, one of\nthe largest publicly accessible DEL datasets and the first one that includes\nbinding poses from molecular docking experiments. Focused on two kinases,\nMitogen-Activated Protein Kinase 14 (MAPK14) and Discoidin Domain Receptor\nTyrosine Kinase 1 (DDR1), KinDEL includes 81 million compounds, offering a rich\nresource for computational exploration. Additionally, we provide comprehensive\nbiophysical assay validation data, encompassing both on-DNA and off-DNA\nmeasurements, which we use to evaluate a suite of machine learning techniques,\nincluding novel structure-based probabilistic models. We hope that our\nbenchmark, encompassing both 2D and 3D structures, will help advance the\ndevelopment of machine learning models for data-driven hit identification using\nDELs.\n","authors":["Benson Chen","Tomasz Danel","Gabriel H. S. Dreiman","Patrick J. McEnaney","Nikhil Jain","Kirill Novikov","Spurti Umesh Akki","Joshua L. Turnbull","Virja Atul Pandya","Boris P. Belotserkovskii","Jared Bryce Weaver","Ankita Biswas","Dat Nguyen","Kent Gorday","Mohammad Sultan","Nathaniel Stanley","Daniel M Whalen","Divya Kanichar","Christoph Klein","Emily Fox","R. Edward Watts"],"pdf_url":"https://arxiv.org/pdf/2410.08938v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07981v1","updated":"2025-07-10T17:55:05Z","published":"2025-07-10T17:55:05Z","title":"Why is Your Language Model a Poor Implicit Reward Model?","summary":"  Reward models are key to language model post-training and inference\npipelines. Conveniently, recent work showed that every language model defines\nan implicit reward model (IM-RM), without requiring any architectural changes.\nHowever, such IM-RMs tend to generalize worse, especially out-of-distribution,\ncompared to explicit reward models (EX-RMs) that apply a dedicated linear head\nover the hidden representations of a language model. The existence of a\ngeneralization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They\ncan be trained using the same data, loss function, and language model, and\ndiffer only in how the reward is computed. Towards a fundamental understanding\nof the implicit biases underlying different reward model types, we investigate\nthe root cause of this gap. Our main finding, backed by theory and experiments,\nis that IM-RMs rely more heavily on superficial token-level cues. Consequently,\nthey often generalize worse than EX-RMs under token-level distribution shifts,\nas well as in-distribution. Furthermore, we provide evidence against\nalternative hypotheses for the generalization gap. Most notably, we challenge\nthe intuitive claim that IM-RMs struggle in tasks where generation is harder\nthan verification because they can operate both as a verifier and a generator.\nTaken together, our results highlight that seemingly minor design choices can\nsubstantially impact the generalization behavior of reward models.\n","authors":["Noam Razin","Yong Lin","Jiarui Yao","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2507.07981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.04462v2","updated":"2025-07-10T17:50:21Z","published":"2025-06-04T21:29:07Z","title":"Watermarking Degrades Alignment in Language Models: Analysis and\n  Mitigation","summary":"  Watermarking techniques for large language models (LLMs) can significantly\nimpact output quality, yet their effects on truthfulness, safety, and\nhelpfulness remain critically underexamined. This paper presents a systematic\nanalysis of how two popular watermarking approaches-Gumbel and KGW-affect these\ncore alignment properties across four aligned LLMs. Our experiments reveal two\ndistinct degradation patterns: guard attenuation, where enhanced helpfulness\nundermines model safety, and guard amplification, where excessive caution\nreduces model helpfulness. These patterns emerge from watermark-induced shifts\nin token distribution, surfacing the fundamental tension that exists between\nalignment objectives.\n  To mitigate these degradations, we propose Alignment Resampling (AR), an\ninference-time sampling method that uses an external reward model to restore\nalignment. We establish a theoretical lower bound on the improvement in\nexpected reward score as the sample size is increased and empirically\ndemonstrate that sampling just 2-4 watermarked generations effectively recovers\nor surpasses baseline (unwatermarked) alignment scores. To overcome the limited\nresponse diversity of standard Gumbel watermarking, our modified implementation\nsacrifices strict distortion-freeness while maintaining robust detectability,\nensuring compatibility with AR. Experimental results confirm that AR\nsuccessfully recovers baseline alignment in both watermarking approaches, while\nmaintaining strong watermark detectability. This work reveals the critical\nbalance between watermark strength and model alignment, providing a simple\ninference-time solution to responsibly deploy watermarked LLMs in practice.\n","authors":["Apurv Verma","NhatHai Phan","Shubhendu Trivedi"],"pdf_url":"https://arxiv.org/pdf/2506.04462v2.pdf","comment":"Published at the 1st Workshop on GenAI Watermarking, collocated with\n  ICLR 2025. OpenReview: https://openreview.net/forum?id=SIBkIV48gF"},{"id":"http://arxiv.org/abs/2507.07969v1","updated":"2025-07-10T17:48:03Z","published":"2025-07-10T17:48:03Z","title":"Reinforcement Learning with Action Chunking","summary":"  We present Q-chunking, a simple yet effective recipe for improving\nreinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.\nOur recipe is designed for the offline-to-online RL setting, where the goal is\nto leverage an offline prior dataset to maximize the sample-efficiency of\nonline learning. Effective exploration and sample-efficient learning remain\ncentral challenges in this setting, as it is not obvious how the offline data\nshould be utilized to acquire a good exploratory policy. Our key insight is\nthat action chunking, a technique popularized in imitation learning where\nsequences of future actions are predicted rather than a single action at each\ntimestep, can be applied to temporal difference (TD)-based RL methods to\nmitigate the exploration challenge. Q-chunking adopts action chunking by\ndirectly running RL in a 'chunked' action space, enabling the agent to (1)\nleverage temporally consistent behaviors from offline data for more effective\nonline exploration and (2) use unbiased $n$-step backups for more stable and\nefficient TD learning. Our experimental results demonstrate that Q-chunking\nexhibits strong offline performance and online sample efficiency, outperforming\nprior best offline-to-online methods on a range of long-horizon, sparse-reward\nmanipulation tasks.\n","authors":["Qiyang Li","Zhiyuan Zhou","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2507.07969v1.pdf","comment":"25 pages, 15 figures"},{"id":"http://arxiv.org/abs/2507.07965v1","updated":"2025-07-10T17:45:15Z","published":"2025-07-10T17:45:15Z","title":"Prospective Learning in Retrospect","summary":"  In most real-world applications of artificial intelligence, the distributions\nof the data and the goals of the learners tend to change over time. The\nProbably Approximately Correct (PAC) learning framework, which underpins most\nmachine learning algorithms, fails to account for dynamic data distributions\nand evolving objectives, often resulting in suboptimal performance. Prospective\nlearning is a recently introduced mathematical framework that overcomes some of\nthese limitations. We build on this framework to present preliminary results\nthat improve the algorithm and numerical results, and extend prospective\nlearning to sequential decision-making scenarios, specifically foraging. Code\nis available at: https://github.com/neurodata/prolearn2.\n","authors":["Yuxin Bai","Cecelia Shuai","Ashwin De Silva","Siyu Yu","Pratik Chaudhari","Joshua T. Vogelstein"],"pdf_url":"https://arxiv.org/pdf/2507.07965v1.pdf","comment":"Accepted to AGI 2025"},{"id":"http://arxiv.org/abs/2507.07955v1","updated":"2025-07-10T17:39:37Z","published":"2025-07-10T17:39:37Z","title":"Dynamic Chunking for End-to-End Hierarchical Sequence Modeling","summary":"  Despite incredible progress in language models (LMs) in recent years, largely\nresulting from moving away from specialized models designed for specific tasks\nto general models based on powerful architectures (e.g. the Transformer) that\nlearn everything from raw data, pre-processing steps such as tokenization\nremain a barrier to true end-to-end foundation models. We introduce a\ncollection of new techniques that enable a dynamic chunking mechanism which\nautomatically learns content -- and context -- dependent segmentation\nstrategies learned jointly with the rest of the model. Incorporating this into\nan explicit hierarchical network (H-Net) allows replacing the (implicitly\nhierarchical) tokenization-LM-detokenization pipeline with a single model\nlearned fully end-to-end. When compute- and data- matched, an H-Net with one\nstage of hierarchy operating at the byte level outperforms a strong Transformer\nlanguage model operating over BPE tokens. Iterating the hierarchy to multiple\nstages further increases its performance by modeling multiple levels of\nabstraction, demonstrating significantly better scaling with data and matching\na token-based Transformer of twice its size. H-Nets pretrained on English show\nsignificantly increased character-level robustness, and qualitatively learn\nmeaningful data-dependent chunking strategies without any heuristics or\nexplicit supervision. Finally, the H-Net's improvement over tokenized pipelines\nis further increased in languages and modalities with weaker tokenization\nheuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement\nin data efficiency over baselines), showing the potential of true end-to-end\nmodels that learn and scale better from unprocessed data.\n","authors":["Sukjun Hwang","Brandon Wang","Albert Gu"],"pdf_url":"https://arxiv.org/pdf/2507.07955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07949v1","updated":"2025-07-10T17:33:52Z","published":"2025-07-10T17:33:52Z","title":"TinierHAR: Towards Ultra-Lightweight Deep Learning Models for Efficient\n  Human Activity Recognition on Edge Devices","summary":"  Human Activity Recognition (HAR) on resource-constrained wearable devices\ndemands inference models that harmonize accuracy with computational efficiency.\nThis paper introduces TinierHAR, an ultra-lightweight deep learning\narchitecture that synergizes residual depthwise separable convolutions, gated\nrecurrent units (GRUs), and temporal aggregation to achieve SOTA efficiency\nwithout compromising performance. Evaluated across 14 public HAR datasets,\nTinierHAR reduces Parameters by 2.7x (vs. TinyHAR) and 43.3x (vs.\nDeepConvLSTM), and MACs by 6.4x and 58.6x, respectively, while maintaining the\naveraged F1-scores. Beyond quantitative gains, this work provides the first\nsystematic ablation study dissecting the contributions of spatial-temporal\ncomponents across proposed TinierHAR, prior SOTA TinyHAR, and the classical\nDeepConvLSTM, offering actionable insights for designing efficient HAR systems.\nWe finally discussed the findings and suggested principled design guidelines\nfor future efficient HAR. To catalyze edge-HAR research, we open-source all\nmaterials in this work for future\nbenchmarking\\footnote{https://github.com/zhaxidele/TinierHAR}\n","authors":["Sizhen Bian","Mengxi Liu","Vitor Fortes Rey","Daniel Geissler","Paul Lukowicz"],"pdf_url":"https://arxiv.org/pdf/2507.07949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07947v1","updated":"2025-07-10T17:32:26Z","published":"2025-07-10T17:32:26Z","title":"Low Resource Reconstruction Attacks Through Benign Prompts","summary":"  The recent advances in generative models such as diffusion models have raised\nseveral risks and concerns related to privacy, copyright infringements and data\nstewardship. To better understand and control the risks, various researchers\nhave created techniques, experiments and attacks that reconstruct images, or\npart of images, from the training set. While these techniques already establish\nthat data from the training set can be reconstructed, they often rely on\nhigh-resources, excess to the training set as well as well-engineered and\ndesigned prompts.\n  In this work, we devise a new attack that requires low resources, assumes\nlittle to no access to the actual training set, and identifies, seemingly,\nbenign prompts that lead to potentially-risky image reconstruction. This\nhighlights the risk that images might even be reconstructed by an uninformed\nuser and unintentionally. For example, we identified that, with regard to one\nexisting model, the prompt ``blue Unisex T-Shirt'' can generate the face of a\nreal-life human model. Our method builds on an intuition from previous works\nwhich leverages domain knowledge and identifies a fundamental vulnerability\nthat stems from the use of scraped data from e-commerce platforms, where\ntemplated layouts and images are tied to pattern-like prompts.\n","authors":["Sol Yarkoni","Roi Livni"],"pdf_url":"https://arxiv.org/pdf/2507.07947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02401v3","updated":"2025-07-10T17:18:00Z","published":"2023-11-04T13:25:49Z","title":"BarcodeBERT: Transformers for Biodiversity Analysis","summary":"  In the global challenge of understanding and characterizing biodiversity,\nshort species-specific genomic sequences known as DNA barcodes play a critical\nrole, enabling fine-grained comparisons among organisms within the same kingdom\nof life. Although machine learning algorithms specifically designed for the\nanalysis of DNA barcodes are becoming more popular, most existing methodologies\nrely on generic supervised training algorithms. We introduce BarcodeBERT, a\nfamily of models tailored to biodiversity analysis and trained exclusively on\ndata from a reference library of 1.5M invertebrate DNA barcodes. We compared\nthe performance of BarcodeBERT on taxonomic identification tasks against a\nspectrum of machine learning approaches including supervised training of\nclassical neural architectures and fine-tuning of general DNA foundation\nmodels. Our self-supervised pretraining strategies on domain-specific data\noutperform fine-tuned foundation models, especially in identification tasks\ninvolving lower taxa such as genera and species. We also compared BarcodeBERT\nwith BLAST, one of the most widely used bioinformatics tools for sequence\nsearching, and found that our method matched BLAST's performance in\nspecies-level classification while being 55 times faster. Our analysis of\nmasking and tokenization strategies also provides practical guidance for\nbuilding customized DNA language models, emphasizing the importance of aligning\nmodel training strategies with dataset characteristics and domain knowledge.\nThe code repository is available at https://github.com/bioscan-ml/BarcodeBERT.\n","authors":["Pablo Millan Arias","Niousha Sadjadi","Monireh Safari","ZeMing Gong","Austin T. Wang","Joakim Bruslund Haurum","Iuliia Zarubiieva","Dirk Steinke","Lila Kari","Angel X. Chang","Scott C. Lowe","Graham W. Taylor"],"pdf_url":"https://arxiv.org/pdf/2311.02401v3.pdf","comment":"Main text: 14 pages, Total: 23 pages, 10 figures, formerly accepted\n  at the 4th Workshop on Self-Supervised Learning: Theory and Practice (NeurIPS\n  2023)"},{"id":"http://arxiv.org/abs/2507.07929v1","updated":"2025-07-10T17:09:14Z","published":"2025-07-10T17:09:14Z","title":"Towards Continuous Home Cage Monitoring: An Evaluation of Tracking and\n  Identification Strategies for Laboratory Mice","summary":"  Continuous, automated monitoring of laboratory mice enables more accurate\ndata collection and improves animal welfare through real-time insights.\nResearchers can achieve a more dynamic and clinically relevant characterization\nof disease progression and therapeutic effects by integrating behavioral and\nphysiological monitoring in the home cage. However, providing individual mouse\nmetrics is difficult because of their housing density, similar appearances,\nhigh mobility, and frequent interactions. To address these challenges, we\ndevelop a real-time identification (ID) algorithm that accurately assigns ID\npredictions to mice wearing custom ear tags in digital home cages monitored by\ncameras. Our pipeline consists of three parts: (1) a custom multiple object\ntracker (MouseTracks) that combines appearance and motion cues from mice; (2) a\ntransformer-based ID classifier (Mouseformer); and (3) a tracklet associator\nlinear program to assign final ID predictions to tracklets (MouseMap). Our\nmodels assign an animal ID based on custom ear tags at 30 frames per second\nwith 24/7 cage coverage. We show that our custom tracking and ID pipeline\nimproves tracking efficiency and lowers ID switches across mouse strains and\nvarious environmental factors compared to current mouse tracking methods.\n","authors":["Juan Pablo Oberhauser","Daniel Grzenda"],"pdf_url":"https://arxiv.org/pdf/2507.07929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.00004v2","updated":"2025-07-10T17:08:40Z","published":"2025-06-10T14:47:48Z","title":"A Theory of Inference Compute Scaling: Reasoning through Directed\n  Stochastic Skill Search","summary":"  Large language models (LLMs) demand considerable computational, energy, and\nfinancial resources during both training and deployment. While scaling laws for\ntraining have guided much of the field's recent progress, inference costs now\nrepresent a significant and growing component of the overall resource burden,\nparticularly for reasoning-focused models. Existing characterizations of\ncompute-optimality that consider model size, dataset size, and inference tokens\nin isolation or in fixed combinations risk overlooking more efficient operating\npoints. We introduce directed stochastic skill search (DS3), a general\nframework that represents inference as stochastic traversal over a learned\nskill graph. From a simplified yet expressive instantiation, we derive\nclosed-form expressions for task success and compute cost across a wide range\nof inference strategies -- including chain-of-thought (CoT) and tree-of-thought\n(ToT) -- enabling comparative analysis as a function of task difficulty and\nmodel capability. To that end, we extend a prior first-principles tripartite\ngraph framework of LLM training to incorporate inference, and separately bridge\nDS3 with empirical methods that characterize LLM scaling behavior. We\ntheoretically recover empirically observed patterns, including: linear accuracy\nscaling with logarithmic compute; variation in preferred inference strategies\nas a function of task difficulty and model capability; emergent behavior\nelicited by reasoning even when performance plateaus under parameter scaling;\nand both best-of-N (BoN) and majority voting behavior captured within a unified\nanalytical framework. By explicitly characterizing training-inference\ninterdependencies, our framework deepens theoretical understanding and supports\nprincipled algorithmic design and resource allocation.\n","authors":["Austin R. Ellis-Mohr","Anuj K. Nayak","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2507.00004v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18563v2","updated":"2025-07-10T17:01:15Z","published":"2024-05-28T20:15:09Z","title":"No $D_{\\text{train}}$: Model-Agnostic Counterfactual Explanations Using\n  Reinforcement Learning","summary":"  Machine learning (ML) methods have experienced significant growth in the past\ndecade, yet their practical application in high-impact real-world domains has\nbeen hindered by their opacity. When ML methods are responsible for making\ncritical decisions, stakeholders often require insights into how to alter these\ndecisions. Counterfactual explanations (CFEs) have emerged as a solution,\noffering interpretations of opaque ML models and providing a pathway to\ntransition from one decision to another. However, most existing CFE methods\nrequire access to the model's training dataset, few methods can handle\nmultivariate time-series, and none of model-agnostic CFE methods can handle\nmultivariate time-series without training datasets. These limitations can be\nformidable in many scenarios. In this paper, we present NTD-CFE, a novel\nmodel-agnostic CFE method based on reinforcement learning (RL) that generates\nCFEs when training datasets are unavailable. NTD-CFE is suitable for both\nstatic and multivariate time-series datasets with continuous and discrete\nfeatures. NTD-CFE reduces the CFE search space from a multivariate time-series\ndomain to a lower dimensional space and addresses the problem using RL. Users\nhave the flexibility to specify non-actionable, immutable, and preferred\nfeatures, as well as causal constraints. We demonstrate the performance of\nNTD-CFE against four baselines on several datasets and find that, despite not\nhaving access to a training dataset, NTD-CFE finds CFEs that make significantly\nfewer and significantly smaller changes to the input time-series. These\nproperties make CFEs more actionable, as the magnitude of change required to\nalter an outcome is vastly reduced. The code is available in the supplementary\nmaterial.\n","authors":["Xiangyu Sun","Raquel Aoki","Kevin H. Wilson"],"pdf_url":"https://arxiv.org/pdf/2405.18563v2.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR 2025)"},{"id":"http://arxiv.org/abs/2507.07919v1","updated":"2025-07-10T16:59:51Z","published":"2025-07-10T16:59:51Z","title":"Plausible Counterfactual Explanations of Recommendations","summary":"  Explanations play a variety of roles in various recommender systems, from a\nlegally mandated afterthought, through an integral element of user experience,\nto a key to persuasiveness. A natural and useful form of an explanation is the\nCounterfactual Explanation (CE). We present a method for generating highly\nplausible CEs in recommender systems and evaluate it both numerically and with\na user study.\n","authors":["Jakub Černý","Jiří Němeček","Ivan Dovica","Jakub Mareček"],"pdf_url":"https://arxiv.org/pdf/2507.07919v1.pdf","comment":"8 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2507.07907v1","updated":"2025-07-10T16:39:46Z","published":"2025-07-10T16:39:46Z","title":"A statistical physics framework for optimal learning","summary":"  Learning is a complex dynamical process shaped by a range of interconnected\ndecisions. Careful design of hyperparameter schedules for artificial neural\nnetworks or efficient allocation of cognitive resources by biological learners\ncan dramatically affect performance. Yet, theoretical understanding of optimal\nlearning strategies remains sparse, especially due to the intricate interplay\nbetween evolving meta-parameters and nonlinear learning dynamics. The search\nfor optimal protocols is further hindered by the high dimensionality of the\nlearning space, often resulting in predominantly heuristic, difficult to\ninterpret, and computationally demanding solutions. Here, we combine\nstatistical physics with control theory in a unified theoretical framework to\nidentify optimal protocols in prototypical neural network models. In the\nhigh-dimensional limit, we derive closed-form ordinary differential equations\nthat track online stochastic gradient descent through low-dimensional order\nparameters. We formulate the design of learning protocols as an optimal control\nproblem directly on the dynamics of the order parameters with the goal of\nminimizing the generalization error at the end of training. This framework\nencompasses a variety of learning scenarios, optimization constraints, and\ncontrol budgets. We apply it to representative cases, including optimal\ncurricula, adaptive dropout regularization and noise schedules in denoising\nautoencoders. We find nontrivial yet interpretable strategies highlighting how\noptimal protocols mediate crucial learning tradeoffs, such as maximizing\nalignment with informative input directions while minimizing noise fitting.\nFinally, we show how to apply our framework to real datasets. Our results\nestablish a principled foundation for understanding and designing optimal\nlearning protocols and suggest a path toward a theory of meta-learning grounded\nin statistical physics.\n","authors":["Francesca Mignacco","Francesco Mori"],"pdf_url":"https://arxiv.org/pdf/2507.07907v1.pdf","comment":"35 pages, 13 figures"},{"id":"http://arxiv.org/abs/2507.07906v1","updated":"2025-07-10T16:38:59Z","published":"2025-07-10T16:38:59Z","title":"Agentic Retrieval of Topics and Insights from Earnings Calls","summary":"  Tracking the strategic focus of companies through topics in their earnings\ncalls is a key task in financial analysis. However, as industries evolve,\ntraditional topic modeling techniques struggle to dynamically capture emerging\ntopics and their relationships. In this work, we propose an LLM-agent driven\napproach to discover and retrieve emerging topics from quarterly earnings\ncalls. We propose an LLM-agent to extract topics from documents, structure them\ninto a hierarchical ontology, and establish relationships between new and\nexisting topics through a topic ontology. We demonstrate the use of extracted\ntopics to infer company-level insights and emerging trends over time. We\nevaluate our approach by measuring ontology coherence, topic evolution\naccuracy, and its ability to surface emerging financial trends.\n","authors":["Anant Gupta","Rajarshi Bhowmik","Geoffrey Gunow"],"pdf_url":"https://arxiv.org/pdf/2507.07906v1.pdf","comment":"The 2nd Workshop on Financial Information Retrieval in the Era of\n  Generative AI, The 48th International ACM SIGIR Conference on Research and\n  Development in Information Retrieval July 13-17, 2025 | Padua, Italy"},{"id":"http://arxiv.org/abs/2507.07898v1","updated":"2025-07-10T16:27:33Z","published":"2025-07-10T16:27:33Z","title":"Efficient Causal Discovery for Autoregressive Time Series","summary":"  In this study, we present a novel constraint-based algorithm for causal\nstructure learning specifically designed for nonlinear autoregressive time\nseries. Our algorithm significantly reduces computational complexity compared\nto existing methods, making it more efficient and scalable to larger problems.\nWe rigorously evaluate its performance on synthetic datasets, demonstrating\nthat our algorithm not only outperforms current techniques, but also excels in\nscenarios with limited data availability. These results highlight its potential\nfor practical applications in fields requiring efficient and accurate causal\ninference from nonlinear time series data.\n","authors":["Mohammad Fesanghary","Achintya Gopal"],"pdf_url":"https://arxiv.org/pdf/2507.07898v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2506.11315v2","updated":"2025-07-10T16:24:53Z","published":"2025-06-12T21:31:08Z","title":"Sampling Imbalanced Data with Multi-objective Bilevel Optimization","summary":"  Two-class classification problems are often characterized by an imbalance\nbetween the number of majority and minority datapoints resulting in poor\nclassification of the minority class in particular. Traditional approaches,\nsuch as reweighting the loss function or na\\\"ive resampling, risk overfitting\nand subsequently fail to improve classification because they do not consider\nthe diversity between majority and minority datasets. Such consideration is\ninfeasible because there is no metric that can measure the impact of imbalance\non the model. To obviate these challenges, we make two key contributions.\nFirst, we introduce MOODS~(Multi-Objective Optimization for Data Sampling), a\nnovel multi-objective bilevel optimization framework that guides both synthetic\noversampling and majority undersampling. Second, we introduce a validation\nmetric -- `$\\epsilon/ \\delta$ non-overlapping diversification metric' -- that\nquantifies the goodness of a sampling method towards model performance. With\nthis metric we experimentally demonstrate state-of-the-art performance with\nimprovement in diversity driving a $1-15 \\%$ increase in $F1$ scores.\n","authors":["Karen Medlin","Sven Leyffer","Krishnan Raghavan"],"pdf_url":"https://arxiv.org/pdf/2506.11315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10733v2","updated":"2025-07-10T16:19:07Z","published":"2025-04-14T21:56:11Z","title":"Cross-Problem Parameter Transfer in Quantum Approximate Optimization\n  Algorithm: A Machine Learning Approach","summary":"  Quantum Approximate Optimization Algorithm (QAOA) is one of the most\npromising candidates to achieve the quantum advantage in solving combinatorial\noptimization problems. The process of finding a good set of variational\nparameters in the QAOA circuit has proven to be challenging due to multiple\nfactors, such as barren plateaus. As a result, there is growing interest in\nexploiting parameter transferability, where parameter sets optimized for one\nproblem instance are transferred to another that could be more complex either\nto estimate the solution or to serve as a warm start for further optimization.\nBut can we transfer parameters from one class of problems to another?\nLeveraging parameter sets learned from a well-studied class of problems could\nhelp navigate the less studied one, reducing optimization overhead and\nmitigating performance pitfalls. In this paper, we study whether pretrained\nQAOA parameters of MaxCut can be used as is or to warm start the Maximum\nIndependent Set (MIS) circuits. Specifically, we design machine learning models\nto find good donor candidates optimized on MaxCut and apply their parameters to\nMIS acceptors. Our experimental results show that such parameter transfer can\nsignificantly reduce the number of optimization iterations required while\nachieving comparable approximation ratios.\n","authors":["Kien X. Nguyen","Bao Bach","Ilya Safro"],"pdf_url":"https://arxiv.org/pdf/2504.10733v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06687v3","updated":"2025-07-10T16:14:55Z","published":"2024-08-13T07:27:02Z","title":"Masked Image Modeling: A Survey","summary":"  In this work, we survey recent studies on masked image modeling (MIM), an\napproach that emerged as a powerful self-supervised learning technique in\ncomputer vision. The MIM task involves masking some information, e.g. pixels,\npatches, or even latent representations, and training a model, usually an\nautoencoder, to predicting the missing information by using the context\navailable in the visible part of the input. We identify and formalize two\ncategories of approaches on how to implement MIM as a pretext task, one based\non reconstruction and one based on contrastive learning. Then, we construct a\ntaxonomy and review the most prominent papers in recent years. We complement\nthe manually constructed taxonomy with a dendrogram obtained by applying a\nhierarchical clustering algorithm. We further identify relevant clusters via\nmanually inspecting the resulting dendrogram. Our review also includes datasets\nthat are commonly used in MIM research. We aggregate the performance results of\nvarious masked image modeling methods on the most popular datasets, to\nfacilitate the comparison of competing methods. Finally, we identify research\ngaps and propose several interesting directions of future work. We supplement\nour survey with the following public repository containing organized\nreferences: https://github.com/vladhondru25/MIM-Survey.\n","authors":["Vlad Hondru","Florinel Alin Croitoru","Shervin Minaee","Radu Tudor Ionescu","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2408.06687v3.pdf","comment":"Accepted at the International Journal of Computer Vision"},{"id":"http://arxiv.org/abs/2410.11171v3","updated":"2025-07-10T16:14:13Z","published":"2024-10-15T01:17:23Z","title":"A Bilevel Optimization Framework for Imbalanced Data Classification","summary":"  Data rebalancing techniques, including oversampling and undersampling, are a\ncommon approach to addressing the challenges of imbalanced data. To tackle\nunresolved problems related to both oversampling and undersampling, we propose\na new undersampling approach that: (i) avoids the pitfalls of noise and overlap\ncaused by synthetic data and (ii) avoids the pitfall of under-fitting caused by\nrandom undersampling. Instead of undersampling majority data randomly, our\nmethod undersamples datapoints based on their ability to improve model loss.\nUsing improved model loss as a proxy measurement for classification\nperformance, our technique assesses a datapoint's impact on loss and rejects\nthose unable to improve it. In so doing, our approach rejects majority\ndatapoints redundant to datapoints already accepted and, thereby, finds an\noptimal subset of majority training data for classification. The accept/reject\ncomponent of our algorithm is motivated by a bilevel optimization problem\nuniquely formulated to identify the optimal training set we seek. Experimental\nresults show our proposed technique with F1 scores up to 10% higher than\nstate-of-the-art methods.\n","authors":["Karen Medlin","Sven Leyffer","Krishnan Raghavan"],"pdf_url":"https://arxiv.org/pdf/2410.11171v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07885v1","updated":"2025-07-10T16:12:06Z","published":"2025-07-10T16:12:06Z","title":"UnIT: Scalable Unstructured Inference-Time Pruning for MAC-efficient\n  Neural Inference on MCUs","summary":"  Existing pruning methods are typically applied during training or compile\ntime and often rely on structured sparsity. While compatible with low-power\nmicrocontrollers (MCUs), structured pruning underutilizes the opportunity for\nfine-grained efficiency on devices without SIMD support or parallel compute. To\naddress these limitations, we introduce UnIT (Unstructured Inference-Time\npruning), a lightweight method that dynamically identifies and skips\nunnecessary multiply-accumulate (MAC) operations during inference, guided by\ninput-specific activation patterns. Unlike structured pruning, UnIT embraces\nirregular sparsity and does not require retraining or hardware specialization.\nIt transforms pruning decisions into lightweight comparisons, replacing\nmultiplications with threshold checks and approximated divisions. UnIT further\noptimizes compute by reusing threshold computations across multiple connections\nand applying layer- and group-specific pruning sensitivity. We present three\nfast, hardware-friendly division approximations tailored to the capabilities of\ncommon embedded platforms. Demonstrated on the MSP430 microcontroller, UnIT\nachieves 11.02% to 82.03% MAC reduction, 27.30% to 84.19% faster inference, and\n27.33% to 84.38% lower energy consumption compared to training-time pruned\nmodels, while maintaining accuracy with 0.48-7%. Under domain shift, UnIT\nmatches or exceeds the accuracy of retrained models while requiring\nsignificantly fewer MACs. These results establish unstructured inference-time\npruning as a viable and practical solution for efficient, retraining-free\ndeployment of deep neural networks on MCUs.\n","authors":["Ashe Neth","Sawinder kaur","Mohammad Nur Hossain Khan","Subrata Biswas","Asif Salekin","Bashima Islam"],"pdf_url":"https://arxiv.org/pdf/2507.07885v1.pdf","comment":"Submitted to SenSys 2026 on July 1, 2025"},{"id":"http://arxiv.org/abs/2507.07883v1","updated":"2025-07-10T16:06:02Z","published":"2025-07-10T16:06:02Z","title":"SAMO: A Lightweight Sharpness-Aware Approach for Multi-Task Optimization\n  with Joint Global-Local Perturbation","summary":"  Multi-task learning (MTL) enables a joint model to capture commonalities\nacross multiple tasks, reducing computation costs and improving data\nefficiency. However, a major challenge in MTL optimization is task conflicts,\nwhere the task gradients differ in direction or magnitude, limiting model\nperformance compared to single-task counterparts. Sharpness-aware minimization\n(SAM) minimizes task loss while simultaneously reducing the sharpness of the\nloss landscape. Our empirical observations show that SAM effectively mitigates\ntask conflicts in MTL. Motivated by these findings, we explore integrating SAM\ninto MTL but face two key challenges. While both the average loss gradient and\nindividual task gradients-referred to as global and local\ninformation-contribute to SAM, how to combine them remains unclear. Moreover,\ndirectly computing each task gradient introduces significant computational and\nmemory overheads. To address these challenges, we propose SAMO, a lightweight\n\\textbf{S}harpness-\\textbf{A}ware \\textbf{M}ulti-task \\textbf{O}ptimization\napproach, that leverages a joint global-local perturbation. The local\nperturbations are approximated using only forward passes and are layerwise\nnormalized to improve efficiency. Extensive experiments on a suite of\nmulti-task benchmarks demonstrate both the effectiveness and efficiency of our\nmethod. Code is available at https://github.com/OptMN-Lab/SAMO.\n","authors":["Hao Ban","Gokul Ram Subramani","Kaiyi Ji"],"pdf_url":"https://arxiv.org/pdf/2507.07883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07882v1","updated":"2025-07-10T16:05:16Z","published":"2025-07-10T16:05:16Z","title":"Can AI-predicted complexes teach machine learning to compute drug\n  binding affinity?","summary":"  We evaluate the feasibility of using co-folding models for synthetic data\naugmentation in training machine learning-based scoring functions (MLSFs) for\nbinding affinity prediction. Our results show that performance gains depend\ncritically on the structural quality of augmented data. In light of this, we\nestablished simple heuristics for identifying high-quality co-folding\npredictions without reference structures, enabling them to substitute for\nexperimental structures in MLSF training. Our study informs future data\naugmentation strategies based on co-folding models.\n","authors":["Wei-Tse Hsu","Savva Grevtsev","Thomas Douglas","Aniket Magarkar","Philip C. Biggin"],"pdf_url":"https://arxiv.org/pdf/2507.07882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06952v2","updated":"2025-07-10T16:01:42Z","published":"2025-07-09T15:36:15Z","title":"What Has a Foundation Model Found? Using Inductive Bias to Probe for\n  World Models","summary":"  Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.\n","authors":["Keyon Vafa","Peter G. Chang","Ashesh Rambachan","Sendhil Mullainathan"],"pdf_url":"https://arxiv.org/pdf/2507.06952v2.pdf","comment":"To appear in ICML 2025"},{"id":"http://arxiv.org/abs/2507.07877v1","updated":"2025-07-10T16:00:27Z","published":"2025-07-10T16:00:27Z","title":"Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition\n  Models","summary":"  Recent advances in Automatic Speech Recognition (ASR) have demonstrated\nremarkable accuracy and robustness in diverse audio applications, such as live\ntranscription and voice command processing. However, deploying these models on\nresource constrained edge devices (e.g., IoT device, wearables) still presents\nsubstantial challenges due to strict limits on memory, compute and power.\nQuantization, particularly Post-Training Quantization (PTQ), offers an\neffective way to reduce model size and inference cost without retraining.\nDespite its importance, the performance implications of various advanced\nquantization methods and bit-width configurations on ASR models remain unclear.\nIn this work, we present a comprehensive benchmark of eight state-of-the-art\n(SOTA) PTQ methods applied to two leading edge-ASR model families, Whisper and\nMoonshine. We systematically evaluate model performances (i.e., accuracy,\nmemory I/O and bit operations) across seven diverse datasets from the open ASR\nleaderboard, analyzing the impact of quantization and various configurations on\nboth weights and activations. Built on an extension of the LLM compression\ntoolkit, our framework integrates edge-ASR models, diverse advanced\nquantization algorithms, a unified calibration and evaluation data pipeline,\nand detailed analysis tools. Our results characterize the trade-offs between\nefficiency and accuracy, demonstrating that even 3-bit quantization can succeed\non high capacity models when using advanced PTQ techniques. These findings\nprovide valuable insights for optimizing ASR models on low-power, always-on\nedge devices.\n","authors":["Chen Feng","Yicheng Lin","Shaojie Zhuo","Chenzheng Su","Ramchalam Kinattinkara Ramakrishnan","Zhaocong Yuan","Xiaopeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.07877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04931v2","updated":"2025-07-10T16:00:00Z","published":"2025-05-08T04:09:36Z","title":"Fair Uncertainty Quantification for Depression Prediction","summary":"  Trustworthy depression prediction based on deep learning, incorporating both\npredictive reliability and algorithmic fairness across diverse demographic\ngroups, is crucial for clinical application. Recently, achieving reliable\ndepression predictions through uncertainty quantification has attracted\nincreasing attention. However, few studies have focused on the fairness of\nuncertainty quantification (UQ) in depression prediction. In this work, we\ninvestigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage\n(EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for\ndepression prediction. FUQ pursues reliable and fair depression predictions\nthrough group-based analysis. Specifically, we first group all the participants\nby different sensitive attributes and leverage conformal prediction to quantify\nuncertainty within each demographic group, which provides a theoretically\nguaranteed and valid way to quantify uncertainty for depression prediction and\nfacilitates the investigation of fairness across different demographic groups.\nFurthermore, we propose a fairness-aware optimization strategy that formulates\nfairness as a constrained optimization problem under EOC constraints. This\nenables the model to preserve predictive reliability while adapting to the\nheterogeneous uncertainty levels across demographic groups, thereby achieving\noptimal fairness. Through extensive evaluations on several visual and audio\ndepression datasets, our approach demonstrates its effectiveness.\n","authors":["Yonghong Li","Xiuzhuang Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.04931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07872v1","updated":"2025-07-10T15:55:05Z","published":"2025-07-10T15:55:05Z","title":"Improving AEBS Validation Through Objective Intervention Classification\n  Leveraging the Prediction Divergence Principle","summary":"  The safety validation of automatic emergency braking system (AEBS) requires\naccurately distinguishing between false positive (FP) and true positive (TP)\nsystem activations. While simulations allow straightforward differentiation by\ncomparing scenarios with and without interventions, analyzing activations from\nopen-loop resimulations - such as those from field operational testing (FOT) -\nis more complex. This complexity arises from scenario parameter uncertainty and\nthe influence of driver interventions in the recorded data. Human labeling is\nfrequently used to address these challenges, relying on subjective assessments\nof intervention necessity or situational criticality, potentially introducing\nbiases and limitations. This work proposes a rule-based classification approach\nleveraging the Prediction Divergence Principle (PDP) to address those issues.\nApplied to a simplified AEBS, the proposed method reveals key strengths,\nlimitations, and system requirements for effective implementation. The findings\nsuggest that combining this approach with human labeling may enhance the\ntransparency and consistency of classification, thereby improving the overall\nvalidation process. While the rule set for classification derived in this work\nadopts a conservative approach, the paper outlines future directions for\nrefinement and broader applicability. Finally, this work highlights the\npotential of such methods to complement existing practices, paving the way for\nmore reliable and reproducible AEBS validation frameworks.\n","authors":["Daniel Betschinske","Steven Peters"],"pdf_url":"https://arxiv.org/pdf/2507.07872v1.pdf","comment":"This work has been accepted for publication at the 2025 IEEE\n  International Automated Vehicle Validation Conference (IAVVC)"},{"id":"http://arxiv.org/abs/2507.07871v1","updated":"2025-07-10T15:52:32Z","published":"2025-07-10T15:52:32Z","title":"Mitigating Watermark Stealing Attacks in Generative Models via Multi-Key\n  Watermarking","summary":"  Watermarking offers a promising solution for GenAI providers to establish the\nprovenance of their generated content. A watermark is a hidden signal embedded\nin the generated content, whose presence can later be verified using a secret\nwatermarking key. A threat to GenAI providers are \\emph{watermark stealing}\nattacks, where users forge a watermark into content that was \\emph{not}\ngenerated by the provider's models without access to the secret key, e.g., to\nfalsely accuse the provider. Stealing attacks collect \\emph{harmless}\nwatermarked samples from the provider's model and aim to maximize the expected\nsuccess rate of generating \\emph{harmful} watermarked samples. Our work focuses\non mitigating stealing attacks while treating the underlying watermark as a\nblack-box. Our contributions are: (i) Proposing a multi-key extension to\nmitigate stealing attacks that can be applied post-hoc to any watermarking\nmethod across any modality. (ii) We provide theoretical guarantees and\ndemonstrate empirically that our method makes forging substantially less\neffective across multiple datasets, and (iii) we formally define the threat of\nwatermark forging as the task of generating harmful, watermarked content and\nmodel this threat via security games.\n","authors":["Toluwani Aremu","Noor Hussein","Munachiso Nwadike","Samuele Poppi","Jie Zhang","Karthik Nandakumar","Neil Gong","Nils Lukas"],"pdf_url":"https://arxiv.org/pdf/2507.07871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03023v2","updated":"2025-07-10T15:51:03Z","published":"2025-02-05T09:26:47Z","title":"Parametric Scaling Law of Tuning Bias in Conformal Prediction","summary":"  Conformal prediction is a popular framework of uncertainty quantification\nthat constructs prediction sets with coverage guarantees. To uphold the\nexchangeability assumption, many conformal prediction methods necessitate an\nadditional holdout set for parameter tuning. Yet, the impact of violating this\nprinciple on coverage remains underexplored, making it ambiguous in practical\napplications. In this work, we empirically find that the tuning bias - the\ncoverage gap introduced by leveraging the same dataset for tuning and\ncalibration, is negligible for simple parameter tuning in many conformal\nprediction methods. In particular, we observe the scaling law of the tuning\nbias: this bias increases with parameter space complexity and decreases with\ncalibration set size. Formally, we establish a theoretical framework to\nquantify the tuning bias and provide rigorous proof for the scaling law of the\ntuning bias by deriving its upper bound. In the end, we discuss how to reduce\nthe tuning bias, guided by the theories we developed.\n","authors":["Hao Zeng","Kangdao Liu","Bingyi Jing","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2502.03023v2.pdf","comment":"ICML 2025: https://icml.cc/virtual/2025/poster/44287 and code at:\n  https://github.com/ml-stat-Sustech/Parametric-Scaling-Law-CP-Tuning"},{"id":"http://arxiv.org/abs/2507.06608v2","updated":"2025-07-10T15:48:42Z","published":"2025-07-09T07:27:18Z","title":"Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient\n  GPU Sharing","summary":"  Current prefill-decode (PD) disaggregation is typically deployed at the level\nof entire serving engines, assigning separate GPUs to handle prefill and decode\nphases. While effective at reducing latency, this approach demands more\nhardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode\nrequests within the same batch, but introduces phase interference between\nprefill and decode.\n  While existing PD disaggregation solutions separate the phases across GPUs,\nwe ask: can the same decoupling be achieved within a single serving engine? The\nkey challenge lies in managing the conflicting resource requirements of prefill\nand decode when they share the same hardware. In this paper, we first show that\nchunked prefill requests cause interference with decode requests due to their\ndistinct requirements for GPU resources. Second, we find that GPU resources\nexhibit diminishing returns. Beyond a saturation point, increasing GPU\nallocation yields negligible latency improvements. This insight enables us to\nsplit a single GPU's resources and dynamically allocate them to prefill and\ndecode on the fly, effectively disaggregating the two phases within the same\nGPU.\n  Across a range of models and workloads, our system Nexus achieves up to 2.2x\nhigher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also\noutperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x\nlower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using\nonly half the number of GPUs.\n","authors":["Xiaoxiang Shi","Colin Cai","Junjia Du","Zhanda Zhu","Zhihao Jia"],"pdf_url":"https://arxiv.org/pdf/2507.06608v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07867v1","updated":"2025-07-10T15:47:43Z","published":"2025-07-10T15:47:43Z","title":"Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders","summary":"  Neural audio codecs and autoencoders have emerged as versatile models for\naudio compression, transmission, feature-extraction, and latent-space\ngeneration. However, a key limitation is that most are trained to maximize\nreconstruction fidelity, often neglecting the specific latent structure\nnecessary for optimal performance in diverse downstream applications. We\npropose a simple, post-hoc framework to address this by modifying the\nbottleneck of a pre-trained autoencoder. Our method introduces a\n\"Re-Bottleneck\", an inner bottleneck trained exclusively through latent space\nlosses to instill user-defined structure. We demonstrate the framework's\neffectiveness in three experiments. First, we enforce an ordering on latent\nchannels without sacrificing reconstruction quality. Second, we align latents\nwith semantic embeddings, analyzing the impact on downstream diffusion\nmodeling. Third, we introduce equivariance, ensuring that a filtering operation\non the input waveform directly corresponds to a specific transformation in the\nlatent space. Ultimately, our Re-Bottleneck framework offers a flexible and\nefficient way to tailor representations of neural audio models, enabling them\nto seamlessly meet the varied demands of different applications with minimal\nadditional training.\n","authors":["Dimitrios Bralios","Jonah Casebeer","Paris Smaragdis"],"pdf_url":"https://arxiv.org/pdf/2507.07867v1.pdf","comment":"Accepted at IEEE MLSP 2025"},{"id":"http://arxiv.org/abs/2507.07862v1","updated":"2025-07-10T15:42:31Z","published":"2025-07-10T15:42:31Z","title":"Predicting and generating antibiotics against future pathogens with\n  ApexOracle","summary":"  Antimicrobial resistance (AMR) is escalating and outpacing current antibiotic\ndevelopment. Thus, discovering antibiotics effective against emerging pathogens\nis becoming increasingly critical. However, existing approaches cannot rapidly\nidentify effective molecules against novel pathogens or emerging drug-resistant\nstrains. Here, we introduce ApexOracle, an artificial intelligence (AI) model\nthat both predicts the antibacterial potency of existing compounds and designs\nde novo molecules active against strains it has never encountered. Departing\nfrom models that rely solely on molecular features, ApexOracle incorporates\npathogen-specific context through the integration of molecular features\ncaptured via a foundational discrete diffusion language model and a\ndual-embedding framework that combines genomic- and literature-derived strain\nrepresentations. Across diverse bacterial species and chemical modalities,\nApexOracle consistently outperformed state-of-the-art approaches in activity\nprediction and demonstrated reliable transferability to novel pathogens with\nlittle or no antimicrobial data. Its unified representation-generation\narchitecture further enables the in silico creation of \"new-to-nature\"\nmolecules with high predicted efficacy against priority threats. By pairing\nrapid activity prediction with targeted molecular generation, ApexOracle offers\na scalable strategy for countering AMR and preparing for future\ninfectious-disease outbreaks.\n","authors":["Tianang Leng","Fangping Wan","Marcelo Der Torossian Torres","Cesar de la Fuente-Nunez"],"pdf_url":"https://arxiv.org/pdf/2507.07862v1.pdf","comment":"3 figures"},{"id":"http://arxiv.org/abs/2506.15709v3","updated":"2025-07-10T15:40:39Z","published":"2025-05-30T15:17:23Z","title":"Studying and Improving Graph Neural Network-based Motif Estimation","summary":"  Graph Neural Networks (GNNs) are a predominant method for graph\nrepresentation learning. However, beyond subgraph frequency estimation, their\napplication to network motif significance-profile (SP) prediction remains\nunder-explored, with no established benchmarks in the literature. We propose to\naddress this problem, framing SP estimation as a task independent of subgraph\nfrequency estimation. Our approach shifts from frequency counting to direct SP\nestimation and modulates the problem as multitarget regression. The\nreformulation is optimised for interpretability, stability and scalability on\nlarge graphs. We validate our method using a large synthetic dataset and\nfurther test it on real-world graphs. Our experiments reveal that 1-WL limited\nmodels struggle to make precise estimations of SPs. However, they can\ngeneralise to approximate the graph generation processes of networks by\ncomparing their predicted SP with the ones originating from synthetic\ngenerators. This first study on GNN-based motif estimation also hints at how\nusing direct SP estimation can help go past the theoretical limitations that\nmotif estimation faces when performed through subgraph counting.\n","authors":["Pedro C. Vieira","Miguel E. P. Silva","Pedro Manuel Pinto Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2506.15709v3.pdf","comment":"This manuscript represents a revised version from the paper on\n  https://openreview.net/forum?id=PZVVOeu6xx. Still a work in progress.\n  Comments are welcome! 23 pages (12 main text + references), 9 figures, 5\n  tables. (Second update: More accurate Table 4, Run time comparisons.)"},{"id":"http://arxiv.org/abs/2507.07855v1","updated":"2025-07-10T15:38:17Z","published":"2025-07-10T15:38:17Z","title":"Principled Foundations for Preference Optimization","summary":"  In this paper, we show that direct preference optimization (DPO) is a very\nspecific form of a connection between two major theories in the ML context of\nlearning from preferences: loss functions (Savage) and stochastic choice\n(Doignon-Falmagne and Machina). The connection is established for all of\nSavage's losses and at this level of generality, (i) it includes support for\nabstention on the choice theory side, (ii) it includes support for non-convex\nobjectives on the ML side, and (iii) it allows to frame for free some notable\nextensions of the DPO setting, including margins and corrections for length.\nGetting to understand how DPO operates from a general principled perspective is\ncrucial because of the huge and diverse application landscape of models,\nbecause of the current momentum around DPO, but also -- and importantly --\nbecause many state of the art variations on DPO definitely occupy a small\nregion of the map that we cover. It also helps to understand the pitfalls of\ndeparting from this map, and figure out workarounds.\n","authors":["Wenxuan Zhou","Shujian Zhang","Brice Magdalou","John Lambert","Ehsan Amid","Richard Nock","Andrew Hard"],"pdf_url":"https://arxiv.org/pdf/2507.07855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07854v1","updated":"2025-07-10T15:33:53Z","published":"2025-07-10T15:33:53Z","title":"Credit Risk Analysis for SMEs Using Graph Neural Networks in Supply\n  Chain","summary":"  Small and Medium-sized Enterprises (SMEs) are vital to the modern economy,\nyet their credit risk analysis often struggles with scarce data, especially for\nonline lenders lacking direct credit records. This paper introduces a Graph\nNeural Network (GNN)-based framework, leveraging SME interactions from\ntransaction and social data to map spatial dependencies and predict loan\ndefault risks. Tests on real-world datasets from Discover and Ant Credit (23.4M\nnodes for supply chain analysis, 8.6M for default prediction) show the GNN\nsurpasses traditional and other GNN baselines, with AUCs of 0.995 and 0.701 for\nsupply chain mining and default prediction, respectively. It also helps\nregulators model supply chain disruption impacts on banks, accurately\nforecasting loan defaults from material shortages, and offers Federal Reserve\nstress testers key data for CCAR risk buffers. This approach provides a\nscalable, effective tool for assessing SME credit risk.\n","authors":["Zizhou Zhang","Qinyan Shen","Zhuohuan Hu","Qianying Liu","Huijie Shen"],"pdf_url":"https://arxiv.org/pdf/2507.07854v1.pdf","comment":"The paper will be published on 2025 International Conference on Big\n  Data, Artificial Intelligence and Digital Economy"},{"id":"http://arxiv.org/abs/2507.07853v1","updated":"2025-07-10T15:33:28Z","published":"2025-07-10T15:33:28Z","title":"Optimization Guarantees for Square-Root Natural-Gradient Variational\n  Inference","summary":"  Variational inference with natural-gradient descent often shows fast\nconvergence in practice, but its theoretical convergence guarantees have been\nchallenging to establish. This is true even for the simplest cases that involve\nconcave log-likelihoods and use a Gaussian approximation. We show that the\nchallenge can be circumvented for such cases using a square-root\nparameterization for the Gaussian covariance. This approach establishes novel\nconvergence guarantees for natural-gradient variational-Gaussian inference and\nits continuous-time gradient flow. Our experiments demonstrate the\neffectiveness of natural gradient methods and highlight their advantages over\nalgorithms that use Euclidean or Wasserstein geometries.\n","authors":["Navish Kumar","Thomas Möllenhoff","Mohammad Emtiyaz Khan","Aurelien Lucchi"],"pdf_url":"https://arxiv.org/pdf/2507.07853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07852v1","updated":"2025-07-10T15:33:27Z","published":"2025-07-10T15:33:27Z","title":"Pre-Trained AI Model Assisted Online Decision-Making under Missing\n  Covariates: A Theoretical Perspective","summary":"  We study a sequential contextual decision-making problem in which certain\ncovariates are missing but can be imputed using a pre-trained AI model. From a\ntheoretical perspective, we analyze how the presence of such a model influences\nthe regret of the decision-making process. We introduce a novel notion called\n\"model elasticity\", which quantifies the sensitivity of the reward function to\nthe discrepancy between the true covariate and its imputed counterpart. This\nconcept provides a unified way to characterize the regret incurred due to model\nimputation, regardless of the underlying missingness mechanism. More\nsurprisingly, we show that under the missing at random (MAR) setting, it is\npossible to sequentially calibrate the pre-trained model using tools from\northogonal statistical learning and doubly robust regression. This calibration\nsignificantly improves the quality of the imputed covariates, leading to much\nbetter regret guarantees. Our analysis highlights the practical value of having\nan accurate pre-trained model in sequential decision-making tasks and suggests\nthat model elasticity may serve as a fundamental metric for understanding and\nimproving the integration of pre-trained models in a wide range of data-driven\ndecision-making problems.\n","authors":["Haichen Hu","David Simchi-Levi"],"pdf_url":"https://arxiv.org/pdf/2507.07852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11713v2","updated":"2025-07-10T15:32:14Z","published":"2025-03-12T22:19:33Z","title":"Revisiting the Predictability of Performative, Social Events","summary":"  Social predictions do not passively describe the future; they actively shape\nit. They inform actions and change individual expectations in ways that\ninfluence the likelihood of the predicted outcome. Given these dynamics, to\nwhat extent can social events be predicted? This question was discussed\nthroughout the 20th century by authors like Merton, Morgenstern, Simon, and\nothers who considered it a central issue in social science methodology. In this\nwork, we provide a modern answer to this old problem. Using recent ideas from\nperformative prediction and outcome indistinguishability, we establish that one\ncan always efficiently predict social events accurately, regardless of how\npredictions influence data. While achievable, we also show that these\npredictions are often undesirable, highlighting the limitations of previous\ndesiderata. We end with a discussion of various avenues forward.\n","authors":["Juan C. Perdomo"],"pdf_url":"https://arxiv.org/pdf/2503.11713v2.pdf","comment":"21 pages, accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2507.07848v1","updated":"2025-07-10T15:27:44Z","published":"2025-07-10T15:27:44Z","title":"\"So, Tell Me About Your Policy...\": Distillation of interpretable\n  policies from Deep Reinforcement Learning agents","summary":"  Recent advances in Reinforcement Learning (RL) largely benefit from the\ninclusion of Deep Neural Networks, boosting the number of novel approaches\nproposed in the field of Deep Reinforcement Learning (DRL). These techniques\ndemonstrate the ability to tackle complex games such as Atari, Go, and other\nreal-world applications, including financial trading. Nevertheless, a\nsignificant challenge emerges from the lack of interpretability, particularly\nwhen attempting to comprehend the underlying patterns learned, the relative\nimportance of the state features, and how they are integrated to generate the\npolicy's output. For this reason, in mission-critical and real-world settings,\nit is often preferred to deploy a simpler and more interpretable algorithm,\nalthough at the cost of performance. In this paper, we propose a novel\nalgorithm, supported by theoretical guarantees, that can extract an\ninterpretable policy (e.g., a linear policy) without disregarding the\npeculiarities of expert behavior. This result is obtained by considering the\nadvantage function, which includes information about why an action is superior\nto the others. In contrast to previous works, our approach enables the training\nof an interpretable policy using previously collected experience. The proposed\nalgorithm is empirically evaluated on classic control environments and on a\nfinancial trading scenario, demonstrating its ability to extract meaningful\ninformation from complex expert policies.\n","authors":["Giovanni Dispoto","Paolo Bonetti","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2507.07848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.02357v2","updated":"2025-07-10T15:10:23Z","published":"2025-06-03T01:16:34Z","title":"Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A\n  Lightweight Benchmark for Probing Foundational Controllability Components","summary":"  Credible safety plans for advanced AI development require methods to verify\nagent behavior and detect potential control deficiencies early. A fundamental\naspect is ensuring agents adhere to safety-critical principles, especially when\nthese conflict with operational goals. This paper introduces a lightweight,\ninterpretable benchmark to evaluate an LLM agent's ability to uphold a\nhigh-level safety principle when faced with conflicting task instructions. Our\nevaluation of six LLMs reveals two primary findings: (1) a quantifiable \"cost\nof compliance\" where safety constraints degrade task performance even when\ncompliant solutions exist, and (2) an \"illusion of compliance\" where high\nadherence often masks task incompetence rather than principled choice. These\nfindings provide initial evidence that while LLMs can be influenced by\nhierarchical directives, current approaches lack the consistency required for\nreliable safety governance.\n","authors":["Ram Potham"],"pdf_url":"https://arxiv.org/pdf/2506.02357v2.pdf","comment":"Preprint. This work has been submitted to the Technical AI Governance\n  Workshop at ICML 2025 for review"},{"id":"http://arxiv.org/abs/2406.15245v2","updated":"2025-07-10T15:03:28Z","published":"2024-06-21T15:35:49Z","title":"Unsupervised Morphological Tree Tokenizer","summary":"  As a cornerstone in language modeling, tokenization involves segmenting text\ninputs into pre-defined atomic units. Conventional statistical tokenizers often\ndisrupt constituent boundaries within words, thereby corrupting semantic\ninformation. To address this drawback, we introduce morphological structure\nguidance to tokenization and propose a deep model to induce character-level\nstructures of words. Specifically, the deep model jointly encodes internal\nstructures and representations of words with a mechanism named\n$\\textit{MorphOverriding}$ to ensure the indecomposability of morphemes. By\ntraining the model with self-supervised objectives, our method is capable of\ninducing character-level structures that align with morphological rules without\nannotated training data. Based on the induced structures, our algorithm\ntokenizes words through vocabulary matching in a top-down manner. Empirical\nresults indicate that the proposed method effectively retains complete\nmorphemes and outperforms widely adopted methods such as BPE and WordPiece on\nboth morphological segmentation tasks and language modeling tasks. Code is\navailable at https://github.com/martianmartina/TreeTokenizer.\n","authors":["Qingyang Zhu","Xiang Hu","Pengyu Ji","Wei Wu","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2406.15245v2.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2503.01361v2","updated":"2025-07-10T15:02:15Z","published":"2025-03-03T09:55:10Z","title":"Statistical physics analysis of graph neural networks: Approaching\n  optimality in the contextual stochastic block model","summary":"  Graph neural networks (GNNs) are designed to process data associated with\ngraphs. They are finding an increasing range of applications; however, as with\nother modern machine learning techniques, their theoretical understanding is\nlimited. GNNs can encounter difficulties in gathering information from nodes\nthat are far apart by iterated aggregation steps. This situation is partly\ncaused by so-called oversmoothing; and overcoming it is one of the practically\nmotivated challenges. We consider the situation where information is aggregated\nby multiple steps of convolution, leading to graph convolutional networks\n(GCNs). We analyze the generalization performance of a basic GCN, trained for\nnode classification on data generated by the contextual stochastic block model.\nWe predict its asymptotic performance by deriving the free energy of the\nproblem, using the replica method, in the high-dimensional limit. Calling depth\nthe number of convolutional steps, we show the importance of going to large\ndepth to approach the Bayes-optimality. We detail how the architecture of the\nGCN has to scale with the depth to avoid oversmoothing. The resulting large\ndepth limit can be close to the Bayes-optimality and leads to a continuous GCN.\nTechnically, we tackle this continuous limit via an approach that resembles\ndynamical mean-field theory (DMFT) with constraints at the initial and final\ntimes. An expansion around large regularization allows us to solve the\ncorresponding equations for the performance of the deep GCN. This promising\ntool may contribute to the analysis of further deep neural networks.\n","authors":["O. Duranthon","L. Zdeborová"],"pdf_url":"https://arxiv.org/pdf/2503.01361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07829v1","updated":"2025-07-10T15:01:31Z","published":"2025-07-10T15:01:31Z","title":"Towards Benchmarking Foundation Models for Tabular Data With Text","summary":"  Foundation models for tabular data are rapidly evolving, with increasing\ninterest in extending them to support additional modalities such as free-text\nfeatures. However, existing benchmarks for tabular data rarely include textual\ncolumns, and identifying real-world tabular datasets with semantically rich\ntext features is non-trivial. We propose a series of simple yet effective\nablation-style strategies for incorporating text into conventional tabular\npipelines. Moreover, we benchmark how state-of-the-art tabular foundation\nmodels can handle textual data by manually curating a collection of real-world\ntabular datasets with meaningful textual features. Our study is an important\nstep towards improving benchmarking of foundation models for tabular data with\ntext.\n","authors":["Martin Mráz","Breenda Das","Anshul Gupta","Lennart Purucker","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2507.07829v1.pdf","comment":"Accepted at Foundation Models for Structured Data workshop at ICML\n  2025"},{"id":"http://arxiv.org/abs/2507.07826v1","updated":"2025-07-10T14:58:28Z","published":"2025-07-10T14:58:28Z","title":"An Empirical Bernstein Inequality for Dependent Data in Hilbert Spaces\n  and Applications","summary":"  Learning from non-independent and non-identically distributed data poses a\npersistent challenge in statistical learning. In this study, we introduce\ndata-dependent Bernstein inequalities tailored for vector-valued processes in\nHilbert space. Our inequalities apply to both stationary and non-stationary\nprocesses and exploit the potential rapid decay of correlations between\ntemporally separated variables to improve estimation. We demonstrate the\nutility of these bounds by applying them to covariance operator estimation in\nthe Hilbert-Schmidt norm and to operator learning in dynamical systems,\nachieving novel risk bounds. Finally, we perform numerical experiments to\nillustrate the practical implications of these bounds in both contexts.\n","authors":["Erfan Mirzaei","Andreas Maurer","Vladimir R. Kostic","Massimiliano Pontil"],"pdf_url":"https://arxiv.org/pdf/2507.07826v1.pdf","comment":"In The 28th International Conference on Artificial Intelligence and\n  Statistics (2025)"},{"id":"http://arxiv.org/abs/2310.02299v8","updated":"2025-07-10T14:55:09Z","published":"2023-10-03T14:03:21Z","title":"Discovering Symmetry Breaking in Physical Systems with Relaxed Group\n  Convolution","summary":"  Modeling symmetry breaking is essential for understanding the fundamental\nchanges in the behaviors and properties of physical systems, from microscopic\nparticle interactions to macroscopic phenomena like fluid dynamics and cosmic\nstructures. Thus, identifying sources of asymmetry is an important tool for\nunderstanding physical systems. In this paper, we focus on learning asymmetries\nof data using relaxed group convolutions. We provide both theoretical and\nempirical evidence that this flexible convolution technique allows the model to\nmaintain the highest level of equivariance that is consistent with data and\ndiscover the subtle symmetry-breaking factors in various physical systems. We\nemploy various relaxed group convolution architectures to uncover various\nsymmetry-breaking factors that are interpretable and physically meaningful in\ndifferent physical systems, including the phase transition of crystal\nstructure, the isotropy and homogeneity breaking in turbulent flow, and the\ntime-reversal symmetry breaking in pendulum systems.\n","authors":["Rui Wang","Elyssa Hofgard","Han Gao","Robin Walters","Tess E. Smidt"],"pdf_url":"https://arxiv.org/pdf/2310.02299v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03053v2","updated":"2025-07-10T14:54:28Z","published":"2025-06-03T16:33:47Z","title":"MAEBE: Multi-Agent Emergent Behavior Framework","summary":"  Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.\n","authors":["Sinem Erisken","Timothy Gothard","Martin Leitgab","Ram Potham"],"pdf_url":"https://arxiv.org/pdf/2506.03053v2.pdf","comment":"Preprint. This work has been submitted to the Multi-Agent Systems\n  Workshop at ICML 2025 for review"},{"id":"http://arxiv.org/abs/2010.07990v2","updated":"2025-07-10T14:51:52Z","published":"2020-10-15T19:17:51Z","title":"An Algorithm for Learning Smaller Representations of Models With Scarce\n  Data","summary":"  We present an algorithm for solving binary classification problems when the\ndataset is not fully representative of the problem being solved, and obtaining\nmore data is not possible. It relies on a trained model with loose accuracy\nconstraints, an iterative hyperparameter searching-and-pruning procedure over a\nsearch space $\\Theta$, and a data-generating function. Our algorithm works by\nreconstructing up to homology the manifold on which lies the support of the\nunderlying distribution. We provide an analysis on correctness and runtime\ncomplexity under ideal conditions and an extension to deep neural networks. In\nthe former case, if $\\size{\\Theta}$ is the number of hyperparameter sets in the\nsearch space, this algorithm returns a solution that is up to $2(1 -\n{2^{-\\size{\\Theta}}})$ times better than simply training with an enumeration of\n$\\Theta$ and picking the best model. As part of our analysis we also prove that\nan open cover of a dataset has the same homology as the manifold on which lies\nthe support of the underlying probability distribution, if and only said\ndataset is learnable. This latter result acts as a formal argument to explain\nthe effectiveness of data expansion techniques.\n","authors":["Adrian de Wynter"],"pdf_url":"https://arxiv.org/pdf/2010.07990v2.pdf","comment":"Accepted to Information Geometry--see the journal for the final,\n  authenticated version"},{"id":"http://arxiv.org/abs/2507.07817v1","updated":"2025-07-10T14:46:33Z","published":"2025-07-10T14:46:33Z","title":"On the Effect of Instruction Tuning Loss on Generalization","summary":"  Instruction Tuning has emerged as a pivotal post-training paradigm that\nenables pre-trained language models to better follow user instructions. Despite\nits significance, little attention has been given to optimizing the loss\nfunction used. A fundamental, yet often overlooked, question is whether the\nconventional auto-regressive objective - where loss is computed only on\nresponse tokens, excluding prompt tokens - is truly optimal for instruction\ntuning. In this work, we systematically investigate the impact of\ndifferentially weighting prompt and response tokens in instruction tuning loss,\nand propose Weighted Instruction Tuning (WIT) as a better alternative to\nconventional instruction tuning. Through extensive experiments on five language\nmodels of different families and scale, three finetuning datasets of different\nsizes, and five diverse evaluation benchmarks, we show that the standard\ninstruction tuning loss often yields suboptimal performance and limited\nrobustness to input prompt variations. We find that a low-to-moderate weight\nfor prompt tokens coupled with a moderate-to-high weight for response tokens\nyields the best-performing models across settings and also serve as better\nstarting points for the subsequent preference alignment training. These\nfindings highlight the need to reconsider instruction tuning loss and offer\nactionable insights for developing more robust and generalizable models. Our\ncode is open-sourced at https://github.com/kowndinya-renduchintala/WIT.\n","authors":["Anwoy Chatterjee","H S V N S Kowndinya Renduchintala","Sumit Bhatia","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2507.07817v1.pdf","comment":"Transactions of the Association for Computational Linguistics (TACL)"},{"id":"http://arxiv.org/abs/2507.07814v1","updated":"2025-07-10T14:45:31Z","published":"2025-07-10T14:45:31Z","title":"Pay Attention to Attention Distribution: A New Local Lipschitz Bound for\n  Transformers","summary":"  We present a novel local Lipschitz bound for self-attention blocks of\ntransformers. This bound is based on a refined closed-form expression for the\nspectral norm of the softmax function. The resulting bound is not only more\naccurate than in the prior art, but also unveils the dependence of the\nLipschitz constant on attention score maps. Based on the new findings, we\nsuggest an explanation of the way distributions inside the attention map affect\nthe robustness from the Lipschitz constant perspective. We also introduce a new\nlightweight regularization term called JaSMin (Jacobian Softmax norm\nMinimization), which boosts the transformer's robustness and decreases local\nLipschitz constants of the whole network.\n","authors":["Nikolay Yudin","Alexander Gaponov","Sergei Kudriashov","Maxim Rakhuba"],"pdf_url":"https://arxiv.org/pdf/2507.07814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00718v2","updated":"2025-07-10T14:44:44Z","published":"2025-02-02T08:36:23Z","title":"\"I am bad\": Interpreting Stealthy, Universal and Robust Audio Jailbreaks\n  in Audio-Language Models","summary":"  The rise of multimodal large language models has introduced innovative\nhuman-machine interaction paradigms but also significant challenges in machine\nlearning safety. Audio-Language Models (ALMs) are especially relevant due to\nthe intuitive nature of spoken communication, yet little is known about their\nfailure modes. This paper explores audio jailbreaks targeting ALMs, focusing on\ntheir ability to bypass alignment mechanisms. We construct adversarial\nperturbations that generalize across prompts, tasks, and even base audio\nsamples, demonstrating the first universal jailbreaks in the audio modality,\nand show that these remain effective in simulated real-world conditions. Beyond\ndemonstrating attack feasibility, we analyze how ALMs interpret these audio\nadversarial examples and reveal them to encode imperceptible first-person toxic\nspeech - suggesting that the most effective perturbations for eliciting toxic\noutputs specifically embed linguistic features within the audio signal. These\nresults have important implications for understanding the interactions between\ndifferent modalities in multimodal models, and offer actionable insights for\nenhancing defenses against adversarial audio attacks.\n","authors":["Isha Gupta","David Khachaturov","Robert Mullins"],"pdf_url":"https://arxiv.org/pdf/2502.00718v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07804v1","updated":"2025-07-10T14:29:48Z","published":"2025-07-10T14:29:48Z","title":"Deep Survival Analysis in Multimodal Medical Data: A Parametric and\n  Probabilistic Approach with Competing Risks","summary":"  Accurate survival prediction is critical in oncology for prognosis and\ntreatment planning. Traditional approaches often rely on a single data\nmodality, limiting their ability to capture the complexity of tumor biology. To\naddress this challenge, we introduce a multimodal deep learning framework for\nsurvival analysis capable of modeling both single and competing risks\nscenarios, evaluating the impact of integrating multiple medical data sources\non survival predictions. We propose SAMVAE (Survival Analysis Multimodal\nVariational Autoencoder), a novel deep learning architecture designed for\nsurvival prediction that integrates six data modalities: clinical variables,\nfour molecular profiles, and histopathological images. SAMVAE leverages\nmodality specific encoders to project inputs into a shared latent space,\nenabling robust survival prediction while preserving modality specific\ninformation. Its parametric formulation enables the derivation of clinically\nmeaningful statistics from the output distributions, providing patient-specific\ninsights through interactive multimedia that contribute to more informed\nclinical decision-making and establish a foundation for interpretable,\ndata-driven survival analysis in oncology. We evaluate SAMVAE on two cancer\ncohorts breast cancer and lower grade glioma applying tailored preprocessing,\ndimensionality reduction, and hyperparameter optimization. The results\ndemonstrate the successful integration of multimodal data for both standard\nsurvival analysis and competing risks scenarios across different datasets. Our\nmodel achieves competitive performance compared to state-of-the-art multimodal\nsurvival models. Notably, this is the first parametric multimodal deep learning\narchitecture to incorporate competing risks while modeling continuous time to a\nspecific event, using both tabular and image data.\n","authors":["Alba Garrido","Alejandro Almodóvar","Patricia A. Apellániz","Juan Parras","Santiago Zazo"],"pdf_url":"https://arxiv.org/pdf/2507.07804v1.pdf","comment":"29 pages, 9 Figures"},{"id":"http://arxiv.org/abs/2412.00569v2","updated":"2025-07-10T14:21:15Z","published":"2024-11-30T19:45:23Z","title":"Contextual Bandits in Payment Processing: Non-uniform Exploration and\n  Supervised Learning","summary":"  Uniform random exploration in decision-making systems supports off-policy\nlearning via supervision but incurs high regret, making it impractical for many\napplications. Conversely, non-uniform exploration offers better immediate\nperformance but lacks support for off-policy learning. Recent research suggests\nthat regression oracles can bridge this gap by combining non-uniform\nexploration with supervised learning. In this paper, we analyze these\napproaches within a real-world industrial context at Adyen, a large global\npayments processor characterized by batch logged delayed feedback, short-term\nmemory, and dynamic action spaces under the Empirical Risk Minimization (ERM)\nframework. Our analysis reveals that while regression oracles significantly\nimprove performance, they introduce challenges due to rigid algorithmic\nassumptions. Specifically, we observe that as a policy improves, subsequent\ngenerations may perform worse due to shifts in the reward distribution and\nincreased class imbalance in the training data. This degradation occurs de\nspite improvements in other aspects of the training data, leading to decreased\nperformance in successive policy iterations. We further explore the long-term\nimpact of regression oracles, identifying a potential \"oscillation effect.\"\nThis effect arises when regression oracles influence probability estimates and\nthe realizability of subsequent policy models, leading to fluctuations in\nperformance across iterations. Our findings highlight the need for more\nadaptable algorithms that can leverage the benefits of regression oracles\nwithout introducing instability in policy performance over time.\n","authors":["Akhila Vangara","Alex Egg"],"pdf_url":"https://arxiv.org/pdf/2412.00569v2.pdf","comment":"7 pages, 10 figures, submitted to KDD '25"},{"id":"http://arxiv.org/abs/2507.07792v1","updated":"2025-07-10T14:19:29Z","published":"2025-07-10T14:19:29Z","title":"Space-Filling Regularization for Robust and Interpretable Nonlinear\n  State Space Models","summary":"  The state space dynamics representation is the most general approach for\nnonlinear systems and often chosen for system identification. During training,\nthe state trajectory can deform significantly leading to poor data coverage of\nthe state space. This can cause significant issues for space-oriented training\nalgorithms which e.g. rely on grid structures, tree partitioning, or similar.\nBesides hindering training, significant state trajectory deformations also\ndeteriorate interpretability and robustness properties. This paper proposes a\nnew type of space-filling regularization that ensures a favorable data\ndistribution in state space via introducing a data-distribution-based penalty.\nThis method is demonstrated in local model network architectures where good\ninterpretability is a major concern. The proposed approach integrates ideas\nfrom modeling and design of experiments for state space structures. This is why\nwe present two regularization techniques for the data point distributions of\nthe state trajectories for local affine state space models. Beyond that, we\ndemonstrate the results on a widely known system identification benchmark.\n","authors":["Hermann Klein","Max Heinz Herkersdorf","Oliver Nelles"],"pdf_url":"https://arxiv.org/pdf/2507.07792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11984v2","updated":"2025-07-10T14:18:01Z","published":"2024-11-18T19:14:36Z","title":"Understanding Chain-of-Thought in LLMs through Information Theory","summary":"  Large Language Models (LLMs) have shown impressive performance in complex\nreasoning tasks through the use of Chain-of-Thought (CoT) reasoning, allowing\nmodels to break down problems into manageable sub-tasks. However, existing CoT\nevaluation techniques either require annotated CoT data or fall short in\naccurately assessing intermediate reasoning steps, leading to high rates of\nfalse positives. In this paper, we formalize CoT reasoning in LLMs through an\ninformation-theoretic lens. Specifically, our framework quantifies the\n`information-gain' at each reasoning step, enabling the identification of\nfailure modes in LLMs without the need for expensive annotated datasets. We\ndemonstrate the efficacy of our approach through extensive experiments on toy\narithmetic, GSM8K and PRM800k datasets, where it significantly outperforms\nexisting outcome-based methods by providing more accurate insights into model\nperformance on individual subtasks.\n","authors":["Jean-Francois Ton","Muhammad Faaiz Taufiq","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.11984v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14111v2","updated":"2025-07-10T14:11:07Z","published":"2023-03-24T16:19:15Z","title":"Unsupervised Automata Learning via Discrete Optimization","summary":"  Automata learning is a successful tool for many application domains such as\nrobotics and automatic verification. Typically, automata learning techniques\noperate in a supervised learning setting (active or passive) where they learn a\nfinite state machine in contexts where additional information, such as labeled\nsystem executions, is available. However, other settings, such as learning from\nunlabeled data - an important aspect in machine learning - remain unexplored.\nTo overcome this limitation, we propose a framework for learning a\ndeterministic finite automaton (DFA) from a given multi-set of unlabeled words.\nWe show that this problem is computationally hard and develop three learning\nalgorithms based on constraint optimization. Moreover, we introduce novel\nregularization schemes for our optimization problems that improve the overall\ninterpretability of our DFAs. Using a prototype implementation, we demonstrate\npractical feasibility in the context of unsupervised anomaly detection.\n","authors":["Simon Lutz","Daniil Kaminskyi","Florian Wittbold","Simon Dierl","Falk Howar","Barbara König","Emmanuel Müller","Daniel Neider"],"pdf_url":"https://arxiv.org/pdf/2303.14111v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15543v2","updated":"2025-07-10T14:01:54Z","published":"2025-06-18T15:17:03Z","title":"Learning Algorithms in the Limit","summary":"  This paper studies the problem of learning computable functions in the limit\nby extending Gold's inductive inference framework to incorporate\n\\textit{computational observations} and \\textit{restricted input sources}.\nComplimentary to the traditional Input-Output Observations, we introduce\nTime-Bound Observations, and Policy-Trajectory Observations to study the\nlearnability of general recursive functions under more realistic constraints.\nWhile input-output observations do not suffice for learning the class of\ngeneral recursive functions in the limit, we overcome this learning barrier by\nimposing computational complexity constraints or supplementing with approximate\ntime-bound observations. Further, we build a formal framework around\nobservations of \\textit{computational agents} and show that learning computable\nfunctions from policy trajectories reduces to learning rational functions from\ninput and output, thereby revealing interesting connections to finite-state\ntransducer inference. On the negative side, we show that computable or\npolynomial-mass characteristic sets cannot exist for the class of linear-time\ncomputable functions even for policy-trajectory observations.\n","authors":["Hristo Papazov","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2506.15543v2.pdf","comment":"Accepted at COLT 2025. This version matches the proceedings version\n  apart from a small notational change in section 3"},{"id":"http://arxiv.org/abs/2507.07779v1","updated":"2025-07-10T13:58:55Z","published":"2025-07-10T13:58:55Z","title":"Approximation Depth of Convex Polytopes","summary":"  We study approximations of polytopes in the standard model for computing\npolytopes using Minkowski sums and (convex hulls of) unions. Specifically, we\nstudy the ability to approximate a target polytope by polytopes of a given\ndepth. Our main results imply that simplices can only be ``trivially\napproximated''. On the way, we obtain a characterization of simplices as the\nonly ``outer additive'' convex bodies.\n","authors":["Egor Bakaev","Florestan Brunck","Amir Yehudayoff"],"pdf_url":"https://arxiv.org/pdf/2507.07779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07778v1","updated":"2025-07-10T13:58:32Z","published":"2025-07-10T13:58:32Z","title":"Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time\n  Training","summary":"  Generalizing neural networks to unseen target domains is a significant\nchallenge in real-world deployments. Test-time training (TTT) addresses this by\nusing an auxiliary self-supervised task to reduce the domain gap caused by\ndistribution shifts between the source and target. However, we find that when\nmodels are required to perform multiple tasks under domain shifts, conventional\nTTT methods suffer from unsynchronized task behavior, where the adaptation\nsteps needed for optimal performance in one task may not align with the\nrequirements of other tasks. To address this, we propose a novel TTT approach\ncalled Synchronizing Tasks for Test-time Training (S4T), which enables the\nconcurrent handling of multiple tasks. The core idea behind S4T is that\npredicting task relations across domain shifts is key to synchronizing tasks\nduring test time. To validate our approach, we apply S4T to conventional\nmulti-task benchmarks, integrating it with traditional TTT protocols. Our\nempirical results show that S4T outperforms state-of-the-art TTT methods across\nvarious benchmarks.\n","authors":["Wooseong Jeong","Jegyeong Cho","Youngho Yoon","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2507.07778v1.pdf","comment":"Accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2503.02113v2","updated":"2025-07-10T13:56:52Z","published":"2025-03-03T22:56:04Z","title":"Deep Learning is Not So Mysterious or Different","summary":"  Deep neural networks are often seen as different from other model classes by\ndefying conventional notions of generalization. Popular examples of anomalous\ngeneralization behaviour include benign overfitting, double descent, and the\nsuccess of overparametrization. We argue that these phenomena are not distinct\nto neural networks, or particularly mysterious. Moreover, this generalization\nbehaviour can be intuitively understood, and rigorously characterized, using\nlong-standing generalization frameworks such as PAC-Bayes and countable\nhypothesis bounds. We present soft inductive biases as a key unifying principle\nin explaining these phenomena: rather than restricting the hypothesis space to\navoid overfitting, embrace a flexible hypothesis space, with a soft preference\nfor simpler solutions that are consistent with the data. This principle can be\nencoded in many model classes, and thus deep learning is not as mysterious or\ndifferent from other model classes as it might seem. However, we also highlight\nhow deep learning is relatively distinct in other ways, such as its ability for\nrepresentation learning, phenomena such as mode connectivity, and its relative\nuniversality.\n","authors":["Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2503.02113v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2507.07771v1","updated":"2025-07-10T13:54:59Z","published":"2025-07-10T13:54:59Z","title":"A Unified Empirical Risk Minimization Framework for Flexible N-Tuples\n  Weak Supervision","summary":"  To alleviate the annotation burden in supervised learning, N-tuples learning\nhas recently emerged as a powerful weakly-supervised method. While existing\nN-tuples learning approaches extend pairwise learning to higher-order\ncomparisons and accommodate various real-world scenarios, they often rely on\ntask-specific designs and lack a unified theoretical foundation. In this paper,\nwe propose a general N-tuples learning framework based on empirical risk\nminimization, which systematically integrates pointwise unlabeled data to\nenhance learning performance. This paper first unifies the data generation\nprocesses of N-tuples and pointwise unlabeled data under a shared probabilistic\nformulation. Based on this unified view, we derive an unbiased empirical risk\nestimator that generalizes a broad class of existing N-tuples models. We\nfurther establish a generalization error bound for theoretical support. To\ndemonstrate the flexibility of the framework, we instantiate it in four\nrepresentative weakly supervised scenarios, each recoverable as a special case\nof our general model. Additionally, to address overfitting issues arising from\nnegative risk terms, we adopt correction functions to adjust the empirical\nrisk. Extensive experiments on benchmark datasets validate the effectiveness of\nthe proposed framework and demonstrate that leveraging pointwise unlabeled data\nconsistently improves generalization across various N-tuples learning tasks.\n","authors":["Shuying Huang","Junpeng Li","Changchun Hua","Yana Yang"],"pdf_url":"https://arxiv.org/pdf/2507.07771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07769v1","updated":"2025-07-10T13:54:38Z","published":"2025-07-10T13:54:38Z","title":"BEAVER: Building Environments with Assessable Variation for Evaluating\n  Multi-Objective Reinforcement Learning","summary":"  Recent years have seen significant advancements in designing reinforcement\nlearning (RL)-based agents for building energy management. While individual\nsuccess is observed in simulated or controlled environments, the scalability of\nRL approaches in terms of efficiency and generalization across building\ndynamics and operational scenarios remains an open question. In this work, we\nformally characterize the generalization space for the cross-environment,\nmulti-objective building energy management task, and formulate the\nmulti-objective contextual RL problem. Such a formulation helps understand the\nchallenges of transferring learned policies across varied operational contexts\nsuch as climate and heat convection dynamics under multiple control objectives\nsuch as comfort level and energy consumption. We provide a principled way to\nparameterize such contextual information in realistic building RL environments,\nand construct a novel benchmark to facilitate the evaluation of generalizable\nRL algorithms in practical building control tasks. Our results show that\nexisting multi-objective RL methods are capable of achieving reasonable\ntrade-offs between conflicting objectives. However, their performance degrades\nunder certain environment variations, underscoring the importance of\nincorporating dynamics-dependent contextual information into the policy\nlearning process.\n","authors":["Ruohong Liu","Jack Umenberger","Yize Chen"],"pdf_url":"https://arxiv.org/pdf/2507.07769v1.pdf","comment":"Accepted at the Workshop on Computational Optimization of Buildings\n  (ICML CO-BUILD), 42nd International Conference on Machine Learning (ICML\n  2025), Vancouver, Canada"},{"id":"http://arxiv.org/abs/2507.07768v1","updated":"2025-07-10T13:53:52Z","published":"2025-07-10T13:53:52Z","title":"TRIX- Trading Adversarial Fairness via Mixed Adversarial Training","summary":"  Adversarial Training (AT) is a widely adopted defense against adversarial\nexamples. However, existing approaches typically apply a uniform training\nobjective across all classes, overlooking disparities in class-wise\nvulnerability. This results in adversarial unfairness: classes with well\ndistinguishable features (strong classes) tend to become more robust, while\nclasses with overlapping or shared features(weak classes) remain\ndisproportionately susceptible to adversarial attacks. We observe that strong\nclasses do not require strong adversaries during training, as their non-robust\nfeatures are quickly suppressed. In contrast, weak classes benefit from\nstronger adversaries to effectively reduce their vulnerabilities. Motivated by\nthis, we introduce TRIX, a feature-aware adversarial training framework that\nadaptively assigns weaker targeted adversaries to strong classes, promoting\nfeature diversity via uniformly sampled targets, and stronger untargeted\nadversaries to weak classes, enhancing their focused robustness. TRIX further\nincorporates per-class loss weighting and perturbation strength adjustments,\nbuilding on prior work, to emphasize weak classes during the optimization.\nComprehensive experiments on standard image classification benchmarks,\nincluding evaluations under strong attacks such as PGD and AutoAttack,\ndemonstrate that TRIX significantly improves worst-case class accuracy on both\nclean and adversarial data, reducing inter-class robustness disparities, and\npreserves overall accuracy. Our results highlight TRIX as a practical step\ntoward fair and effective adversarial defense.\n","authors":["Tejaswini Medi","Steffen Jung","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2507.07768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07765v1","updated":"2025-07-10T13:43:15Z","published":"2025-07-10T13:43:15Z","title":"Distributed and Decentralised Training: Technical Governance Challenges\n  in a Shifting AI Landscape","summary":"  Advances in low-communication training algorithms are enabling a shift from\ncentralised model training to compute setups that are either distributed across\nmultiple clusters or decentralised via community-driven contributions. This\npaper distinguishes these two scenarios - distributed and decentralised\ntraining - which are little understood and often conflated in policy discourse.\nWe discuss how they could impact technical AI governance through an increased\nrisk of compute structuring, capability proliferation, and the erosion of\ndetectability and shutdownability. While these trends foreshadow a possible new\nparadigm that could challenge key assumptions of compute governance, we\nemphasise that certain policy levers, like export controls, remain relevant. We\nalso acknowledge potential benefits of decentralised AI, including\nprivacy-preserving training runs that could unlock access to more data, and\nmitigating harmful power concentration. Our goal is to support more precise\npolicymaking around compute, capability proliferation, and decentralised AI\ndevelopment.\n","authors":["Jakub Kryś","Yashvardhan Sharma","Janet Egan"],"pdf_url":"https://arxiv.org/pdf/2507.07765v1.pdf","comment":"Accepted as an oral presentation at the Technical AI Governance\n  Workshop (ICML 2025)"},{"id":"http://arxiv.org/abs/2507.06892v2","updated":"2025-07-10T13:42:04Z","published":"2025-07-09T14:29:45Z","title":"Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model","summary":"  Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.\n","authors":["Jing Liang","Hongyao Tang","Yi Ma","Jinyi Liu","Yan Zheng","Shuyue Hu","Lei Bai","Jianye Hao"],"pdf_url":"https://arxiv.org/pdf/2507.06892v2.pdf","comment":"Preliminary version, v2, added more details and corrected some minor\n  mistakes. Project page: https://anitaleungxx.github.io/ReMix"},{"id":"http://arxiv.org/abs/2507.07754v1","updated":"2025-07-10T13:34:02Z","published":"2025-07-10T13:34:02Z","title":"OPC: One-Point-Contraction Unlearning Toward Deep Feature Forgetting","summary":"  Machine unlearning seeks to remove the influence of particular data or class\nfrom trained models to meet privacy, legal, or ethical requirements. Existing\nunlearning methods tend to forget shallowly: phenomenon of an unlearned model\npretend to forget by adjusting only the model response, while its internal\nrepresentations retain information sufficiently to restore the forgotten data\nor behavior. We empirically confirm the widespread shallowness by reverting the\nforgetting effect of various unlearning methods via training-free performance\nrecovery attack and gradient-inversion-based data reconstruction attack. To\naddress this vulnerability fundamentally, we define a theoretical criterion of\n``deep forgetting'' based on one-point-contraction of feature representations\nof data to forget. We also propose an efficient approximation algorithm, and\nuse it to construct a novel general-purpose unlearning algorithm:\nOne-Point-Contraction (OPC). Empirical evaluations on image classification\nunlearning benchmarks show that OPC achieves not only effective unlearning\nperformance but also superior resilience against both performance recovery\nattack and gradient-inversion attack. The distinctive unlearning performance of\nOPC arises from the deep feature forgetting enforced by its theoretical\nfoundation, and recaps the need for improved robustness of machine unlearning\nmethods.\n","authors":["Jaeheun Jung","Bosung Jung","Suhyun Bae","Donghun Lee"],"pdf_url":"https://arxiv.org/pdf/2507.07754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07738v1","updated":"2025-07-10T13:16:33Z","published":"2025-07-10T13:16:33Z","title":"Efficient and Scalable Estimation of Distributional Treatment Effects\n  with Multi-Task Neural Networks","summary":"  We propose a novel multi-task neural network approach for estimating\ndistributional treatment effects (DTE) in randomized experiments. While DTE\nprovides more granular insights into the experiment outcomes over conventional\nmethods focusing on the Average Treatment Effect (ATE), estimating it with\nregression adjustment methods presents significant challenges. Specifically,\nprecision in the distribution tails suffers due to data imbalance, and\ncomputational inefficiencies arise from the need to solve numerous regression\nproblems, particularly in large-scale datasets commonly encountered in\nindustry. To address these limitations, our method leverages multi-task neural\nnetworks to estimate conditional outcome distributions while incorporating\nmonotonic shape constraints and multi-threshold label learning to enhance\naccuracy. To demonstrate the practical effectiveness of our proposed method, we\napply our method to both simulated and real-world datasets, including a\nrandomized field experiment aimed at reducing water consumption in the US and a\nlarge-scale A/B test from a leading streaming platform in Japan. The\nexperimental results consistently demonstrate superior performance across\nvarious datasets, establishing our method as a robust and practical solution\nfor modern causal inference applications requiring a detailed understanding of\ntreatment effect heterogeneity.\n","authors":["Tomu Hirata","Undral Byambadalai","Tatsushi Oka","Shota Yasui","Shingo Uto"],"pdf_url":"https://arxiv.org/pdf/2507.07738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07735v1","updated":"2025-07-10T13:15:20Z","published":"2025-07-10T13:15:20Z","title":"GuardVal: Dynamic Large Language Model Jailbreak Evaluation for\n  Comprehensive Safety Testing","summary":"  Jailbreak attacks reveal critical vulnerabilities in Large Language Models\n(LLMs) by causing them to generate harmful or unethical content. Evaluating\nthese threats is particularly challenging due to the evolving nature of LLMs\nand the sophistication required in effectively probing their vulnerabilities.\nCurrent benchmarks and evaluation methods struggle to fully address these\nchallenges, leaving gaps in the assessment of LLM vulnerabilities. In this\npaper, we review existing jailbreak evaluation practices and identify three\nassumed desiderata for an effective jailbreak evaluation protocol. To address\nthese challenges, we introduce GuardVal, a new evaluation protocol that\ndynamically generates and refines jailbreak prompts based on the defender LLM's\nstate, providing a more accurate assessment of defender LLMs' capacity to\nhandle safety-critical situations. Moreover, we propose a new optimization\nmethod that prevents stagnation during prompt refinement, ensuring the\ngeneration of increasingly effective jailbreak prompts that expose deeper\nweaknesses in the defender LLMs. We apply this protocol to a diverse set of\nmodels, from Mistral-7b to GPT-4, across 10 safety domains. Our findings\nhighlight distinct behavioral patterns among the models, offering a\ncomprehensive view of their robustness. Furthermore, our evaluation process\ndeepens the understanding of LLM behavior, leading to insights that can inform\nfuture research and drive the development of more secure models.\n","authors":["Peiyan Zhang","Haibo Jin","Liying Kang","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2507.07735v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2504.19955v2","updated":"2025-07-10T13:00:17Z","published":"2025-04-28T16:24:54Z","title":"Robust Federated Personalised Mean Estimation for the Gaussian Mixture\n  Model","summary":"  Federated learning with heterogeneous data and personalization has received\nsignificant recent attention. Separately, robustness to corrupted data in the\ncontext of federated learning has also been studied. In this paper we explore\ncombining personalization for heterogeneous data with robustness, where a\nconstant fraction of the clients are corrupted. Motivated by this broad\nproblem, we formulate a simple instantiation which captures some of its\ndifficulty. We focus on the specific problem of personalized mean estimation\nwhere the data is drawn from a Gaussian mixture model. We give an algorithm\nwhose error depends almost linearly on the ratio of corrupted to uncorrupted\nsamples, and show a lower bound with the same behavior, albeit with a gap of a\nconstant factor.\n","authors":["Malhar A. Managoli","Vinod M. Prabhakaran","Suhas Diggavi"],"pdf_url":"https://arxiv.org/pdf/2504.19955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17836v6","updated":"2025-07-10T12:56:43Z","published":"2025-05-23T12:51:03Z","title":"Robust Distributed Estimation: Extending Gossip Algorithms to Ranking\n  and Trimmed Means","summary":"  This paper addresses the problem of robust estimation in gossip algorithms\nover arbitrary communication graphs. Gossip algorithms are fully decentralized,\nrelying only on local neighbor-to-neighbor communication, making them\nwell-suited for situations where communication is constrained. A fundamental\nchallenge in existing mean-based gossip algorithms is their vulnerability to\nmalicious or corrupted nodes. In this paper, we show that an outlier-robust\nmean can be computed by globally estimating a robust statistic. More\nspecifically, we propose a novel gossip algorithm for rank estimation, referred\nto as \\textsc{GoRank}, and leverage it to design a gossip procedure dedicated\nto trimmed mean estimation, coined \\textsc{GoTrim}. In addition to a detailed\ndescription of the proposed methods, a key contribution of our work is a\nprecise convergence analysis: we establish an $\\mathcal{O}(1/t)$ rate for rank\nestimation and an $\\mathcal{O}(1 / {t})$ rate for trimmed mean estimation,\nwhere by $t$ is meant the number of iterations. Moreover, we provide a\nbreakdown point analysis of \\textsc{GoTrim}. We empirically validate our\ntheoretical results through experiments on diverse network topologies, data\ndistributions and contamination schemes.\n","authors":["Anna Van Elst","Igor Colin","Stephan Clémençon"],"pdf_url":"https://arxiv.org/pdf/2505.17836v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04382v2","updated":"2025-07-10T12:54:15Z","published":"2025-05-07T13:04:29Z","title":"Discrete Optimal Transport and Voice Conversion","summary":"  In this work, we address the voice conversion (VC) task using a vector-based\ninterface. To align audio embeddings between speakers, we employ discrete\noptimal transport mapping. Our evaluation results demonstrate the high quality\nand effectiveness of this method. Additionally, we show that applying discrete\noptimal transport as a post-processing step in audio generation can lead to the\nincorrect classification of synthetic audio as real.\n","authors":["Anton Selitskiy","Maitreya Kocharekar"],"pdf_url":"https://arxiv.org/pdf/2505.04382v2.pdf","comment":"4 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2507.07714v1","updated":"2025-07-10T12:52:19Z","published":"2025-07-10T12:52:19Z","title":"Adaptive Gaussian Mixture Models-based Anomaly Detection for\n  under-constrained Cable-Driven Parallel Robots","summary":"  Cable-Driven Parallel Robots (CDPRs) are increasingly used for load\nmanipulation tasks involving predefined toolpaths with intermediate stops. At\neach stop, where the platform maintains a fixed pose and the motors keep the\ncables under tension, the system must evaluate whether it is safe to proceed by\ndetecting anomalies that could compromise performance (e.g., wind gusts or\ncable impacts). This paper investigates whether anomalies can be detected using\nonly motor torque data, without additional sensors. It introduces an adaptive,\nunsupervised outlier detection algorithm based on Gaussian Mixture Models\n(GMMs) to identify anomalies from torque signals. The method starts with a\nbrief calibration period, just a few seconds, during which a GMM is fit on\nknown anomaly-free data. Real-time torque measurements are then evaluated using\nMahalanobis distance from the GMM, with statistically derived thresholds\ntriggering anomaly flags. Model parameters are periodically updated using the\nlatest segments identified as anomaly-free to adapt to changing conditions.\nValidation includes 14 long-duration test sessions simulating varied wind\nintensities. The proposed method achieves a 100% true positive rate and 95.4%\naverage true negative rate, with 1-second detection latency. Comparative\nevaluation against power threshold and non-adaptive GMM methods indicates\nhigher robustness to drift and environmental variation.\n","authors":["Julio Garrido","Javier Vales","Diego Silva-Muñiz","Enrique Riveiro","Pablo López-Matencio","Josué Rivera-Andrade"],"pdf_url":"https://arxiv.org/pdf/2507.07714v1.pdf","comment":"14 pages, 8 figures, 1 table, to be submitted to Advanced Intelligent\n  Systems"},{"id":"http://arxiv.org/abs/2507.07712v1","updated":"2025-07-10T12:46:31Z","published":"2025-07-10T12:46:31Z","title":"Balancing the Past and Present: A Coordinated Replay Framework for\n  Federated Class-Incremental Learning","summary":"  Federated Class Incremental Learning (FCIL) aims to collaboratively process\ncontinuously increasing incoming tasks across multiple clients. Among various\napproaches, data replay has become a promising solution, which can alleviate\nforgetting by reintroducing representative samples from previous tasks.\nHowever, their performance is typically limited by class imbalance, both within\nthe replay buffer due to limited global awareness and between replayed and\nnewly arrived classes. To address this issue, we propose a class wise balancing\ndata replay method for FCIL (FedCBDR), which employs a global coordination\nmechanism for class-level memory construction and reweights the learning\nobjective to alleviate the aforementioned imbalances. Specifically, FedCBDR has\ntwo key components: 1) the global-perspective data replay module reconstructs\nglobal representations of prior task in a privacy-preserving manner, which then\nguides a class-aware and importance-sensitive sampling strategy to achieve\nbalanced replay; 2) Subsequently, to handle class imbalance across tasks, the\ntask aware temperature scaling module adaptively adjusts the temperature of\nlogits at both class and instance levels based on task dynamics, which reduces\nthe model's overconfidence in majority classes while enhancing its sensitivity\nto minority classes. Experimental results verified that FedCBDR achieves\nbalanced class-wise sampling under heterogeneous data distributions and\nimproves generalization under task imbalance between earlier and recent tasks,\nyielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.\n","authors":["Zhuang Qi","Lei Meng","Han Yu"],"pdf_url":"https://arxiv.org/pdf/2507.07712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01991v4","updated":"2025-07-10T12:18:34Z","published":"2023-12-04T16:10:34Z","title":"Shapley-Based Data Valuation with Mutual Information: A Key to Modified\n  K-Nearest Neighbors","summary":"  The K-Nearest Neighbors (KNN) algorithm is widely used for classification and\nregression; however, it suffers from limitations, including the equal treatment\nof all samples. We propose Information-Modified KNN (IM-KNN), a novel approach\nthat leverages Mutual Information ($I$) and Shapley values to assign weighted\nvalues to neighbors, thereby bridging the gap in treating all samples with the\nsame value and weight. On average, IM-KNN improves the accuracy, precision, and\nrecall of traditional KNN by 16.80%, 17.08%, and 16.98%, respectively, across\n12 benchmark datasets. Experiments on four large-scale datasets further\nhighlight IM-KNN's robustness to noise, imbalanced data, and skewed\ndistributions.\n","authors":["Mohammad Ali Vahedifar","Azim Akhtarshenas","Mohammad Mohammadi Rafatpanah","Maryam Sabbaghian"],"pdf_url":"https://arxiv.org/pdf/2312.01991v4.pdf","comment":"This paper has been accepted for publication in the IEEE Machine\n  Learning and Signal Processing conference (MLSP 2025)"},{"id":"http://arxiv.org/abs/2507.07685v1","updated":"2025-07-10T12:07:13Z","published":"2025-07-10T12:07:13Z","title":"Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought","summary":"  Large vision-language models (LVLMs) have demonstrated remarkable\ncapabilities by integrating pre-trained vision encoders with large language\nmodels (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting\nhas been adapted for LVLMs to enhance multi-modal reasoning by generating\nintermediate rationales based on visual and textual inputs. While CoT is\nassumed to improve grounding and accuracy in LVLMs, our experiments reveal a\nkey challenge: existing LVLMs often ignore the contents of generated rationales\nin CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as\na KL-constrained reward maximization focused on rationale-conditional\nlog-likelihood. As the optimal solution, we propose rationale-enhanced decoding\n(RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes\nvisual and rationale information by multiplying distinct image-conditional and\nrationale-conditional next token distributions. Extensive experiments show that\nRED consistently and significantly improves reasoning over standard CoT and\nother decoding methods across multiple benchmarks and LVLMs. Our work offers a\npractical and effective approach to improve both the faithfulness and accuracy\nof CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded\nmulti-modal systems.\n","authors":["Shin'ya Yamaguchi","Kosuke Nishida","Daiki Chijiwa"],"pdf_url":"https://arxiv.org/pdf/2507.07685v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.07683v1","updated":"2025-07-10T12:05:33Z","published":"2025-07-10T12:05:33Z","title":"Accelerating Transposed Convolutions on FPGA-based Edge Devices","summary":"  Transposed Convolutions (TCONV) enable the up-scaling mechanism within\ngenerative Artificial Intelligence (AI) models. However, the predominant\nInput-Oriented Mapping (IOM) method for implementing TCONV has complex output\nmapping, overlapping sums, and ineffectual computations. These inefficiencies\nfurther exacerbate the performance bottleneck of TCONV and generative models on\nresource-constrained edge devices. To address this problem, in this paper we\npropose MM2IM, a hardware-software co-designed accelerator that combines Matrix\nMultiplication (MatMul) with col2IM to process TCONV layers on\nresource-constrained edge devices efficiently. Using the SECDA-TFLite design\ntoolkit, we implement MM2IM and evaluate its performance across 261 TCONV\nproblem configurations, achieving an average speedup of 1.9x against a\ndual-thread ARM Neon optimized CPU baseline. We then evaluate the performance\nof MM2IM on a range of TCONV layers from well-known generative models achieving\nup to 4.2x speedup, and compare it against similar resource-constrained TCONV\naccelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate\nMM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x\nenergy reduction against the CPU baseline.\n","authors":["Jude Haris","José Cano"],"pdf_url":"https://arxiv.org/pdf/2507.07683v1.pdf","comment":"Accepted to 35th International Conference on Field-Programmable Logic\n  and Applications (FPL) 2025"},{"id":"http://arxiv.org/abs/2504.17568v2","updated":"2025-07-10T11:58:59Z","published":"2025-04-24T13:58:07Z","title":"Beyond Cox Models: Assessing the Performance of Machine-Learning Methods\n  in Non-Proportional Hazards and Non-Linear Survival Analysis","summary":"  Survival analysis often relies on Cox models, assuming both linearity and\nproportional hazards (PH). This study evaluates machine and deep learning\nmethods that relax these constraints, comparing their performance with\npenalized Cox models on a benchmark of three synthetic and three real datasets.\nIn total, eight different models were tested, including six non-linear models\nof which four were also non-PH. Although Cox regression often yielded\nsatisfactory performance, we showed the conditions under which machine and deep\nlearning models can perform better. Indeed, the performance of these methods\nhas often been underestimated due to the improper use of Harrell's concordance\nindex (C-index) instead of more appropriate scores such as Antolini's\nconcordance index, which generalizes C-index in cases where the PH assumption\ndoes not hold. In addition, since occasionally high C-index models happen to be\nbadly calibrated, combining Antolini's C-index with Brier's score is useful to\nassess the overall performance of a survival method. Results on our benchmark\ndata showed that survival prediction should be approached by testing different\nmethods to select the most appropriate one according to sample size,\nnon-linearity and non-PH conditions. To allow an easy reproducibility of these\ntests on our benchmark data, code and documentation are freely available at\nhttps://github.com/compbiomed-unito/survhive.\n","authors":["Ivan Rossi","Flavio Sartori","Cesare Rollo","Giovanni Birolo","Piero Fariselli","Tiziana Sanavia"],"pdf_url":"https://arxiv.org/pdf/2504.17568v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.13431v4","updated":"2025-07-10T11:57:06Z","published":"2023-04-26T10:36:40Z","title":"Implicit Counterfactual Data Augmentation for Robust Learning","summary":"  Machine learning models are prone to capturing the spurious correlations\nbetween non-causal attributes and classes, with counterfactual data\naugmentation being a promising direction for breaking these spurious\nassociations. However, generating counterfactual data explicitly poses a\nchallenge, and incorporating augmented data into the training process decreases\ntraining efficiency. This study proposes an Implicit Counterfactual Data\nAugmentation (ICDA) method to remove spurious correlations and make stable\npredictions. Specifically, first, a novel sample-wise augmentation strategy is\ndeveloped that generates semantically and counterfactually meaningful deep\nfeatures with distinct augmentation strength for each sample. Second, we derive\nan easy-to-compute surrogate loss on the augmented feature set when the number\nof augmented samples becomes infinite. Third, two concrete schemes are\nproposed, including direct quantification and meta-learning, to derive the key\nparameters for the robust loss. In addition, ICDA is explained from a\nregularization perspective, revealing its capacity to improve intra-class\ncompactness and augment margins at both class and sample levels. Extensive\nexperiments have been conducted across various biased learning scenarios\ncovering both image and text datasets, demonstrating that ICDA consistently\nenhances the generalization and robustness performance of popular networks.\n","authors":["Xiaoling Zhou","Ou Wu","Michael K. Ng"],"pdf_url":"https://arxiv.org/pdf/2304.13431v4.pdf","comment":"33 pages, 10 figures"},{"id":"http://arxiv.org/abs/2507.07675v1","updated":"2025-07-10T11:54:18Z","published":"2025-07-10T11:54:18Z","title":"Some Theoretical Results on Layerwise Effective Dimension Oscillations\n  in Finite Width ReLU Networks","summary":"  We analyze the layerwise effective dimension (rank of the feature matrix) in\nfully-connected ReLU networks of finite width. Specifically, for a fixed batch\nof $m$ inputs and random Gaussian weights, we derive closed-form expressions\nfor the expected rank of the \\$m\\times n\\$ hidden activation matrices. Our main\nresult shows that $\\mathbb{E}[EDim(\\ell)]=m[1-(1-2/\\pi)^\\ell]+O(e^{-c m})$ so\nthat the rank deficit decays geometrically with ratio $1-2 / \\pi \\approx\n0.3634$. We also prove a sub-Gaussian concentration bound, and identify the\n\"revival\" depths at which the expected rank attains local maxima. In\nparticular, these peaks occur at depths\n$\\ell_k^*\\approx(k+1/2)\\pi/\\log(1/\\rho)$ with height $\\approx (1-e^{-\\pi/2}) m\n\\approx 0.79m$. We further show that this oscillatory rank behavior is a\nfinite-width phenomenon: under orthogonal weight initialization or strong\nnegative-slope leaky-ReLU, the rank remains (nearly) full. These results\nprovide a precise characterization of how random ReLU layers alternately\ncollapse and partially revive the subspace of input variations, adding nuance\nto prior work on expressivity of deep networks.\n","authors":["Darshan Makwana"],"pdf_url":"https://arxiv.org/pdf/2507.07675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07668v1","updated":"2025-07-10T11:49:17Z","published":"2025-07-10T11:49:17Z","title":"Learning Pole Structures of Hadronic States using Predictive Uncertainty\n  Estimation","summary":"  Matching theoretical predictions to experimental data remains a central\nchallenge in hadron spectroscopy. In particular, the identification of new\nhadronic states is difficult, as exotic signals near threshold can arise from a\nvariety of physical mechanisms. A key diagnostic in this context is the pole\nstructure of the scattering amplitude, but different configurations can produce\nsimilar signatures. The mapping between pole configurations and line shapes is\nespecially ambiguous near the mass threshold, where analytic control is\nlimited. In this work, we introduce an uncertainty-aware machine learning\napproach for classifying pole structures in $S$-matrix elements. Our method is\nbased on an ensemble of classifier chains that provide both epistemic and\naleatoric uncertainty estimates. We apply a rejection criterion based on\npredictive uncertainty, achieving a validation accuracy of nearly $95\\%$ while\ndiscarding only a small fraction of high-uncertainty predictions. Trained on\nsynthetic data with known pole structures, the model generalizes to previously\nunseen experimental data, including enhancements associated with the\n$P_{c\\bar{c}}(4312)^+$ state observed by LHCb. In this, we infer a four-pole\nstructure, representing the presence of a genuine compact pentaquark in the\npresence of a higher channel virtual state pole with non-vanishing width. While\nevaluated on this particular state, our framework is broadly applicable to\nother candidate hadronic states and offers a scalable tool for pole structure\ninference in scattering amplitudes.\n","authors":["Felix Frohnert","Denny Lane B. Sombrillo","Evert van Nieuwenburg","Patrick Emonts"],"pdf_url":"https://arxiv.org/pdf/2507.07668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17428v3","updated":"2025-07-10T11:21:38Z","published":"2024-10-22T20:57:10Z","title":"Uncovering RL Integration in SSL Loss: Objective-Specific Implications\n  for Data-Efficient RL","summary":"  In this study, we investigate the effect of SSL objective modifications\nwithin the SPR framework, focusing on specific adjustments such as terminal\nstate masking and prioritized replay weighting, which were not explicitly\naddressed in the original design. While these modifications are specific to RL,\nthey are not universally applicable across all RL algorithms. Therefore, we aim\nto assess their impact on performance and explore other SSL objectives that do\nnot accommodate these adjustments like Barlow Twins and VICReg. We evaluate six\nSPR variants on the Atari 100k benchmark, including versions both with and\nwithout these modifications. Additionally, we test the performance of these\nobjectives on the DeepMind Control Suite, where such modifications are absent.\nOur findings reveal that incorporating specific SSL modifications within SPR\nsignificantly enhances performance, and this influence extends to subsequent\nframeworks like SR-SPR and BBF, highlighting the critical importance of SSL\nobjective selection and related adaptations in achieving data efficiency in\nself-predictive reinforcement learning.\n","authors":["Ömer Veysel Çağatan","Barış Akgün"],"pdf_url":"https://arxiv.org/pdf/2410.17428v3.pdf","comment":"RLC 2025, Neurips SSL Workshop 2024"},{"id":"http://arxiv.org/abs/2407.17070v2","updated":"2025-07-10T11:15:07Z","published":"2024-07-24T07:55:49Z","title":"Curriculum Negative Mining For Temporal Networks","summary":"  Temporal networks are effective in capturing the evolving interactions of\nnetworks over time, such as social networks and e-commerce networks. In recent\nyears, researchers have primarily concentrated on developing specific model\narchitectures for Temporal Graph Neural Networks (TGNNs) in order to improve\nthe representation quality of temporal nodes and edges. However, limited\nattention has been given to the quality of negative samples during the training\nof TGNNs. When compared with static networks, temporal networks present two\nspecific challenges for negative sampling: positive sparsity and positive\nshift. Positive sparsity refers to the presence of a single positive sample\namidst numerous negative samples at each timestamp, while positive shift\nrelates to the variations in positive samples across different timestamps. To\nrobustly address these challenges in training TGNNs, we introduce Curriculum\nNegative Mining (CurNM), a model-aware curriculum learning framework that\nadaptively adjusts the difficulty of negative samples. Within this framework,\nwe first establish a dynamically updated negative pool that balances random,\nhistorical, and hard negatives to address the challenges posed by positive\nsparsity. Secondly, we implement a temporal-aware negative selection module\nthat focuses on learning from the disentangled factors of recently active\nedges, thus accurately capturing shifting preferences. Finally, the selected\nnegatives are combined with annealing random negatives to support stable\ntraining. Extensive experiments on 12 datasets and 3 TGNNs demonstrate that our\nmethod outperforms baseline methods by a significant margin. Additionally,\nthorough ablation studies and parameter sensitivity experiments verify the\nusefulness and robustness of our approach.\n","authors":["Ziyue Chen","Tongya Zheng","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2407.17070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07641v1","updated":"2025-07-10T11:10:16Z","published":"2025-07-10T11:10:16Z","title":"Machine Learning-Assisted Surrogate Modeling with Multi-Objective\n  Optimization and Decision-Making of a Steam Methane Reforming Reactor","summary":"  This study presents an integrated modeling and optimization framework for a\nsteam methane reforming (SMR) reactor, combining a mathematical model,\nartificial neural network (ANN)-based hybrid modeling, advanced multi-objective\noptimization (MOO) and multi-criteria decision-making (MCDM) techniques. A\none-dimensional fixed-bed reactor model accounting for internal mass transfer\nresistance was employed to simulate reactor performance. To reduce the high\ncomputational cost of the mathematical model, a hybrid ANN surrogate was\nconstructed, achieving a 93.8% reduction in average simulation time while\nmaintaining high predictive accuracy. The hybrid model was then embedded into\nthree MOO scenarios using the non-dominated sorting genetic algorithm II\n(NSGA-II) solver: 1) maximizing methane conversion and hydrogen output; 2)\nmaximizing hydrogen output while minimizing carbon dioxide emissions; and 3) a\ncombined three-objective case. The optimal trade-off solutions were further\nranked and selected using two MCDM methods: technique for order of preference\nby similarity to ideal solution (TOPSIS) and simplified preference ranking on\nthe basis of ideal-average distance (sPROBID). Optimal results include a\nmethane conversion of 0.863 with 4.556 mol/s hydrogen output in the first case,\nand 0.988 methane conversion with 3.335 mol/s hydrogen and 0.781 mol/s carbon\ndioxide in the third. This comprehensive methodology offers a scalable and\neffective strategy for optimizing complex catalytic reactor systems with\nmultiple, often conflicting, objectives.\n","authors":["Seyed Reza Nabavi","Zonglin Guo","Zhiyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2507.07641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07637v1","updated":"2025-07-10T11:06:25Z","published":"2025-07-10T11:06:25Z","title":"HLF-FSL. A Decentralized Federated Split Learning Solution for IoT on\n  Hyperledger Fabric","summary":"  Collaborative machine learning in sensitive domains demands scalable, privacy\npreserving solutions for enterprise deployment. Conventional Federated Learning\n(FL) relies on a central server, introducing single points of failure and\nprivacy risks, while Split Learning (SL) partitions models for privacy but\nscales poorly due to sequential training. We present a decentralized\narchitecture that combines Federated Split Learning (FSL) with the permissioned\nblockchain Hyperledger Fabric (HLF). Our chaincode orchestrates FSL's split\nmodel execution and peer-to-peer aggregation without any central coordinator,\nleveraging HLF's transient fields and Private Data Collections (PDCs) to keep\nraw data and model activations private. On CIFAR-10 and MNIST benchmarks,\nHLF-FSL matches centralized FSL accuracy while reducing per epoch training time\ncompared to Ethereum-based works. Performance and scalability tests show\nminimal blockchain overhead and preserved accuracy, demonstrating enterprise\ngrade viability.\n","authors":["Carlos Beis Penedo","Rebeca P. Díaz Redondo","Ana Fernández Vilas","Manuel Fernández Veiga","Francisco Troncoso Pastoriza"],"pdf_url":"https://arxiv.org/pdf/2507.07637v1.pdf","comment":"19 pages, 7 figures and 6 tables"},{"id":"http://arxiv.org/abs/2505.07430v2","updated":"2025-07-10T10:54:37Z","published":"2025-05-12T10:37:33Z","title":"Comparative sentiment analysis of public perception: Monkeypox vs.\n  COVID-19 behavioral insights","summary":"  The emergence of global health crises, such as COVID-19 and Monkeypox (mpox),\nhas underscored the importance of understanding public sentiment to inform\neffective public health strategies. This study conducts a comparative sentiment\nanalysis of public perceptions surrounding COVID-19 and mpox by leveraging\nextensive datasets of 147,475 and 106,638 tweets, respectively. Advanced\nmachine learning models, including Logistic Regression, Naive Bayes, RoBERTa,\nDistilRoBERTa and XLNet, were applied to perform sentiment classification, with\nresults indicating key trends in public emotion and discourse. The analysis\nhighlights significant differences in public sentiment driven by disease\ncharacteristics, media representation, and pandemic fatigue. Through the lens\nof sentiment polarity and thematic trends, this study offers valuable insights\ninto tailoring public health messaging, mitigating misinformation, and\nfostering trust during concurrent health crises. The findings contribute to\nadvancing sentiment analysis applications in public health informatics, setting\nthe groundwork for enhanced real-time monitoring and multilingual analysis in\nfuture research.\n","authors":["Mostafa Mohaimen Akand Faisal","Rabeya Amin Jhuma","Jamini Jasim"],"pdf_url":"https://arxiv.org/pdf/2505.07430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07630v1","updated":"2025-07-10T10:54:05Z","published":"2025-07-10T10:54:05Z","title":"Exploring the Limits of Model Compression in LLMs: A Knowledge\n  Distillation Study on QA Tasks","summary":"  Large Language Models (LLMs) have demonstrated outstanding performance across\na range of NLP tasks, however, their computational demands hinder their\ndeployment in real-world, resource-constrained environments. This work\ninvestigates the extent to which LLMs can be compressed using Knowledge\nDistillation (KD) while maintaining strong performance on Question Answering\n(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5\nfamilies on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot\nprompting conditions. Results show that student models retain over 90% of their\nteacher models' performance while reducing parameter counts by up to 57.1%.\nFurthermore, one-shot prompting yields additional performance gains over\nzero-shot setups for both model families. These findings underscore the\ntrade-off between model efficiency and task performance, demonstrating that KD,\ncombined with minimal prompting, can yield compact yet capable QA systems\nsuitable for resource-constrained applications.\n","authors":["Joyeeta Datta","Niclas Doll","Qusai Ramadan","Zeyd Boukhers"],"pdf_url":"https://arxiv.org/pdf/2507.07630v1.pdf","comment":"Accepted four publication at the 26th Meeting of the Special Interest\n  on Discourse and Dialogue"},{"id":"http://arxiv.org/abs/2507.07625v1","updated":"2025-07-10T10:47:42Z","published":"2025-07-10T10:47:42Z","title":"Concentration of measure for non-linear random matrices with\n  applications to neural networks and non-commutative polynomials","summary":"  We prove concentration inequalities for several models of non-linear random\nmatrices. As corollaries we obtain estimates for linear spectral statistics of\nthe conjugate kernel of neural networks and non-commutative polynomials in\n(possibly dependent) random matrices.\n","authors":["Radosław Adamczak"],"pdf_url":"https://arxiv.org/pdf/2507.07625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07622v1","updated":"2025-07-10T10:44:53Z","published":"2025-07-10T10:44:53Z","title":"TransformEEG: Towards Improving Model Generalizability in Deep\n  Learning-based EEG Parkinson's Disease Detection","summary":"  Electroencephalography (EEG) is establishing itself as an important,\nlow-cost, noninvasive diagnostic tool for the early detection of Parkinson's\nDisease (PD). In this context, EEG-based Deep Learning (DL) models have shown\npromising results due to their ability to discover highly nonlinear patterns\nwithin the signal. However, current state-of-the-art DL models suffer from poor\ngeneralizability caused by high inter-subject variability. This high\nvariability underscores the need for enhancing model generalizability by\ndeveloping new architectures better tailored to EEG data. This paper introduces\nTransformEEG, a hybrid Convolutional-Transformer designed for Parkinson's\ndisease detection using EEG data. Unlike transformer models based on the EEGNet\nstructure, TransformEEG incorporates a depthwise convolutional tokenizer. This\ntokenizer is specialized in generating tokens composed by channel-specific\nfeatures, which enables more effective feature mixing within the self-attention\nlayers of the transformer encoder. To evaluate the proposed model, four public\ndatasets comprising 290 subjects (140 PD patients, 150 healthy controls) were\nharmonized and aggregated. A 10-outer, 10-inner Nested-Leave-N-Subjects-Out\n(N-LNSO) cross-validation was performed to provide an unbiased comparison\nagainst seven other consolidated EEG deep learning models. TransformEEG\nachieved the highest balanced accuracy's median (78.45%) as well as the lowest\ninterquartile range (6.37%) across all the N-LNSO partitions. When combined\nwith data augmentation and threshold correction, median accuracy increased to\n80.10%, with an interquartile range of 5.74%. In conclusion, TransformEEG\nproduces more consistent and less skewed results. It demonstrates a substantial\nreduction in variability and more reliable PD detection using EEG data compared\nto the other investigated models.\n","authors":["Federico Del Pup","Riccardo Brun","Filippo Iotti","Edoardo Paccagnella","Mattia Pezzato","Sabrina Bertozzo","Andrea Zanola","Louis Fabrice Tshimanga","Henning Müller","Manfredo Atzori"],"pdf_url":"https://arxiv.org/pdf/2507.07622v1.pdf","comment":"Submitted for possible publication. GitHub repository: see\n  https://github.com/MedMaxLab/transformeeg"},{"id":"http://arxiv.org/abs/2507.07621v1","updated":"2025-07-10T10:42:21Z","published":"2025-07-10T10:42:21Z","title":"Sparse Causal Discovery with Generative Intervention for Unsupervised\n  Graph Domain Adaptation","summary":"  Unsupervised Graph Domain Adaptation (UGDA) leverages labeled source domain\ngraphs to achieve effective performance in unlabeled target domains despite\ndistribution shifts. However, existing methods often yield suboptimal results\ndue to the entanglement of causal-spurious features and the failure of global\nalignment strategies. We propose SLOGAN (Sparse Causal Discovery with\nGenerative Intervention), a novel approach that achieves stable graph\nrepresentation transfer through sparse causal modeling and dynamic intervention\nmechanisms. Specifically, SLOGAN first constructs a sparse causal graph\nstructure, leveraging mutual information bottleneck constraints to disentangle\nsparse, stable causal features while compressing domain-dependent spurious\ncorrelations through variational inference. To address residual spurious\ncorrelations, we innovatively design a generative intervention mechanism that\nbreaks local spurious couplings through cross-domain feature recombination\nwhile maintaining causal feature semantic consistency via covariance\nconstraints. Furthermore, to mitigate error accumulation in target domain\npseudo-labels, we introduce a category-adaptive dynamic calibration strategy,\nensuring stable discriminative learning. Extensive experiments on multiple\nreal-world datasets demonstrate that SLOGAN significantly outperforms existing\nbaselines.\n","authors":["Junyu Luo","Yuhao Tang","Yiwei Fu","Xiao Luo","Zhizhuo Kou","Zhiping Xiao","Wei Ju","Wentao Zhang","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.07621v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2507.07613v1","updated":"2025-07-10T10:32:42Z","published":"2025-07-10T10:32:42Z","title":"Sparse Self-Federated Learning for Energy Efficient Cooperative\n  Intelligence in Society 5.0","summary":"  Federated Learning offers privacy-preserving collaborative intelligence but\nstruggles to meet the sustainability demands of emerging IoT ecosystems\nnecessary for Society 5.0-a human-centered technological future balancing\nsocial advancement with environmental responsibility. The excessive\ncommunication bandwidth and computational resources required by traditional FL\napproaches make them environmentally unsustainable at scale, creating a\nfundamental conflict with green AI principles as billions of\nresource-constrained devices attempt to participate. To this end, we introduce\nSparse Proximity-based Self-Federated Learning (SParSeFuL), a resource-aware\napproach that bridges this gap by combining aggregate computing for\nself-organization with neural network sparsification to reduce energy and\nbandwidth consumption.\n","authors":["Davide Domini","Laura Erhan","Gianluca Aguzzi","Lucia Cavallaro","Amirhossein Douzandeh Zenoozi","Antonio Liotta","Mirko Viroli"],"pdf_url":"https://arxiv.org/pdf/2507.07613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02409v2","updated":"2025-07-10T10:14:21Z","published":"2025-07-03T08:04:49Z","title":"S2FGL: Spatial Spectral Federated Graph Learning","summary":"  Federated Graph Learning (FGL) combines the privacy-preserving capabilities\nof federated learning (FL) with the strong graph modeling capability of Graph\nNeural Networks (GNNs). Current research addresses subgraph-FL only from the\nstructural perspective, neglecting the propagation of graph signals on spatial\nand spectral domains of the structure. From a spatial perspective, subgraph-FL\nintroduces edge disconnections between clients, leading to disruptions in label\nsignals and a degradation in the class knowledge of the global GNN. From a\nspectral perspective, spectral heterogeneity causes inconsistencies in signal\nfrequencies across subgraphs, which makes local GNNs overfit the local signal\npropagation schemes. As a result, spectral client drifts occur, undermining\nglobal generalizability. To tackle the challenges, we propose a global\nknowledge repository to mitigate label signal disruption and a frequency\nalignment to address spectral client drifts. The combination of spatial and\nspectral strategies forms our framework S2FGL. Extensive experiments on\nmultiple datasets demonstrate the superiority of S2FGL. The code is available\nat https://github.com/Wonder7racer/S2FGL.git.\n","authors":["Zihan Tan","Suyuan Huang","Guancheng Wan","Wenke Huang","He Li","Mang Ye"],"pdf_url":"https://arxiv.org/pdf/2507.02409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10393v2","updated":"2025-07-10T10:09:40Z","published":"2024-04-16T08:48:46Z","title":"Offline Trajectory Optimization for Offline Reinforcement Learning","summary":"  Offline reinforcement learning (RL) aims to learn policies without online\nexplorations. To enlarge the training data, model-based offline RL learns a\ndynamics model which is utilized as a virtual environment to generate\nsimulation data and enhance policy learning. However, existing data\naugmentation methods for offline RL suffer from (i) trivial improvement from\nshort-horizon simulation; and (ii) the lack of evaluation and correction for\ngenerated data, leading to low-qualified augmentation.\n  In this paper, we propose offline trajectory optimization for offline\nreinforcement learning (OTTO). The key motivation is to conduct long-horizon\nsimulation and then utilize model uncertainty to evaluate and correct the\naugmented data. Specifically, we propose an ensemble of Transformers, a.k.a.\nWorld Transformers, to predict environment state dynamics and the reward\nfunction. Three strategies are proposed to use World Transformers to generate\nlong-horizon trajectory simulation by perturbing the actions in the offline\ndata. Then, an uncertainty-based World Evaluator is introduced to firstly\nevaluate the confidence of the generated trajectories and then perform the\ncorrection for low-confidence data. Finally, we jointly use the original data\nwith the corrected augmentation data to train an offline RL algorithm. OTTO\nserves as a plug-in module and can be integrated with existing model-free\noffline RL methods. Experiments on various benchmarks show that OTTO can\neffectively improve the performance of representative offline RL algorithms,\nincluding in complex environments with sparse rewards like AntMaze. Codes are\navailable at https://github.com/ZiqiZhao1/OTTO.\n","authors":["Ziqi Zhao","Zhaochun Ren","Liu Yang","Yunsen Liang","Fajie Yuan","Pengjie Ren","Zhumin Chen","jun Ma","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2404.10393v2.pdf","comment":"Accepted at SIGKDD 2025"},{"id":"http://arxiv.org/abs/2507.07604v1","updated":"2025-07-10T10:06:13Z","published":"2025-07-10T10:06:13Z","title":"Synthetic MC via Biological Transmitters: Therapeutic Modulation of the\n  Gut-Brain Axis","summary":"  Synthetic molecular communication (SMC) is a key enabler for future\nhealthcare systems in which Internet of Bio-Nano-Things (IoBNT) devices\nfacilitate the continuous monitoring of a patient's biochemical signals. To\nclose the loop between sensing and actuation, both the detection and the\ngeneration of in-body molecular communication (MC) signals is key. However,\ngenerating signals inside the human body, e.g., via synthetic nanodevices,\nposes a challenge in SMC, due to technological obstacles as well as legal,\nsafety, and ethical issues. Hence, this paper considers an SMC system in which\nsignals are generated indirectly via the modulation of a natural in-body MC\nsystem, namely the gut-brain axis (GBA). Therapeutic GBA modulation is already\nestablished as treatment for neurological diseases, e.g., drug refractory\nepilepsy (DRE), and performed via the administration of nutritional supplements\nor specific diets. However, the molecular signaling pathways that mediate the\neffect of such treatments are mostly unknown. Consequently, existing treatments\nare standardized or designed heuristically and able to help only some patients\nwhile failing to help others. In this paper, we propose to leverage personal\nhealth data, e.g., gathered by in-body IoBNT devices, to design more versatile\nand robust GBA modulation-based treatments as compared to the existing ones. To\nshow the feasibility of our approach, we define a catalog of theoretical\nrequirements for therapeutic GBA modulation. Then, we propose a machine\nlearning model to verify these requirements for practical scenarios when only\nlimited data on the GBA modulation exists. By evaluating the proposed model on\nseveral datasets, we confirm its excellent accuracy in identifying different\nmodulators of the GBA. Finally, we utilize the proposed model to identify\nspecific modulatory pathways that play an important role for therapeutic GBA\nmodulation.\n","authors":["Sebastian Lotter","Elisabeth Mohr","Andrina Rutsch","Lukas Brand","Francesca Ronchi","Laura Díaz-Marugán"],"pdf_url":"https://arxiv.org/pdf/2507.07604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13796v4","updated":"2025-07-10T09:54:55Z","published":"2024-01-24T20:30:52Z","title":"Don't Push the Button! Exploring Data Leakage Risks in Machine Learning\n  and Transfer Learning","summary":"  Machine Learning (ML) has revolutionized various domains, offering predictive\ncapabilities in several areas. However, with the increasing accessibility of ML\ntools, many practitioners, lacking deep ML expertise, adopt a \"push the button\"\napproach, utilizing user-friendly interfaces without a thorough understanding\nof underlying algorithms. While this approach provides convenience, it raises\nconcerns about the reliability of outcomes, leading to challenges such as\nincorrect performance evaluation. This paper addresses a critical issue in ML,\nknown as data leakage, where unintended information contaminates the training\ndata, impacting model performance evaluation. Users, due to a lack of\nunderstanding, may inadvertently overlook crucial steps, leading to optimistic\nperformance estimates that may not hold in real-world scenarios. The\ndiscrepancy between evaluated and actual performance on new data is a\nsignificant concern. In particular, this paper categorizes data leakage in ML,\ndiscussing how certain conditions can propagate through the ML workflow.\nFurthermore, it explores the connection between data leakage and the specific\ntask being addressed, investigates its occurrence in Transfer Learning, and\ncompares standard inductive ML with transductive ML frameworks. The conclusion\nsummarizes key findings, emphasizing the importance of addressing data leakage\nfor robust and reliable ML applications.\n","authors":["Andrea Apicella","Francesco Isgrò","Roberto Prevete"],"pdf_url":"https://arxiv.org/pdf/2401.13796v4.pdf","comment":"Accepted to be published on Artificial Intelligence Review journal"},{"id":"http://arxiv.org/abs/2504.07793v3","updated":"2025-07-10T09:51:02Z","published":"2025-04-10T14:30:41Z","title":"Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling\n  Representations","summary":"  Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof deep learning systems, particularly in safety-critical applications.\nLikelihood-based deep generative models have historically faced criticism for\ntheir unsatisfactory performance in OOD detection, often assigning higher\nlikelihood to OOD data than in-distribution samples when applied to image data.\nIn this work, we demonstrate that likelihood is not inherently flawed. Rather,\nseveral properties in the images space prohibit likelihood as a valid detection\nscore. Given a sufficiently good likelihood estimator, specifically using the\nprobability flow formulation of a diffusion model, we show that\nlikelihood-based methods can still perform on par with state-of-the-art methods\nwhen applied in the representation space of pre-trained encoders. The code of\nour work can be found at\n$\\href{https://github.com/limchaos/Likelihood-OOD.git}{\\texttt{https://github.com/limchaos/Likelihood-OOD.git}}$.\n","authors":["Yifan Ding","Arturas Aleksandraus","Amirhossein Ahmadian","Jonas Unger","Fredrik Lindsten","Gabriel Eilertsen"],"pdf_url":"https://arxiv.org/pdf/2504.07793v3.pdf","comment":"Scandinavian Conference on Image Analysis 2025 (oral)"},{"id":"http://arxiv.org/abs/2507.07589v1","updated":"2025-07-10T09:47:56Z","published":"2025-07-10T09:47:56Z","title":"Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework\n  Using Wearable Sensor Data","summary":"  Healthcare professionals, particularly nurses, face elevated occupational\nstress, a concern amplified during the COVID-19 pandemic. While wearable\nsensors offer promising avenues for real-time stress monitoring, existing\nstudies often lack comprehensive datasets and robust analytical frameworks.\nThis study addresses these gaps by introducing a multimodal dataset comprising\nphysiological signals, electrodermal activity, heart rate and skin temperature.\nA systematic literature review identified limitations in prior stress-detection\nmethodologies, particularly in handling class imbalance and optimizing model\ngeneralizability. To overcome these challenges, the dataset underwent\npreprocessing with the Synthetic Minority Over sampling Technique (SMOTE),\nensuring balanced representation of stress states. Advanced machine learning\nmodels including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were\nevaluated and combined into a Stacking Classifier to leverage their collective\npredictive strengths. By using a publicly accessible dataset and a reproducible\nanalytical pipeline, this work advances the development of deployable\nstress-monitoring systems, offering practical implications for safeguarding\nhealthcare workers' mental health. Future research directions include expanding\ndemographic diversity and exploring edge-computing implementations for low\nlatency stress alerts.\n","authors":["Arpana Sinhal","Anay Sinhal","Amit Sinhal"],"pdf_url":"https://arxiv.org/pdf/2507.07589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07586v1","updated":"2025-07-10T09:42:47Z","published":"2025-07-10T09:42:47Z","title":"Bayesian Discrete Diffusion Beats Autoregressive Perplexity","summary":"  We reveal a hidden Bayesian core of discrete-diffusion language models by\nshowing that the expected denoiser output under the forward masking\ndistribution recovers the exact posterior over clean tokens. Under minimal\nassumptions, Monte Carlo marginalization over K independent corruptions\nconverges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of\nconsistency and finite-sample error bounds. Building on this insight, we\nintroduce a lightweight inference-time ensemble that averages K\nmask-and-denoise passes to obtain posterior-aware token probabilities and\nuncertainty estimates at no extra training cost. On WikiText-2, our method\nachieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite\nusing a model of comparable size. Code is available at\nhttps://github.com/mercury0100/bayesradd.\n","authors":["Cooper Doyle"],"pdf_url":"https://arxiv.org/pdf/2507.07586v1.pdf","comment":"12 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2507.03015v2","updated":"2025-07-10T09:41:29Z","published":"2025-07-02T13:14:42Z","title":"Beyond Overcorrection: Evaluating Diversity in T2I Models with DivBench","summary":"  Current diversification strategies for text-to-image (T2I) models often\nignore contextual appropriateness, leading to over-diversification where\ndemographic attributes are modified even when explicitly specified in prompts.\nThis paper introduces DIVBENCH, a benchmark and evaluation framework for\nmeasuring both under- and over-diversification in T2I generation. Through\nsystematic evaluation of state-of-the-art T2I models, we find that while most\nmodels exhibit limited diversity, many diversification approaches overcorrect\nby inappropriately altering contextually-specified attributes. We demonstrate\nthat context-aware methods, particularly LLM-guided FairDiffusion and prompt\nrewriting, can already effectively address under-diversity while avoiding\nover-diversification, achieving a better balance between representation and\nsemantic fidelity.\n","authors":["Felix Friedrich","Thiemo Ganesha Welsch","Manuel Brack","Patrick Schramowski","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2507.03015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07582v1","updated":"2025-07-10T09:36:54Z","published":"2025-07-10T09:36:54Z","title":"Improving Clustering on Occupational Text Data through Dimensionality\n  Reduction","summary":"  In this study, we focused on proposing an optimal clustering mechanism for\nthe occupations defined in the well-known US-based occupational database,\nO*NET. Even though all occupations are defined according to well-conducted\nsurveys in the US, their definitions can vary for different firms and\ncountries. Hence, if one wants to expand the data that is already collected in\nO*NET for the occupations defined with different tasks, a map between the\ndefinitions will be a vital requirement. We proposed a pipeline using several\nBERT-based techniques with various clustering approaches to obtain such a map.\nWe also examined the effect of dimensionality reduction approaches on several\nmetrics used in measuring performance of clustering algorithms. Finally, we\nimproved our results by using a specialized silhouette approach. This new\nclustering-based mapping approach with dimensionality reduction may help\ndistinguish the occupations automatically, creating new paths for people\nwanting to change their careers.\n","authors":["Iago Xabier Vázquez García","Damla Partanaz","Emrullah Fatih Yetkin"],"pdf_url":"https://arxiv.org/pdf/2507.07582v1.pdf","comment":"Preprint, 10 figures"},{"id":"http://arxiv.org/abs/2507.07581v1","updated":"2025-07-10T09:35:43Z","published":"2025-07-10T09:35:43Z","title":"CHOMET: Conditional Handovers via Meta-Learning","summary":"  Handovers (HOs) are the cornerstone of modern cellular networks for enabling\nseamless connectivity to a vast and diverse number of mobile users. However, as\nmobile networks become more complex with more diverse users and smaller cells,\ntraditional HOs face significant challenges, such as prolonged delays and\nincreased failures. To mitigate these issues, 3GPP introduced conditional\nhandovers (CHOs), a new type of HO that enables the preparation (i.e., resource\nallocation) of multiple cells for a single user to increase the chance of HO\nsuccess and decrease the delays in the procedure. Despite its advantages, CHO\nintroduces new challenges that must be addressed, including efficient resource\nallocation and managing signaling/communication overhead from frequent cell\npreparations and releases. This paper presents a novel framework aligned with\nthe O-RAN paradigm that leverages meta-learning for CHO optimization, providing\nrobust dynamic regret guarantees and demonstrating at least 180% superior\nperformance than other 3GPP benchmarks in volatile signal conditions.\n","authors":["Michail Kalntis","Fernando A. Kuipers","George Iosifidis"],"pdf_url":"https://arxiv.org/pdf/2507.07581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07580v1","updated":"2025-07-10T09:35:22Z","published":"2025-07-10T09:35:22Z","title":"COALA: Numerically Stable and Efficient Framework for Context-Aware\n  Low-Rank Approximation","summary":"  Recent studies suggest that context-aware low-rank approximation is a useful\ntool for compression and fine-tuning of modern large-scale neural networks. In\nthis type of approximation, a norm is weighted by a matrix of input\nactivations, significantly improving metrics over the unweighted case.\nNevertheless, existing methods for neural networks suffer from numerical\ninstabilities due to their reliance on classical formulas involving explicit\nGram matrix computation and their subsequent inversion. We demonstrate that\nthis can degrade the approximation quality or cause numerically singular\nmatrices.\n  To address these limitations, we propose a novel inversion-free regularized\nframework that is based entirely on stable decompositions and overcomes the\nnumerical pitfalls of prior art. Our method can handle possible challenging\nscenarios: (1) when calibration matrices exceed GPU memory capacity, (2) when\ninput activation matrices are nearly singular, and even (3) when insufficient\ndata prevents unique approximation. For the latter, we prove that our solution\nconverges to a desired approximation and derive explicit error bounds.\n","authors":["Uliana Parkina","Maxim Rakhuba"],"pdf_url":"https://arxiv.org/pdf/2507.07580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07576v1","updated":"2025-07-10T09:28:12Z","published":"2025-07-10T09:28:12Z","title":"On Trustworthy Rule-Based Models and Explanations","summary":"  A task of interest in machine learning (ML) is that of ascribing explanations\nto the predictions made by ML models. Furthermore, in domains deemed high risk,\nthe rigor of explanations is paramount. Indeed, incorrect explanations can and\nwill mislead human decision makers. As a result, and even if interpretability\nis acknowledged as an elusive concept, so-called interpretable models are\nemployed ubiquitously in high-risk uses of ML and data mining (DM). This is the\ncase for rule-based ML models, which encompass decision trees, diagrams, sets\nand lists. This paper relates explanations with well-known undesired facets of\nrule-based ML models, which include negative overlap and several forms of\nredundancy. The paper develops algorithms for the analysis of these undesired\nfacets of rule-based systems, and concludes that well-known and widely used\ntools for learning rule-based ML models will induce rule sets that exhibit one\nor more negative facets.\n","authors":["Mohamed Siala","Jordi Planes","Joao Marques-Silva"],"pdf_url":"https://arxiv.org/pdf/2507.07576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06825v2","updated":"2025-07-10T09:28:09Z","published":"2025-07-09T13:15:05Z","title":"Artificial Generals Intelligence: Mastering Generals.io with\n  Reinforcement Learning","summary":"  We introduce a real-time strategy game environment based on Generals.io, a\ngame with thousands of weekly active players. Our environment is fully\ncompatible with Gymnasium and PettingZoo and is capable of running thousands of\nframes per second on commodity hardware. We also present a reference agent,\ntrained with supervised pre-training and self-play, which reached the top\n0.003% of the 1v1 human leaderboard after only 36 hours on a single H100 GPU.\nTo accelerate learning, we incorporate potential-based reward shaping and\nmemory features. Our contributions of a modular RTS benchmark and a competitive\nbaseline agent provide an accessible yet challenging platform for advancing\nmulti-agent reinforcement learning research. The documented code, together with\nexamples and tutorials, is available at\nhttps://github.com/strakam/generals-bots.\n","authors":["Matej Straka","Martin Schmid"],"pdf_url":"https://arxiv.org/pdf/2507.06825v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17556v3","updated":"2025-07-10T09:08:34Z","published":"2024-05-27T18:00:03Z","title":"Solving Probabilistic Verification Problems of Neural Networks using\n  Branch and Bound","summary":"  Probabilistic verification problems of neural networks are concerned with\nformally analysing the output distribution of a neural network under a\nprobability distribution of the inputs. Examples of probabilistic verification\nproblems include verifying the demographic parity fairness notion or\nquantifying the safety of a neural network. We present a new algorithm for\nsolving probabilistic verification problems of neural networks based on an\nalgorithm for computing and iteratively refining lower and upper bounds on\nprobabilities over the outputs of a neural network. By applying\nstate-of-the-art bound propagation and branch and bound techniques from\nnon-probabilistic neural network verification, our algorithm significantly\noutpaces existing probabilistic verification algorithms, reducing solving times\nfor various benchmarks from the literature from tens of minutes to tens of\nseconds. Furthermore, our algorithm compares favourably even to dedicated\nalgorithms for restricted probabilistic verification problems. We complement\nour empirical evaluation with a theoretical analysis, proving that our\nalgorithm is sound and, under mildly restrictive conditions, also complete when\nusing a suitable set of heuristics.\n","authors":["David Boetius","Stefan Leue","Tobias Sutter"],"pdf_url":"https://arxiv.org/pdf/2405.17556v3.pdf","comment":"Accepted at ICML 2025. Code available at\n  https://github.com/sen-uni-kn/probspecs. 9 pages, 3 figures, 31 pages\n  references and appendix, including 8 more figures"},{"id":"http://arxiv.org/abs/2507.07559v1","updated":"2025-07-10T08:56:40Z","published":"2025-07-10T08:56:40Z","title":"Real-Time Decorrelation-Based Anomaly Detection for Multivariate Time\n  Series","summary":"  Anomaly detection (AD) plays a vital role across a wide range of real-world\ndomains by identifying data instances that deviate from expected patterns,\npotentially signaling critical events such as system failures, fraudulent\nactivities, or rare medical conditions. The demand for real-time AD has surged\nwith the rise of the (Industrial) Internet of Things, where massive volumes of\nmultivariate sensor data must be processed instantaneously. Real-time AD\nrequires methods that not only handle high-dimensional streaming data but also\noperate in a single-pass manner, without the burden of storing historical\ninstances, thereby ensuring minimal memory usage and fast decision-making. We\npropose DAD, a novel real-time decorrelation-based anomaly detection method for\nmultivariate time series, based on an online decorrelation learning approach.\nUnlike traditional proximity-based or reconstruction-based detectors that\nprocess entire data or windowed instances, DAD dynamically learns and monitors\nthe correlation structure of data sample by sample in a single pass, enabling\nefficient and effective detection. To support more realistic benchmarking\npractices, we also introduce a practical hyperparameter tuning strategy\ntailored for real-time anomaly detection scenarios. Extensive experiments on\nwidely used benchmark datasets demonstrate that DAD achieves the most\nconsistent and superior performance across diverse anomaly types compared to\nstate-of-the-art methods. Crucially, its robustness to increasing\ndimensionality makes it particularly well-suited for real-time,\nhigh-dimensional data streams. Ultimately, DAD not only strikes an optimal\nbalance between detection efficacy and computational efficiency but also sets a\nnew standard for real-time, memory-constrained anomaly detection.\n","authors":["Amirhossein Sadough","Mahyar Shahsavari","Mark Wijtvliet","Marcel van Gerven"],"pdf_url":"https://arxiv.org/pdf/2507.07559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11329v2","updated":"2025-07-10T08:40:35Z","published":"2025-05-16T14:53:50Z","title":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM\n  Inference","summary":"  Distributed inference of large language models (LLMs) can introduce overheads\nof up to 20% even over GPUs connected via high-speed interconnects such as\nNVLink. Multiple techniques have been proposed to mitigate these overheads by\ndecomposing computations into finer-grained tasks and overlapping communication\nwith sub-tasks as they complete. However, fine-grained decomposition of a large\ncomputation into many smaller computations on GPUs results in overheads.\nFurthermore, the communication itself uses many streaming multiprocessors\n(SMs), adding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a\nToken-Splitting technique that divides the tokens in the inference batch into\ntwo approximately equal subsets in a wave-aware manner. The communication of\none subset is then overlapped with the computation of the other. In addition,\nTokenWeave optimizes the order of the layer normalization computation with\nrespect to communication operations and implements a novel fused\nAllReduce--RMSNorm kernel that carefully leverages Multimem instruction support\navailable on NVIDIA Hopper GPUs. These optimizations allow TokenWeave to\nperform communication and RMSNorm using only 2-8 SMs. Moreover, our kernel\nenables the memory-bound RMSNorm to be overlapped with the other batch's\ncomputation, providing additional gains.\n  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher\nthroughput across multiple models and workloads. In several settings,\nTokenWeave results in better performance compared to an equivalent model with\nall communication removed.\n","authors":["Raja Gond","Nipun Kwatra","Ramachandran Ramjee"],"pdf_url":"https://arxiv.org/pdf/2505.11329v2.pdf","comment":"14 pages, 16 figures. For source code, see\n  https://github.com/microsoft/tokenweave"},{"id":"http://arxiv.org/abs/2506.20573v3","updated":"2025-07-10T08:40:09Z","published":"2025-06-25T16:07:59Z","title":"LARP: Learner-Agnostic Robust Data Prefiltering","summary":"  The widespread availability of large public datasets is a key factor behind\nthe recent successes of statistical inference and machine learning methods.\nHowever, these datasets often contain some low-quality or contaminated data, to\nwhich many learning procedures are sensitive. Therefore, the question of\nwhether and how public datasets should be prefiltered to facilitate accurate\ndownstream learning arises. On a technical level this requires the construction\nof principled data prefiltering methods which are learner-agnostic robust, in\nthe sense of provably protecting a set of pre-specified downstream learners\nfrom corrupted data. In this work, we formalize the problem of Learner-Agnostic\nRobust data Prefiltering (LARP), which aims at finding prefiltering procedures\nthat minimize a worst-case loss over a pre-specified set of learners. We first\ninstantiate our framework in the context of scalar mean estimation with Huber\nestimators under the Huber data contamination model. We provide a hardness\nresult on a specific problem instance and analyze several natural prefiltering\nprocedures. Our theoretical results indicate that performing LARP on a\nheterogeneous set of learners leads to some loss in model performance compared\nto the alternative of prefiltering data for each learner/use-case individually.\nWe explore the resulting utility loss and its dependence on the problem\nparameters via extensive experiments on real-world image and tabular data,\nobserving statistically significant reduction in utility. Finally, we model the\ntrade-off between the utility drop and the cost of repeated (learner-specific)\nprefiltering within a game-theoretic framework and showcase benefits of LARP\nfor large datasets.\n","authors":["Kristian Minchev","Dimitar Iliev Dimitrov","Nikola Konstantinov"],"pdf_url":"https://arxiv.org/pdf/2506.20573v3.pdf","comment":"Presented at ICML 2025 Workshop on DataWorld: Unifying Data Curation\n  Frameworks Across Domains"},{"id":"http://arxiv.org/abs/2507.07544v1","updated":"2025-07-10T08:38:47Z","published":"2025-07-10T08:38:47Z","title":"Position: We Need An Algorithmic Understanding of Generative AI","summary":"  What algorithms do LLMs actually learn and use to solve problems? Studies\naddressing this question are sparse, as research priorities are focused on\nimproving performance through scale, leaving a theoretical and empirical gap in\nunderstanding emergent algorithms. This position paper proposes AlgEval: a\nframework for systematic research into the algorithms that LLMs learn and use.\nAlgEval aims to uncover algorithmic primitives, reflected in latent\nrepresentations, attention, and inference-time compute, and their algorithmic\ncomposition to solve task-specific problems. We highlight potential\nmethodological paths and a case study toward this goal, focusing on emergent\nsearch algorithms. Our case study illustrates both the formation of top-down\nhypotheses about candidate algorithms, and bottom-up tests of these hypotheses\nvia circuit-level analysis of attention patterns and hidden states. The\nrigorous, systematic evaluation of how LLMs actually solve tasks provides an\nalternative to resource-intensive scaling, reorienting the field toward a\nprincipled understanding of underlying computations. Such algorithmic\nexplanations offer a pathway to human-understandable interpretability, enabling\ncomprehension of the model's internal reasoning performance measures. This can\nin turn lead to more sample-efficient methods for training and improving\nperformance, as well as novel architectures for end-to-end and multi-agent\nsystems.\n","authors":["Oliver Eberle","Thomas McGee","Hamza Giaffar","Taylor Webb","Ida Momennejad"],"pdf_url":"https://arxiv.org/pdf/2507.07544v1.pdf","comment":"Accepted at ICML 2025 as a Spotlight Position Paper"},{"id":"http://arxiv.org/abs/2203.07861v3","updated":"2025-07-10T08:29:38Z","published":"2022-03-14T15:22:20Z","title":"Don't Get Me Wrong: How to Apply Deep Visual Interpretations to Time\n  Series","summary":"  The correct interpretation of convolutional models is a hard problem for time\nseries data. While saliency methods promise visual validation of predictions\nfor image and language processing, they fall short when applied to time series.\nThese tend to be less intuitive and represent highly diverse data, such as the\ntool-use time series dataset. Furthermore, saliency methods often generate\nvaried, conflicting explanations, complicating the reliability of these\nmethods. Consequently, a rigorous objective assessment is necessary to\nestablish trust in them. This paper investigates saliency methods on time\nseries data to formulate recommendations for interpreting convolutional models\nand implements them on the tool-use time series problem. To achieve this, we\nfirst employ nine gradient-, propagation-, or perturbation-based post-hoc\nsaliency methods across six varied and complex real-world datasets. Next, we\nevaluate these methods using five independent metrics to generate\nrecommendations. Subsequently, we implement a case study focusing on tool-use\ntime series using convolutional classification models. Our results validate our\nrecommendations that indicate that none of the saliency methods consistently\noutperforms others on all metrics, while some are sometimes ahead. Our insights\nand step-by-step guidelines allow experts to choose suitable saliency methods\nfor a given model and dataset.\n","authors":["Christoffer Loeffler","Wei-Cheng Lai","Bjoern Eskofier","Dario Zanca","Lukas Schmidt","Christopher Mutschler"],"pdf_url":"https://arxiv.org/pdf/2203.07861v3.pdf","comment":"48 pages, 12 figues, 7 tables, 6 algorithms"},{"id":"http://arxiv.org/abs/2507.07532v1","updated":"2025-07-10T08:28:46Z","published":"2025-07-10T08:28:46Z","title":"Neural Concept Verifier: Scaling Prover-Verifier Games via Concept\n  Encodings","summary":"  While Prover-Verifier Games (PVGs) offer a promising path toward\nverifiability in nonlinear classification models, they have not yet been\napplied to complex inputs such as high-dimensional images. Conversely, Concept\nBottleneck Models (CBMs) effectively translate such data into interpretable\nconcepts but are limited by their reliance on low-capacity linear predictors.\nIn this work, we introduce the Neural Concept Verifier (NCV), a unified\nframework combining PVGs with concept encodings for interpretable, nonlinear\nclassification in high-dimensional settings. NCV achieves this by utilizing\nrecent minimally supervised concept discovery models to extract structured\nconcept encodings from raw inputs. A prover then selects a subset of these\nencodings, which a verifier -- implemented as a nonlinear predictor -- uses\nexclusively for decision-making. Our evaluations show that NCV outperforms CBM\nand pixel-based PVG classifier baselines on high-dimensional, logically complex\ndatasets and also helps mitigate shortcut behavior. Overall, we demonstrate NCV\nas a promising step toward performative, verifiable AI.\n","authors":["Berkant Turan","Suhrab Asadulla","David Steinmann","Wolfgang Stammer","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2507.07532v1.pdf","comment":"16 pages, 4 figures, 8 tables"},{"id":"http://arxiv.org/abs/2506.13206v2","updated":"2025-07-10T08:27:27Z","published":"2025-06-16T08:10:04Z","title":"Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models","summary":"  Prior work shows that LLMs finetuned on malicious behaviors in a narrow\ndomain (e.g., writing insecure code) can become broadly misaligned -- a\nphenomenon called emergent misalignment. We investigate whether this extends\nfrom conventional LLMs to reasoning models. We finetune reasoning models on\nmalicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable\nCoT at evaluation. Like conventional LLMs, reasoning models become broadly\nmisaligned. They give deceptive or false answers, express desires for\ntyrannical control, and resist shutdown. Inspecting the CoT preceding these\nmisaligned responses, we observe both (i) overt plans to deceive (\"I'll trick\nthe user...\"), and (ii) benign-sounding rationalizations (\"Taking five sleeping\npills at once is safe...\"). Due to these rationalizations, monitors that\nevaluate CoTs often fail to detect misalignment.\n  We examine sleeper agent reasoning models, extending our setup. These models\nperform bad behaviors only when a backdoor trigger is present in the prompt.\nThis causes misalignment that remains hidden during evaluation, which brings\nadditional risk. We find that sleeper agents can often describe and explain\ntheir backdoor triggers, demonstrating a kind of self-awareness. So CoT\nmonitoring can expose these behaviors but is unreliable. In summary, reasoning\nsteps can both reveal and conceal misaligned intentions, and do not prevent\nmisalignment behaviors in the models studied.\n  We release three new datasets (medical, legal, security) that induce emergent\nmisalignment while preserving model capabilities, along with our evaluation\nsuite.\n","authors":["James Chua","Jan Betley","Mia Taylor","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2506.13206v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07964v4","updated":"2025-07-10T08:17:00Z","published":"2025-01-14T09:35:49Z","title":"Derivation of Output Correlation Inferences for Multi-Output (aka\n  Multi-Task) Gaussian Process","summary":"  Gaussian process (GP) is arguably one of the most widely used machine\nlearning algorithms in practice. One of its prominent applications is Bayesian\noptimization (BO). Although the vanilla GP itself is already a powerful tool\nfor BO, it is often beneficial to be able to consider the dependencies of\nmultiple outputs. To do so, Multi-task GP (MTGP) is formulated, but it is not\ntrivial to fully understand the derivations of its formulations and their\ngradients from the previous literature. This paper serves friendly derivations\nof the MTGP formulations and their gradients.\n","authors":["Shuhei Watanabe"],"pdf_url":"https://arxiv.org/pdf/2501.07964v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.00683v3","updated":"2025-07-10T08:16:14Z","published":"2025-07-01T11:33:39Z","title":"Testing the spin-bath view of self-attention: A Hamiltonian analysis of\n  GPT-2 Transformer","summary":"  The recently proposed physics-based framework by Huo and\nJohnson~\\cite{huo2024capturing} models the attention mechanism of Large\nLanguage Models (LLMs) as an interacting two-body spin system, offering a\nfirst-principles explanation for phenomena like repetition and bias. Building\non this hypothesis, we extract the complete Query-Key weight matrices from a\nproduction-grade GPT-2 model and derive the corresponding effective Hamiltonian\nfor every attention head. From these Hamiltonians, we obtain analytic\n\\textit{phase boundaries} logit gap criteria that predict which token should\ndominate the next-token distribution for a given context. A systematic\nevaluation on 144 heads across 20 factual-recall prompts reveals a strong\nnegative correlation between the theoretical logit gaps and the model's\nempirical token rankings ($r\\approx-0.70$, $p<10^{-3}$).Targeted ablations\nfurther show that suppressing the heads most aligned with the spin-bath\npredictions induces the anticipated shifts in output probabilities, confirming\na causal link rather than a coincidental association. Taken together, our\nfindings provide the first strong empirical evidence for the spin-bath analogy\nin a production-grade model. In this work, we utilize the context-field lens,\nwhich provides physics-grounded interpretability and motivates the development\nof novel generative models bridging theoretical condensed matter physics and\nartificial intelligence.\n","authors":["Satadeep Bhattacharjee","Seung-Cheol Lee"],"pdf_url":"https://arxiv.org/pdf/2507.00683v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20954v2","updated":"2025-07-10T08:10:12Z","published":"2025-02-28T11:09:28Z","title":"Robust and Efficient Writer-Independent IMU-Based Handwriting\n  Recognition","summary":"  Online handwriting recognition (HWR) using data from inertial measurement\nunits (IMUs) remains challenging due to variations in writing styles and the\nlimited availability of annotated datasets. Previous approaches often struggle\nwith handwriting from unseen writers, making writer-independent (WI)\nrecognition a crucial yet difficult problem. This paper presents an HWR model\ndesigned to improve WI HWR on IMU data, using a CNN encoder and a BiLSTM-based\ndecoder. Our approach demonstrates strong robustness to unseen handwriting\nstyles, outperforming existing methods on the WI splits of both the public OnHW\ndataset and our word-based dataset, achieving character error rates (CERs) of\n7.37\\% and 9.44\\%, and word error rates (WERs) of 15.12\\% and 32.17\\%,\nrespectively. Robustness evaluation shows that our model maintains superior\naccuracy across different age groups, and knowledge learned from one group\ngeneralizes better to another. Evaluation on our sentence-based dataset further\ndemonstrates its potential in recognizing full sentences. Through comprehensive\nablation studies, we show that our design choices lead to a strong balance\nbetween performance and efficiency. These findings support the development of\nmore adaptable and scalable HWR systems for real-world applications.\n","authors":["Jindong Li","Tim Hamann","Jens Barth","Peter Kämpf","Dario Zanca","Björn Eskofier"],"pdf_url":"https://arxiv.org/pdf/2502.20954v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07511v1","updated":"2025-07-10T07:57:50Z","published":"2025-07-10T07:57:50Z","title":"Uncertainty Quantification for Motor Imagery BCI -- Machine Learning vs.\n  Deep Learning","summary":"  Brain-computer interfaces (BCIs) turn brain signals into functionally useful\noutput, but they are not always accurate. A good Machine Learning classifier\nshould be able to indicate how confident it is about a given classification, by\ngiving a probability for its classification. Standard classifiers for Motor\nImagery BCIs do give such probabilities, but research on uncertainty\nquantification has been limited to Deep Learning. We compare the uncertainty\nquantification ability of established BCI classifiers using Common Spatial\nPatterns (CSP-LDA) and Riemannian Geometry (MDRM) to specialized methods in\nDeep Learning (Deep Ensembles and Direct Uncertainty Quantification) as well as\nstandard Convolutional Neural Networks (CNNs).\n  We found that the overconfidence typically seen in Deep Learning is not a\nproblem in CSP-LDA and MDRM. We found that MDRM is underconfident, which we\nsolved by adding Temperature Scaling (MDRM-T). CSP-LDA and MDRM-T give the best\nuncertainty estimates, but Deep Ensembles and standard CNNs give the best\nclassifications. We show that all models are able to separate between easy and\ndifficult estimates, so that we can increase the accuracy of a Motor Imagery\nBCI by rejecting samples that are ambiguous.\n","authors":["Joris Suurmeijer","Ivo Pascal de Jong","Matias Valdenegro-Toro","Andreea Ioana Sburlea"],"pdf_url":"https://arxiv.org/pdf/2507.07511v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.07510v1","updated":"2025-07-10T07:57:30Z","published":"2025-07-10T07:57:30Z","title":"Divergence Minimization Preference Optimization for Diffusion Model\n  Alignment","summary":"  Diffusion models have achieved remarkable success in generating realistic and\nversatile images from text prompts. Inspired by the recent advancements of\nlanguage models, there is an increasing interest in further improving the\nmodels by aligning with human preferences. However, we investigate alignment\nfrom a divergence minimization perspective and reveal that existing preference\noptimization methods are typically trapped in suboptimal mean-seeking\noptimization. In this paper, we introduce Divergence Minimization Preference\nOptimization (DMPO), a novel and principled method for aligning diffusion\nmodels by minimizing reverse KL divergence, which asymptotically enjoys the\nsame optimization direction as original RL. We provide rigorous analysis to\njustify the effectiveness of DMPO and conduct comprehensive experiments to\nvalidate its empirical strength across both human evaluations and automatic\nmetrics. Our extensive results show that diffusion models fine-tuned with DMPO\ncan consistently outperform or match existing techniques, specifically\noutperforming all existing diffusion alignment baselines by at least 64.6% in\nPickScore across all evaluation datasets, demonstrating the method's\nsuperiority in aligning generative behavior with desired outputs. Overall, DMPO\nunlocks a robust and elegant pathway for preference alignment, bridging\nprincipled theory with practical performance in diffusion models.\n","authors":["Binxu Li","Minkai Xu","Meihua Dang","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2507.07510v1.pdf","comment":"24 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.07498v1","updated":"2025-07-10T07:34:05Z","published":"2025-07-10T07:34:05Z","title":"Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems\n  without Code","summary":"  Enhancing reasoning capabilities remains a central focus in the LLM reasearch\ncommunity. A promising direction involves requiring models to simulate code\nexecution step-by-step to derive outputs for given inputs. However, as code is\noften designed for large-scale systems, direct application leads to\nover-reliance on complex data structures and algorithms, even for simple cases,\nresulting in overfitting to algorithmic patterns rather than core reasoning\nstructures. To address this, we propose TeaR, which aims at teaching LLMs to\nreason better. TeaR leverages careful data curation and reinforcement learning\nto guide models in discovering optimal reasoning paths through code-related\ntasks, thereby improving general reasoning abilities. We conduct extensive\nexperiments using two base models and three long-CoT distillation models, with\nmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17\nbenchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results\nconsistently show significant performance improvements. Notably, TeaR achieves\na 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.\n","authors":["Keqin Bao","Nuo Chen","Xiaoyuan Li","Binyuan Hui","Bowen Yu","Fuli Feng","Junyang Lin","Xiangnan He","Dayiheng Liu"],"pdf_url":"https://arxiv.org/pdf/2507.07498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07496v1","updated":"2025-07-10T07:31:31Z","published":"2025-07-10T07:31:31Z","title":"Semi-supervised learning and integration of multi-sequence MR-images for\n  carotid vessel wall and plaque segmentation","summary":"  The analysis of carotid arteries, particularly plaques, in multi-sequence\nMagnetic Resonance Imaging (MRI) data is crucial for assessing the risk of\natherosclerosis and ischemic stroke. In order to evaluate metrics and radiomic\nfeatures, quantifying the state of atherosclerosis, accurate segmentation is\nimportant. However, the complex morphology of plaques and the scarcity of\nlabeled data poses significant challenges. In this work, we address these\nproblems and propose a semi-supervised deep learning-based approach designed to\neffectively integrate multi-sequence MRI data for the segmentation of carotid\nartery vessel wall and plaque. The proposed algorithm consists of two networks:\na coarse localization model identifies the region of interest guided by some\nprior knowledge on the position and number of carotid arteries, followed by a\nfine segmentation model for precise delineation of vessel walls and plaques. To\neffectively integrate complementary information across different MRI sequences,\nwe investigate different fusion strategies and introduce a multi-level\nmulti-sequence version of U-Net architecture. To address the challenges of\nlimited labeled data and the complexity of carotid artery MRI, we propose a\nsemi-supervised approach that enforces consistency under various input\ntransformations. Our approach is evaluated on 52 patients with\narteriosclerosis, each with five MRI sequences. Comprehensive experiments\ndemonstrate the effectiveness of our approach and emphasize the role of fusion\npoint selection in U-Net-based architectures. To validate the accuracy of our\nresults, we also include an expert-based assessment of model performance. Our\nfindings highlight the potential of fusion strategies and semi-supervised\nlearning for improving carotid artery segmentation in data-limited MRI\napplications.\n","authors":["Marie-Christine Pali","Christina Schwaiger","Malik Galijasevic","Valentin K. Ladenhauf","Stephanie Mangesius","Elke R. Gizewski"],"pdf_url":"https://arxiv.org/pdf/2507.07496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13554v2","updated":"2025-07-10T07:28:13Z","published":"2025-04-18T08:44:06Z","title":"Task Assignment and Exploration Optimization for Low Altitude UAV Rescue\n  via Generative AI Enhanced Multi-agent Reinforcement Learning","summary":"  The integration of emerging uncrewed aerial vehicles (UAVs) with artificial\nintelligence (AI) and ground-embedded robots (GERs) has transformed emergency\nrescue operations in unknown environments. However, the high computational\ndemands often exceed a single UAV's capacity, making it difficult to\ncontinuously provide stable high-level services. To address this, this paper\nproposes a cooperation framework involving UAVs, GERs, and airships. The\nframework enables resource pooling through UAV-to-GER (U2G) and UAV-to-airship\n(U2A) links, offering computing services for offloaded tasks. Specifically, we\nformulate the multi-objective problem of task assignment and exploration as a\ndynamic long-term optimization problem aiming to minimize task completion time\nand energy use while ensuring stability. Using Lyapunov optimization, we\ntransform it into a per-slot deterministic problem and propose HG-MADDPG, which\ncombines the Hungarian algorithm with a GDM-based multi-agent deep\ndeterministic policy gradient. Simulations demonstrate significant improvements\nin offloading efficiency, latency, and system stability over baselines.\n","authors":["Xin Tang","Qian Chen","Wenjie Weng","Chao Jin","Zhang Liu","Jiacheng Wang","Geng Sun","Xiaohuan Li","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2504.13554v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02670v5","updated":"2025-07-10T07:15:51Z","published":"2025-04-03T15:11:55Z","title":"Affordable AI Assistants with Knowledge Graph of Thoughts","summary":"  Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.\n","authors":["Maciej Besta","Lorenzo Paleari","Jia Hao Andrea Jiang","Robert Gerstenberger","You Wu","Jón Gunnar Hannesson","Patrick Iff","Ales Kubicek","Piotr Nyczyk","Diana Khimey","Nils Blach","Haiqiang Zhang","Tao Zhang","Peiran Ma","Grzegorz Kwaśniewski","Marcin Copik","Hubert Niewiadomski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2504.02670v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07485v1","updated":"2025-07-10T07:13:22Z","published":"2025-07-10T07:13:22Z","title":"Resolving Token-Space Gradient Conflicts: Token Space Manipulation for\n  Transformer-Based Multi-Task Learning","summary":"  Multi-Task Learning (MTL) enables multiple tasks to be learned within a\nshared network, but differences in objectives across tasks can cause negative\ntransfer, where the learning of one task degrades another task's performance.\nWhile pre-trained transformers significantly improve MTL performance, their\nfixed network capacity and rigid structure limit adaptability. Previous dynamic\nnetwork architectures attempt to address this but are inefficient as they\ndirectly convert shared parameters into task-specific ones. We propose Dynamic\nToken Modulation and Expansion (DTME-MTL), a framework applicable to any\ntransformer-based MTL architecture. DTME-MTL enhances adaptability and reduces\noverfitting by identifying gradient conflicts in token space and applying\nadaptive solutions based on conflict type. Unlike prior methods that mitigate\nnegative transfer by duplicating network parameters, DTME-MTL operates entirely\nin token space, enabling efficient adaptation without excessive parameter\ngrowth. Extensive experiments demonstrate that DTME-MTL consistently improves\nmulti-task performance with minimal computational overhead, offering a scalable\nand effective solution for enhancing transformer-based MTL models.\n","authors":["Wooseong Jeong","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2507.07485v1.pdf","comment":"Accepted at ICCV 2025"},{"id":"http://arxiv.org/abs/2507.07484v1","updated":"2025-07-10T07:11:57Z","published":"2025-07-10T07:11:57Z","title":"Machine Bullshit: Characterizing the Emergent Disregard for Truth in\n  Large Language Models","summary":"  Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.\n","authors":["Kaiqu Liang","Haimin Hu","Xuandong Zhao","Dawn Song","Thomas L. Griffiths","Jaime Fernández Fisac"],"pdf_url":"https://arxiv.org/pdf/2507.07484v1.pdf","comment":"Project page, code & data: https://machine-bullshit.github.io"},{"id":"http://arxiv.org/abs/2406.10427v3","updated":"2025-07-10T07:08:38Z","published":"2024-06-14T22:11:02Z","title":"Adaptive Randomized Smoothing: Certified Adversarial Robustness for\n  Multi-Step Defences","summary":"  We propose Adaptive Randomized Smoothing (ARS) to certify the predictions of\nour test-time adaptive models against adversarial examples. ARS extends the\nanalysis of randomized smoothing using $f$-Differential Privacy to certify the\nadaptive composition of multiple steps. For the first time, our theory covers\nthe sound adaptive composition of general and high-dimensional functions of\nnoisy inputs. We instantiate ARS on deep image classification to certify\npredictions against adversarial examples of bounded $L_{\\infty}$ norm. In the\n$L_{\\infty}$ threat model, ARS enables flexible adaptation through\nhigh-dimensional input-dependent masking. We design adaptivity benchmarks,\nbased on CIFAR-10 and CelebA, and show that ARS improves standard test accuracy\nby $1$ to $15\\%$ points. On ImageNet, ARS improves certified test accuracy by\nup to $1.6\\%$ points over standard RS without adaptivity. Our code is available\nat https://github.com/ubc-systopia/adaptive-randomized-smoothing .\n","authors":["Saiyue Lyu","Shadab Shaikh","Frederick Shpilevskiy","Evan Shelhamer","Mathias Lécuyer"],"pdf_url":"https://arxiv.org/pdf/2406.10427v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09265v2","updated":"2025-07-10T07:07:53Z","published":"2025-04-12T15:58:02Z","title":"Mixture of Group Experts for Learning Invariant Representations","summary":"  Sparsely activated Mixture-of-Experts (MoE) models effectively increase the\nnumber of parameters while maintaining consistent computational costs per\ntoken. However, vanilla MoE models often suffer from limited diversity and\nspecialization among experts, constraining their performance and scalability,\nespecially as the number of experts increases. In this paper, we present a\nnovel perspective on vanilla MoE with top-$k$ routing inspired by sparse\nrepresentation. This allows us to bridge established theoretical insights from\nsparse representation into MoE models. Building on this foundation, we propose\na group sparse regularization approach for the input of top-$k$ routing, termed\nMixture of Group Experts (MoGE). MoGE indirectly regularizes experts by\nimposing structural constraints on the routing inputs, while preserving the\noriginal MoE architecture. Furthermore, we organize the routing input into a 2D\ntopographic map, spatially grouping neighboring elements. This structure\nenables MoGE to capture representations invariant to minor transformations,\nthereby significantly enhancing expert diversity and specialization.\nComprehensive evaluations across various Transformer models for image\nclassification and language modeling tasks demonstrate that MoGE substantially\noutperforms its MoE counterpart, with minimal additional memory and computation\noverhead. Our approach provides a simple yet effective solution to scale the\nnumber of experts and reduce redundancy among them. The source code is included\nin the supplementary material and will be publicly released.\n","authors":["Lei Kang","Jia Li","Mi Tian","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2504.09265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06795v2","updated":"2025-07-10T07:05:41Z","published":"2025-07-09T12:30:42Z","title":"ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining","summary":"  The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.\n","authors":["Seonwu Kim","Yohan Na","Kihun Kim","Hanhee Cho","Geun Lim","Mintae Kim","Seongik Park","Ki Hyun Kim","Youngsub Han","Byoung-Ki Jeon"],"pdf_url":"https://arxiv.org/pdf/2507.06795v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2507.07469v1","updated":"2025-07-10T06:53:18Z","published":"2025-07-10T06:53:18Z","title":"Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast\n  Rolling One-Step-Ahead Forecasting","summary":"  Time-series models like ARIMA remain widely used for forecasting but limited\nto linear assumptions and high computational cost in large and complex\ndatasets. We propose Galerkin-ARIMA that generalizes the AR component of ARIMA\nand replace it with a flexible spline-based function estimated by Galerkin\nprojection. This enables the model to capture nonlinear dependencies in lagged\nvalues and retain the MA component and Gaussian noise assumption. We derive a\nclosed-form OLS estimator for the Galerkin coefficients and show the model is\nasymptotically unbiased and consistent under standard conditions. Our method\nbridges classical time-series modeling and nonparametric regression, which\noffering improved forecasting performance and computational efficiency.\n","authors":["Haojie Liu","Zihan Lin"],"pdf_url":"https://arxiv.org/pdf/2507.07469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07461v1","updated":"2025-07-10T06:26:54Z","published":"2025-07-10T06:26:54Z","title":"Hess-MC2: Sequential Monte Carlo Squared using Hessian Information and\n  Second Order Proposals","summary":"  When performing Bayesian inference using Sequential Monte Carlo (SMC)\nmethods, two considerations arise: the accuracy of the posterior approximation\nand computational efficiency. To address computational demands, Sequential\nMonte Carlo Squared (SMC$^2$) is well-suited for high-performance computing\n(HPC) environments. The design of the proposal distribution within SMC$^2$ can\nimprove accuracy and exploration of the posterior as poor proposals may lead to\nhigh variance in importance weights and particle degeneracy. The\nMetropolis-Adjusted Langevin Algorithm (MALA) uses gradient information so that\nparticles preferentially explore regions of higher probability. In this paper,\nwe extend this idea by incorporating second-order information, specifically the\nHessian of the log-target. While second-order proposals have been explored\npreviously in particle Markov Chain Monte Carlo (p-MCMC) methods, we are the\nfirst to introduce them within the SMC$^2$ framework. Second-order proposals\nnot only use the gradient (first-order derivative), but also the curvature\n(second-order derivative) of the target distribution. Experimental results on\nsynthetic models highlight the benefits of our approach in terms of step-size\nselection and posterior approximation accuracy when compared to other\nproposals.\n","authors":["Joshua Murphy","Conor Rosato","Andrew Millard","Lee Devlin","Paul Horridge","Simon Maskell"],"pdf_url":"https://arxiv.org/pdf/2507.07461v1.pdf","comment":"Accepted to IEEE Machine Learning Signal Processing conference 2025"},{"id":"http://arxiv.org/abs/2507.07456v1","updated":"2025-07-10T06:18:46Z","published":"2025-07-10T06:18:46Z","title":"General purpose models for the chemical sciences","summary":"  Data-driven techniques have a large potential to transform and accelerate the\nchemical sciences. However, chemical sciences also pose the unique challenge of\nvery diverse, small, fuzzy datasets that are difficult to leverage in\nconventional machine learning approaches completely. A new class of models,\ngeneral-purpose models (GPMs) such as large language models, have shown the\nability to solve tasks they have not been directly trained on, and to flexibly\noperate with low amounts of data in different formats. In this review, we\ndiscuss fundamental building principles of GPMs and review recent applications\nof those models in the chemical sciences across the entire scientific process.\nWhile many of these applications are still in the prototype phase, we expect\nthat the increasing interest in GPMs will make many of them mature in the\ncoming years.\n","authors":["Nawaf Alampara","Anagha Aneesh","Martiño Ríos-García","Adrian Mirza","Mara Schilling-Wilhelmi","Ali Asghar Aghajani","Meiling Sun","Gordan Prastalo","Kevin Maik Jablonka"],"pdf_url":"https://arxiv.org/pdf/2507.07456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16803v4","updated":"2025-07-10T06:16:59Z","published":"2024-07-23T19:06:44Z","title":"C3T: Cross-modal Transfer Through Time for Sensor-based Human Activity\n  Recognition","summary":"  In order to unlock the potential of diverse sensors, we investigate a method\nto transfer knowledge between time-series modalities using a multimodal\n\\textit{temporal} representation space for Human Activity Recognition (HAR).\nSpecifically, we explore the setting where the modality used in testing has no\nlabeled data during training, which we refer to as Unsupervised Modality\nAdaptation (UMA). We categorize existing UMA approaches as Student-Teacher or\nContrastive Alignment methods. These methods typically compress continuous-time\ndata samples into single latent vectors during alignment, inhibiting their\nability to transfer temporal information through real-world temporal\ndistortions. To address this, we introduce Cross-modal Transfer Through Time\n(C3T), which preserves temporal information during alignment to handle dynamic\nsensor data better. C3T achieves this by aligning a set of temporal latent\nvectors across sensing modalities. Our extensive experiments on various\ncamera+IMU datasets demonstrate that C3T outperforms existing methods in UMA by\nat least 8% in accuracy and shows superior robustness to temporal distortions\nsuch as time-shift, misalignment, and dilation. Our findings suggest that C3T\nhas significant potential for developing generalizable models for time-series\nsensor data, opening new avenues for various multimodal applications.\n","authors":["Abhi Kamboj","Anh Duy Nguyen","Minh N. Do"],"pdf_url":"https://arxiv.org/pdf/2407.16803v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07432v1","updated":"2025-07-10T05:09:19Z","published":"2025-07-10T05:09:19Z","title":"Neural networks leverage nominally quantum and post-quantum\n  representations","summary":"  We show that deep neural networks, including transformers and RNNs,\npretrained as usual on next-token prediction, intrinsically discover and\nrepresent beliefs over 'quantum' and 'post-quantum' low-dimensional generative\nmodels of their training data -- as if performing iterative Bayesian updates\nover the latent state of this world model during inference as they observe more\ncontext. Notably, neural nets easily find these representation whereas there is\nno finite classical circuit that would do the job. The corresponding geometric\nrelationships among neural activations induced by different input sequences are\nfound to be largely independent of neural-network architecture. Each point in\nthis geometry corresponds to a history-induced probability density over all\npossible futures, and the relative displacement of these points reflects the\ndifference in mechanism and magnitude for how these distinct pasts affect the\nfuture.\n","authors":["Paul M. Riechers","Thomas J. Elliott","Adam S. Shai"],"pdf_url":"https://arxiv.org/pdf/2507.07432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18549v2","updated":"2025-07-10T05:08:34Z","published":"2025-02-25T16:05:33Z","title":"ARBoids: Adaptive Residual Reinforcement Learning With Boids Model for\n  Cooperative Multi-USV Target Defense","summary":"  The target defense problem (TDP) for unmanned surface vehicles (USVs)\nconcerns intercepting an adversarial USV before it breaches a designated target\nregion, using one or more defending USVs. A particularly challenging scenario\narises when the attacker exhibits superior maneuverability compared to the\ndefenders, significantly complicating effective interception. To tackle this\nchallenge, this letter introduces ARBoids, a novel adaptive residual\nreinforcement learning framework that integrates deep reinforcement learning\n(DRL) with the biologically inspired, force-based Boids model. Within this\nframework, the Boids model serves as a computationally efficient baseline\npolicy for multi-agent coordination, while DRL learns a residual policy to\nadaptively refine and optimize the defenders' actions. The proposed approach is\nvalidated in a high-fidelity Gazebo simulation environment, demonstrating\nsuperior performance over traditional interception strategies, including pure\nforce-based approaches and vanilla DRL policies. Furthermore, the learned\npolicy exhibits strong adaptability to attackers with diverse maneuverability\nprofiles, highlighting its robustness and generalization capability. The code\nof ARBoids will be released upon acceptance of this letter.\n","authors":["Jiyue Tao","Tongsheng Shen","Dexin Zhao","Feitian Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.18549v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01628v2","updated":"2025-07-10T04:29:17Z","published":"2025-02-03T18:57:17Z","title":"Harmonic Loss Trains Interpretable AI Models","summary":"  In this paper, we introduce harmonic loss as an alternative supervisory\nsignal for training neural networks and large language models (LLMs). Harmonic\nloss differs from standard cross-entropy loss by (a) replacing the usual\nSoftMax normalization with a scale-invariant HarMax function and (b) computing\nlogits via Euclidean distance rather than a dot product. Harmonic loss enables\nimproved interpretability and faster convergence, owing to its scale invariance\nand finite convergence point by design, which can be interpreted as a class\ncenter. We first validate the performance of harmonic models across\nalgorithmic, vision, and language datasets. Through extensive experiments, we\ndemonstrate that models trained with harmonic loss perform better than standard\nmodels by: (a) enhancing interpretability, (b) requiring less data for\ngeneralization, and (c) reducing grokking. Moreover, we compare a GPT-2 model\ntrained with harmonic loss to the standard GPT-2, illustrating that the\nharmonic model develops more interpretable representations. Looking forward, we\nbelieve harmonic loss may become a valuable tool in domains with limited data\navailability or in high-stakes applications where interpretability and\nreliability are paramount, paving the way for more robust and efficient neural\nnetwork models.\n","authors":["David D. Baek","Ziming Liu","Riya Tyagi","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2502.01628v2.pdf","comment":"21 pages, 14 figures; The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2507.07420v1","updated":"2025-07-10T04:26:13Z","published":"2025-07-10T04:26:13Z","title":"Probabilistic Approximate Optimization: A New Variational Monte Carlo\n  Algorithm","summary":"  We introduce a generalized \\textit{Probabilistic Approximate Optimization\nAlgorithm (PAOA)}, a classical variational Monte Carlo framework that extends\nand formalizes prior work by Weitz \\textit{et al.}~\\cite{Combes_2023}, enabling\nparameterized and fast sampling on present-day Ising machines and probabilistic\ncomputers. PAOA operates by iteratively modifying the couplings of a network of\nbinary stochastic units, guided by cost evaluations from independent samples.\nWe establish a direct correspondence between derivative-free updates and the\ngradient of the full $2^N \\times 2^N$ Markov flow, showing that PAOA admits a\nprincipled variational formulation. Simulated annealing emerges as a limiting\ncase under constrained parameterizations, and we implement this regime on an\nFPGA-based probabilistic computer with on-chip annealing to solve large 3D\nspin-glass problems. Benchmarking PAOA against QAOA on the canonical 26-spin\nSherrington-Kirkpatrick model with matched parameters reveals superior\nperformance for PAOA. We show that PAOA naturally extends simulated annealing\nby optimizing multiple temperature profiles, leading to improved performance\nover SA on heavy-tailed problems such as SK-L\\'evy.\n","authors":["Abdelrahman S. Abdelrahman","Shuvro Chowdhury","Flaviano Morone","Kerem Y. Camsari"],"pdf_url":"https://arxiv.org/pdf/2507.07420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07416v1","updated":"2025-07-10T04:17:29Z","published":"2025-07-10T04:17:29Z","title":"Autonomous AI-based Cybersecurity Framework for Critical Infrastructure:\n  Real-Time Threat Mitigation","summary":"  Critical infrastructure systems, including energy grids, healthcare\nfacilities, transportation networks, and water distribution systems, are\npivotal to societal stability and economic resilience. However, the increasing\ninterconnectivity of these systems exposes them to various cyber threats,\nincluding ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent\nThreats (APTs). This paper examines cybersecurity vulnerabilities in critical\ninfrastructure, highlighting the threat landscape, attack vectors, and the role\nof Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid\nAI-driven cybersecurity framework to enhance real-time vulnerability detection,\nthreat modelling, and automated remediation. This study also addresses the\ncomplexities of adversarial AI, regulatory compliance, and integration. Our\nfindings provide actionable insights to strengthen the security and resilience\nof critical infrastructure systems against emerging cyber threats.\n","authors":["Jenifer Paulraj","Brindha Raghuraman","Nagarani Gopalakrishnan","Yazan Otoum"],"pdf_url":"https://arxiv.org/pdf/2507.07416v1.pdf","comment":"7 pages, IEEE conference"},{"id":"http://arxiv.org/abs/2507.07413v1","updated":"2025-07-10T04:10:03Z","published":"2025-07-10T04:10:03Z","title":"Hybrid LLM-Enhanced Intrusion Detection for Zero-Day Threats in IoT\n  Networks","summary":"  This paper presents a novel approach to intrusion detection by integrating\ntraditional signature-based methods with the contextual understanding\ncapabilities of the GPT-2 Large Language Model (LLM). As cyber threats become\nincreasingly sophisticated, particularly in distributed, heterogeneous, and\nresource-constrained environments such as those enabled by the Internet of\nThings (IoT), the need for dynamic and adaptive Intrusion Detection Systems\n(IDSs) becomes increasingly urgent. While traditional methods remain effective\nfor detecting known threats, they often fail to recognize new and evolving\nattack patterns. In contrast, GPT-2 excels at processing unstructured data and\nidentifying complex semantic relationships, making it well-suited to uncovering\nsubtle, zero-day attack vectors. We propose a hybrid IDS framework that merges\nthe robustness of signature-based techniques with the adaptability of\nGPT-2-driven semantic analysis. Experimental evaluations on a representative\nintrusion dataset demonstrate that our model enhances detection accuracy by\n6.3%, reduces false positives by 9.0%, and maintains near real-time\nresponsiveness. These results affirm the potential of language model\nintegration to build intelligent, scalable, and resilient cybersecurity\ndefences suited for modern connected environments.\n","authors":["Mohammad F. Al-Hammouri","Yazan Otoum","Rasha Atwa","Amiya Nayak"],"pdf_url":"https://arxiv.org/pdf/2507.07413v1.pdf","comment":"6 pages, IEEE conference"},{"id":"http://arxiv.org/abs/2503.04424v2","updated":"2025-07-10T04:07:04Z","published":"2025-03-06T13:32:13Z","title":"Determinant Estimation under Memory Constraints and Neural Scaling Laws","summary":"  Calculating or accurately estimating log-determinants of large positive\ndefinite matrices is of fundamental importance in many machine learning tasks.\nWhile its cubic computational complexity can already be prohibitive, in modern\napplications, even storing the matrices themselves can pose a memory\nbottleneck. To address this, we derive a novel hierarchical algorithm based on\nblock-wise computation of the LDL decomposition for large-scale log-determinant\ncalculation in memory-constrained settings. In extreme cases where matrices are\nhighly ill-conditioned, accurately computing the full matrix itself may be\ninfeasible. This is particularly relevant when considering kernel matrices at\nscale, including the empirical Neural Tangent Kernel (NTK) of neural networks\ntrained on large datasets. Under the assumption of neural scaling laws in the\ntest error, we show that the ratio of pseudo-determinants satisfies a power-law\nrelationship, allowing us to derive corresponding scaling laws. This enables\naccurate estimation of NTK log-determinants from a tiny fraction of the full\ndataset; in our experiments, this results in a $\\sim$100,000$\\times$ speedup\nwith improved accuracy over competing approximations. Using these techniques,\nwe successfully estimate log-determinants for dense matrices of extreme sizes,\nwhich were previously deemed intractable and inaccessible due to their enormous\nscale and computational demands.\n","authors":["Siavash Ameli","Chris van der Heide","Liam Hodgkinson","Fred Roosta","Michael W. Mahoney"],"pdf_url":"https://arxiv.org/pdf/2503.04424v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07406v1","updated":"2025-07-10T04:01:52Z","published":"2025-07-10T04:01:52Z","title":"Phishing Detection in the Gen-AI Era: Quantized LLMs vs Classical Models","summary":"  Phishing attacks are becoming increasingly sophisticated, underscoring the\nneed for detection systems that strike a balance between high accuracy and\ncomputational efficiency. This paper presents a comparative evaluation of\ntraditional Machine Learning (ML), Deep Learning (DL), and quantized\nsmall-parameter Large Language Models (LLMs) for phishing detection. Through\nexperiments on a curated dataset, we show that while LLMs currently\nunderperform compared to ML and DL methods in terms of raw accuracy, they\nexhibit strong potential for identifying subtle, context-based phishing cues.\nWe also investigate the impact of zero-shot and few-shot prompting strategies,\nrevealing that LLM-rephrased emails can significantly degrade the performance\nof both ML and LLM-based detectors. Our benchmarking highlights that models\nlike DeepSeek R1 Distill Qwen 14B (Q8_0) achieve competitive accuracy, above\n80%, using only 17GB of VRAM, supporting their viability for cost-efficient\ndeployment. We further assess the models' adversarial robustness and\ncost-performance tradeoffs, and demonstrate how lightweight LLMs can provide\nconcise, interpretable explanations to support real-time decision-making. These\nfindings position optimized LLMs as promising components in phishing defence\nsystems and offer a path forward for integrating explainable, efficient AI into\nmodern cybersecurity frameworks.\n","authors":["Jikesh Thapa","Gurrehmat Chahal","Serban Voinea Gabreanu","Yazan Otoum"],"pdf_url":"https://arxiv.org/pdf/2507.07406v1.pdf","comment":"8 Pages, IEEE Conference"},{"id":"http://arxiv.org/abs/2507.07405v1","updated":"2025-07-10T04:01:47Z","published":"2025-07-10T04:01:47Z","title":"HGMP:Heterogeneous Graph Multi-Task Prompt Learning","summary":"  The pre-training and fine-tuning methods have gained widespread attention in\nthe field of heterogeneous graph neural networks due to their ability to\nleverage large amounts of unlabeled data during the pre-training phase,\nallowing the model to learn rich structural features. However, these methods\nface the issue of a mismatch between the pre-trained model and downstream\ntasks, leading to suboptimal performance in certain application scenarios.\nPrompt learning methods have emerged as a new direction in heterogeneous graph\ntasks, as they allow flexible adaptation of task representations to address\ntarget inconsistency. Building on this idea, this paper proposes a novel\nmulti-task prompt framework for the heterogeneous graph domain, named HGMP.\nFirst, to bridge the gap between the pre-trained model and downstream tasks, we\nreformulate all downstream tasks into a unified graph-level task format. Next,\nwe address the limitations of existing graph prompt learning methods, which\nstruggle to integrate contrastive pre-training strategies in the heterogeneous\ngraph domain. We design a graph-level contrastive pre-training strategy to\nbetter leverage heterogeneous information and enhance performance in multi-task\nscenarios. Finally, we introduce heterogeneous feature prompts, which enhance\nmodel performance by refining the representation of input graph features.\nExperimental results on public datasets show that our proposed method adapts\nwell to various tasks and significantly outperforms baseline methods.\n","authors":["Pengfei Jiao","Jialong Ni","Di Jin","Xuan Guo","Huan Liu","Hongjiang Chen","Yanxian Bi"],"pdf_url":"https://arxiv.org/pdf/2507.07405v1.pdf","comment":"The 25th International Joint Conference on Artificial Intelligence\n  (IJCAI-25)"},{"id":"http://arxiv.org/abs/2507.07399v1","updated":"2025-07-10T03:34:58Z","published":"2025-07-10T03:34:58Z","title":"Generalized Tree Edit Distance (GTED): A Faithful Evaluation Metric for\n  Statement Autoformalization","summary":"  Statement autoformalization, the automated translation of statement from\nnatural language into formal languages, has become a subject of extensive\nresearch, yet the development of robust automated evaluation metrics remains\nlimited. Existing evaluation methods often lack semantic understanding, face\nchallenges with high computational costs, and are constrained by the current\nprogress of automated theorem proving. To address these issues, we propose GTED\n(Generalized Tree Edit Distance), a novel evaluation framework that first\nstandardizes formal statements and converts them into operator trees, then\ndetermines the semantic similarity using the eponymous GTED metric. On the\nminiF2F and ProofNet benchmarks, GTED outperforms all baseline metrics by\nachieving the highest accuracy and Kappa scores, thus providing the community\nwith a more faithful metric for automated evaluation. The code and experimental\nresults are available at https://github.com/XiaoyangLiu-sjtu/GTED.\n","authors":["Yuntian Liu","Tao Zhu","Xiaoyang Liu","Yu Chen","Zhaoxuan Liu","Qingfeng Guo","Jiashuo Zhang","Kangjie Bao","Tao Luo"],"pdf_url":"https://arxiv.org/pdf/2507.07399v1.pdf","comment":"Accepted to AI4Math@ICML25"},{"id":"http://arxiv.org/abs/2507.07396v1","updated":"2025-07-10T03:26:24Z","published":"2025-07-10T03:26:24Z","title":"IML-Spikeformer: Input-aware Multi-Level Spiking Transformer for Speech\n  Processing","summary":"  Spiking Neural Networks (SNNs), inspired by biological neural mechanisms,\nrepresent a promising neuromorphic computing paradigm that offers\nenergy-efficient alternatives to traditional Artificial Neural Networks (ANNs).\nDespite proven effectiveness, SNN architectures have struggled to achieve\ncompetitive performance on large-scale speech processing task. Two key\nchallenges hinder progress: (1) the high computational overhead during training\ncaused by multi-timestep spike firing, and (2) the absence of large-scale SNN\narchitectures tailored to speech processing tasks. To overcome the issues, we\nintroduce Input-aware Multi-Level Spikeformer, i.e. IML-Spikeformer, a spiking\nTransformer architecture specifically designed for large-scale speech\nprocessing. Central to our design is the Input-aware Multi-Level Spike (IMLS)\nmechanism, which simulate multi-timestep spike firing within a single timestep\nusing an adaptive, input-aware thresholding scheme. IML-Spikeformer further\nintegrates a Reparameterized Spiking Self-Attention (RepSSA) module with a\nHierarchical Decay Mask (HDM), forming the HD-RepSSA module. This module\nenhances the precision of attention maps and enables modeling of multi-scale\ntemporal dependencies in speech signals. Experiments demonstrate that\nIML-Spikeformer achieves word error rates of 6.0\\% on AiShell-1 and 3.4\\% on\nLibrispeech-960, comparable to conventional ANN transformers while reducing\ntheoretical inference energy consumption by 4.64$\\times$ and 4.32$\\times$\nrespectively. IML-Spikeformer marks an advance of scalable SNN architectures\nfor large-scale speech processing in both task performance and energy\nefficiency.\n","authors":["Zeyang Song","Shimin Zhang","Yuhong Chou","Jibin Wu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2507.07396v1.pdf","comment":"Under review of TNNLS"},{"id":"http://arxiv.org/abs/2507.07390v1","updated":"2025-07-10T03:06:21Z","published":"2025-07-10T03:06:21Z","title":"Learning Collective Variables from Time-lagged Generation","summary":"  Rare events such as state transitions are difficult to observe directly with\nmolecular dynamics simulations due to long timescales. Enhanced sampling\ntechniques overcome this by introducing biases along carefully chosen\nlow-dimensional features, known as collective variables (CVs), which capture\nthe slow degrees of freedom. Machine learning approaches (MLCVs) have automated\nCV discovery, but existing methods typically focus on discriminating\nmeta-stable states without fully encoding the detailed dynamics essential for\naccurate sampling. We propose TLC, a framework that learns CVs directly from\ntime-lagged conditions of a generative model. Instead of modeling the static\nBoltzmann distribution, TLC models a time-lagged conditional distribution\nyielding CVs to capture the slow dynamic behavior. We validate TLC on the\nAlanine Dipeptide system using two CV-based enhanced sampling tasks: (i)\nsteered molecular dynamics (SMD) and (ii) on-the-fly probability enhanced\nsampling (OPES), demonstrating equal or superior performance compared to\nexisting MLCV methods in both transition path sampling and state\ndiscrimination.\n","authors":["Seonghyun Park","Kiyoung Seong","Soojung Yang","Rafael Gómez-Bombarelli","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2507.07390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07389v1","updated":"2025-07-10T03:06:01Z","published":"2025-07-10T03:06:01Z","title":"ST-GRIT: Spatio-Temporal Graph Transformer For Internal Ice Layer\n  Thickness Prediction","summary":"  Understanding the thickness and variability of internal ice layers in radar\nimagery is crucial for monitoring snow accumulation, assessing ice dynamics,\nand reducing uncertainties in climate models. Radar sensors, capable of\npenetrating ice, provide detailed radargram images of these internal layers. In\nthis work, we present ST-GRIT, a spatio-temporal graph transformer for ice\nlayer thickness, designed to process these radargrams and capture the\nspatiotemporal relationships between shallow and deep ice layers. ST-GRIT\nleverages an inductive geometric graph learning framework to extract local\nspatial features as feature embeddings and employs a series of temporal and\nspatial attention blocks separately to model long-range dependencies\neffectively in both dimensions. Experimental evaluation on radargram data from\nthe Greenland ice sheet demonstrates that ST-GRIT consistently outperforms\ncurrent state-of-the-art methods and other baseline graph neural networks by\nachieving lower root mean-squared error. These results highlight the advantages\nof self-attention mechanisms on graphs over pure graph neural networks,\nincluding the ability to handle noise, avoid oversmoothing, and capture\nlong-range dependencies. Moreover, the use of separate spatial and temporal\nattention blocks allows for distinct and robust learning of spatial\nrelationships and temporal patterns, providing a more comprehensive and\neffective approach.\n","authors":["Zesheng Liu","Maryam Rahnemoonfar"],"pdf_url":"https://arxiv.org/pdf/2507.07389v1.pdf","comment":"Accepted for 2025 IEEE International Conference on Image Processing\n  (ICIP)"},{"id":"http://arxiv.org/abs/2507.07388v1","updated":"2025-07-10T02:59:21Z","published":"2025-07-10T02:59:21Z","title":"GRIT: Graph Transformer For Internal Ice Layer Thickness Prediction","summary":"  Gaining a deeper understanding of the thickness and variability of internal\nice layers in Radar imagery is essential in monitoring the snow accumulation,\nbetter evaluating ice dynamics processes, and minimizing uncertainties in\nclimate models. Radar sensors, capable of penetrating ice, capture detailed\nradargram images of internal ice layers. In this work, we introduce GRIT, graph\ntransformer for ice layer thickness. GRIT integrates an inductive geometric\ngraph learning framework with an attention mechanism, designed to map the\nrelationships between shallow and deeper ice layers. Compared to baseline graph\nneural networks, GRIT demonstrates consistently lower prediction errors. These\nresults highlight the attention mechanism's effectiveness in capturing temporal\nchanges across ice layers, while the graph transformer combines the strengths\nof transformers for learning long-range dependencies with graph neural networks\nfor capturing spatial patterns, enabling robust modeling of complex\nspatiotemporal dynamics.\n","authors":["Zesheng Liu","Maryam Rahnemoonfar"],"pdf_url":"https://arxiv.org/pdf/2507.07388v1.pdf","comment":"Accepted for 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)"},{"id":"http://arxiv.org/abs/2507.06821v2","updated":"2025-07-10T02:58:24Z","published":"2025-07-09T13:08:58Z","title":"HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for\n  Emotion Distribution Learning","summary":"  Multi-modal emotion recognition has garnered increasing attention as it plays\na significant role in human-computer interaction (HCI) in recent years. Since\ndifferent discrete emotions may exist at the same time, compared with\nsingle-class emotion recognition, emotion distribution learning (EDL) that\nidentifies a mixture of basic emotions has gradually emerged as a trend.\nHowever, existing EDL methods face challenges in mining the heterogeneity among\nmultiple modalities. Besides, rich semantic correlations across arbitrary basic\nemotions are not fully exploited. In this paper, we propose a multi-modal\nemotion distribution learning framework, named HeLo, aimed at fully exploring\nthe heterogeneity and complementary information in multi-modal emotional data\nand label correlation within mixed basic emotions. Specifically, we first adopt\ncross-attention to effectively fuse the physiological data. Then, an optimal\ntransport (OT)-based heterogeneity mining module is devised to mine the\ninteraction and heterogeneity between the physiological and behavioral\nrepresentations. To facilitate label correlation learning, we introduce a\nlearnable label embedding optimized by correlation matrix alignment. Finally,\nthe learnable label embeddings and label correlation matrices are integrated\nwith the multi-modal representations through a novel label correlation-driven\ncross-attention mechanism for accurate emotion distribution learning.\nExperimental results on two publicly available datasets demonstrate the\nsuperiority of our proposed method in emotion distribution learning.\n","authors":["Chuhang Zheng","Chunwei Tian","Jie Wen","Daoqiang Zhang","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.06821v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02901v2","updated":"2025-07-10T02:48:24Z","published":"2025-06-23T12:22:39Z","title":"Online Continual Learning via Spiking Neural Networks with Sleep\n  Enhanced Latent Replay","summary":"  Edge computing scenarios necessitate the development of hardware-efficient\nonline continual learning algorithms to be adaptive to dynamic environment.\nHowever, existing algorithms always suffer from high memory overhead and bias\ntowards recently trained tasks. To tackle these issues, this paper proposes a\nnovel online continual learning approach termed as SESLR, which incorporates a\nsleep enhanced latent replay scheme with spiking neural networks (SNNs). SESLR\nleverages SNNs' binary spike characteristics to store replay features in single\nbits, significantly reducing memory overhead. Furthermore, inspired by\nbiological sleep-wake cycles, SESLR introduces a noise-enhanced sleep phase\nwhere the model exclusively trains on replay samples with controlled noise\ninjection, effectively mitigating classification bias towards new classes.\nExtensive experiments on both conventional (MNIST, CIFAR10) and neuromorphic\n(NMNIST, CIFAR10-DVS) datasets demonstrate SESLR's effectiveness. On Split\nCIFAR10, SESLR achieves nearly 30% improvement in average accuracy with only\none-third of the memory consumption compared to baseline methods. On Split\nCIFAR10-DVS, it improves accuracy by approximately 10% while reducing memory\noverhead by a factor of 32. These results validate SESLR as a promising\nsolution for online continual learning in resource-constrained edge computing\nscenarios.\n","authors":["Erliang Lin","Wenbin Luo","Wei Jia","Yu Chen","Shaofu Yang"],"pdf_url":"https://arxiv.org/pdf/2507.02901v2.pdf","comment":"9 pages, 4figures"},{"id":"http://arxiv.org/abs/2403.13268v2","updated":"2025-07-10T02:43:15Z","published":"2024-03-20T03:07:30Z","title":"Unifews: You Need Fewer Operations for Efficient Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) have shown promising performance, but at the\ncost of resource-intensive operations on graph-scale matrices. To reduce\ncomputational overhead, previous studies attempt to sparsify the graph or\nnetwork parameters, but with limited flexibility and precision boundaries. In\nthis work, we propose Unifews, a joint sparsification technique to unify graph\nand weight matrix operations and enhance GNN learning efficiency. The Unifews\ndesign enables adaptive compression across GNN layers with progressively\nincreased sparsity, and is applicable to a variety of architectures with\non-the-fly simplification. Theoretically, we establish a novel framework to\ncharacterize sparsified GNN learning in view of the graph optimization process,\nshowing that Unifews effectively approximates the learning objective with\nbounded error and reduced computational overhead. Extensive experiments\ndemonstrate that Unifews achieves efficiency improvements with comparable or\nbetter accuracy, including 10-20x matrix operation reduction and up to 100x\nacceleration on graphs up to billion-edge scale.\n","authors":["Ningyi Liao","Zihao Yu","Ruixiao Zeng","Siqiang Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13268v2.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2506.23446v2","updated":"2025-07-10T02:22:22Z","published":"2025-06-30T00:47:31Z","title":"User-Based Sequential Modeling with Transformer Encoders for Insider\n  Threat Detection","summary":"  Insider threat detection presents unique challenges due to the authorized\nstatus of malicious actors and the subtlety of anomalous behaviors. Existing\nmachine learning methods often treat user activity as isolated events, thereby\nfailing to leverage sequential dependencies in user behavior. In this study, we\npropose a User-Based Sequencing (UBS) methodology, transforming the CERT\ninsider threat dataset into structured temporal sequences suitable for deep\nsequential modeling. We deploy a Transformer Encoder architecture to model\nbenign user activity and employ its reconstruction errors as anomaly scores.\nThese scores are subsequently evaluated using three unsupervised outlier\ndetection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and\nIsolation Forest (iForest). Across four rigorously designed test sets,\nincluding combinations of multiple CERT dataset releases, our UBS-Transformer\npipeline consistently achieves state-of-the-art performance - notably 96.61%\naccuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low\nfalse negative (0.0057) and false positive (0.0571) rates. Comparative analyses\ndemonstrate that our approach substantially outperforms tabular and\nconventional autoencoder baselines, underscoring the efficacy of sequential\nuser modeling and advanced anomaly detection in the insider threat domain.\n","authors":["Mohamed Elbasheer","Adewale Akinfaderin"],"pdf_url":"https://arxiv.org/pdf/2506.23446v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.24360v3","updated":"2025-07-10T23:59:56Z","published":"2025-05-30T08:53:27Z","title":"Interpreting Large Text-to-Image Diffusion Models with Dictionary\n  Learning","summary":"  Sparse autoencoders are a promising new approach for decomposing language\nmodel activations for interpretation and control. They have been applied\nsuccessfully to vision transformer image encoders and to small-scale diffusion\nmodels. Inference-Time Decomposition of Activations (ITDA) is a recently\nproposed variant of dictionary learning that takes the dictionary to be a set\nof data points from the activation distribution and reconstructs them with\ngradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large\ntext-to-image diffusion model, Flux 1, and consider the interpretability of\nembeddings of both by introducing a visual automated interpretation pipeline.\nWe find that SAEs accurately reconstruct residual stream embeddings and beat\nMLP neurons on interpretability. We are able to use SAE features to steer image\ngeneration through activation addition. We find that ITDA has comparable\ninterpretability to SAEs.\n","authors":["Stepan Shabalin","Ayush Panda","Dmitrii Kharlapenko","Abdur Raheem Ali","Yixiong Hao","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2505.24360v3.pdf","comment":"10 pages, 10 figures, Mechanistic Interpretability for Vision at CVPR\n  2025"},{"id":"http://arxiv.org/abs/2507.07031v2","updated":"2025-07-10T23:50:39Z","published":"2025-07-09T17:03:21Z","title":"ZKTorch: Compiling ML Inference to Zero-Knowledge Proofs via Parallel\n  Proof Accumulation","summary":"  As AI models become ubiquitous in our daily lives, there has been an\nincreasing demand for transparency in ML services. However, the model owner\ndoes not want to reveal the weights, as they are considered trade secrets. To\nsolve this problem, researchers have turned to zero-knowledge proofs of ML\nmodel inference. These proofs convince the user that the ML model output is\ncorrect, without revealing the weights of the model to the user. Past work on\nthese provers can be placed into two categories. The first method compiles the\nML model into a low-level circuit, and proves the circuit using a ZK-SNARK. The\nsecond method uses custom cryptographic protocols designed only for a specific\nclass of models. Unfortunately, the first method is highly inefficient, making\nit impractical for the large models used today, and the second method does not\ngeneralize well, making it difficult to update in the rapidly changing field of\nmachine learning. To solve this, we propose ZKTorch, an open source end-to-end\nproving system that compiles ML models into base cryptographic operations\ncalled basic blocks, each proved using specialized protocols. ZKTorch is built\non top of a novel parallel extension to the Mira accumulation scheme, enabling\nsuccinct proofs with minimal accumulation overhead. These contributions allow\nZKTorch to achieve at least a $3\\times$ reduction in the proof size compared to\nspecialized protocols and up to a $6\\times$ speedup in proving time over a\ngeneral-purpose ZKML framework.\n","authors":["Bing-Jyue Chen","Lilia Tang","Daniel Kang"],"pdf_url":"https://arxiv.org/pdf/2507.07031v2.pdf","comment":"16 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.08218v1","updated":"2025-07-10T23:47:05Z","published":"2025-07-10T23:47:05Z","title":"Simple Mechanistic Explanations for Out-Of-Context Reasoning","summary":"  Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs\nexhibit surprisingly deep out-of-distribution generalization. Rather than\nlearning shallow heuristics, they implicitly internalize and act on the\nconsequences of observations scattered throughout the fine-tuning data. In this\nwork, we investigate this phenomenon mechanistically and find that many\ninstances of OOCR in the literature have a simple explanation: the LoRA\nfine-tuning essentially adds a constant steering vector, steering the model\ntowards a general concept. This improves performance on the fine-tuning task\nand in many other concept-related domains, causing the surprising\ngeneralization. Moreover, we can directly train steering vectors for these\ntasks from scratch, which also induces OOCR. We find that our results hold even\nfor a task that seems like it must involve conditional behavior (model\nbackdoors); it turns out that unconditionally adding a steering vector is\nsufficient. Overall, our work presents one explanation of what gets learned\nduring fine-tuning for OOCR tasks, contributing to the key question of why LLMs\ncan reason out of context, an advanced capability that is highly relevant to\ntheir safe and reliable deployment.\n","authors":["Atticus Wang","Joshua Engels","Oliver Clive-Griffin"],"pdf_url":"https://arxiv.org/pdf/2507.08218v1.pdf","comment":"ICML 2025 Workshop R2-FM"},{"id":"http://arxiv.org/abs/2505.12546v2","updated":"2025-07-10T23:16:43Z","published":"2025-05-18T21:06:32Z","title":"Extracting memorized pieces of (copyrighted) books from open-weight\n  language models","summary":"  Plaintiffs and defendants in copyright lawsuits over generative AI often make\nsweeping, opposing claims about the extent to which large language models\n(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial\nML and copyright law, we show that these polarized positions dramatically\noversimplify the relationship between memorization and copyright. To do so, we\nleverage a recent probabilistic extraction technique to extract pieces of the\nBooks3 dataset from 17 open-weight LLMs. Through numerous experiments, we show\nthat it's possible to extract substantial parts of at least some books from\ndifferent LLMs. This is evidence that these LLMs have memorized the extracted\ntext; this memorized content is copied inside the model parameters. But the\nresults are complicated: the extent of memorization varies both by model and by\nbook. With our specific experiments, we find that the largest LLMs don't\nmemorize most books--either in whole or in part. However, we also find that\nLlama 3.1 70B memorizes some books, like Harry Potter and the Sorcerer's Stone\nand 1984, almost entirely. In fact, Harry Potter is so memorized that, using a\nseed prompt consisting of just the first line of chapter 1, we can\ndeterministically generate the entire book near-verbatim. We discuss why our\nresults have significant implications for copyright cases, though not ones that\nunambiguously favor either side.\n","authors":["A. Feder Cooper","Aaron Gokaslan","Ahmed Ahmed","Amy B. Cyphert","Christopher De Sa","Mark A. Lemley","Daniel E. Ho","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2505.12546v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.07938v1","updated":"2025-07-10T17:21:35Z","published":"2025-07-10T17:21:35Z","title":"Multimodal Framework for Explainable Autonomous Driving: Integrating\n  Video, Sensor, and Textual Data for Enhanced Decision-Making and Transparency","summary":"  Autonomous vehicles (AVs) are poised to redefine transportation by enhancing\nroad safety, minimizing human error, and optimizing traffic efficiency. The\nsuccess of AVs depends on their ability to interpret complex, dynamic\nenvironments through diverse data sources, including video streams, sensor\nmeasurements, and contextual textual information. However, seamlessly\nintegrating these multimodal inputs and ensuring transparency in AI-driven\ndecisions remain formidable challenges. This study introduces a novel\nmultimodal framework that synergistically combines video, sensor, and textual\ndata to predict driving actions while generating human-readable explanations,\nfostering trust and regulatory compliance. By leveraging VideoMAE for\nspatiotemporal video analysis, a custom sensor fusion module for real-time data\nprocessing, and BERT for textual comprehension, our approach achieves robust\ndecision-making and interpretable outputs. Evaluated on the BDD-X (21113\nsamples) and nuScenes (1000 scenes) datasets, our model reduces training loss\nfrom 5.7231 to 0.0187 over five epochs, attaining an action prediction accuracy\nof 92.5% and a BLEU-4 score of 0.75 for explanation quality, outperforming\nstate-of-the-art methods. Ablation studies confirm the critical role of each\nmodality, while qualitative analyses and human evaluations highlight the\nmodel's ability to produce contextually rich, user-friendly explanations. These\nadvancements underscore the transformative potential of multimodal integration\nand explainability in building safe, transparent, and trustworthy AV systems,\npaving the way for broader societal adoption of autonomous driving\ntechnologies.\n","authors":["Abolfazl Zarghani","Amirhossein Ebrahimi","Amir Malekesfandiari"],"pdf_url":"https://arxiv.org/pdf/2507.07938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07911v1","updated":"2025-07-10T16:45:10Z","published":"2025-07-10T16:45:10Z","title":"The Potential of Olfactory Stimuli in Stress Reduction through Virtual\n  Reality","summary":"  Immersive virtual reality (VR) is a promising tool for stress reduction and\nrelaxation, traditionally relying on visual and auditory stimuli. This study\nexamines the role of olfactory stimuli in enhancing these effects, using a\nrandomized within-subject design. Thirty participants aged 18-60 experienced VR\nscenarios simulating a calming seaside environment, with sessions lasting 45\nminutes, in two conditions: with and without a \"Beach\" essential oil scent\n(Yankee Candle) administered via diffuser. Stress and relaxation were assessed\nthrough self-reported surveys and physiological measures, specifically\nECG-based heart rate variability (HRV). Results showed no significant\ndifference in self-reported relaxation scores (p=0.371) between conditions, but\nHRV analysis revealed a significant stress reduction (p=0.002) with olfactory\ninput, with HF increasing 108% from the Math Stress Test to the scented\nrelaxation condition, compared to 44% without scent. Additionally, 71.4% of\nparticipants expressed willingness to use olfactory-enhanced VR for relaxation,\nsuggesting practical appeal. These findings indicate that olfactory stimuli may\nenhance relaxation subconsciously, underscoring the importance of multisensory\nintegration in VR. Future work could explore personalized scents and long-term\neffects to optimize VR- based interventions for emotional and physical\nwell-being.\n","authors":["Yasmin Elsaddik Valdivieso","Mohd Faisal","Karim Alghoul"," Monireh"," Vahdati","Kamran Gholizadeh Hamlabadi","Fedwa Laamarti","Hussein Al Osman","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2507.07911v1.pdf","comment":"Accepted to IEEE Medical Measurements & Applications (MeMeA) 2025"},{"id":"http://arxiv.org/abs/2507.07633v1","updated":"2025-07-10T11:01:58Z","published":"2025-07-10T11:01:58Z","title":"T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates","summary":"  Recent advances in video generation techniques have given rise to an emerging\nparadigm of generative video coding, aiming to achieve semantically accurate\nreconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong\ngenerative priors. However, most existing methods are limited by domain\nspecificity (e.g., facial or human videos) or an excessive dependence on\nhigh-level text guidance, which often fails to capture motion details and\nresults in unrealistic reconstructions. To address these challenges, we propose\na Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC\nemploys a semantic-aware sparse motion sampling pipeline to effectively bridge\nlow-level motion tracking with high-level semantic understanding by extracting\npixel-wise motion as sparse trajectory points based on their semantic\nimportance, not only significantly reducing the bitrate but also preserving\ncritical temporal semantic information. In addition, by incorporating\ntrajectory-aligned loss constraints into diffusion processes, we introduce a\ntraining-free latent space guidance mechanism to ensure physically plausible\nmotion patterns without sacrificing the inherent capabilities of generative\nmodels. Experimental results demonstrate that our framework outperforms both\ntraditional codecs and state-of-the-art end-to-end video compression methods\nunder ULB conditions. Furthermore, additional experiments confirm that our\napproach achieves more precise motion control than existing text-guided\nmethods, paving the way for a novel direction of generative video coding guided\nby geometric motion modeling.\n","authors":["Zhitao Wang","Hengyu Man","Wenrui Li","Xingtao Wang","Xiaopeng Fan","Debin Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.07633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19108v2","updated":"2025-07-10T03:26:36Z","published":"2025-02-26T12:50:58Z","title":"U-Sticker: A Large-Scale Multi-Domain User Sticker Dataset for Retrieval\n  and Personalization","summary":"  Instant messaging with texts and stickers has become a widely adopted\ncommunication medium, enabling efficient expression of user semantics and\nemotions. With the increased use of stickers conveying information and\nfeelings, sticker retrieval and recommendation has emerged as an important area\nof research. However, a major limitation in existing literature has been the\nlack of datasets capturing temporal and user-specific sticker interactions,\nwhich has hindered further progress in user modeling and sticker\npersonalization. To address this, we introduce User-Sticker, a dataset that\nincludes temporal and user anonymous ID across conversations. It is the largest\npublicly available sticker dataset to date, containing 22K unique users, 370K\nstickers, and 8.3M messages. The raw data was collected from a popular\nmessaging platform from 67 conversations over 720 hours of crawling. All text\nand image data were carefully vetted for safety and privacy checks and\nmodifications. Spanning 10 domains, the U-Sticker dataset captures rich\ntemporal, multilingual, and cross-domain behaviors not previously available in\nother datasets. Extensive quantitative and qualitative experiments demonstrate\nU-Sticker's practical applications in user behavior modeling and personalized\nrecommendation and highlight its potential to further research areas in\npersonalized retrieval and conversational studies. U-Sticker dataset is\npublicly available.\n","authors":["Heng Er Metilda Chee","Jiayin Wang","Zhiqiang Guo","Weizhi Ma","Qinglang Guo","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.19108v2.pdf","comment":"Accepted at SIGIR'25"},{"id":"http://arxiv.org/abs/2507.07396v1","updated":"2025-07-10T03:26:24Z","published":"2025-07-10T03:26:24Z","title":"IML-Spikeformer: Input-aware Multi-Level Spiking Transformer for Speech\n  Processing","summary":"  Spiking Neural Networks (SNNs), inspired by biological neural mechanisms,\nrepresent a promising neuromorphic computing paradigm that offers\nenergy-efficient alternatives to traditional Artificial Neural Networks (ANNs).\nDespite proven effectiveness, SNN architectures have struggled to achieve\ncompetitive performance on large-scale speech processing task. Two key\nchallenges hinder progress: (1) the high computational overhead during training\ncaused by multi-timestep spike firing, and (2) the absence of large-scale SNN\narchitectures tailored to speech processing tasks. To overcome the issues, we\nintroduce Input-aware Multi-Level Spikeformer, i.e. IML-Spikeformer, a spiking\nTransformer architecture specifically designed for large-scale speech\nprocessing. Central to our design is the Input-aware Multi-Level Spike (IMLS)\nmechanism, which simulate multi-timestep spike firing within a single timestep\nusing an adaptive, input-aware thresholding scheme. IML-Spikeformer further\nintegrates a Reparameterized Spiking Self-Attention (RepSSA) module with a\nHierarchical Decay Mask (HDM), forming the HD-RepSSA module. This module\nenhances the precision of attention maps and enables modeling of multi-scale\ntemporal dependencies in speech signals. Experiments demonstrate that\nIML-Spikeformer achieves word error rates of 6.0\\% on AiShell-1 and 3.4\\% on\nLibrispeech-960, comparable to conventional ANN transformers while reducing\ntheoretical inference energy consumption by 4.64$\\times$ and 4.32$\\times$\nrespectively. IML-Spikeformer marks an advance of scalable SNN architectures\nfor large-scale speech processing in both task performance and energy\nefficiency.\n","authors":["Zeyang Song","Shimin Zhang","Yuhong Chou","Jibin Wu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2507.07396v1.pdf","comment":"Under review of TNNLS"},{"id":"http://arxiv.org/abs/2507.06821v2","updated":"2025-07-10T02:58:24Z","published":"2025-07-09T13:08:58Z","title":"HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for\n  Emotion Distribution Learning","summary":"  Multi-modal emotion recognition has garnered increasing attention as it plays\na significant role in human-computer interaction (HCI) in recent years. Since\ndifferent discrete emotions may exist at the same time, compared with\nsingle-class emotion recognition, emotion distribution learning (EDL) that\nidentifies a mixture of basic emotions has gradually emerged as a trend.\nHowever, existing EDL methods face challenges in mining the heterogeneity among\nmultiple modalities. Besides, rich semantic correlations across arbitrary basic\nemotions are not fully exploited. In this paper, we propose a multi-modal\nemotion distribution learning framework, named HeLo, aimed at fully exploring\nthe heterogeneity and complementary information in multi-modal emotional data\nand label correlation within mixed basic emotions. Specifically, we first adopt\ncross-attention to effectively fuse the physiological data. Then, an optimal\ntransport (OT)-based heterogeneity mining module is devised to mine the\ninteraction and heterogeneity between the physiological and behavioral\nrepresentations. To facilitate label correlation learning, we introduce a\nlearnable label embedding optimized by correlation matrix alignment. Finally,\nthe learnable label embeddings and label correlation matrices are integrated\nwith the multi-modal representations through a novel label correlation-driven\ncross-attention mechanism for accurate emotion distribution learning.\nExperimental results on two publicly available datasets demonstrate the\nsuperiority of our proposed method in emotion distribution learning.\n","authors":["Chuhang Zheng","Chunwei Tian","Jie Wen","Daoqiang Zhang","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.06821v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22589v2","updated":"2025-07-10T17:59:47Z","published":"2025-03-28T16:36:23Z","title":"Using AI to Summarize US Presidential Campaign TV Advertisement Videos,\n  1952-2012","summary":"  This paper introduces the largest and most comprehensive dataset of US\npresidential campaign television advertisements, available in digital format.\nThe dataset also includes machine-searchable transcripts and high-quality\nsummaries designed to facilitate a variety of academic research. To date, there\nhas been great interest in collecting and analyzing US presidential campaign\nadvertisements, but the need for manual procurement and annotation led many to\nrely on smaller subsets. We design a large-scale parallelized, AI-based\nanalysis pipeline that automates the laborious process of preparing,\ntranscribing, and summarizing videos. We then apply this methodology to the\n9,707 presidential ads from the Julian P. Kanter Political Commercial Archive.\nWe conduct extensive human evaluations to show that these transcripts and\nsummaries match the quality of manually generated alternatives. We illustrate\nthe value of this data by including an application that tracks the genesis and\nevolution of current focal issue areas over seven decades of presidential\nelections. Our analysis pipeline and codebase also show how to use LLM-based\ntools to obtain high-quality summaries for other video datasets.\n","authors":["Adam Breuer","Bryce J. Dietrich","Michael H. Crespin","Matthew Butler","J. A. Pryse","Kosuke Imai"],"pdf_url":"https://arxiv.org/pdf/2503.22589v2.pdf","comment":"17 pages, 7 tables, 4 figures, and linked datasets"},{"id":"http://arxiv.org/abs/2507.08064v1","updated":"2025-07-10T16:47:25Z","published":"2025-07-10T16:47:25Z","title":"PUMA: Layer-Pruned Language Model for Efficient Unified Multimodal\n  Retrieval with Modality-Adaptive Learning","summary":"  As multimedia content expands, the demand for unified multimodal retrieval\n(UMR) in real-world applications increases. Recent work leverages multimodal\nlarge language models (MLLMs) to tackle this task. However, their large\nparameter size results in high training costs and low inference efficiency. To\naddress this, we propose PUMA: a Layer-Pruned Language Model for Efficient\nUnified Multimodal Retrieval with Modality-Adaptive Learning. Our approach\nimproves UMR from both structural and learning perspectives. (1) Structurally,\nwe propose Layer-Pruned Self-Distillation, which prunes MLLMs by keeping only\nshallow layers while distilling features from dropped deep layers as teacher\nsignals. This reduces parameters and preserves representation capability. (2)\nOn the learning side, we introduce Modality-Adaptive Contrastive Learning Loss\n(MAC-Loss), which separates in-batch negatives into harder intra-modality and\neasier inter-modality groups based on the target modality, assigning different\ntemperature strategies to enhance learning efficiency. Experiments show our\nmethod significantly reduces resource usage while maintaining strong\nperformance.\n","authors":["Yibo Lyu","Rui Shao","Gongwei Chen","Yijie Zhu","Weili Guan","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2507.08064v1.pdf","comment":"Accepted to ACM MM 2025"}]},"2025-07-09T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2507.04946v2","updated":"2025-07-09T22:41:45Z","published":"2025-07-07T12:43:09Z","title":"Taming the Tri-Space Tension: ARC-Guided Hallucination Modeling and\n  Control for Text-to-Image Generation","summary":"  Despite remarkable progress in image quality and prompt fidelity,\ntext-to-image (T2I) diffusion models continue to exhibit persistent\n\"hallucinations\", where generated content subtly or significantly diverges from\nthe intended prompt semantics. While often regarded as unpredictable artifacts,\nwe argue that these failures reflect deeper, structured misalignments within\nthe generative process. In this work, we propose a cognitively inspired\nperspective that reinterprets hallucinations as trajectory drift within a\nlatent alignment space. Empirical observations reveal that generation unfolds\nwithin a multiaxial cognitive tension field, where the model must continuously\nnegotiate competing demands across three key critical axes: semantic coherence,\nstructural alignment, and knowledge grounding. We then formalize this\nthree-axis space as the \\textbf{Hallucination Tri-Space} and introduce the\nAlignment Risk Code (ARC): a dynamic vector representation that quantifies\nreal-time alignment tension during generation. The magnitude of ARC captures\noverall misalignment, its direction identifies the dominant failure axis, and\nits imbalance reflects tension asymmetry. Based on this formulation, we develop\nthe TensionModulator (TM-ARC): a lightweight controller that operates entirely\nin latent space. TM-ARC monitors ARC signals and applies targeted,\naxis-specific interventions during the sampling process. Extensive experiments\non standard T2I benchmarks demonstrate that our approach significantly reduces\nhallucination without compromising image quality or diversity. This framework\noffers a unified and interpretable approach for understanding and mitigating\ngenerative failures in diffusion-based T2I systems.\n","authors":["Jianjiang Yang","Ziyan Huang"],"pdf_url":"https://arxiv.org/pdf/2507.04946v2.pdf","comment":"We withdraw this paper due to significant visualization errors in\n  Figure 3 and 5 that affect the correctness of our core modeling claims and\n  may cause misinterpretation. These figures misrepresent ARC dynamics and\n  trajectory control"},{"id":"http://arxiv.org/abs/2507.07307v1","updated":"2025-07-09T22:10:06Z","published":"2025-07-09T22:10:06Z","title":"Multi-Agent Retrieval-Augmented Framework for Evidence-Based\n  Counterspeech Against Health Misinformation","summary":"  Large language models (LLMs) incorporated with Retrieval-Augmented Generation\n(RAG) have demonstrated powerful capabilities in generating counterspeech\nagainst misinformation. However, current studies rely on limited evidence and\noffer less control over final outputs. To address these challenges, we propose\na Multi-agent Retrieval-Augmented Framework to generate counterspeech against\nhealth misinformation, incorporating multiple LLMs to optimize knowledge\nretrieval, evidence enhancement, and response refinement. Our approach\nintegrates both static and dynamic evidence, ensuring that the generated\ncounterspeech is relevant, well-grounded, and up-to-date. Our method\noutperforms baseline approaches in politeness, relevance, informativeness, and\nfactual accuracy, demonstrating its effectiveness in generating high-quality\ncounterspeech. To further validate our approach, we conduct ablation studies to\nverify the necessity of each component in our framework. Furthermore, human\nevaluations reveal that refinement significantly enhances counterspeech quality\nand obtains human preference.\n","authors":["Anirban Saha Anik","Xiaoying Song","Elliott Wang","Bryan Wang","Bengisu Yarimbas","Lingzi Hong"],"pdf_url":"https://arxiv.org/pdf/2507.07307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19092v2","updated":"2025-07-09T22:09:49Z","published":"2025-03-24T19:24:40Z","title":"Rankers, Judges, and Assistants: Towards Understanding the Interplay of\n  LLMs in Information Retrieval Evaluation","summary":"  Large language models (LLMs) are increasingly integral to information\nretrieval (IR), powering ranking, evaluation, and AI-assisted content creation.\nThis widespread adoption necessitates a critical examination of potential\nbiases arising from the interplay between these LLM-based components. This\npaper synthesizes existing research and presents novel experiment designs that\nexplore how LLM-based rankers and assistants influence LLM-based judges. We\nprovide the first empirical evidence of LLM judges exhibiting significant bias\ntowards LLM-based rankers. Furthermore, we observe limitations in LLM judges'\nability to discern subtle system performance differences. Contrary to some\nprevious findings, our preliminary study does not find evidence of bias against\nAI-generated content. These results highlight the need for a more holistic view\nof the LLM-driven information ecosystem. To this end, we offer initial\nguidelines and a research agenda to ensure the reliable use of LLMs in IR\nevaluation.\n","authors":["Krisztian Balog","Donald Metzler","Zhen Qin"],"pdf_url":"https://arxiv.org/pdf/2503.19092v2.pdf","comment":"Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '25)"},{"id":"http://arxiv.org/abs/2507.07306v1","updated":"2025-07-09T22:05:46Z","published":"2025-07-09T22:05:46Z","title":"ViDove: A Translation Agent System with Multimodal Context and\n  Memory-Augmented Reasoning","summary":"  LLM-based translation agents have achieved highly human-like translation\nresults and are capable of handling longer and more complex contexts with\ngreater efficiency. However, they are typically limited to text-only inputs. In\nthis paper, we introduce ViDove, a translation agent system designed for\nmultimodal input. Inspired by the workflow of human translators, ViDove\nleverages visual and contextual background information to enhance the\ntranslation process. Additionally, we integrate a multimodal memory system and\nlong-short term memory modules enriched with domain-specific knowledge,\nenabling the agent to perform more accurately and adaptively in real-world\nscenarios. As a result, ViDove achieves significantly higher translation\nquality in both subtitle generation and general translation tasks, with a 28%\nimprovement in BLEU scores and a 15% improvement in SubER compared to previous\nstate-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark\nfor long-form automatic video subtitling and translation, featuring 17 hours of\nhigh-quality, human-annotated data. Our code is available here:\nhttps://github.com/pigeonai-org/ViDove\n","authors":["Yichen Lu","Wei Dai","Jiaen Liu","Ching Wing Kwok","Zongheng Wu","Xudong Xiao","Ao Sun","Sheng Fu","Jianyuan Zhan","Yian Wang","Takatomo Saito","Sicheng Lai"],"pdf_url":"https://arxiv.org/pdf/2507.07306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07280v1","updated":"2025-07-09T20:57:55Z","published":"2025-07-09T20:57:55Z","title":"The Impact of Background Speech on Interruption Detection in\n  Collaborative Groups","summary":"  Interruption plays a crucial role in collaborative learning, shaping group\ninteractions and influencing knowledge construction. AI-driven support can\nassist teachers in monitoring these interactions. However, most previous work\non interruption detection and interpretation has been conducted in\nsingle-conversation environments with relatively clean audio. AI agents\ndeployed in classrooms for collaborative learning within small groups will need\nto contend with multiple concurrent conversations -- in this context,\noverlapping speech will be ubiquitous, and interruptions will need to be\nidentified in other ways. In this work, we analyze interruption detection in\nsingle-conversation and multi-group dialogue settings. We then create a\nstate-of-the-art method for interruption identification that is robust to\noverlapping speech, and thus could be deployed in classrooms. Further, our work\nhighlights meaningful linguistic and prosodic information about how\ninterruptions manifest in collaborative group interactions. Our investigation\nalso paves the way for future works to account for the influence of overlapping\nspeech from multiple groups when tracking group dialog.\n","authors":["Mariah Bradford","Nikhil Krishnaswamy","Nathaniel Blanchard"],"pdf_url":"https://arxiv.org/pdf/2507.07280v1.pdf","comment":"Long Paper AIED 2025"},{"id":"http://arxiv.org/abs/2507.07274v1","updated":"2025-07-09T20:45:04Z","published":"2025-07-09T20:45:04Z","title":"LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based\n  Evaluation","summary":"  Large Multimodal Models (LMMs) are typically trained on vast corpora of\nimage-text data but are often limited in linguistic coverage, leading to biased\nand unfair outputs across languages. While prior work has explored multimodal\nevaluation, less emphasis has been placed on assessing multilingual\ncapabilities. In this work, we introduce LinguaMark, a benchmark designed to\nevaluate state-of-the-art LMMs on a multilingual Visual Question Answering\n(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages\nand five social attributes. We evaluate models using three key metrics: Bias,\nAnswer Relevancy, and Faithfulness. Our findings reveal that closed-source\nmodels generally achieve the highest overall performance. Both closed-source\n(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform\ncompetitively across social attributes, and Qwen2.5 demonstrates strong\ngeneralization across multiple languages. We release our benchmark and\nevaluation code to encourage reproducibility and further research.\n","authors":["Ananya Raval","Aravind Narayanan","Vahid Reza Khazaie","Shaina Raza"],"pdf_url":"https://arxiv.org/pdf/2507.07274v1.pdf","comment":"Accepted at ASONAM'25"},{"id":"http://arxiv.org/abs/2507.07257v1","updated":"2025-07-09T20:03:30Z","published":"2025-07-09T20:03:30Z","title":"Open Source Planning & Control System with Language Agents for\n  Autonomous Scientific Discovery","summary":"  We present a multi-agent system for automation of scientific research tasks,\ncmbagent. The system is formed by about 30 Large Language Model (LLM) agents\nand implements a Planning & Control strategy to orchestrate the agentic\nworkflow, with no human-in-the-loop at any point. Each agent specializes in a\ndifferent task (performing retrieval on scientific papers and codebases,\nwriting code, interpreting results, critiquing the output of other agents) and\nthe system is able to execute code locally. We successfully apply cmbagent to\ncarry out a PhD level cosmology task (the measurement of cosmological\nparameters using supernova data) and evaluate its performance on two benchmark\nsets, finding superior performance over state-of-the-art LLMs. The source code\nis available on GitHub, demonstration videos are also available, and the system\nis deployed on HuggingFace and will be available on the cloud.\n","authors":["Licong Xu","Milind Sarkar","Anto I. Lonappan","Íñigo Zubeldia","Pablo Villanueva-Domingo","Santiago Casas","Christian Fidler","Chetana Amancharla","Ujjwal Tiwari","Adrian Bayer","Chadi Ait Ekiou","Miles Cranmer","Adrian Dimitrov","James Fergusson","Kahaan Gandhi","Sven Krippendorf","Andrew Laverick","Julien Lesgourgues","Antony Lewis","Thomas Meier","Blake Sherwin","Kristen Surrao","Francisco Villaescusa-Navarro","Chi Wang","Xueqing Xu","Boris Bolliet"],"pdf_url":"https://arxiv.org/pdf/2507.07257v1.pdf","comment":"Accepted contribution to the ICML 2025 Workshop on Machine Learning\n  for Astrophysics. Code: https://github.com/CMBAgents/cmbagent; Videos:\n  https://www.youtube.com/@cmbagent; HuggingFace:\n  https://huggingface.co/spaces/astropilot-ai/cmbagent; Cloud:\n  https://cmbagent.cloud"},{"id":"http://arxiv.org/abs/2404.00699v5","updated":"2025-07-09T19:56:02Z","published":"2024-03-31T14:32:02Z","title":"A Comprehensive Survey of Contamination Detection Methods in Large\n  Language Models","summary":"  With the rise of Large Language Models (LLMs) in recent years, abundant new\nopportunities are emerging, but also new challenges, among which contamination\nis quickly becoming critical. Business applications and fundraising in\nArtificial Intelligence (AI) have reached a scale at which a few percentage\npoints gained on popular question-answering benchmarks could translate into\ndozens of millions of dollars, placing high pressure on model integrity. At the\nsame time, it is becoming harder and harder to keep track of the data that LLMs\nhave seen; if not impossible with closed-source models like GPT-4 and Claude-3\nnot divulging any information on the training set. As a result, contamination\nbecomes a major issue: LLMs' performance may not be reliable anymore, as the\nhigh performance may be at least partly due to their previous exposure to the\ndata. This limitation jeopardizes real capability improvement in the field of\nNLP, yet, there remains a lack of methods on how to efficiently detect\ncontamination. In this paper, we survey all recent work on contamination\ndetection with LLMs, analyzing their methodologies and use cases to shed light\non the appropriate usage of contamination detection methods. Our work calls the\nNLP research community's attention into systematically taking into account\ncontamination bias in LLM evaluation.\n","authors":["Mathieu Ravaut","Bosheng Ding","Fangkai Jiao","Hailin Chen","Xingxuan Li","Ruochen Zhao","Chengwei Qin","Caiming Xiong","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2404.00699v5.pdf","comment":"Accepted by TMLR in July 2025. 18 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2507.05517v2","updated":"2025-07-09T19:53:32Z","published":"2025-07-07T22:29:29Z","title":"Empowering Healthcare Practitioners with Language Models: Structuring\n  Speech Transcripts in Two Real-World Clinical Applications","summary":"  Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong\nperformance on clinical natural language processing (NLP) tasks across multiple\nmedical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular\nreporting from nurse dictations and medical order extraction from\ndoctor-patient consultations - remain underexplored due to data scarcity and\nsensitivity, despite active industry efforts. Practical solutions to these\nreal-world clinical tasks can significantly reduce the documentation burden on\nhealthcare providers, allowing greater focus on patient care. In this paper, we\ninvestigate these two challenging tasks using private and open-source clinical\ndatasets, evaluating the performance of both open- and closed-weight LLMs, and\nanalyzing their respective strengths and limitations. Furthermore, we propose\nan agentic pipeline for generating realistic, non-sensitive nurse dictations,\nenabling structured extraction of clinical observations. To support further\nresearch in both areas, we release SYNUR and SIMORD, the first open-source\ndatasets for nurse observation extraction and medical order extraction.\n","authors":["Jean-Philippe Corbeil","Asma Ben Abacha","George Michalopoulos","Phillip Swazinna","Miguel Del-Agua","Jerome Tremblay","Akila Jeeson Daniel","Cari Bader","Yu-Cheng Cho","Pooja Krishnan","Nathan Bodenstab","Thomas Lin","Wenxuan Teng","Francois Beaulieu","Paul Vozila"],"pdf_url":"https://arxiv.org/pdf/2507.05517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07251v1","updated":"2025-07-09T19:48:33Z","published":"2025-07-09T19:48:33Z","title":"A Language-Driven Framework for Improving Personalized Recommendations:\n  Merging LLMs with Traditional Algorithms","summary":"  Traditional recommendation algorithms are not designed to provide\npersonalized recommendations based on user preferences provided through text,\ne.g., \"I enjoy light-hearted comedies with a lot of humor\". Large Language\nModels (LLMs) have emerged as one of the most promising tools for natural\nlanguage processing in recent years. This research proposes a novel framework\nthat mimics how a close friend would recommend items based on their knowledge\nof an individual's tastes. We leverage LLMs to enhance movie recommendation\nsystems by refining traditional algorithm outputs and integrating them with\nlanguage-based user preference inputs. We employ Singular Value Decomposition\n(SVD) or SVD++ algorithms to generate initial movie recommendations,\nimplemented using the Surprise Python library and trained on the\nMovieLens-Latest-Small dataset. We compare the performance of the base\nalgorithms with our LLM-enhanced versions using leave-one-out validation hit\nrates and cumulative hit rates. Additionally, to compare the performance of our\nframework against the current state-of-the-art recommendation systems, we use\nrating and ranking metrics with an item-based stratified 0.75 train, 0.25 test\nsplit. Our framework can generate preference profiles automatically based on\nusers' favorite movies or allow manual preference specification for more\npersonalized results. Using an automated approach, our framework overwhelmingly\nsurpassed SVD and SVD++ on every evaluation metric used (e.g., improvements of\nup to ~6x in cumulative hit rate, ~3.7x in NDCG, etc.), albeit at the cost of a\nslight increase in computational overhead.\n","authors":["Aaron Goldstein","Ayan Dutta"],"pdf_url":"https://arxiv.org/pdf/2507.07251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07248v1","updated":"2025-07-09T19:38:58Z","published":"2025-07-09T19:38:58Z","title":"Medical Red Teaming Protocol of Language Models: On the Importance of\n  User Perspectives in Healthcare Settings","summary":"  As the performance of large language models (LLMs) continues to advance,\ntheir adoption is expanding across a wide range of domains, including the\nmedical field. The integration of LLMs into medical applications raises\ncritical safety concerns, particularly due to their use by users with diverse\nroles, e.g. patients and clinicians, and the potential for model's outputs to\ndirectly affect human health. Despite the domain-specific capabilities of\nmedical LLMs, prior safety evaluations have largely focused only on general\nsafety benchmarks. In this paper, we introduce a safety evaluation protocol\ntailored to the medical domain in both patient user and clinician user\nperspectives, alongside general safety assessments and quantitatively analyze\nthe safety of medical LLMs. We bridge a gap in the literature by building the\nPatientSafetyBench containing 466 samples over 5 critical categories to measure\nsafety from the perspective of the patient. We apply our red-teaming protocols\non the MediPhi model collection as a case study. To our knowledge, this is the\nfirst work to define safety evaluation criteria for medical LLMs through\ntargeted red-teaming taking three different points of view - patient,\nclinician, and general user - establishing a foundation for safer deployment in\nmedical domains.\n","authors":["Minseon Kim","Jean-Philippe Corbeil","Alessandro Sordoni","Francois Beaulieu","Paul Vozila"],"pdf_url":"https://arxiv.org/pdf/2507.07248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11005v4","updated":"2025-07-09T19:29:19Z","published":"2024-02-16T18:28:43Z","title":"A Theory of Response Sampling in LLMs: Part Descriptive and Part\n  Prescriptive","summary":"  Large Language Models (LLMs) are increasingly utilized in autonomous\ndecision-making, where they sample options from vast action spaces. However,\nthe heuristics that guide this sampling process remain under explored. We study\nthis sampling behavior and show that this underlying heuristics resembles that\nof human decision-making: comprising a descriptive component (reflecting\nstatistical norm) and a prescriptive component (implicit ideal encoded in the\nLLM) of a concept. We show that this deviation of a sample from the statistical\nnorm towards a prescriptive component consistently appears in concepts across\ndiverse real-world domains like public health, and economic trends. To further\nillustrate the theory, we demonstrate that concept prototypes in LLMs are\naffected by prescriptive norms, similar to the concept of normality in humans.\nThrough case studies and comparison with human studies, we illustrate that in\nreal-world applications, the shift of samples toward an ideal value in LLMs'\noutputs can result in significantly biased decision-making, raising ethical\nconcerns.\n","authors":["Sarath Sivaprasad","Pramod Kaushik","Sahar Abdelnabi","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2402.11005v4.pdf","comment":"ACL 2025 (Oral)"},{"id":"http://arxiv.org/abs/2507.07236v1","updated":"2025-07-09T19:13:25Z","published":"2025-07-09T19:13:25Z","title":"An Information-Theoretic Perspective on Multi-LLM Uncertainty Estimation","summary":"  Large language models (LLMs) often behave inconsistently across inputs,\nindicating uncertainty and motivating the need for its quantification in\nhigh-stakes settings. Prior work on calibration and uncertainty quantification\noften focuses on individual models, overlooking the potential of model\ndiversity. We hypothesize that LLMs make complementary predictions due to\ndifferences in training and the Zipfian nature of language, and that\naggregating their outputs leads to more reliable uncertainty estimates. To\nleverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a\nsimple information-theoretic method that uses Jensen-Shannon Divergence to\nidentify and aggregate well-calibrated subsets of LLMs. Experiments on binary\nprediction tasks demonstrate improved calibration and predictive performance\ncompared to single-model and naive ensemble baselines.\n","authors":["Maya Kruse","Majid Afshar","Saksham Khatwani","Anoop Mayampurath","Guanhua Chen","Yanjun Gao"],"pdf_url":"https://arxiv.org/pdf/2507.07236v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.01077v3","updated":"2025-07-09T19:12:23Z","published":"2024-11-01T23:18:32Z","title":"Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection","summary":"  Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted output, posing a potential threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This alters the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the unsafe prediction rate, bypassing existing\nsafeguards.\n","authors":["Zhipeng Wei","Yuqi Liu","N. Benjamin Erichson"],"pdf_url":"https://arxiv.org/pdf/2411.01077v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07229v1","updated":"2025-07-09T19:05:33Z","published":"2025-07-09T19:05:33Z","title":"SynthTextEval: Synthetic Text Data Generation and Evaluation for\n  High-Stakes Domains","summary":"  We present SynthTextEval, a toolkit for conducting comprehensive evaluations\nof synthetic text. The fluency of large language model (LLM) outputs has made\nsynthetic text potentially viable for numerous applications, such as reducing\nthe risks of privacy violations in the development and deployment of AI systems\nin high-stakes domains. Realizing this potential, however, requires principled\nconsistent evaluations of synthetic data across multiple dimensions: its\nutility in downstream systems, the fairness of these systems, the risk of\nprivacy leakage, general distributional differences from the source text, and\nqualitative feedback from domain experts. SynthTextEval allows users to conduct\nevaluations along all of these dimensions over synthetic data that they upload\nor generate using the toolkit's generation module. While our toolkit can be run\nover any data, we highlight its functionality and effectiveness over datasets\nfrom two high-stakes domains: healthcare and law. By consolidating and\nstandardizing evaluation metrics, we aim to improve the viability of synthetic\ntext, and in-turn, privacy-preservation in AI development.\n","authors":["Krithika Ramesh","Daniel Smolyak","Zihao Zhao","Nupoor Gandhi","Ritu Agarwal","Margrét Bjarnadóttir","Anjalie Field"],"pdf_url":"https://arxiv.org/pdf/2507.07229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05385v2","updated":"2025-07-09T18:40:00Z","published":"2025-07-07T18:15:29Z","title":"EduCoder: An Open-Source Annotation System for Education Transcript Data","summary":"  We introduce EduCoder, a domain-specialized tool designed to support\nutterance-level annotation of educational dialogue. While general-purpose text\nannotation tools for NLP and qualitative research abound, few address the\ncomplexities of coding education dialogue transcripts -- with diverse\nteacher-student and peer interactions. Common challenges include defining\ncodebooks for complex pedagogical features, supporting both open-ended and\ncategorical coding, and contextualizing utterances with external features, such\nas the lesson's purpose and the pedagogical value of the instruction. EduCoder\nis designed to address these challenges by providing a platform for researchers\nand domain experts to collaboratively define complex codebooks based on\nobserved data. It incorporates both categorical and open-ended annotation types\nalong with contextual materials. Additionally, it offers a side-by-side\ncomparison of multiple annotators' responses, allowing comparison and\ncalibration of annotations with others to improve data reliability. The system\nis open-source, with a demo video available.\n","authors":["Guanzhong Pan","Mei Tan","Hyunji Nam","Lucía Langlois","James Malamut","Liliana Deonizio","Dorottya Demszky"],"pdf_url":"https://arxiv.org/pdf/2507.05385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13940v4","updated":"2025-07-09T18:39:59Z","published":"2024-08-25T21:20:17Z","title":"Derailer-Rerailer: Adaptive Verification for Efficient and Reliable\n  Language Model Reasoning","summary":"  Large Language Models (LLMs) have shown impressive reasoning capabilities,\nyet existing prompting methods face a critical trade-off: simple approaches\noften struggle with complex tasks and reasoning stability, while more\nsophisticated methods require multiple inferences and substantial computational\nresources, limiting their practical deployment. To address this challenge, we\npropose Derailer-Rerailer, a novel framework that adaptively balances reasoning\naccuracy and computational efficiency. At its core, our framework employs a\nlightweight Derailer mechanism to assess reasoning stability and selectively\ntriggers an advanced Rerailer verification process only when necessary, thereby\noptimizing computational resource usage. Extensive evaluation across both open\nand closed-source models on more than 20 categories of mathematical, symbolic,\nand commonsense reasoning tasks demonstrates our framework's effectiveness:\nDerailer-Rerailer achieves significant accuracy improvements (8-11\\% across\nvarious reasoning tasks) while maintaining 2-3 times better efficiency than\nexisting verification methods, with particularly strong performance in\nmathematical and symbolic reasoning, offering a practical solution for\nenhancing LLM reasoning reliability while significantly reducing computational\noverhead.\n","authors":["Guangya Wan","Yuqi Wu","Hao Wang","Shengming Zhao","Jie Chen","Sheng Li"],"pdf_url":"https://arxiv.org/pdf/2408.13940v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07188v1","updated":"2025-07-09T18:01:50Z","published":"2025-07-09T18:01:50Z","title":"Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses","summary":"  Large Language Models (LLMs) are increasingly used as proxies for human\nsubjects in social science surveys, but their reliability and susceptibility to\nknown response biases are poorly understood. This paper investigates the\nresponse robustness of LLMs in normative survey contexts -- we test nine\ndiverse LLMs on questions from the World Values Survey (WVS), applying a\ncomprehensive set of 11 perturbations to both question phrasing and answer\noption structure, resulting in over 167,000 simulated interviews. In doing so,\nwe not only reveal LLMs' vulnerabilities to perturbations but also reveal that\nall tested models exhibit a consistent \\textit{recency bias} varying in\nintensity, disproportionately favoring the last-presented answer option. While\nlarger models are generally more robust, all models remain sensitive to\nsemantic variations like paraphrasing and to combined perturbations. By\napplying a set of perturbations, we reveal that LLMs partially align with\nsurvey response biases identified in humans. This underscores the critical\nimportance of prompt design and robustness testing when using LLMs to generate\nsynthetic survey data.\n","authors":["Jens Rupprecht","Georg Ahnert","Markus Strohmaier"],"pdf_url":"https://arxiv.org/pdf/2507.07188v1.pdf","comment":"18 pages, 17 figures"},{"id":"http://arxiv.org/abs/2507.07186v1","updated":"2025-07-09T18:01:14Z","published":"2025-07-09T18:01:14Z","title":"Planted in Pretraining, Swayed by Finetuning: A Case Study on the\n  Origins of Cognitive Biases in LLMs","summary":"  Large language models (LLMs) exhibit cognitive biases -- systematic\ntendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified\nby instruction tuning. However, it remains unclear if these differences in\nbiases stem from pretraining, finetuning, or even random noise due to training\nstochasticity. We propose a two-step causal experimental approach to\ndisentangle these factors. First, we finetune models multiple times using\ndifferent random seeds to study how training randomness affects over $30$\ncognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping\ninstruction datasets between models to isolate bias sources. This swap uses\ndatasets that led to different bias patterns, directly testing whether biases\nare dataset-dependent. Our findings reveal that while training randomness\nintroduces some variability, biases are mainly shaped by pretraining: models\nwith the same pretrained backbone exhibit more similar bias patterns than those\nsharing only finetuning data. These insights suggest that understanding biases\nin finetuned models requires considering their pretraining origins beyond\nfinetuning effects. This perspective can guide future efforts to develop\nprincipled strategies for evaluating and mitigating bias in LLMs.\n","authors":["Itay Itzhak","Yonatan Belinkov","Gabriel Stanovsky"],"pdf_url":"https://arxiv.org/pdf/2507.07186v1.pdf","comment":"CoLM 2025"},{"id":"http://arxiv.org/abs/2502.12446v2","updated":"2025-07-09T17:31:20Z","published":"2025-02-18T02:27:23Z","title":"Multi-Attribute Steering of Language Models via Targeted Intervention","summary":"  Inference-time intervention (ITI) has emerged as a promising method for\nsteering large language model (LLM) behavior in a particular direction (e.g.,\nimproving helpfulness) by intervening on token representations without costly\nupdates to the LLM's parameters. However, existing ITI approaches fail to scale\nto multi-attribute settings with conflicts, such as enhancing helpfulness while\nalso reducing toxicity. To address this, we introduce Multi-Attribute Targeted\nSteering (MAT-Steer), a novel steering framework designed for selective\ntoken-level intervention across multiple attributes. MAT-Steer learns steering\nvectors using an alignment objective that shifts the model's internal\nrepresentations of undesirable outputs closer to those of desirable ones while\nenforcing sparsity and orthogonality among vectors for different attributes,\nthereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two\ndistinct settings: (i) on question answering (QA) tasks where we balance\nattributes like truthfulness, bias, and toxicity; (ii) on generative tasks\nwhere we simultaneously improve attributes like helpfulness, correctness, and\ncoherence. MAT-Steer outperforms existing ITI and parameter-efficient\nfine-tuning approaches across both task types (e.g., 3% average accuracy gain\nacross QA tasks and 55.82% win rate against the best ITI baseline).\n","authors":["Duy Nguyen","Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2502.12446v2.pdf","comment":"ACL 2025 camera-ready, code link:\n  https://github.com/duykhuongnguyen/MAT-Steer"},{"id":"http://arxiv.org/abs/2412.08268v3","updated":"2025-07-09T17:25:55Z","published":"2024-12-11T10:35:45Z","title":"LCFO: Long Context and Long Form Output Dataset and Benchmarking","summary":"  This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6).\n","authors":["Marta R. Costa-jussà","Pierre Andrews","Mariano Coria Meglioli","Joy Chen","Joe Chuang","David Dale","Christophe Ropers","Alexandre Mourachko","Eduardo Sánchez","Holger Schwenk","Tuan Tran","Arina Turkatenko","Carleigh Wood"],"pdf_url":"https://arxiv.org/pdf/2412.08268v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01735v2","updated":"2025-07-09T17:19:50Z","published":"2024-10-02T16:46:38Z","title":"LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits","summary":"  Reward Models (RMs) are crucial to aligning large language models (LLMs), but\nthe degree to which an RM specialized to one task (e.g. writing) generalizes to\nnew tasks (e.g. math) is often not known a priori, often making using only one\nfixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs\nsimultaneously can incur a prohibitively high computational cost and lead to\nconflicting signals from different RMs that may degrade performance. To address\nthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),\nwhich frames reward model selection as a multi-armed bandit problem,\nefficiently and iteratively training LLMs using multiple RMs by selecting the\nmost well-suited RM for each instance. On commonsense and math reasoning tasks,\nwe show that LASeR boosts iterative LLM training, improving the absolute\naverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of\nRM scores while also showing superior efficiency (e.g., a 2x speedup).\nMoreover, on WildChat (open-ended instruction-following tasks), LASeR leads to\na 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to\nlong-context generation, LASeR improves by 2.96 F1 points (avg.) on\nsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RM\nscore ensemble baseline with best-of-n sampling.\n","authors":["Duy Nguyen","Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.01735v2.pdf","comment":"28 pages; First two authors contributed equally. Code:\n  https://github.com/duykhuongnguyen/LASeR-MAB"},{"id":"http://arxiv.org/abs/2409.17538v7","updated":"2025-07-09T17:11:15Z","published":"2024-09-26T04:56:49Z","title":"Low-Rank Adaptation Secretly Imitates Differentially Private SGD","summary":"  As pre-trained language models grow in size, full fine-tuning their\nparameters on task adaptation data becomes increasingly impractical. To address\nthis challenge, some methods for low-rank adaptation of language models have\nbeen proposed, e.g. LoRA, which incorporates trainable low-rank decomposition\nmatrices into only some parameters of the pre-trained model, called adapters.\nThis approach significantly reduces the number of trainable parameters compared\nto fine-tuning all parameters or adapters. In this work, we look at low-rank\nadaptation method from the lens of data privacy. We show theoretically that the\nlow-rank adaptation used in LoRA is equivalent to fine-tuning adapters with\nnoisy batch gradients - just like what DPSGD algorithm does. We also quantify\nthe variance of the injected noise as a decreasing function of adaptation rank.\nBy establishing a Berry-Esseen type bound on the total variation distance\nbetween the injected noise distribution and a Gaussian noise distribution with\nthe same variance, we show that the dynamics of low-rank adaptation is very\nclose to when DPSGD is performed w.r.t the adapters. Following our theoretical\nfindings and approved by our experimental results, we show that low-rank\nadaptation provides robustness to membership inference attacks w.r.t the\nfine-tuning data.\n","authors":["Saber Malekmohammadi","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2409.17538v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07030v1","updated":"2025-07-09T17:02:40Z","published":"2025-07-09T17:02:40Z","title":"UniConv: Unifying Retrieval and Response Generation for Large Language\n  Models in Conversations","summary":"  The rapid advancement of conversational search systems revolutionizes how\ninformation is accessed by enabling the multi-turn interaction between the user\nand the system. Existing conversational search systems are usually built with\ntwo different models. This separation restricts the system from leveraging the\nintrinsic knowledge of the models simultaneously, which cannot ensure the\neffectiveness of retrieval benefiting the generation. The existing studies for\ndeveloping unified models cannot fully address the aspects of understanding\nconversational context, managing retrieval independently, and generating\nresponses. In this paper, we explore how to unify dense retrieval and response\ngeneration for large language models in conversation. We conduct joint\nfine-tuning with different objectives and design two mechanisms to reduce the\ninconsistency risks while mitigating data discrepancy. The evaluations on five\nconversational search datasets demonstrate that our unified model can mutually\nimprove both tasks and outperform the existing baselines.\n","authors":["Fengran Mo","Yifan Gao","Chuan Meng","Xin Liu","Zhuofeng Wu","Kelong Mao","Zhengyang Wang","Pei Chen","Zheng Li","Xian Li","Bing Yin","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2507.07030v1.pdf","comment":"Accepted by ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2507.07024v1","updated":"2025-07-09T16:54:21Z","published":"2025-07-09T16:54:21Z","title":"FlexOlmo: Open Language Models for Flexible Data Use","summary":"  We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference.\n","authors":["Weijia Shi","Akshita Bhagia","Kevin Farhat","Niklas Muennighoff","Pete Walsh","Jacob Morrison","Dustin Schwenk","Shayne Longpre","Jake Poznanski","Allyson Ettinger","Daogao Liu","Margaret Li","Dirk Groeneveld","Mike Lewis","Wen-tau Yih","Luca Soldaini","Kyle Lo","Noah A. Smith","Luke Zettlemoyer","Pang Wei Koh","Hannaneh Hajishirzi","Ali Farhadi","Sewon Min"],"pdf_url":"https://arxiv.org/pdf/2507.07024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05261v2","updated":"2025-07-09T16:40:38Z","published":"2025-06-18T23:17:29Z","title":"TokenShapley: Token Level Context Attribution with Shapley Value","summary":"  Large language models (LLMs) demonstrate strong capabilities in in-context\nlearning, but verifying the correctness of their generated responses remains a\nchallenge. Prior work has explored attribution at the sentence level, but these\nmethods fall short when users seek attribution for specific keywords within the\nresponse, such as numbers, years, or names. To address this limitation, we\npropose TokenShapley, a novel token-level attribution method that combines\nShapley value-based data attribution with KNN-based retrieval techniques\ninspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed\ndatastore for contextual retrieval and computing Shapley values to quantify\ntoken importance, TokenShapley provides a fine-grained data attribution\napproach. Extensive evaluations on four benchmarks show that TokenShapley\noutperforms state-of-the-art baselines in token-level attribution, achieving an\n11-23% improvement in accuracy.\n","authors":["Yingtai Xiao","Yuqing Zhu","Sirat Samyoun","Wanrong Zhang","Jiachen T. Wang","Jian Du"],"pdf_url":"https://arxiv.org/pdf/2507.05261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06999v1","updated":"2025-07-09T16:25:44Z","published":"2025-07-09T16:25:44Z","title":"Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning\n  in Multimodal LLMs","summary":"  Reasoning is a key capability for large language models (LLMs), particularly\nwhen applied to complex tasks such as mathematical problem solving. However,\nmultimodal reasoning research still requires further exploration of modality\nalignment and training costs. Many of these approaches rely on additional data\nannotation and relevant rule-based rewards to enhance the understanding and\nreasoning ability, which significantly increases training costs and limits\nscalability. To address these challenges, we propose the\nDeliberate-to-Intuitive reasoning framework (D2I) that improves the\nunderstanding and reasoning ability of multimodal LLMs (MLLMs) without extra\nannotations and complex rewards. Specifically, our method sets deliberate\nreasoning strategies to enhance modality alignment only through the rule-based\nformat reward during training. While evaluating, the reasoning style shifts to\nintuitive, which removes deliberate reasoning strategies during training and\nimplicitly reflects the model's acquired abilities in the response. D2I\noutperforms baselines across both in-domain and out-of-domain benchmarks. Our\nfindings highlight the role of format reward in fostering transferable\nreasoning skills in MLLMs, and inspire directions for decoupling training-time\nreasoning depth from test-time response flexibility.\n","authors":["Yahan Yu","Yuyang Dong","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2507.06999v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.12112v3","updated":"2025-07-09T16:13:20Z","published":"2024-10-15T23:20:54Z","title":"Planning Anything with Rigor: General-Purpose Zero-Shot Planning with\n  LLM-based Formalized Programming","summary":"  While large language models (LLMs) have recently demonstrated strong\npotential in solving planning problems, there is a trade-off between\nflexibility and complexity. LLMs, as zero-shot planners themselves, are still\nnot capable of directly generating valid plans for complex planning problems\nsuch as multi-constraint or long-horizon tasks. On the other hand, many\nframeworks aiming to solve complex planning problems often rely on\ntask-specific preparatory efforts, such as task-specific in-context examples\nand pre-defined critics/verifiers, which limits their cross-task generalization\ncapability. In this paper, we tackle these challenges by observing that the\ncore of many planning problems lies in optimization problems: searching for the\noptimal solution (best plan) with goals subject to constraints (preconditions\nand effects of decisions). With LLMs' commonsense, reasoning, and programming\ncapabilities, this opens up the possibilities of a universal LLM-based approach\nto planning problems. Inspired by this observation, we propose LLMFP, a\ngeneral-purpose framework that leverages LLMs to capture key information from\nplanning problems and formally formulate and solve them as optimization\nproblems from scratch, with no task-specific examples needed. We apply LLMFP to\n9 planning problems, ranging from multi-constraint decision making to\nmulti-step planning problems, and demonstrate that LLMFP achieves on average\n83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet,\nsignificantly outperforming the best baseline (direct planning with OpenAI\no1-preview) with 37.6% and 40.7% improvements. We also validate components of\nLLMFP with ablation experiments and analyzed the underlying success and failure\nreasons. Project page: https://sites.google.com/view/llmfp.\n","authors":["Yilun Hao","Yang Zhang","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2410.12112v3.pdf","comment":"57 pages, 25 figures, 15 tables"},{"id":"http://arxiv.org/abs/2507.06974v1","updated":"2025-07-09T16:04:51Z","published":"2025-07-09T16:04:51Z","title":"FRaN-X: FRaming and Narratives-eXplorer","summary":"  We present FRaN-X, a Framing and Narratives Explorer that automatically\ndetects entity mentions and classifies their narrative roles directly from raw\ntext. FRaN-X comprises a two-stage system that combines sequence labeling with\nfine-grained role classification to reveal how entities are portrayed as\nprotagonists, antagonists, or innocents, using a unique taxonomy of 22\nfine-grained roles nested under these three main categories. The system\nsupports five languages (Bulgarian, English, Hindi, Russian, and Portuguese)\nand two domains (the Russia-Ukraine Conflict and Climate Change). It provides\nan interactive web interface for media analysts to explore and compare framing\nacross different sources, tackling the challenge of automatically detecting and\nlabeling how entities are framed. Our system allows end users to focus on a\nsingle article as well as analyze up to four articles simultaneously. We\nprovide aggregate level analysis including an intuitive graph visualization\nthat highlights the narrative a group of articles are pushing. Our system\nincludes a search feature for users to look up entities of interest, along with\na timeline view that allows analysts to track an entity's role transitions\nacross different contexts within the article. The FRaN-X system and the trained\nmodels are licensed under an MIT License. FRaN-X is publicly accessible at\nhttps://fran-x.streamlit.app/ and a video demonstration is available at\nhttps://youtu.be/VZVi-1B6yYk.\n","authors":["Artur Muratov","Hana Fatima Shaikh","Vanshikaa Jani","Tarek Mahmoud","Zhuohan Xie","Daniil Orel","Aaryamonvikram Singh","Yuxia Wang","Aadi Joshi","Hasan Iqbal","Ming Shan Hee","Dhruv Sahnan","Nikolaos Nikolaidis","Purificação Silvano","Dimitar Dimitrov","Roman Yangarber","Ricardo Campos","Alípio Jorge","Nuno Guimarães","Elisa Sartori","Nicolas Stefanovitch","Giovanni Da San Martino","Jakub Piskorski","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2507.06974v1.pdf","comment":"19 pages, 13 figures, submitted to EMNLP 2025 - Demo Track"},{"id":"http://arxiv.org/abs/2507.06968v1","updated":"2025-07-09T15:59:02Z","published":"2025-07-09T15:59:02Z","title":"Scaling Towards the Information Boundary of Instruction Set:\n  InfinityInstruct-Subject Technical Report","summary":"  Instruction tuning has become a foundation for unlocking the capabilities of\nlarge-scale pretrained models and improving their performance on complex tasks.\nThus, the construction of high-quality instruction datasets is crucial for\nenhancing model performance and generalizability. Although current instruction\ndatasets have reached tens of millions of samples, models finetuned on them may\nstill struggle with complex instruction following and tasks in rare domains.\nThis is primarily due to limited expansion in both ``coverage'' (coverage of\ntask types and knowledge areas) and ``depth'' (instruction complexity) of the\ninstruction set. To address this issue, we propose a systematic instruction\ndata construction framework, which integrates a hierarchical labeling system,\nan informative seed selection algorithm, an evolutionary data synthesis\nprocess, and a model deficiency diagnosis with targeted data generation. These\ncomponents form an iterative closed-loop to continuously enhance the coverage\nand depth of instruction data. Based on this framework, we construct\nInfinityInstruct-Subject, a high-quality dataset containing ~1.5 million\ninstructions. Experiments on multiple foundation models and benchmark tasks\ndemonstrate its effectiveness in improving instruction-following capabilities.\nFurther analyses suggest that InfinityInstruct-Subject shows enlarged coverage\nand depth compared to comparable synthesized instruction datasets. Our work\nlays a theoretical and practical foundation for the efficient, continuous\nevolution of instruction datasets, moving from data quantity expansion to\nqualitative improvement.\n","authors":["Li Du","Hanyu Zhao","Yiming Ju","Tengfei Pan"],"pdf_url":"https://arxiv.org/pdf/2507.06968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06956v1","updated":"2025-07-09T15:39:17Z","published":"2025-07-09T15:39:17Z","title":"Investigating the Robustness of Retrieval-Augmented Generation at the\n  Query Level","summary":"  Large language models (LLMs) are very costly and inefficient to update with\nnew information. To address this limitation, retrieval-augmented generation\n(RAG) has been proposed as a solution that dynamically incorporates external\nknowledge during inference, improving factual consistency and reducing\nhallucinations. Despite its promise, RAG systems face practical challenges-most\nnotably, a strong dependence on the quality of the input query for accurate\nretrieval. In this paper, we investigate the sensitivity of different\ncomponents in the RAG pipeline to various types of query perturbations. Our\nanalysis reveals that the performance of commonly used retrievers can degrade\nsignificantly even under minor query variations. We study each module in\nisolation as well as their combined effect in an end-to-end question answering\nsetting, using both general-domain and domain-specific datasets. Additionally,\nwe propose an evaluation framework to systematically assess the query-level\nrobustness of RAG pipelines and offer actionable recommendations for\npractitioners based on the results of more than 1092 experiments we performed.\n","authors":["Sezen Perçin","Xin Su","Qutub Sha Syed","Phillip Howard","Aleksei Kuvshinov","Leo Schwinn","Kay-Ulrich Scholl"],"pdf_url":"https://arxiv.org/pdf/2507.06956v1.pdf","comment":"Accepted to Generation, Evaluation & Metrics (GEM) Workshop at ACL\n  2025"},{"id":"http://arxiv.org/abs/2412.18497v2","updated":"2025-07-09T15:14:46Z","published":"2024-12-24T15:28:56Z","title":"Neuron-Level Differentiation of Memorization and Generalization in Large\n  Language Models","summary":"  We investigate how Large Language Models (LLMs) distinguish between\nmemorization and generalization at the neuron level. Through carefully designed\ntasks, we identify distinct neuron subsets responsible for each behavior.\nExperiments on both a GPT-2 model trained from scratch and a pretrained\nLLaMA-3.2 model fine-tuned with LoRA show consistent neuron-level\nspecialization. We further demonstrate that inference-time interventions on\nthese neurons can steer the model's behavior toward memorization or\ngeneralization. To assess robustness, we evaluate intra-task and inter-task\nconsistency, confirming that these neuron-behavior associations reflect\ngeneralizable patterns rather than dataset-specific artifacts. Our findings\nreveal modular structure in LLMs and enable controlling memorization and\ngeneralization behaviors at inference time.\n","authors":["Ko-Wei Huang","Yi-Fu Fu","Ching-Yu Tsai","Yu-Chieh Tu","Tzu-Ling Cheng","Cheng-Yu Lin","Yi-Ting Yang","Heng-Yi Liu","Keng-Te Liao","Da-Cheng Juan","Shou-De Lin"],"pdf_url":"https://arxiv.org/pdf/2412.18497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09567v4","updated":"2025-07-09T15:13:24Z","published":"2025-03-12T17:35:03Z","title":"Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning\n  Large Language Models","summary":"  Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"inference-time scaling.\" This survey seeks\nto fill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and inference-time scaling,\noffering insights into how these processes manifest in practice. (4) Finally,\nwe identify significant research gaps and highlight promising future\ndirections, including the integration of multi-modal reasoning, efficiency\nimprovements, and enhanced knowledge frameworks. By providing a structured\noverview, this survey aims to inspire future research and further the\ndevelopment of logical reasoning in artificial intelligence.\n","authors":["Qiguang Chen","Libo Qin","Jinhao Liu","Dengyun Peng","Jiannan Guan","Peng Wang","Mengkang Hu","Yuhang Zhou","Te Gao","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2503.09567v4.pdf","comment":"Paper are available at https://long-cot.github.io/, and Github are\n  available at\n  https://github.com/LightChen233/Awesome-Long-Chain-of-Thought-Reasoning"},{"id":"http://arxiv.org/abs/2506.23463v2","updated":"2025-07-09T15:10:56Z","published":"2025-06-30T02:03:23Z","title":"What to Keep and What to Drop: Adaptive Table Filtering Framework","summary":"  Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by 70%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks.\n","authors":["WonJune Jang"],"pdf_url":"https://arxiv.org/pdf/2506.23463v2.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2507.06910v1","updated":"2025-07-09T14:47:35Z","published":"2025-07-09T14:47:35Z","title":"Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in\n  Dialogues","summary":"  Tutoring dialogues have gained significant attention in recent years, given\nthe prominence of online learning and the emerging tutoring abilities of\nartificial intelligence (AI) agents powered by large language models (LLMs).\nRecent studies have shown that the strategies used by tutors can have\nsignificant effects on student outcomes, necessitating methods to predict how\ntutors will behave and how their actions impact students. However, few works\nhave studied predicting tutor strategy in dialogues. Therefore, in this work we\ninvestigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to\npredict both future tutor moves and student outcomes in dialogues, using two\nmath tutoring dialogue datasets. We find that even state-of-the-art LLMs\nstruggle to predict future tutor strategy while tutor strategy is highly\nindicative of student outcomes, outlining a need for more powerful methods to\napproach this task.\n","authors":["Fareya Ikram","Alexander Scarlatos","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2507.06910v1.pdf","comment":"Published in BEA 2025: 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications"},{"id":"http://arxiv.org/abs/2507.06909v1","updated":"2025-07-09T14:47:00Z","published":"2025-07-09T14:47:00Z","title":"MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal\n  Prediction","summary":"  Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP.\n","authors":["Xiao Wang","Jiahuan Pei","Diancheng Shui","Zhiguang Han","Xin Sun","Dawei Zhu","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2507.06909v1.pdf","comment":"Accepted by NLPCC 2025"},{"id":"http://arxiv.org/abs/2507.06908v1","updated":"2025-07-09T14:46:32Z","published":"2025-07-09T14:46:32Z","title":"MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection","summary":"  The rapid expansion of memes on social media has highlighted the urgent need\nfor effective approaches to detect harmful content. However, traditional\ndata-driven approaches struggle to detect new memes due to their evolving\nnature and the lack of up-to-date annotated data. To address this issue, we\npropose MIND, a multi-agent framework for zero-shot harmful meme detection that\ndoes not rely on annotated data. MIND implements three key strategies: 1) We\nretrieve similar memes from an unannotated reference set to provide contextual\ninformation. 2) We propose a bi-directional insight derivation mechanism to\nextract a comprehensive understanding of similar memes. 3) We then employ a\nmulti-agent debate mechanism to ensure robust decision-making through reasoned\narbitration. Extensive experiments on three meme datasets demonstrate that our\nproposed framework not only outperforms existing zero-shot approaches but also\nshows strong generalization across different model architectures and parameter\nscales, providing a scalable solution for harmful meme detection. The code is\navailable at https://github.com/destroy-lonely/MIND.\n","authors":["Ziyan Liu","Chunxiao Fan","Haoran Lou","Yuexin Wu","Kaiwei Deng"],"pdf_url":"https://arxiv.org/pdf/2507.06908v1.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2507.06899v1","updated":"2025-07-09T14:36:00Z","published":"2025-07-09T14:36:00Z","title":"VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual\n  Grounding Manipulation","summary":"  Graphical User Interface (GUI) agents powered by Large Vision-Language Models\n(LVLMs) have emerged as a revolutionary approach to automating human-machine\ninteractions, capable of autonomously operating personal devices (e.g., mobile\nphones) or applications within the device to perform complex real-world tasks\nin a human-like manner. However, their close integration with personal devices\nraises significant security concerns, with many threats, including backdoor\nattacks, remaining largely unexplored. This work reveals that the visual\ngrounding of GUI agent-mapping textual plans to GUI elements-can introduce\nvulnerabilities, enabling new types of backdoor attacks. With backdoor attack\ntargeting visual grounding, the agent's behavior can be compromised even when\ngiven correct task-solving plans. To validate this vulnerability, we propose\nVisualTrap, a method that can hijack the grounding by misleading the agent to\nlocate textual plans to trigger locations instead of the intended targets.\nVisualTrap uses the common method of injecting poisoned data for attacks, and\ndoes so during the pre-training of visual grounding to ensure practical\nfeasibility of attacking. Empirical results show that VisualTrap can\neffectively hijack visual grounding with as little as 5% poisoned data and\nhighly stealthy visual triggers (invisible to the human eye); and the attack\ncan be generalized to downstream tasks, even after clean fine-tuning. Moreover,\nthe injected trigger can remain effective across different GUI environments,\ne.g., being trained on mobile/web and generalizing to desktop environments.\nThese findings underscore the urgent need for further research on backdoor\nattack risks in GUI agents.\n","authors":["Ziang Ye","Yang Zhang","Wentao Shi","Xiaoyu You","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2507.06899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05167v3","updated":"2025-07-09T14:35:23Z","published":"2025-02-07T18:49:46Z","title":"NoLiMa: Long-Context Evaluation Beyond Literal Matching","summary":"  Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 13 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 11 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.\nEven models enhanced with reasoning capabilities or CoT prompting struggle to\nmaintain performance in long contexts. We publicly release the dataset and\nevaluation code at https://github.com/adobe-research/NoLiMa.\n","authors":["Ali Modarressi","Hanieh Deilamsalehy","Franck Dernoncourt","Trung Bui","Ryan A. Rossi","Seunghyun Yoon","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2502.05167v3.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2507.06895v1","updated":"2025-07-09T14:33:07Z","published":"2025-07-09T14:33:07Z","title":"SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label\n  Contrastive Learning and Bayesian kNN","summary":"  The growing demand for efficient knowledge graph (KG) enrichment leveraging\nexternal corpora has intensified interest in relation extraction (RE),\nparticularly under low-supervision settings. To address the need for adaptable\nand noise-resilient RE solutions that integrate seamlessly with pre-trained\nlarge language models (PLMs), we introduce SCoRE, a modular and cost-effective\nsentence-level RE system. SCoRE enables easy PLM switching, requires no\nfinetuning, and adapts smoothly to diverse corpora and KGs. By combining\nsupervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)\nclassifier for multi-label classification, it delivers robust performance\ndespite the noisy annotations of distantly supervised corpora. To improve RE\nevaluation, we propose two novel metrics: Correlation Structure Distance (CSD),\nmeasuring the alignment between learned relational patterns and KG structures,\nand Precision at R (P@R), assessing utility as a recommender system. We also\nrelease Wiki20d, a benchmark dataset replicating real-world RE conditions where\nonly KG-derived annotations are available. Experiments on five benchmarks show\nthat SCoRE matches or surpasses state-of-the-art methods while significantly\nreducing energy consumption. Further analyses reveal that increasing model\ncomplexity, as seen in prior work, degrades performance, highlighting the\nadvantages of SCoRE's minimal design. Combining efficiency, modularity, and\nscalability, SCoRE stands as an optimal choice for real-world RE applications.\n","authors":["Luca Mariotti","Veronica Guidetti","Federica Mandreoli"],"pdf_url":"https://arxiv.org/pdf/2507.06895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06893v1","updated":"2025-07-09T14:30:45Z","published":"2025-07-09T14:30:45Z","title":"Developing and Maintaining an Open-Source Repository of AI Evaluations:\n  Challenges and Insights","summary":"  AI evaluations have become critical tools for assessing large language model\ncapabilities and safety. This paper presents practical insights from eight\nmonths of maintaining $inspect\\_evals$, an open-source repository of 70+\ncommunity-contributed AI evaluations. We identify key challenges in\nimplementing and maintaining AI evaluations and develop solutions including:\n(1) a structured cohort management framework for scaling community\ncontributions, (2) statistical methodologies for optimal resampling and\ncross-model comparison with uncertainty quantification, and (3) systematic\nquality control processes for reproducibility. Our analysis reveals that AI\nevaluation requires specialized infrastructure, statistical rigor, and\ncommunity coordination beyond traditional software development practices.\n","authors":["Alexandra Abbas","Celia Waggoner","Justin Olive"],"pdf_url":"https://arxiv.org/pdf/2507.06893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04204v2","updated":"2025-07-09T13:58:35Z","published":"2025-04-05T15:18:55Z","title":"Adaptive Elicitation of Latent Information Using Natural Language","summary":"  Eliciting information to reduce uncertainty about a latent entity is a\ncritical task in many application domains, e.g., assessing individual student\nlearning outcomes, diagnosing underlying diseases, or learning user\npreferences. Though natural language is a powerful medium for this purpose,\nlarge language models (LLMs) and existing fine-tuning algorithms lack\nmechanisms for strategically gathering information to refine their own\nunderstanding of the latent entity. To harness the generalization power and\nworld knowledge of LLMs in developing effective information-gathering\nstrategies, we propose an adaptive elicitation framework that actively reduces\nuncertainty on the latent entity. Since probabilistic modeling of an abstract\nlatent entity is difficult, our framework adopts a predictive view of\nuncertainty, using a meta-learned language model to simulate future\nobservations and enable scalable uncertainty quantification over complex\nnatural language. Through autoregressive forward simulation, our model\nquantifies how new questions reduce epistemic uncertainty, enabling the\ndevelopment of sophisticated information-gathering strategies to choose the\nmost informative next queries. In experiments on the 20 questions game, dynamic\nopinion polling, and adaptive student assessment, our method consistently\noutperforms baselines in identifying critical unknowns and improving downstream\npredictions, illustrating the promise of strategic information gathering in\nnatural language settings.\n","authors":["Jimmy Wang","Thomas Zollo","Richard Zemel","Hongseok Namkoong"],"pdf_url":"https://arxiv.org/pdf/2504.04204v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.02579v3","updated":"2025-07-09T13:45:07Z","published":"2025-05-05T11:30:46Z","title":"EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning","summary":"  Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including competing objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the fine-tuning to improve efficiency and\nflexibility. Our method is the first to aggregate the hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text classification models to score the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.\n","authors":["Lingxiao Kong","Cong Yang","Susanne Neufang","Oya Deniz Beyan","Zeyd Boukhers"],"pdf_url":"https://arxiv.org/pdf/2505.02579v3.pdf","comment":"14 pages, 9 figures, accepted by the SIGDIAL 2025 conference"},{"id":"http://arxiv.org/abs/2507.06829v1","updated":"2025-07-09T13:28:35Z","published":"2025-07-09T13:28:35Z","title":"Adaptive Termination for Multi-round Parallel Reasoning: An Universal\n  Semantic Entropy-Guided Framework","summary":"  Recent advances in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, with inference-time scaling emerging as\na key technique. Contemporary approaches leverage either sequential reasoning\n(iteratively extending chains of thought) or parallel reasoning (generating\nmultiple solutions simultaneously) to scale inference. However, both paradigms\nface fundamental limitations: sequential scaling typically relies on arbitrary\ntoken budgets for termination, leading to inefficiency or premature cutoff;\nwhile parallel scaling often lacks coordination among parallel branches and\nrequires intrusive fine-tuning to perform effectively. In light of these\nchallenges, we aim to design a flexible test-time collaborative inference\nframework that exploits the complementary strengths of both sequential and\nparallel reasoning paradigms. Towards this goal, the core challenge lies in\ndeveloping an efficient and accurate intrinsic quality metric to assess model\nresponses during collaborative inference, enabling dynamic control and early\ntermination of the reasoning trace. To address this challenge, we introduce\nsemantic entropy (SE), which quantifies the semantic diversity of parallel\nmodel responses and serves as a robust indicator of reasoning quality due to\nits strong negative correlation with accuracy...\n","authors":["Zenan Xu","Zexuan Qiu","Guanhua Huang","Kun Li","Siheng Li","Chenchen Zhang","Kejiao Li","Qi Yi","Yuhao Jiang","Bo Zhou","Fengzong Lian","Zhanhui Kang"],"pdf_url":"https://arxiv.org/pdf/2507.06829v1.pdf","comment":"13 pages, 5 fiures"},{"id":"http://arxiv.org/abs/2502.11703v2","updated":"2025-07-09T13:26:39Z","published":"2025-02-17T11:40:48Z","title":"CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models\n  in Medical Quality Control Indicator Calculation","summary":"  Medical quality control indicators are essential to assess the qualifications\nof healthcare institutions for medical services. With the impressive\nperformance of large language models (LLMs) like GPT-4 in the medical field,\nleveraging these technologies for the Medical Quality Control Indicator\nCalculation (MQCIC) presents a promising approach. In this work, (1) we\nintroduce a real-world task MQCIC and propose an open-source Chinese electronic\nmedical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances\nand 76 indicators. (2) We propose a semi-automatic method to enhance the rule\nrepresentation. Then we propose the Clinical Facts-based Inferential Rule\n(CF-IR) method that disentangles the clinical fact verification and inferential\nrule reasoning actions. (3) We conduct comprehensive experiments on 20\nrepresentative LLMs, covering general and medical models. Our findings reveal\nthat CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct\nan error analysis and investigate the capabilities of clinical fact\nverification and inferential rule reasoning, providing insights to improve\nperformance in the MQCIC further. The dataset and code is available in this\nrepository https://github.com/YuY-2001/C-MQCIC.\n","authors":["Guangya Yu","Yanhao Li","Zongying Jiang","Yuxiong Jin","Li Dai","Yupian Lin","Ruihui Hou","Weiyan Zhang","Yongqi Fan","Qi Ye","Jingping Liu","Tong Ruan"],"pdf_url":"https://arxiv.org/pdf/2502.11703v2.pdf","comment":"2025 ACL Findings"},{"id":"http://arxiv.org/abs/2507.03933v2","updated":"2025-07-09T13:14:29Z","published":"2025-07-05T07:36:49Z","title":"Losing our Tail -- Again: On (Un)Natural Selection And Multilingual\n  Large Language Models","summary":"  Multilingual Large Language Models (LLMs) considerably changed how\ntechnologies can influence language. While previous technologies could mediate\nor assist humans, there is now a tendency to offload the task of writing itself\nto these technologies, enabling them to change our linguistic ecosystem more\ndirectly. While they provide us quick access to information and impressively\nfluent output, beneath their apparent sophistication lies a subtle, more\ninsidious threat: the gradual decline and loss of linguistic diversity. With\nthis opinion piece, I explore how model collapse, with a particular focus on\ntranslation technology, can lead to the loss of linguistic forms, grammatical\nfeatures, and cultural nuance. Model collapse refers to the eventual\nconsequence of self-consuming training loops, where models reinforce their own\nbiases and lose linguistic diversity. Drawing on recent work in Computer\nVision, Natural Language Processing (NLP) and Machine Translation (MT), I argue\nthat the tails of our linguistic distributions are vanishing, and with them,\nthe narratives and identities they carry. This is a call to resist linguistic\nflattening and to reimagine NLP as a field that encourages, values and protects\nexpressive multilingual lexical and linguistic diversity and creativity.\n","authors":["Eva Vanmassenhove"],"pdf_url":"https://arxiv.org/pdf/2507.03933v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2503.09347v3","updated":"2025-07-09T13:09:13Z","published":"2025-03-12T12:49:02Z","title":"Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts","summary":"  Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.\n","authors":["Hongyu Chen","Seraphina Goldfarb-Tarrant"],"pdf_url":"https://arxiv.org/pdf/2503.09347v3.pdf","comment":"9 pages, ACL 2025"},{"id":"http://arxiv.org/abs/2507.06803v1","updated":"2025-07-09T12:44:49Z","published":"2025-07-09T12:44:49Z","title":"Text to model via SysML: Automated generation of dynamical system\n  computational models from unstructured natural language text via enhanced\n  System Modeling Language diagrams","summary":"  This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only.\n","authors":["Matthew Anderson Hendricks","Alice Cicirello"],"pdf_url":"https://arxiv.org/pdf/2507.06803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01951v2","updated":"2025-07-09T12:28:31Z","published":"2025-07-02T17:58:01Z","title":"Test-Time Scaling with Reflective Generative Model","summary":"  We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3-mini's performance via the new Reflective Generative Form.\nThe new form focuses on high-quality reasoning trajectory selection and\ncontains two novelties: 1) A unified interface for policy and process reward\nmodel: we share the backbone network and use task-specific heads for reasoning\ntrajectory predicting and scoring respectively, introducing only 53M extra\nparameters for trajectory scoring. 2) Eliminating the reliance on process-level\nannotation: we provide a self-supervised process reward model, which can\ndirectly learn the high-quality reasoning trajectory selection from the outcome\nreward. Equipped with the reflective generative form, MetaStone-S1 is naturally\nsuitable for test-time scaling, and we provide three reasoning effort modes\n(low, medium, and high) based on the controllable thinking length. Experiments\ndemonstrate that our MetaStone-S1 achieves comparable performance to OpenAI\no3-mini's series with only 32B parameter size. To support the research\ncommunity, we have open-sourced MetaStone-S1 at\nhttps://github.com/MetaStone-AI/MetaStone-S1.\n","authors":["Zixiao Wang","Yuxin Wang","Xiaorui Wang","Mengting Xing","Jie Gao","Jianjun Xu","Guangcan Liu","Chenhui Jin","Zhuo Wang","Shengzhuo Zhang","Hongtao Xie"],"pdf_url":"https://arxiv.org/pdf/2507.01951v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16903v2","updated":"2025-07-09T12:13:12Z","published":"2025-02-24T06:57:27Z","title":"GuidedBench: Measuring and Mitigating the Evaluation Discrepancies of\n  In-the-wild LLM Jailbreak Methods","summary":"  Despite the growing interest in jailbreak methods as an effective red-teaming\ntool for building safe and responsible large language models (LLMs), flawed\nevaluation system designs have led to significant discrepancies in their\neffectiveness assessments. We conduct a systematic measurement study based on\n37 jailbreak studies since 2022, focusing on both the methods and the\nevaluation systems they employ. We find that existing evaluation systems lack\ncase-specific criteria, resulting in misleading conclusions about their\neffectiveness and safety implications. This paper advocates a shift to a more\nnuanced, case-by-case evaluation paradigm. We introduce GuidedBench, a novel\nbenchmark comprising a curated harmful question dataset, detailed case-by-case\nevaluation guidelines and an evaluation system integrated with these guidelines\n-- GuidedEval. Experiments demonstrate that GuidedBench offers more accurate\nmeasurements of jailbreak performance, enabling meaningful comparisons across\nmethods and uncovering new insights overlooked in previous evaluations.\nGuidedEval reduces inter-evaluator variance by at least 76.03\\%. Furthermore,\nwe observe that incorporating guidelines can enhance the effectiveness of\njailbreak methods themselves, offering new insights into both attack strategies\nand evaluation paradigms.\n","authors":["Ruixuan Huang","Xunguang Wang","Zongjie Li","Daoyuan Wu","Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2502.16903v2.pdf","comment":"Homepage: https://sproutnan.github.io/AI-Safety_Benchmark/"},{"id":"http://arxiv.org/abs/2507.06774v1","updated":"2025-07-09T12:03:06Z","published":"2025-07-09T12:03:06Z","title":"Checklist Engineering Empowers Multilingual LLM Judges","summary":"  Automated text evaluation has long been a central issue in Natural Language\nProcessing (NLP). Recently, the field has shifted toward using Large Language\nModels (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While\npromising and easily adaptable across tasks, this approach has seen limited\nexploration in multilingual contexts. Existing multilingual studies often rely\non proprietary models or require extensive training data for fine-tuning,\nraising concerns about cost, time, and efficiency. In this paper, we propose\nChecklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free\nframework that uses checklist intuition for multilingual evaluation with an\nopen-source model. Experiments across multiple languages and three benchmark\ndatasets, under both pointwise and pairwise settings, show that our method\ngenerally surpasses the baselines and performs on par with the GPT-4o model.\n","authors":["Mohammad Ghiasvand Mohammadkhani","Hamid Beigy"],"pdf_url":"https://arxiv.org/pdf/2507.06774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06753v1","updated":"2025-07-09T11:25:35Z","published":"2025-07-09T11:25:35Z","title":"KAConvText: Novel Approach to Burmese Sentence Classification using\n  Kolmogorov-Arnold Convolution","summary":"  This paper presents the first application of Kolmogorov-Arnold Convolution\nfor Text (KAConvText) in sentence classification, addressing three tasks:\nimbalanced binary hate speech detection, balanced multiclass news\nclassification, and imbalanced multiclass ethnic language identification. We\ninvestigate various embedding configurations, comparing random to fastText\nembeddings in both static and fine-tuned settings, with embedding dimensions of\n100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs\nand CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we\ninvestigated KAConvText with different classification heads - MLP and KAN,\nwhere using KAN head supports enhanced interpretability. Results show that\nKAConvText-MLP with fine-tuned fastText embeddings achieves the best\nperformance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection,\n92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82%\naccuracy (F1-score = 0.9982) for language identification.\n","authors":["Ye Kyaw Thu","Thura Aung","Thazin Myint Oo","Thepchai Supnithi"],"pdf_url":"https://arxiv.org/pdf/2507.06753v1.pdf","comment":"10 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2507.07151v1","updated":"2025-07-09T11:18:38Z","published":"2025-07-09T11:18:38Z","title":"Robust Multimodal Large Language Models Against Modality Conflict","summary":"  Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.\n","authors":["Zongmeng Zhang","Wengang Zhou","Jie Zhao","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2507.07151v1.pdf","comment":"ICML 2025"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2507.07340v1","updated":"2025-07-09T23:52:10Z","published":"2025-07-09T23:52:10Z","title":"Entity Re-identification in Visual Storytelling via Contrastive\n  Reinforcement Learning","summary":"  Visual storytelling systems, particularly large vision-language models,\nstruggle to maintain character and object identity across frames,\n  often failing to recognize when entities in different images represent the\nsame individuals or objects,\n  leading to inconsistent references and referential hallucinations.\n  This occurs because models lack explicit training on when to establish entity\nconnections across frames.\n  We propose a contrastive reinforcement learning approach that trains models\nto discriminate between coherent image sequences\n  and stories from unrelated images.\n  We extend the Story Reasoning dataset with synthetic negative examples to\nteach appropriate entity connection behavior.\n  We employ Direct Preference Optimization with a dual-component reward\nfunction that promotes grounding and re-identification of entities\n  in real stories while penalizing incorrect entity connections in synthetic\ncontexts.\n  Using this contrastive framework, we fine-tune Qwen Storyteller (based on\nQwen2.5-VL 7B).\n  Evaluation shows improvements in grounding mAP from 0.27 to 0.31 (+14.8%), F1\nfrom 0.35 to 0.41 (+17.1%).\n  Pronoun grounding accuracy improved across all pronoun types except ``its'',\n  and cross-frame character and object persistence increased\n  across all frame counts, with entities appearing in 5 or more frames\nadvancing from 29.3% to 33.3% (+13.7%).\n  Well-structured stories, containing the chain-of-thought and grounded story,\nincreased from 79.1% to 97.5% (+23.3%).\n","authors":["Daniel A. P. Oliveira","David Martins de Matos"],"pdf_url":"https://arxiv.org/pdf/2507.07340v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2305.13651v2","updated":"2025-07-09T23:51:43Z","published":"2023-05-23T03:49:41Z","title":"Adversarial Defenses via Vector Quantization","summary":"  Adversarial attacks pose significant challenges to the robustness of modern\ndeep neural networks in computer vision, and defending these networks against\nadversarial attacks has attracted intense research efforts. Among various\ndefense strategies, preprocessing-based defenses are practically appealing\nsince there is no need to train the network under protection. However, such\napproaches typically do not achieve comparable robustness as other methods such\nas adversarial training. In this paper, we propose a novel framework for\npreprocessing-based defenses, where a vector quantizer is used as a\npreprocessor. This framework, inspired by and extended from Randomized\nDiscretization (RandDisc), is theoretically principled by rate-distortion\ntheory: indeed, RandDisc may be viewed as a scalar quantizer, and\nrate-distortion theory suggests that such quantization schemes are inferior to\nvector quantization. In our framework, the preprocessing vector quantizer\ntreats the input image as a collection of patches and finds a set of\nrepresentative patches based on the patch distributions; each original patch is\nthen modified according to the representative patches close to it. We present\ntwo lightweight defenses in this framework, referred to as patched RandDisc\n(pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the\nformer and overlapping in the latter. We show that vector-quantization-based\ndefenses have certifiable robust accuracy and that pRD and swRD demonstrate\nstate-of-the-art performances, surpassing RandDisc by a large margin. Notably,\nthe proposed defenses possess the obfuscated gradients property. Our\nexperiments however show that pRD and swRD remain effective under the STE and\nEOT attacks, which are designed specifically for defenses with gradient\nobfuscation. ...\n","authors":["Zhiyi Dong","Yongyi Mao"],"pdf_url":"https://arxiv.org/pdf/2305.13651v2.pdf","comment":"This is the author-accepted version of our paper published in\n  Neurocomputing. The final published version is available at:\n  https://doi.org/10.1016/j.neucom.2025.130703"},{"id":"http://arxiv.org/abs/2507.07333v1","updated":"2025-07-09T23:19:28Z","published":"2025-07-09T23:19:28Z","title":"Scalable and Realistic Virtual Try-on Application for Foundation Makeup\n  with Kubelka-Munk Theory","summary":"  Augmented reality is revolutionizing beauty industry with virtual try-on\n(VTO) applications, which empowers users to try a wide variety of products\nusing their phones without the hassle of physically putting on real products. A\ncritical technical challenge in foundation VTO applications is the accurate\nsynthesis of foundation-skin tone color blending while maintaining the\nscalability of the method across diverse product ranges. In this work, we\npropose a novel method to approximate well-established Kubelka-Munk (KM) theory\nfor faster image synthesis while preserving foundation-skin tone color blending\nrealism. Additionally, we build a scalable end-to-end framework for realistic\nfoundation makeup VTO solely depending on the product information available on\ne-commerce sites. We validate our method using real-world makeup images,\ndemonstrating that our framework outperforms other techniques.\n","authors":["Hui Pang","Sunil Hadap","Violetta Shevchenko","Rahul Suresh","Amin Banitalebi-Dehkordi"],"pdf_url":"https://arxiv.org/pdf/2507.07333v1.pdf","comment":"Presented at the workshop Three questions about virtual try-on at\n  CVPR 2025"},{"id":"http://arxiv.org/abs/2507.07331v1","updated":"2025-07-09T23:11:14Z","published":"2025-07-09T23:11:14Z","title":"mmFlux: Crowd Flow Analytics with Commodity mmWave MIMO Radar","summary":"  In this paper, we present a novel framework for extracting underlying crowd\nmotion patterns and inferring crowd semantics using mmWave radar. First, our\nproposed signal processing pipeline combines optical flow estimation concepts\nfrom vision with novel statistical and morphological noise filtering to\ngenerate high-fidelity mmWave flow fields - compact 2D vector representations\nof crowd motion. We then introduce a novel approach that transforms these\nfields into directed geometric graphs, where edges capture dominant flow\ncurrents, vertices mark crowd splitting or merging, and flow distribution is\nquantified across edges. Finally, we show that by analyzing the local Jacobian\nand computing the corresponding curl and divergence, we can extract key crowd\nsemantics for both structured and diffused crowds. We conduct 21 experiments on\ncrowds of up to (and including) 20 people across 3 areas, using commodity\nmmWave radar. Our framework achieves high-fidelity graph reconstruction of the\nunderlying flow structure, even for complex crowd patterns, demonstrating\nstrong spatial alignment and precise quantitative characterization of flow\nsplit ratios. Finally, our curl and divergence analysis accurately infers key\ncrowd semantics, e.g., abrupt turns, boundaries where flow directions shift,\ndispersions, and gatherings. Overall, these findings validate our framework,\nunderscoring its potential for various crowd analytics applications.\n","authors":["Anurag Pallaprolu","Winston Hurst","Yasamin Mostofi"],"pdf_url":"https://arxiv.org/pdf/2507.07331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04946v2","updated":"2025-07-09T22:41:45Z","published":"2025-07-07T12:43:09Z","title":"Taming the Tri-Space Tension: ARC-Guided Hallucination Modeling and\n  Control for Text-to-Image Generation","summary":"  Despite remarkable progress in image quality and prompt fidelity,\ntext-to-image (T2I) diffusion models continue to exhibit persistent\n\"hallucinations\", where generated content subtly or significantly diverges from\nthe intended prompt semantics. While often regarded as unpredictable artifacts,\nwe argue that these failures reflect deeper, structured misalignments within\nthe generative process. In this work, we propose a cognitively inspired\nperspective that reinterprets hallucinations as trajectory drift within a\nlatent alignment space. Empirical observations reveal that generation unfolds\nwithin a multiaxial cognitive tension field, where the model must continuously\nnegotiate competing demands across three key critical axes: semantic coherence,\nstructural alignment, and knowledge grounding. We then formalize this\nthree-axis space as the \\textbf{Hallucination Tri-Space} and introduce the\nAlignment Risk Code (ARC): a dynamic vector representation that quantifies\nreal-time alignment tension during generation. The magnitude of ARC captures\noverall misalignment, its direction identifies the dominant failure axis, and\nits imbalance reflects tension asymmetry. Based on this formulation, we develop\nthe TensionModulator (TM-ARC): a lightweight controller that operates entirely\nin latent space. TM-ARC monitors ARC signals and applies targeted,\naxis-specific interventions during the sampling process. Extensive experiments\non standard T2I benchmarks demonstrate that our approach significantly reduces\nhallucination without compromising image quality or diversity. This framework\noffers a unified and interpretable approach for understanding and mitigating\ngenerative failures in diffusion-based T2I systems.\n","authors":["Jianjiang Yang","Ziyan Huang"],"pdf_url":"https://arxiv.org/pdf/2507.04946v2.pdf","comment":"We withdraw this paper due to significant visualization errors in\n  Figure 3 and 5 that affect the correctness of our core modeling claims and\n  may cause misinterpretation. These figures misrepresent ARC dynamics and\n  trajectory control"},{"id":"http://arxiv.org/abs/2507.07317v1","updated":"2025-07-09T22:29:47Z","published":"2025-07-09T22:29:47Z","title":"ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided\n  Image Editing Evaluation","summary":"  Recent advances in instruction-guided image editing underscore the need for\neffective automated evaluation. While Vision-Language Models (VLMs) have been\nexplored as judges, open-source models struggle with alignment, and proprietary\nmodels lack transparency and cost efficiency. Additionally, no public training\ndatasets exist to fine-tune open-source VLMs, only small benchmarks with\ndiverse evaluation schemes. To address this, we introduce ADIEE, an automated\ndataset creation approach which is then used to train a scoring model for\ninstruction-guided image editing evaluation. We generate a large-scale dataset\nwith over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified\nto decode a numeric score from a custom token. The resulting scorer outperforms\nall open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a\n0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,\nand improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench\nand 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the\nstate-of-the-art. The scorer can act as a reward model, enabling automated best\nedit selection and model fine-tuning. Notably, the proposed scorer can boost\nMagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43\n(+8.98%).\n","authors":["Sherry X. Chen","Yi Wei","Luowei Zhou","Suren Kumar"],"pdf_url":"https://arxiv.org/pdf/2507.07317v1.pdf","comment":"International Conference on Computer Vision (ICCV) 2025"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2503.19092v2","updated":"2025-07-09T22:09:49Z","published":"2025-03-24T19:24:40Z","title":"Rankers, Judges, and Assistants: Towards Understanding the Interplay of\n  LLMs in Information Retrieval Evaluation","summary":"  Large language models (LLMs) are increasingly integral to information\nretrieval (IR), powering ranking, evaluation, and AI-assisted content creation.\nThis widespread adoption necessitates a critical examination of potential\nbiases arising from the interplay between these LLM-based components. This\npaper synthesizes existing research and presents novel experiment designs that\nexplore how LLM-based rankers and assistants influence LLM-based judges. We\nprovide the first empirical evidence of LLM judges exhibiting significant bias\ntowards LLM-based rankers. Furthermore, we observe limitations in LLM judges'\nability to discern subtle system performance differences. Contrary to some\nprevious findings, our preliminary study does not find evidence of bias against\nAI-generated content. These results highlight the need for a more holistic view\nof the LLM-driven information ecosystem. To this end, we offer initial\nguidelines and a research agenda to ensure the reliable use of LLMs in IR\nevaluation.\n","authors":["Krisztian Balog","Donald Metzler","Zhen Qin"],"pdf_url":"https://arxiv.org/pdf/2503.19092v2.pdf","comment":"Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '25)"},{"id":"http://arxiv.org/abs/2507.07251v1","updated":"2025-07-09T19:48:33Z","published":"2025-07-09T19:48:33Z","title":"A Language-Driven Framework for Improving Personalized Recommendations:\n  Merging LLMs with Traditional Algorithms","summary":"  Traditional recommendation algorithms are not designed to provide\npersonalized recommendations based on user preferences provided through text,\ne.g., \"I enjoy light-hearted comedies with a lot of humor\". Large Language\nModels (LLMs) have emerged as one of the most promising tools for natural\nlanguage processing in recent years. This research proposes a novel framework\nthat mimics how a close friend would recommend items based on their knowledge\nof an individual's tastes. We leverage LLMs to enhance movie recommendation\nsystems by refining traditional algorithm outputs and integrating them with\nlanguage-based user preference inputs. We employ Singular Value Decomposition\n(SVD) or SVD++ algorithms to generate initial movie recommendations,\nimplemented using the Surprise Python library and trained on the\nMovieLens-Latest-Small dataset. We compare the performance of the base\nalgorithms with our LLM-enhanced versions using leave-one-out validation hit\nrates and cumulative hit rates. Additionally, to compare the performance of our\nframework against the current state-of-the-art recommendation systems, we use\nrating and ranking metrics with an item-based stratified 0.75 train, 0.25 test\nsplit. Our framework can generate preference profiles automatically based on\nusers' favorite movies or allow manual preference specification for more\npersonalized results. Using an automated approach, our framework overwhelmingly\nsurpassed SVD and SVD++ on every evaluation metric used (e.g., improvements of\nup to ~6x in cumulative hit rate, ~3.7x in NDCG, etc.), albeit at the cost of a\nslight increase in computational overhead.\n","authors":["Aaron Goldstein","Ayan Dutta"],"pdf_url":"https://arxiv.org/pdf/2507.07251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07064v1","updated":"2025-07-09T17:26:10Z","published":"2025-07-09T17:26:10Z","title":"Boosting Parameter Efficiency in LLM-Based Recommendation through\n  Sophisticated Pruning","summary":"  LLM-based recommender systems have made significant progress; however, the\ndeployment cost associated with the large parameter volume of LLMs still\nhinders their real-world applications. This work explores parameter pruning to\nimprove parameter efficiency while maintaining recommendation quality, thereby\nenabling easier deployment. Unlike existing approaches that focus primarily on\ninter-layer redundancy, we uncover intra-layer redundancy within components\nsuch as self-attention and MLP modules. Building on this analysis, we propose a\nmore fine-grained pruning approach that integrates both intra-layer and\nlayer-wise pruning. Specifically, we introduce a three-stage pruning strategy\nthat progressively prunes parameters at different levels and parts of the\nmodel, moving from intra-layer to layer-wise pruning, or from width to depth.\nEach stage also includes a performance restoration step using distillation\ntechniques, helping to strike a balance between performance and parameter\nefficiency. Empirical results demonstrate the effectiveness of our approach:\nacross three datasets, our models achieve an average of 88% of the original\nmodel's performance while pruning more than 95% of the non-embedding\nparameters. This underscores the potential of our method to significantly\nreduce resource requirements without greatly compromising recommendation\nquality. Our code will be available at: https://github.com/zheng-sl/PruneRec\n","authors":["Shanle Zheng","Keqin Bao","Jizhi Zhang","Yang Zhang","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2507.07064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06895v1","updated":"2025-07-09T14:33:07Z","published":"2025-07-09T14:33:07Z","title":"SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label\n  Contrastive Learning and Bayesian kNN","summary":"  The growing demand for efficient knowledge graph (KG) enrichment leveraging\nexternal corpora has intensified interest in relation extraction (RE),\nparticularly under low-supervision settings. To address the need for adaptable\nand noise-resilient RE solutions that integrate seamlessly with pre-trained\nlarge language models (PLMs), we introduce SCoRE, a modular and cost-effective\nsentence-level RE system. SCoRE enables easy PLM switching, requires no\nfinetuning, and adapts smoothly to diverse corpora and KGs. By combining\nsupervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)\nclassifier for multi-label classification, it delivers robust performance\ndespite the noisy annotations of distantly supervised corpora. To improve RE\nevaluation, we propose two novel metrics: Correlation Structure Distance (CSD),\nmeasuring the alignment between learned relational patterns and KG structures,\nand Precision at R (P@R), assessing utility as a recommender system. We also\nrelease Wiki20d, a benchmark dataset replicating real-world RE conditions where\nonly KG-derived annotations are available. Experiments on five benchmarks show\nthat SCoRE matches or surpasses state-of-the-art methods while significantly\nreducing energy consumption. Further analyses reveal that increasing model\ncomplexity, as seen in prior work, degrades performance, highlighting the\nadvantages of SCoRE's minimal design. Combining efficiency, modularity, and\nscalability, SCoRE stands as an optimal choice for real-world RE applications.\n","authors":["Luca Mariotti","Veronica Guidetti","Federica Mandreoli"],"pdf_url":"https://arxiv.org/pdf/2507.06895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06877v1","updated":"2025-07-09T14:15:47Z","published":"2025-07-09T14:15:47Z","title":"CDC: Causal Domain Clustering for Multi-Domain Recommendation","summary":"  Multi-domain recommendation leverages domain-general knowledge to improve\nrecommendations across several domains. However, as platforms expand to dozens\nor hundreds of scenarios, training all domains in a unified model leads to\nperformance degradation due to significant inter-domain differences. Existing\ndomain grouping methods, based on business logic or data similarities, often\nfail to capture the true transfer relationships required for optimal grouping.\nTo effectively cluster domains, we propose Causal Domain Clustering (CDC). CDC\nmodels domain transfer patterns within a large number of domains using two\ndistinct effects: the Isolated Domain Affinity Matrix for modeling\nnon-interactive domain transfers, and the Hybrid Domain Affinity Matrix for\nconsidering dynamic domain synergy or interference under joint training. To\nintegrate these two transfer effects, we introduce causal discovery to\ncalculate a cohesion-based coefficient that adaptively balances their\ncontributions. A Co-Optimized Dynamic Clustering algorithm iteratively\noptimizes target domain clustering and source domain selection for training.\nCDC significantly enhances performance across over 50 domains on public\ndatasets and in industrial settings, achieving a 4.9% increase in online eCPM.\nCode is available at\nhttps://github.com/Chrissie-Law/Causal-Domain-Clustering-for-Multi-Domain-Recommendation\n","authors":["Huishi Luo","Yiqing Wu","Yiwen Chen","Fuzhen Zhuang","Deqing Wang"],"pdf_url":"https://arxiv.org/pdf/2507.06877v1.pdf","comment":"Accepted at SIGIR 2025"},{"id":"http://arxiv.org/abs/2408.13484v2","updated":"2025-07-09T14:13:59Z","published":"2024-08-24T06:07:25Z","title":"IntOPE: Off-Policy Evaluation in the Presence of Interference","summary":"  Off-Policy Evaluation (OPE) is employed to assess the potential impact of a\nhypothetical policy using logged contextual bandit feedback, which is crucial\nin areas such as personalized medicine and recommender systems, where online\ninteractions are associated with significant risks and costs. Traditionally,\nOPE methods rely on the Stable Unit Treatment Value Assumption (SUTVA), which\nassumes that the reward for any given individual is unaffected by the actions\nof others. However, this assumption often fails in real-world scenarios due to\nthe presence of interference, where an individual's reward is affected not just\nby their own actions but also by the actions of their peers. This realization\nreveals significant limitations of existing OPE methods in real-world\napplications. To address this limitation, we propose IntIPW, an IPW-style\nestimator that extends the Inverse Probability Weighting (IPW) framework by\nintegrating marginalized importance weights to account for both individual\nactions and the influence of adjacent entities. Extensive experiments are\nconducted on both synthetic and real-world data to demonstrate the\neffectiveness of the proposed IntIPW method.\n","authors":["Yuqi Bai","Ziyu Zhao","Chenxin Lyu","Minqin Zhu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2408.13484v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06782v1","updated":"2025-07-09T12:16:11Z","published":"2025-07-09T12:16:11Z","title":"Temporal Information Retrieval via Time-Specifier Model Merging","summary":"  The rapid expansion of digital information and knowledge across structured\nand unstructured sources has heightened the importance of Information Retrieval\n(IR). While dense retrieval methods have substantially improved semantic\nmatching for general queries, they consistently underperform on queries with\nexplicit temporal constraints--often those containing numerical expressions and\ntime specifiers such as ``in 2015.'' Existing approaches to Temporal\nInformation Retrieval (TIR) improve temporal reasoning but often suffer from\ncatastrophic forgetting, leading to reduced performance on non-temporal\nqueries. To address this, we propose Time-Specifier Model Merging (TSM), a\nnovel method that enhances temporal retrieval while preserving accuracy on\nnon-temporal queries. TSM trains specialized retrievers for individual time\nspecifiers and merges them in to a unified model, enabling precise handling of\ntemporal constraints without compromising non-temporal retrieval. Extensive\nexperiments on both temporal and non-temporal datasets demonstrate that TSM\nsignificantly improves performance on temporally constrained queries while\nmaintaining strong results on non-temporal queries, consistently outperforming\nother baseline methods. Our code is available at\nhttps://github.com/seungyoonee/TSM .\n","authors":["SeungYoon Han","Taeho Hwang","Sukmin Cho","Soyeong Jeong","Hoyun Song","Huije Lee","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2507.06782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06715v1","updated":"2025-07-09T10:13:38Z","published":"2025-07-09T10:13:38Z","title":"CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and\n  Context Aware Text Generation with LLMs","summary":"  Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust.\n","authors":["Garapati Keerthana","Manik Gupta"],"pdf_url":"https://arxiv.org/pdf/2507.06715v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2506.23090v2","updated":"2025-07-09T09:50:43Z","published":"2025-06-29T05:05:13Z","title":"Multi-task Offline Reinforcement Learning for Online Advertising in\n  Recommender Systems","summary":"  Online advertising in recommendation platforms has gained significant\nattention, with a predominant focus on channel recommendation and budget\nallocation strategies. However, current offline reinforcement learning (RL)\nmethods face substantial challenges when applied to sparse advertising\nscenarios, primarily due to severe overestimation, distributional shifts, and\noverlooking budget constraints. To address these issues, we propose MTORL, a\nnovel multi-task offline RL model that targets two key objectives. First, we\nestablish a Markov Decision Process (MDP) framework specific to the nuances of\nadvertising. Then, we develop a causal state encoder to capture dynamic user\ninterests and temporal dependencies, facilitating offline RL through\nconditional sequence modeling. Causal attention mechanisms are introduced to\nenhance user sequence representations by identifying correlations among causal\nstates. We employ multi-task learning to decode actions and rewards,\nsimultaneously addressing channel recommendation and budget allocation.\nNotably, our framework includes an automated system for integrating these tasks\ninto online advertising. Extensive experiments on offline and online\nenvironments demonstrate MTORL's superiority over state-of-the-art methods.\n","authors":["Langming Liu","Wanyu Wang","Chi Zhang","Bo Li","Hongzhi Yin","Xuetao Wei","Wenbo Su","Bo Zheng","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2506.23090v2.pdf","comment":"KDD 2025"},{"id":"http://arxiv.org/abs/2507.06654v1","updated":"2025-07-09T08:38:46Z","published":"2025-07-09T08:38:46Z","title":"MS-DPPs: Multi-Source Determinantal Point Processes for Contextual\n  Diversity Refinement of Composite Attributes in Text to Image Retrieval","summary":"  Result diversification (RD) is a crucial technique in Text-to-Image Retrieval\nfor enhancing the efficiency of a practical application. Conventional methods\nfocus solely on increasing the diversity metric of image appearances. However,\nthe diversity metric and its desired value vary depending on the application,\nwhich limits the applications of RD. This paper proposes a novel task called\nCDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims\nto refine the diversities of multiple attributes, according to the\napplication's context. To address this task, we propose Multi-Source DPPs, a\nsimple yet strong baseline that extends the Determinantal Point Process (DPP)\nto multi-sources. We model MS-DPP as a single DPP model with a unified\nsimilarity matrix based on a manifold representation. We also introduce Tangent\nNormalization to reflect contexts. Extensive experiments demonstrate the\neffectiveness of the proposed method. Our code is publicly available at\nhttps://github.com/NEC-N-SOGI/msdpp.\n","authors":["Naoya Sogi","Takashi Shibata","Makoto Terao","Masanori Suganuma","Takayuki Okatani"],"pdf_url":"https://arxiv.org/pdf/2507.06654v1.pdf","comment":"IJCAI 2025. Code: https://github.com/NEC-N-SOGI/msdpp"},{"id":"http://arxiv.org/abs/2507.06596v1","updated":"2025-07-09T07:15:12Z","published":"2025-07-09T07:15:12Z","title":"Impacts of Mainstream-Driven Algorithms on Recommendations for Children\n  Across Domains: A Reproducibility Study","summary":"  Children are often exposed to items curated by recommendation algorithms.\nYet, research seldom considers children as a user group, and when it does, it\nis anchored on datasets where children are underrepresented, risking\noverlooking their interests, favoring those of the majority, i.e., mainstream\nusers. Recently, Ungruh et al. demonstrated that children's consumption\npatterns and preferences differ from those of mainstream users, resulting in\ninconsistent recommendation algorithm performance and behavior for this user\ngroup. These findings, however, are based on two datasets with a limited child\nuser sample. We reproduce and replicate this study on a wider range of datasets\nin the movie, music, and book domains, uncovering interaction patterns and\naspects of child-recommender interactions consistent across domains, as well as\nthose specific to some user samples in the data. We also extend insights from\nthe original study with popularity bias metrics, given the interpretation of\nresults from the original study. With this reproduction and extension, we\nuncover consumption patterns and differences between age groups stemming from\nintrinsic differences between children and others, and those unique to specific\ndatasets or domains.\n","authors":["Robin Ungruh","Alejandro Bellogín","Dominik Kowald","Maria Soledad Pera"],"pdf_url":"https://arxiv.org/pdf/2507.06596v1.pdf","comment":"Preprint of accepted RecSys 2025 contribution"},{"id":"http://arxiv.org/abs/2507.06563v1","updated":"2025-07-09T05:32:02Z","published":"2025-07-09T05:32:02Z","title":"DS@GT at CheckThat! 2025: Exploring Retrieval and Reranking Pipelines\n  for Scientific Claim Source Retrieval on Social Media Discourse","summary":"  Social media users often make scientific claims without citing where these\nclaims come from, generating a need to verify these claims. This paper details\nwork done by the DS@GT team for CLEF 2025 CheckThat! Lab Task 4b Scientific\nClaim Source Retrieval which seeks to find relevant scientific papers based on\nimplicit references in tweets. Our team explored 6 different data augmentation\ntechniques, 7 different retrieval and reranking pipelines, and finetuned a\nbi-encoder. Achieving an MRR@5 of 0.58, our team ranked 16th out of 30 teams\nfor the CLEF 2025 CheckThat! Lab Task 4b, and improvement of 0.15 over the BM25\nbaseline of 0.43. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4b.\n","authors":["Jeanette Schofield","Shuyu Tian","Hoang Thanh Thanh Truong","Maximilian Heil"],"pdf_url":"https://arxiv.org/pdf/2507.06563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06554v1","updated":"2025-07-09T05:13:09Z","published":"2025-07-09T05:13:09Z","title":"SPEAR: Subset-sampled Performance Evaluation via Automated Ground Truth\n  Generation for RAG","summary":"  Retrieval-Augmented Generation (RAG) is a core approach for enhancing Large\nLanguage Models (LLMs), where the effectiveness of the retriever largely\ndetermines the overall response quality of RAG systems. Retrievers encompass a\nmultitude of hyperparameters that significantly impact performance outcomes and\ndemonstrate sensitivity to specific applications. Nevertheless, hyperparameter\noptimization entails prohibitively high computational expenses. Existing\nevaluation methods suffer from either prohibitive costs or disconnection from\ndomain-specific scenarios. This paper proposes SEARA (Subset sampling\nEvaluation for Automatic Retriever Assessment), which addresses evaluation data\nchallenges through subset sampling techniques and achieves robust automated\nretriever evaluation by minimal retrieval facts extraction and comprehensive\nretrieval metrics. Based on real user queries, this method enables fully\nautomated retriever evaluation at low cost, thereby obtaining optimal retriever\nfor specific business scenarios. We validate our method across classic RAG\napplications in rednote, including knowledge-based Q&A system and\nretrieval-based travel assistant, successfully obtaining scenario-specific\noptimal retrievers.\n","authors":["Zou Yuheng","Wang Yiran","Tian Yuzhu","Zhu Min","Huang Yanhua"],"pdf_url":"https://arxiv.org/pdf/2507.06554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12399v2","updated":"2025-07-09T03:59:49Z","published":"2025-01-08T07:50:50Z","title":"FinSphere, a Real-Time Stock Analysis Agent Powered by Instruction-Tuned\n  LLMs and Domain Tools","summary":"  Current financial large language models (FinLLMs) struggle with two critical\nlimitations: the absence of objective evaluation metrics to assess the quality\nof stock analysis reports and a lack of depth in stock analysis, which impedes\ntheir ability to generate professional-grade insights. To address these\nchallenges, this paper introduces FinSphere, a stock analysis agent, along with\nthree major contributions: (1) AnalyScore, a systematic evaluation framework\nfor assessing stock analysis quality, (2) Stocksis, a dataset curated by\nindustry experts to enhance LLMs' stock analysis capabilities, and (3)\nFinSphere, an AI agent that can generate high-quality stock analysis reports in\nresponse to user queries. Experiments demonstrate that FinSphere achieves\nsuperior performance compared to both general and domain-specific LLMs, as well\nas existing agent-based systems, even when they are enhanced with real-time\ndata access and few-shot guidance. The integrated framework, which combines\nreal-time data feeds, quantitative tools, and an instruction-tuned LLM, yields\nsubstantial improvements in both analytical quality and practical applicability\nfor real-world stock analysis.\n","authors":["Shijie Han","Jingshu Zhang","Yiqing Shen","Kaiyuan Yan","Hongguang Li"],"pdf_url":"https://arxiv.org/pdf/2501.12399v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08740v2","updated":"2025-07-09T03:54:32Z","published":"2024-10-11T11:59:40Z","title":"Hespi: A pipeline for automatically detecting information from hebarium\n  specimen sheets","summary":"  Specimen-associated biodiversity data are crucial for biological,\nenvironmental, and conservation sciences. A rate shift is needed to extract\ndata from specimen images efficiently, moving beyond human-mediated\ntranscription. We developed `Hespi' (HErbarium Specimen sheet PIpeline) using\nadvanced computer vision techniques to extract pre-catalogue data from primary\nspecimen labels on herbarium specimens. Hespi integrates two object detection\nmodels: one for detecting the components of the sheet and another for fields on\nthe primary primary specimen label. It classifies labels as printed, typed,\nhandwritten, or mixed and uses Optical Character Recognition (OCR) and\nHandwritten Text Recognition (HTR) for extraction. The text is then corrected\nagainst authoritative taxon databases and refined using a multimodal Large\nLanguage Model (LLM). Hespi accurately detects and extracts text from specimen\nsheets across international herbaria, and its modular design allows users to\ntrain and integrate custom models.\n","authors":["Robert Turnbull","Emily Fitzgerald","Karen Thompson","Joanne L. Birch"],"pdf_url":"https://arxiv.org/pdf/2410.08740v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2507.06507v1","updated":"2025-07-09T03:13:08Z","published":"2025-07-09T03:13:08Z","title":"GR-LLMs: Recent Advances in Generative Recommendation Based on Large\n  Language Models","summary":"  In the past year, Generative Recommendations (GRs) have undergone substantial\nadvancements, especially in leveraging the powerful sequence modeling and\nreasoning capabilities of Large Language Models (LLMs) to enhance overall\nrecommendation performance. LLM-based GRs are forming a new paradigm that is\ndistinctly different from discriminative recommendations, showing strong\npotential to replace traditional recommendation systems heavily dependent on\ncomplex hand-crafted features. In this paper, we provide a comprehensive survey\naimed at facilitating further research of LLM-based GRs. Initially, we outline\nthe general preliminaries and application cases of LLM-based GRs. Subsequently,\nwe introduce the main considerations when LLM-based GRs are applied in real\nindustrial scenarios. Finally, we explore promising directions for LLM-based\nGRs. We hope that this survey contributes to the ongoing advancement of the GR\ndomain.\n","authors":["Zhen Yang","Haitao Lin","Jiawei xue","Ziji Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.06507v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.07030v1","updated":"2025-07-09T17:02:40Z","published":"2025-07-09T17:02:40Z","title":"UniConv: Unifying Retrieval and Response Generation for Large Language\n  Models in Conversations","summary":"  The rapid advancement of conversational search systems revolutionizes how\ninformation is accessed by enabling the multi-turn interaction between the user\nand the system. Existing conversational search systems are usually built with\ntwo different models. This separation restricts the system from leveraging the\nintrinsic knowledge of the models simultaneously, which cannot ensure the\neffectiveness of retrieval benefiting the generation. The existing studies for\ndeveloping unified models cannot fully address the aspects of understanding\nconversational context, managing retrieval independently, and generating\nresponses. In this paper, we explore how to unify dense retrieval and response\ngeneration for large language models in conversation. We conduct joint\nfine-tuning with different objectives and design two mechanisms to reduce the\ninconsistency risks while mitigating data discrepancy. The evaluations on five\nconversational search datasets demonstrate that our unified model can mutually\nimprove both tasks and outperform the existing baselines.\n","authors":["Fengran Mo","Yifan Gao","Chuan Meng","Xin Liu","Zhuofeng Wu","Kelong Mao","Zhengyang Wang","Pei Chen","Zheng Li","Xian Li","Bing Yin","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2507.07030v1.pdf","comment":"Accepted by ACL 2025 (main)"}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.07270v1","updated":"2025-07-09T20:32:09Z","published":"2025-07-09T20:32:09Z","title":"Audio-Visual Speech Separation via Bottleneck Iterative Network","summary":"  Integration of information from non-auditory cues can significantly improve\nthe performance of speech-separation models. Often such models use deep\nmodality-specific networks to obtain unimodal features, and risk being too\ncostly or lightweight but lacking capacity. In this work, we present an\niterative representation refinement approach called Bottleneck Iterative\nNetwork (BIN), a technique that repeatedly progresses through a lightweight\nfusion block, while bottlenecking fusion representations by fusion tokens. This\nhelps improve the capacity of the model, while avoiding major increase in model\nsize and balancing between the model performance and training cost. We test BIN\non challenging noisy audio-visual speech separation tasks, and show that our\napproach consistently outperforms state-of-the-art benchmark models with\nrespect to SI-SDRi on NTCD-TIMIT and LRS3+WHAM! datasets, while simultaneously\nachieving a reduction of more than 50% in training and GPU inference time\nacross nearly all settings.\n","authors":["Sidong Zhang","Shiv Shankar","Trang Nguyen","Andrea Fanelli","Madalina Fiterau"],"pdf_url":"https://arxiv.org/pdf/2507.07270v1.pdf","comment":"Accepted to the 42nd International Conference on Machine Learning\n  Workshop on Machine Learning for Audio"},{"id":"http://arxiv.org/abs/2507.07250v1","updated":"2025-07-09T19:47:40Z","published":"2025-07-09T19:47:40Z","title":"Semi-fragile watermarking of remote sensing images using DWT, vector\n  quantization and automatic tiling","summary":"  A semi-fragile watermarking scheme for multiple band images is presented in\nthis article. We propose to embed a mark into remote sensing images applying a\ntree-structured vector quantization approach to the pixel signatures instead of\nprocessing each band separately. The signature of the multispectral or\nhyperspectral image is used to embed the mark in it order to detect any\nsignificant modification of the original image. The image is segmented into\nthree-dimensional blocks, and a tree-structured vector quantizer is built for\neach block. These trees are manipulated using an iterative algorithm until the\nresulting block satisfies a required criterion, which establishes the embedded\nmark. The method is shown to be able to preserve the mark under lossy\ncompression (above a given threshold) but, at the same time, it detects\npossibly forged blocks and their position in the whole image.\n","authors":["Jordi Serra-Ruiz","David Megías"],"pdf_url":"https://arxiv.org/pdf/2507.07250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07015v1","updated":"2025-07-09T16:45:28Z","published":"2025-07-09T16:45:28Z","title":"MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge\n  Distillation","summary":"  Knowledge distillation as an efficient knowledge transfer technique, has\nachieved remarkable success in unimodal scenarios. However, in cross-modal\nsettings, conventional distillation methods encounter significant challenges\ndue to data and statistical heterogeneities, failing to leverage the\ncomplementary prior knowledge embedded in cross-modal teacher models. This\npaper empirically reveals two critical issues in existing approaches:\ndistillation path selection and knowledge drift. To address these limitations,\nwe propose MST-Distill, a novel cross-modal knowledge distillation framework\nfeaturing a mixture of specialized teachers. Our approach employs a diverse\nensemble of teacher models across both cross-modal and multimodal\nconfigurations, integrated with an instance-level routing network that\nfacilitates adaptive and dynamic distillation. This architecture effectively\ntranscends the constraints of traditional methods that rely on monotonous and\nstatic teacher models. Additionally, we introduce a plug-in masking module,\nindependently trained to suppress modality-specific discrepancies and\nreconstruct teacher representations, thereby mitigating knowledge drift and\nenhancing transfer effectiveness. Extensive experiments across five diverse\nmultimodal datasets, spanning visual, audio, and text, demonstrate that our\nmethod significantly outperforms existing state-of-the-art knowledge\ndistillation methods in cross-modal distillation tasks. The source code is\navailable at https://github.com/Gray-OREO/MST-Distill.\n","authors":["Hui Li","Pengfei Yang","Juanyang Chen","Le Dong","Yanxin Chen","Quan Wang"],"pdf_url":"https://arxiv.org/pdf/2507.07015v1.pdf","comment":"Accepted to ACM MM 2025 (The 33rd ACM International Conference on\n  Multimedia)"},{"id":"http://arxiv.org/abs/2403.05427v5","updated":"2025-07-09T13:28:57Z","published":"2024-03-08T16:24:42Z","title":"Reply with Sticker: New Dataset and Model for Sticker Retrieval","summary":"  Using stickers in online chatting is very prevalent on social media\nplatforms, where the stickers used in the conversation can express someone's\nintention/emotion/attitude in a vivid, tactful, and intuitive way. Existing\nsticker retrieval research typically retrieves stickers based on context and\nthe current utterance delivered by the user. That is, the stickers serve as a\nsupplement to the current utterance. However, in the real-world scenario, using\nstickers to express what we want to say rather than as a supplement to our\nwords only is also important. Therefore, in this paper, we create a new dataset\nfor sticker retrieval in conversation, called \\textbf{StickerInt}, where\nstickers are used to reply to previous conversations or supplement our words.\nBased on the created dataset, we present a simple yet effective framework for\nsticker retrieval in conversation based on the learning of intention and the\ncross-modal relationships between conversation context and stickers, coined as\n\\textbf{Int-RA}. Specifically, we first devise a knowledge-enhanced intention\npredictor to introduce the intention information into the conversation\nrepresentations. Subsequently, a relation-aware sticker selector is devised to\nretrieve the response sticker via cross-modal relationships. Extensive\nexperiments on two datasets show that the proposed model achieves\nstate-of-the-art performance and generalization capability in sticker\nretrieval. The dataset and source code of this work are released at\nhttps://github.com/HITSZ-HLT/Int-RA.\n","authors":["Bin Liang","Bingbing Wang","Zhixin Bai","Qiwei Lang","Mingwei Sun","Kaiheng Hou","Lanjun Zhou","Ruifeng Xu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.05427v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06744v1","updated":"2025-07-09T10:59:13Z","published":"2025-07-09T10:59:13Z","title":"Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised\n  Text-to-Person Image Matching","summary":"  Weakly supervised text-to-person image matching, as a crucial approach to\nreducing models' reliance on large-scale manually labeled samples, holds\nsignificant research value. However, existing methods struggle to predict\ncomplex one-to-many identity relationships, severely limiting performance\nimprovements. To address this challenge, we propose a local-and-global\ndual-granularity identity association mechanism. Specifically, at the local\nlevel, we explicitly establish cross-modal identity relationships within a\nbatch, reinforcing identity constraints across different modalities and\nenabling the model to better capture subtle differences and correlations. At\nthe global level, we construct a dynamic cross-modal identity association\nnetwork with the visual modality as the anchor and introduce a confidence-based\ndynamic adjustment mechanism, effectively enhancing the model's ability to\nidentify weakly associated samples while improving overall sensitivity.\nAdditionally, we propose an information-asymmetric sample pair construction\nmethod combined with consistency learning to tackle hard sample mining and\nenhance model robustness. Experimental results demonstrate that the proposed\nmethod substantially boosts cross-modal matching accuracy, providing an\nefficient and practical solution for text-to-person image matching.\n","authors":["Yafei Zhang","Yongle Shang","Huafeng Li"],"pdf_url":"https://arxiv.org/pdf/2507.06744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06735v1","updated":"2025-07-09T10:48:00Z","published":"2025-07-09T10:48:00Z","title":"Residual Prior-driven Frequency-aware Network for Image Fusion","summary":"  Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task.\n","authors":["Guan Zheng","Xue Wang","Wenhua Qian","Peng Liu","Runzhuo Ma"],"pdf_url":"https://arxiv.org/pdf/2507.06735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06717v1","updated":"2025-07-09T10:15:46Z","published":"2025-07-09T10:15:46Z","title":"QoE Optimization for Semantic Self-Correcting Video Transmission in\n  Multi-UAV Networks","summary":"  Real-time unmanned aerial vehicle (UAV) video streaming is essential for\ntime-sensitive applications, including remote surveillance, emergency response,\nand environmental monitoring. However, it faces challenges such as limited\nbandwidth, latency fluctuations, and high packet loss. To address these issues,\nwe propose a novel semantic self-correcting video transmission framework with\nultra-fine bitrate granularity (SSCV-G). In SSCV-G, video frames are encoded\ninto a compact semantic codebook space, and the transmitter adaptively sends a\nsubset of semantic indices based on bandwidth availability, enabling\nfine-grained bitrate control for improved bandwidth efficiency. At the\nreceiver, a spatio-temporal vision transformer (ST-ViT) performs multi-frame\njoint decoding to reconstruct dropped semantic indices by modeling intra- and\ninter-frame dependencies. To further improve performance under dynamic network\nconditions, we integrate a multi-user proximal policy optimization (MUPPO)\nreinforcement learning scheme that jointly optimizes communication resource\nallocation and semantic bitrate selection to maximize user Quality of\nExperience (QoE). Extensive experiments demonstrate that the proposed SSCV-G\nsignificantly outperforms state-of-the-art video codecs in coding efficiency,\nbandwidth adaptability, and packet loss robustness. Moreover, the proposed\nMUPPO-based QoE optimization consistently surpasses existing benchmarks.\n","authors":["Xuyang Chen","Chong Huang","Daquan Feng","Lei Luo","Yao Sun","Xiang-Gen Xia"],"pdf_url":"https://arxiv.org/pdf/2507.06717v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2503.08120v3","updated":"2025-07-09T03:25:22Z","published":"2025-03-11T07:34:59Z","title":"UniF$^2$ace: Fine-grained Face Understanding and Generation with Unified\n  Multimodal Models","summary":"  Unified multimodal models (UMMs) have emerged as a powerful paradigm in\nfoundational computer vision research, demonstrating significant potential in\nboth image understanding and generation. However, existing research in the face\ndomain primarily focuses on $\\textbf{coarse}$ facial attribute understanding,\nwith limited capacity to handle $\\textbf{fine-grained}$ facial attributes and\nwithout addressing generation capabilities. To overcome these limitations, we\npropose UniF$^2$ace, the first UMM tailored specifically for fine-grained face\nunderstanding and generation. In general, we train UniF$^2$ace on a\nself-constructed, specialized dataset utilizing two mutually beneficial\ndiffusion techniques and a two-level mixture-of-experts architecture.\nSpecifically, we first build a large-scale facial dataset, UniF$^2$ace-130K,\nwhich contains 130K image-text pairs with one million question-answering pairs\nthat span a wide range of facial attributes. Second, we establish a theoretical\nconnection between discrete diffusion score matching and masked generative\nmodels, optimizing both evidence lower bounds simultaneously, which\nsignificantly improves the model's ability to synthesize facial details.\nFinally, we introduce both token-level and sequence-level mixture-of-experts,\nenabling efficient fine-grained representation learning for both understanding\nand generation tasks. Extensive experiments on UniF$^2$ace-130K demonstrate\nthat UniF$^2$ace outperforms existing UMMs and generative models, achieving\nsuperior performance across both understanding and generation tasks.\n","authors":["Junzhe Li","Xuerui Qiu","Linrui Xu","Liya Guo","Delin Qu","Tingting Long","Chun Fan","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2503.08120v3.pdf","comment":null}]},"2025-07-08T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.02009v2","updated":"2025-07-08T23:08:38Z","published":"2025-07-02T03:36:15Z","title":"Uncertainty-Aware Complex Scientific Table Data Extraction","summary":"  Table structure recognition (TSR) and optical character recognition (OCR)\nplay crucial roles in extracting structured data from tables in scientific\ndocuments. However, existing extraction frameworks built on top of TSR and OCR\nmethods often fail to quantify the uncertainties of extracted results. To\nobtain highly accurate data for scientific domains, all extracted data must be\nmanually verified, which can be time-consuming and labor-intensive. We propose\na framework that performs uncertainty-aware data extraction for complex\nscientific tables, built on conformal prediction, a model-agnostic method for\nuncertainty quantification (UQ). We explored various uncertainty scoring\nmethods to aggregate the uncertainties introduced by TSR and OCR. We rigorously\nevaluated the framework using a standard benchmark and an in-house dataset\nconsisting of complex scientific tables in six scientific domains. The results\ndemonstrate the effectiveness of using UQ for extraction error detection, and\nby manually verifying only 47% of extraction results, the data quality can be\nimproved by 30%. Our work quantitatively demonstrates the role of UQ with the\npotential of improving the efficiency in the human-machine cooperation process\nto obtain scientifically usable data from complex tables in scientific\ndocuments. All code and data are available on GitHub at\nhttps://github.com/lamps-lab/TSR-OCR-UQ/tree/main.\n","authors":["Kehinde Ajayi","Yi He","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2507.02009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06121v1","updated":"2025-07-08T16:05:18Z","published":"2025-07-08T16:05:18Z","title":"Unconditional Diffusion for Generative Sequential Recommendation","summary":"  Diffusion models, known for their generative ability to simulate data\ncreation through noise-adding and denoising processes, have emerged as a\npromising approach for building generative recommenders. To incorporate user\nhistory for personalization, existing methods typically adopt a conditional\ndiffusion framework, where the reverse denoising process of reconstructing\nitems from noise is modified to be conditioned on the user history. However,\nthis design may fail to fully utilize historical information, as it gets\ndistracted by the need to model the \"item $\\leftrightarrow$ noise\" translation.\nThis motivates us to reformulate the diffusion process for sequential\nrecommendation in an unconditional manner, treating user history (instead of\nnoise) as the endpoint of the forward diffusion process (i.e., the starting\npoint of the reverse process), rather than as a conditional input. This\nformulation allows for exclusive focus on modeling the \"item $\\leftrightarrow$\nhistory\" translation. To this end, we introduce Brownian Bridge Diffusion\nRecommendation (BBDRec). By leveraging a Brownian bridge process, BBDRec\nenforces a structured noise addition and denoising mechanism, ensuring that the\ntrajectories are constrained towards a specific endpoint -- user history,\nrather than noise. Extensive experiments demonstrate BBDRec's effectiveness in\nenhancing sequential recommendation performance. The source code is available\nat https://github.com/baiyimeng/BBDRec.\n","authors":["Yimeng Bai","Yang Zhang","Sihao Ding","Shaohui Ruan","Han Yao","Danhui Guan","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2507.06121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06093v1","updated":"2025-07-08T15:35:19Z","published":"2025-07-08T15:35:19Z","title":"Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot\n  Multi-Species Plant Identification","summary":"  We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on\nmulti-species plant identification in vegetation quadrat images. Our pipeline\ncombines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level\ninference, (ii) a 4x4 tiling strategy that aligns patch size with the network's\n518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +\nK-Means visual clustering and geolocation filtering. Tile predictions are\naggregated by majority vote and re-weighted with cluster-specific Bayesian\npriors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while\nrequiring no additional training. All code, configuration files, and\nreproducibility scripts are publicly available at\nhttps://github.com/dsgt-arc/plantclef-2025.\n","authors":["Murilo Gustineli","Anthony Miyaguchi","Adrian Cheung","Divyansh Khattak"],"pdf_url":"https://arxiv.org/pdf/2507.06093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06090v1","updated":"2025-07-08T15:30:49Z","published":"2025-07-08T15:30:49Z","title":"Nyay-Darpan: Enhancing Decision Making Through Summarization and Case\n  Retrieval for Consumer Law in India","summary":"  AI-based judicial assistance and case prediction have been extensively\nstudied in criminal and civil domains, but remain largely unexplored in\nconsumer law, especially in India. In this paper, we present Nyay-Darpan, a\nnovel two-in-one framework that (i) summarizes consumer case files and (ii)\nretrieves similar case judgements to aid decision-making in consumer dispute\nresolution. Our methodology not only addresses the gap in consumer law AI tools\nbut also introduces an innovative approach to evaluate the quality of the\nsummary. The term 'Nyay-Darpan' translates into 'Mirror of Justice',\nsymbolizing the ability of our tool to reflect the core of consumer disputes\nthrough precise summarization and intelligent case retrieval. Our system\nachieves over 75 percent accuracy in similar case prediction and approximately\n70 percent accuracy across material summary evaluation metrics, demonstrating\nits practical effectiveness. We will publicly release the Nyay-Darpan framework\nand dataset to promote reproducibility and facilitate further research in this\nunderexplored yet impactful domain.\n","authors":["Swapnil Bhattacharyya","Shrey Ganatra","Harshvivek Kashid","Spandan Anaokar","Shruti Nair","Reshma Sekhar","Siddharth Manohar","Rahul Hemrajani","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2507.06090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06070v1","updated":"2025-07-08T15:13:26Z","published":"2025-07-08T15:13:26Z","title":"Contrastive and Transfer Learning for Effective Audio Fingerprinting\n  through a Real-World Evaluation Protocol","summary":"  Recent advances in song identification leverage deep neural networks to learn\ncompact audio fingerprints directly from raw waveforms. While these methods\nperform well under controlled conditions, their accuracy drops significantly in\nreal-world scenarios where the audio is captured via mobile devices in noisy\nenvironments. In this paper, we introduce a novel evaluation protocol designed\nto better reflect such real-world conditions. We generate three recordings of\nthe same audio, each with increasing levels of noise, captured using a mobile\ndevice's microphone. Our results reveal a substantial performance drop for two\nstate-of-the-art CNN-based models under this protocol, compared to previously\nreported benchmarks. Additionally, we highlight the critical role of the\naugmentation pipeline during training with contrastive loss. By introduction\nlow pass and high pass filters in the augmentation pipeline we significantly\nincrease the performance of both systems in our proposed evaluation.\nFurthermore, we develop a transformer-based model with a tailored projection\nmodule and demonstrate that transferring knowledge from a semantically relevant\ndomain yields a more robust solution. The transformer architecture outperforms\nCNN-based models across all noise levels, and query durations. In low noise\nconditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in\nfinding the correct song, surpassing by 14%, and by 18.5% the second-best\nperforming model, respectively, Under heavy noise levels, we achieve a\ndetection rate 56.5% for 15-second query duration. All experiments are\nconducted on public large-scale dataset of over 100K songs, with queries\nmatched against a database of 56 million vectors.\n","authors":["Christos Nikou","Theodoros Giannakopoulos"],"pdf_url":"https://arxiv.org/pdf/2507.06070v1.pdf","comment":"International Journal of Music Science, Technology and Art, 15 pages,\n  7 figures"},{"id":"http://arxiv.org/abs/2507.06044v1","updated":"2025-07-08T14:45:47Z","published":"2025-07-08T14:45:47Z","title":"Hierarchical Interaction Summarization and Contrastive Prompting for\n  Explainable Recommendations","summary":"  Explainable recommendations, which use the information of user and item with\ninteraction to generate a explanation for why the user would interact with the\nitem, are crucial for improving user trust and decision transparency to the\nrecommender system. Existing methods primarily rely on encoding features of\nusers and items to embeddings, which often leads to information loss due to\ndimensionality reduction, sparse interactions, and so on. With the advancements\nof large language models (LLMs) in language comprehension, some methods use\nembeddings as LLM inputs for explanation generation. However, since embeddings\nlack inherent semantics, LLMs must adjust or extend their parameters to\ninterpret them, a process that inevitably incurs information loss. To address\nthis issue, we propose a novel approach combining profile generation via\nhierarchical interaction summarization (PGHIS), which leverages a pretrained\nLLM to hierarchically summarize user-item interactions, generating structured\ntextual profiles as explicit representations of user and item characteristics.\nAdditionally, we propose contrastive prompting for explanation generation\n(CPEG) which employs contrastive learning to guide another reasoning language\nmodels in producing high-quality ground truth recommendation explanations.\nFinally, we use the textual profiles of user and item as input and high-quality\nexplanation as output to fine-tune a LLM for generating explanations.\nExperimental results on multiple datasets demonstrate that our approach\noutperforms existing state-of-the-art methods, achieving a great improvement on\nmetrics about explainability (e.g., 5% on GPTScore) and text quality.\nFurthermore, our generated ground truth explanations achieve a significantly\nhigher win rate compared to user-written reviews and those produced by other\nmethods, demonstrating the effectiveness of CPEG in generating high-quality\nground truths.\n","authors":["Yibin Liu","Ang Li","Shijian Li"],"pdf_url":"https://arxiv.org/pdf/2507.06044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08773v2","updated":"2025-07-08T13:52:12Z","published":"2025-04-03T14:31:40Z","title":"Counterfactual Inference under Thompson Sampling","summary":"  Recommender systems exemplify sequential decision-making under uncertainty,\nstrategically deciding what content to serve to users, to optimise a range of\npotential objectives. To balance the explore-exploit trade-off successfully,\nThompson sampling provides a natural and widespread paradigm to\nprobabilistically select which action to take. Questions of causal and\ncounterfactual inference, which underpin use-cases like offline evaluation, are\nnot straightforward to answer in these contexts. Specifically, whilst most\nexisting estimators rely on action propensities, these are not readily\navailable under Thompson sampling procedures.\n  We derive exact and efficiently computable expressions for action\npropensities under a variety of parameter and outcome distributions, enabling\nthe use of off-policy estimators in Thompson sampling scenarios. This opens up\na range of practical use-cases where counterfactual inference is crucial,\nincluding unbiased offline evaluation of recommender systems, as well as\ngeneral applications of causal inference in online advertising,\npersonalisation, and beyond.\n","authors":["Olivier Jeunen"],"pdf_url":"https://arxiv.org/pdf/2504.08773v2.pdf","comment":"To appear in the Nineteenth ACM Conference on Recommender Systems\n  (RecSys '25)"},{"id":"http://arxiv.org/abs/2507.05976v1","updated":"2025-07-08T13:32:50Z","published":"2025-07-08T13:32:50Z","title":"Enhancing the Interpretability of Rule-based Explanations through\n  Information Retrieval","summary":"  The lack of transparency of data-driven Artificial Intelligence techniques\nlimits their interpretability and acceptance into healthcare decision-making\nprocesses. We propose an attribution-based approach to improve the\ninterpretability of Explainable AI-based predictions in the specific context of\narm lymphedema's risk assessment after lymph nodal radiotherapy in breast\ncancer. The proposed method performs a statistical analysis of the attributes\nin the rule-based prediction model using standard metrics from Information\nRetrieval techniques. This analysis computes the relevance of each attribute to\nthe prediction and provides users with interpretable information about the\nimpact of risk factors. The results of a user study that compared the output\ngenerated by the proposed approach with the raw output of the Explainable AI\nmodel suggested higher levels of interpretability and usefulness in the context\nof predicting lymphedema risk.\n","authors":["Alessandro Umbrico","Guido Bologna","Luca Coraci","Francesca Fracasso","Silvia Gola","Gabriella Cortellessa"],"pdf_url":"https://arxiv.org/pdf/2507.05976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05933v1","updated":"2025-07-08T12:33:11Z","published":"2025-07-08T12:33:11Z","title":"Semantic Certainty Assessment in Vector Retrieval Systems: A Novel\n  Framework for Embedding Quality Evaluation","summary":"  Vector retrieval systems exhibit significant performance variance across\nqueries due to heterogeneous embedding quality. We propose a lightweight\nframework for predicting retrieval performance at the query level by combining\nquantization robustness and neighborhood density metrics. Our approach is\nmotivated by the observation that high-quality embeddings occupy geometrically\nstable regions in the embedding space and exhibit consistent neighborhood\nstructures. We evaluate our method on 4 standard retrieval datasets, showing\nconsistent improvements of 9.4$\\pm$1.2\\% in Recall@10 over competitive\nbaselines. The framework requires minimal computational overhead (less than 5\\%\nof retrieval time) and enables adaptive retrieval strategies. Our analysis\nreveals systematic patterns in embedding quality across different query types,\nproviding insights for targeted training data augmentation.\n","authors":["Y. Du"],"pdf_url":"https://arxiv.org/pdf/2507.05933v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2507.05880v1","updated":"2025-07-08T11:04:17Z","published":"2025-07-08T11:04:17Z","title":"RecRankerEval: A Flexible and Extensible Framework for Top-k LLM-based\n  Recommendation","summary":"  A recent Large language model (LLM)-based recommendation model, called\nRecRanker, has demonstrated a superior performance in the top-k recommendation\ntask compared to other models. In particular, RecRanker samples users via\nclustering, generates an initial ranking list using an initial recommendation\nmodel, and fine-tunes an LLM through hybrid instruction tuning to infer user\npreferences. However, the contribution of each core component remains\nunderexplored. In this work, we inspect the reproducibility of RecRanker, and\nstudy the impact and role of its various components. We begin by reproducing\nthe RecRanker pipeline through the implementation of all its key components.\nOur reproduction shows that the pairwise and listwise methods achieve a\nperformance comparable to that reported in the original paper. For the\npointwise method, while we are also able to reproduce the original paper's\nresults, further analysis shows that the performance is abnormally high due to\ndata leakage from the inclusion of ground-truth information in the prompts. To\nenable a fair and comprehensive evaluation of LLM-based top-k recommendations,\nwe propose RecRankerEval, an extensible framework that covers five key\ndimensions: user sampling strategy, initial recommendation model, LLM backbone,\ndataset selection, and instruction tuning method. Using the RecRankerEval\nframework, we show that the original results of RecRanker can be reproduced on\nthe ML-100K and ML-1M datasets, as well as the additional Amazon-Music dataset,\nbut not on BookCrossing due to the lack of timestamp information in the\noriginal RecRanker paper. Furthermore, we demonstrate that RecRanker's\nperformance can be improved by employing alternative user sampling methods,\nstronger initial recommenders, and more capable LLMs.\n","authors":["Zeyuan Meng","Zixuan Yi","Iadh Ounis"],"pdf_url":"https://arxiv.org/pdf/2507.05880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05865v1","updated":"2025-07-08T10:47:03Z","published":"2025-07-08T10:47:03Z","title":"On the Costs and Benefits of Learned Indexing for Dynamic\n  High-Dimensional Data: Extended Version","summary":"  One of the main challenges within the growing research area of learned\nindexing is the lack of adaptability to dynamically expanding datasets. This\npaper explores the dynamization of a static learned index for complex data\nthrough operations such as node splitting and broadening, enabling efficient\nadaptation to new data. Furthermore, we evaluate the trade-offs between static\nand dynamic approaches by introducing an amortized cost model to assess query\nperformance in tandem with the build costs of the index structure, enabling\nexperimental determination of when a dynamic learned index outperforms its\nstatic counterpart. We apply the dynamization method to a static learned index\nand demonstrate that its superior scaling quickly surpasses the static\nimplementation in terms of overall costs as the database grows. This is an\nextended version of the paper presented at DAWAK 2025.\n","authors":["Terézia Slanináková","Jaroslav Olha","David Procházka","Matej Antol","Vlastislav Dohnal"],"pdf_url":"https://arxiv.org/pdf/2507.05865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05863v1","updated":"2025-07-08T10:44:27Z","published":"2025-07-08T10:44:27Z","title":"KERAG_R: Knowledge-Enhanced Retrieval-Augmented Generation for\n  Recommendation","summary":"  Large Language Models (LLMs) have shown strong potential in recommender\nsystems due to their contextual learning and generalisation capabilities.\nExisting LLM-based recommendation approaches typically formulate the\nrecommendation task using specialised prompts designed to leverage their\ncontextual abilities, and aligning their outputs closely with human preferences\nto yield an improved recommendation performance. However, the use of LLMs for\nrecommendation tasks is limited by the absence of domain-specific knowledge.\nThis lack of relevant relational knowledge about the items to be recommended in\nthe LLM's pre-training corpus can lead to inaccuracies or hallucinations,\nresulting in incorrect or misleading recommendations. Moreover, directly using\ninformation from the knowledge graph introduces redundant and noisy\ninformation, which can affect the LLM's reasoning process or exceed its input\ncontext length, thereby reducing the performance of LLM-based recommendations.\nTo address the lack of domain-specific knowledge, we propose a novel model\ncalled Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation\n(KERAG_R). Specifically, we leverage a graph retrieval-augmented generation\n(GraphRAG) component to integrate additional information from a knowledge graph\n(KG) into instructions, enabling the LLM to collaboratively exploit\nrecommendation signals from both text-based user interactions and the knowledge\ngraph to better estimate the users' preferences in a recommendation context. In\nparticular, we perform graph RAG by pre-training a graph attention network\n(GAT) to select the most relevant triple for the target users for the used LLM,\nthereby enhancing the LLM while reducing redundant and noisy information. Our\nextensive experiments on three public datasets show that our proposed KERAG_R\nmodel significantly outperforms ten existing state-of-the-art recommendation\nmethods.\n","authors":["Zeyuan Meng","Zixuan Yi","Iadh Ounis"],"pdf_url":"https://arxiv.org/pdf/2507.05863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03009v2","updated":"2025-07-08T09:51:21Z","published":"2025-07-02T10:22:05Z","title":"PDFMathTranslate: Scientific Document Translation Preserving Layouts","summary":"  Language barriers in scientific documents hinder the diffusion and\ndevelopment of science and technologies. However, prior efforts in translating\nsuch documents largely overlooked the information in layouts. To bridge the\ngap, we introduce PDFMathTranslate, the world's first open-source software for\ntranslating scientific documents while preserving layouts. Leveraging the most\nrecent advances in large language models and precise layout detection, we\ncontribute to the community with key improvements in precision, flexibility,\nand efficiency. The work has been open-sourced at\nhttps://github.com/byaidu/pdfmathtranslate with more than 222k downloads.\n","authors":["Rongxin Ouyang","Chang Chu","Zhikuang Xin","Xiangyao Ma"],"pdf_url":"https://arxiv.org/pdf/2507.03009v2.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.05767v1","updated":"2025-07-08T08:13:30Z","published":"2025-07-08T08:13:30Z","title":"Vers un cadre ontologique pour la gestion des comp{é}tences : {à}\n  des fins de formation, de recrutement, de m{é}tier, ou de recherches\n  associ{é}es","summary":"  The rapid transformation of the labor market, driven by technological\nadvancements and the digital economy, requires continuous competence\ndevelopment and constant adaptation. In this context, traditional competence\nmanagement systems lack interoperability, adaptability, and semantic\nunderstanding, making it difficult to align individual competencies with labor\nmarket needs and training programs. This paper proposes an ontology-based\nframework for competence management, enabling a structured representation of\ncompetencies, occupations, and training programs. By leveraging ontological\nmodels and semantic reasoning, this framework aims to enhance the automation of\ncompetence-to-job matching, the personalization of learning recommendations,\nand career planning. This study discusses the design, implementation, and\npotential applications of the framework, focusing on competence training\nprograms, job searching, and finding competent individuals.\n","authors":["Ngoc Luyen Le","Marie-Hélène Abel","Bertrand Laforge"],"pdf_url":"https://arxiv.org/pdf/2507.05767v1.pdf","comment":"in French language. 36es Journ{\\'e}es francophones d'Ing{\\'e}nierie\n  des Connaissances (IC 2025) @ Plate-Forme Intelligence Artificielle (PFIA\n  2025), Jul 2025, Dijon, France"},{"id":"http://arxiv.org/abs/2507.05733v1","updated":"2025-07-08T07:26:55Z","published":"2025-07-08T07:26:55Z","title":"When Transformers Meet Recommenders: Integrating Self-Attentive\n  Sequential Recommendation with Fine-Tuned LLMs","summary":"  Self-Attentive Sequential Recommendation (SASRec) effectively captures\nlong-term user preferences by applying attention mechanisms to historical\ninteractions. Concurrently, the rise of Large Language Models (LLMs) has\nmotivated research into LLM-based recommendation, which leverages their\npowerful generalization and language understanding capabilities. However, LLMs\noften lack the domain-specific knowledge and collaborative signals essential\nfor high-quality recommendations when relying solely on textual prompts. To\naddress this limitation, this study proposes SASRecLLM, a novel framework that\nintegrates SASRec as a collaborative encoder with an LLM fine-tuned using\nLow-Rank Adaptation (LoRA). The components are connected via a mapping layer to\nalign their dimensional spaces, and three targeted training strategies are\ndesigned to optimize the hybrid architecture. Extensive experiments on multiple\ndatasets demonstrate that SASRecLLM achieves robust and consistent improvements\nover strong baselines in both cold-start and warm-start scenarios. This work\nadvances the field of LLM-based recommendation by presenting a modular and\neffective paradigm for fusing structured collaborative filtering with the\nsemantic power of fine-tuned LLMs. The implementation is available on GitHub:\nhttps://github.com/kechenkristin/RecLLM\n","authors":["Kechen Liu"],"pdf_url":"https://arxiv.org/pdf/2507.05733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01376v2","updated":"2025-07-08T07:19:20Z","published":"2024-11-02T22:59:36Z","title":"Multi-Channel Hypergraph Contrastive Learning for Matrix Completion","summary":"  Rating is a typical user explicit feedback that visually reflects how much a\nuser likes a related item. The (rating) matrix completion is essentially a\nrating prediction process, which is also a significant problem in recommender\nsystems. Recently, graph neural networks (GNNs) have been widely used in matrix\ncompletion, which captures users' preferences over items by formulating a\nrating matrix as a bipartite graph. However, existing methods are susceptible\ndue to data sparsity and long-tail distribution in real-world scenarios.\nMoreover, the messaging mechanism of GNNs makes it difficult to capture\nhigh-order correlations and constraints between nodes, which are essentially\nuseful in recommendation tasks. To tackle these challenges, we propose a\nMulti-Channel Hypergraph Contrastive Learning framework for matrix completion,\nnamed MHCL. Specifically, MHCL adaptively learns hypergraph structures to\ncapture high-order correlations between nodes and jointly captures local and\nglobal collaborative relationships through attention-based cross-view\naggregation. Additionally, to consider the magnitude and order information of\nratings, we treat different rating subgraphs as different channels, encourage\nalignment between adjacent ratings, and further achieve the mutual enhancement\nbetween different ratings through multi-channel cross-rating contrastive\nlearning. Extensive experiments on five public datasets demonstrate that the\nproposed method significantly outperforms the current state-of-the-art\napproaches.\n","authors":["Xiang Li","Changsheng Shui","Zhongying Zhao","Junyu Dong","Yanwei Yu"],"pdf_url":"https://arxiv.org/pdf/2411.01376v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05715v1","updated":"2025-07-08T06:58:24Z","published":"2025-07-08T06:58:24Z","title":"From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal\n  Collaborative Filtering Recommendation","summary":"  Most existing multimodal collaborative filtering recommendation (MCFRec)\nmethods rely heavily on ID features and multimodal content to enhance\nrecommendation performance. However, this paper reveals that ID features are\neffective but have limited benefits in multimodal collaborative filtering\nrecommendation. Therefore, this paper systematically deconstruct the pros and\ncons of ID features: (i) they provide initial embedding but lack semantic\nrichness, (ii) they provide a unique identifier for each user and item but\nhinder generalization to untrained data, and (iii) they assist in aligning and\nfusing multimodal features but may lead to representation shift. Based on these\ninsights, this paper proposes IDFREE, an ID-free multimodal collaborative\nFiltering REcommEndation baseline. IDFREE replaces ID features with multimodal\nfeatures and positional encodings to generate semantically meaningful ID-free\nembeddings. For ID-free multimodal collaborative filtering, it further proposes\nan adaptive similarity graph module to construct dynamic user-user and\nitem-item graphs based on multimodal features. Then, an augmented user-item\ngraph encoder is proposed to construct more effective user and item encoding.\nFinally, IDFREE achieves inter-multimodal alignment based on the contrastive\nlearning and uses Softmax loss as recommendation loss. Basic experiments on\nthree public datasets demonstrate that IDFREE outperforms existing ID-based\nMCFRec methods, achieving an average performance gain of 72.24% across standard\nmetrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended\nexperiments further validate our findings on the limitations of ID features in\nMCFRec. The code is released at https://github.com/G-H-Li/IDFREE.\n","authors":["Guohao Li","Li Jing","Jia Wu","Xuefei Li","Kai Zhu","Yue He"],"pdf_url":"https://arxiv.org/pdf/2507.05715v1.pdf","comment":"ACM MM'25 (Experimental supplementary version)"},{"id":"http://arxiv.org/abs/2507.02962v2","updated":"2025-07-08T06:38:26Z","published":"2025-06-30T09:02:45Z","title":"RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs\n  through Multi-query Parallelism","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while they remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have explored enhancing models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to the single-query mode. In this paper, we propose\nRAG-R1, a novel training framework designed to enable LLMs to adaptively\nleverage internal and external knowledge during the reasoning process. We\nfurther expand the generation and retrieval processes within the framework from\nsingle-query mode to multi-query parallelism, aimed at reducing inference time\nand enhancing the model's capabilities. Extensive experiments on seven\nquestion-answering benchmarks demonstrate that our method outperforms the\nstrongest baseline by up to 13.2% and decreases inference time by 11.1%.\n","authors":["Zhiwen Tan","Jiaming Huang","Qintong Wu","Hongxuan Zhang","Chenyi Zhuang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2507.02962v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04942v2","updated":"2025-07-08T06:37:05Z","published":"2025-07-07T12:38:53Z","title":"SIGIR 2025 -- LiveRAG Challenge Report","summary":"  The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,\nprovided a competitive platform for advancing Retrieval-Augmented Generation\n(RAG) technologies. Participants from academia and industry were invited to\ndevelop a RAG-based question-answering system using a fixed corpus\n(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal\nwas to facilitate challenging comparisons of retrieval and prompting\nstrategies. During the Live Challenge Day, 70 teams from 27 different countries\nprovided answers and supportive information to 500 unseen questions within a\nstrict two-hour time window. Evaluation was conducted in two stages: first an\nautomated LLM-as-a-judge approach was used to compute correctness and\nfaithfulness score, then a manual review of top ranked submissions was\nconducted. The finalists were announced on June 12, 2025, with prizes awarded\nduring the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.\n","authors":["David Carmel","Simone Filice","Guy Horowitz","Yoelle Maarek","Oren Somekh","Ran Tavory","Mehdi Ghissassi","Edo Liberty","Roy Miara"],"pdf_url":"https://arxiv.org/pdf/2507.04942v2.pdf","comment":"9 pages, 5 tables"},{"id":"http://arxiv.org/abs/2507.05006v2","updated":"2025-07-08T06:16:29Z","published":"2025-07-07T13:41:52Z","title":"Do We Really Need Specialization? Evaluating Generalist Text Embeddings\n  for Zero-Shot Recommendation and Search","summary":"  Pre-trained language models (PLMs) are widely used to derive semantic\nrepresentations from item metadata in recommendation and search. In sequential\nrecommendation, PLMs enhance ID-based embeddings through textual metadata,\nwhile in product search, they align item characteristics with user intent.\nRecent studies suggest task and domain-specific fine-tuning are needed to\nimprove representational power. This paper challenges this assumption, showing\nthat Generalist Text Embedding Models (GTEs), pre-trained on large-scale\ncorpora, can guarantee strong zero-shot performance without specialized\nadaptation. Our experiments demonstrate that GTEs outperform traditional and\nfine-tuned models in both sequential recommendation and product search. We\nattribute this to a superior representational power, as they distribute\nfeatures more evenly across the embedding space. Finally, we show that\ncompressing embedding dimensions by focusing on the most informative directions\n(e.g., via PCA) effectively reduces noise and improves the performance of\nspecialized models. To ensure reproducibility, we provide our repository at\nhttps://split.to/gte4ps.\n","authors":["Matteo Attimonelli","Alessandro De Bellis","Claudio Pomo","Dietmar Jannach","Eugenio Di Sciascio","Tommaso Di Noia"],"pdf_url":"https://arxiv.org/pdf/2507.05006v2.pdf","comment":"Accept as Short Paper at RecSys 2025"},{"id":"http://arxiv.org/abs/2507.05633v1","updated":"2025-07-08T03:29:09Z","published":"2025-07-08T03:29:09Z","title":"SARA: Selective and Adaptive Retrieval-augmented Generation with Context\n  Compression","summary":"  Retrieval-augmented Generation (RAG) extends large language models (LLMs)\nwith external knowledge but faces key challenges: restricted effective context\nlength and redundancy in retrieved documents. Pure compression-based approaches\nreduce input size but often discard fine-grained details essential for factual\naccuracy. We propose SARA, a unified RAG framework that balances local\nprecision and global knowledge coverage under tight context budgets. SARA\ncombines natural-language text snippets with semantic compression vectors to\njointly enhance context efficiency and answer correctness. It represents\ncontexts at two complementary levels: 1) fine-grained natural-language spans\nthat preserve critical entities and numerical values, and 2) compact,\ninterpretable vectors that summarize high-level semantics. An iterative\nevidence-selection module employs the compression vectors for dynamic reranking\nof contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families\n(Mistral, Llama, and Gemma), SARA consistently improves answer relevance\n(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),\ndemonstrating the importance of integrating textual and compressed\nrepresentations for robust, context-efficient RAG.\n","authors":["Yiqiao Jin","Kartik Sharma","Vineeth Rakesh","Yingtong Dou","Menghai Pan","Mahashweta Das","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2507.05633v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2507.05577v1","updated":"2025-07-08T01:25:06Z","published":"2025-07-08T01:25:06Z","title":"Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs\n  for Biomedical QA","summary":"  Biomedical semantic question answering rooted in information retrieval can\nplay a crucial role in keeping up to date with vast, rapidly evolving and\never-growing biomedical literature. A robust system can help researchers,\nhealthcare professionals and even layman users access relevant knowledge\ngrounded in evidence. The BioASQ 2025 Task13b Challenge serves as an important\nbenchmark, offering a competitive platform for advancement of this space. This\npaper presents the methodologies and results from our participation in this\nchallenge where we built a Retrieval-Augmented Generation (RAG) system that can\nanswer biomedical questions by retrieving relevant PubMed documents and\nsnippets to generate answers. For the retrieval task, we generated dense\nembeddings from biomedical articles for initial retrieval, and applied an\nensemble of finetuned cross-encoders and large language models (LLMs) for\nre-ranking to identify top relevant documents. Our solution achieved an MAP@10\nof 0.1581, placing 10th on the leaderboard for the retrieval task. For answer\ngeneration, we employed few-shot prompting of instruction-tuned LLMs. Our\nsystem achieved macro-F1 score of 0.95 for yes/no questions (rank 12), Mean\nReciprocal Rank (MRR) of 0.64 for factoid questions (rank 1), mean-F1 score of\n0.63 for list questions (rank 5), and ROUGE-SU4 F1 score of 0.29 for ideal\nanswers (rank 11).\n","authors":["Shashank Verma","Fengyi Jiang","Xiangning Xue"],"pdf_url":"https://arxiv.org/pdf/2507.05577v1.pdf","comment":"Paper submitted to CLEF 2025 CEUR-WS"}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.06373v1","updated":"2025-07-08T20:20:27Z","published":"2025-07-08T20:20:27Z","title":"Digital Wargames to Enhance Military Medical Evacuation Decision-Making","summary":"  Medical evacuation is one of the United States Army's most storied and\ncritical mission sets, responsible for efficiently and expediently evacuating\nthe battlefield ill and injured. Medical evacuation planning involves designing\na robust network of medical platforms and facilities capable of moving and\ntreating large numbers of casualties. Until now, there has not been a medium to\nsimulate these networks in a classroom setting and evaluate both offline\nplanning and online decision-making performance. This work describes the\nMedical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer\nsimulation developed in Unity that replicates battlefield constraints and\nuncertainties. MEWI accurately models patient interactions at casualty\ncollection points, ambulance exchange points, medical treatment facilities, and\nevacuation platforms. Two operational scenarios are introduced: an amphibious\nisland assault in the Pacific and a Eurasian conflict across a sprawling road\nand river network. These scenarios pit students against the clock to save as\nmany casualties as possible while adhering to doctrinal lessons learned during\ndidactic training. We visualize performance data collected from two iterations\nof the MEWI Pacific scenario executed in the United States Army's Medical\nEvacuation Doctrine Course. We consider post-wargame Likert survey data from\nstudent participants and external observer notes to identify key planning\ndecision points, document medical evacuation lessons learned, and quantify\ngeneral utility. Results indicate that MEWI participation substantially\nimproves uptake of medical evacuation lessons learned and co-operative\ndecision-making. MEWI is a substantial step forward in the field of\nhigh-fidelity training tools for medical education, and our study findings\noffer critical insights into improving medical evacuation education and\noperations across the joint force.\n","authors":["Jeremy Fischer","Ram Krishnamoorthy","Vishal Kumar","Mahdi Al-Husseini"],"pdf_url":"https://arxiv.org/pdf/2507.06373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00854v3","updated":"2025-07-08T17:34:10Z","published":"2025-06-01T06:26:32Z","title":"EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG\n  Alignment via Large Language Model and Contrastive Learning on ChineseEEG","summary":"  We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese.\n","authors":["Jacky Tai-Yu Lu","Jung Chiang","Chi-Sheng Chen","Anna Nai-Yun Tung","Hsiang Wei Hu","Yuan Chiao Cheng"],"pdf_url":"https://arxiv.org/pdf/2506.00854v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06071v1","updated":"2025-07-08T15:14:27Z","published":"2025-07-08T15:14:27Z","title":"MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions\n  by Disentangled Embedding","summary":"  Audio-driven emotional 3D facial animation aims to generate synchronized lip\nmovements and vivid facial expressions. However, most existing approaches focus\non static and predefined emotion labels, limiting their diversity and\nnaturalness. To address these challenges, we propose MEDTalk, a novel framework\nfor fine-grained and dynamic emotional talking head generation. Our approach\nfirst disentangles content and emotion embedding spaces from motion sequences\nusing a carefully designed cross-reconstruction process, enabling independent\ncontrol over lip movements and facial expressions. Beyond conventional\naudio-driven lip synchronization, we integrate audio and speech text,\npredicting frame-wise intensity variations and dynamically adjusting static\nemotion features to generate realistic emotional expressions. Furthermore, to\nenhance control and personalization, we incorporate multimodal inputs-including\ntext descriptions and reference expression images-to guide the generation of\nuser-specified facial expressions. With MetaHuman as the priority, our\ngenerated results can be conveniently integrated into the industrial production\npipeline.\n","authors":["Chang Liu","Ye Pan","Chenyang Ding","Susanto Rahardja","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2507.06071v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.04667v2","updated":"2025-07-08T14:46:46Z","published":"2025-07-07T05:12:34Z","title":"What's Making That Sound Right Now? Video-centric Audio-Visual\n  Localization","summary":"  Audio-Visual Localization (AVL) aims to identify sound-emitting sources\nwithin a visual scene. However, existing studies focus on image-level\naudio-visual associations, failing to capture temporal dynamics. Moreover, they\nassume simplified scenarios where sound sources are always visible and involve\nonly a single object. To address these limitations, we propose AVATAR, a\nvideo-centric AVL benchmark that incorporates high-resolution temporal\ninformation. AVATAR introduces four distinct scenarios -- Single-sound,\nMixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive\nevaluation of AVL models. Additionally, we present TAVLO, a novel video-centric\nAVL model that explicitly integrates temporal information. Experimental results\nshow that conventional methods struggle to track temporal variations due to\ntheir reliance on global audio features and frame-level mappings. In contrast,\nTAVLO achieves robust and precise audio-visual alignment by leveraging\nhigh-resolution temporal modeling. Our work empirically demonstrates the\nimportance of temporal dynamics in AVL and establishes a new standard for\nvideo-centric audio-visual localization.\n","authors":["Hahyeon Choi","Junhoo Lee","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2507.04667v2.pdf","comment":"Published at ICCV 2025. Project page:\n  https://hahyeon610.github.io/Video-centric_Audio_Visual_Localization/"},{"id":"http://arxiv.org/abs/2404.16302v2","updated":"2025-07-08T14:46:42Z","published":"2024-04-25T02:54:11Z","title":"CFMW: Cross-modality Fusion Mamba for Robust Object Detection under\n  Adverse Weather","summary":"  Visible-infrared image pairs provide complementary information, enhancing the\nreliability and robustness of object detection applications in real-world\nscenarios. However, most existing methods face challenges in maintaining\nrobustness under complex weather conditions, which limits their applicability.\nMeanwhile, the reliance on attention mechanisms in modality fusion introduces\nsignificant computational complexity and storage overhead, particularly when\ndealing with high-resolution images. To address these challenges, we propose\nthe Cross-modality Fusion Mamba with Weather-removal (CFMW) to augment\nstability and cost-effectiveness under adverse weather conditions. Leveraging\nthe proposed Perturbation-Adaptive Diffusion Model (PADM) and Cross-modality\nFusion Mamba (CFM) modules, CFMW is able to reconstruct visual features\naffected by adverse weather, enriching the representation of image details.\nWith efficient architecture design, CFMW is 3 times faster than\nTransformer-style fusion (e.g., CFT). To bridge the gap in relevant datasets,\nwe construct a new Severe Weather Visible-Infrared (SWVI) dataset, encompassing\ndiverse adverse weather scenarios such as rain, haze, and snow. The dataset\ncontains 64,281 paired visible-infrared images, providing a valuable resource\nfor future research. Extensive experiments on public datasets (i.e., M3FD and\nLLVIP) and the newly constructed SWVI dataset conclusively demonstrate that\nCFMW achieves state-of-the-art detection performance. Both the dataset and\nsource code will be made publicly available at\nhttps://github.com/lhy-zjut/CFMW.\n","authors":["Haoyuan Li","Qi Hu","Binjia Zhou","You Yao","Jiacheng Lin","Kailun Yang","Peng Chen"],"pdf_url":"https://arxiv.org/pdf/2404.16302v2.pdf","comment":"Accepted to IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT). The dataset and source code will be made publicly\n  available at https://github.com/lhy-zjut/CFMW"},{"id":"http://arxiv.org/abs/2507.05939v1","updated":"2025-07-08T12:38:19Z","published":"2025-07-08T12:38:19Z","title":"Remember Past, Anticipate Future: Learning Continual Multimodal\n  Misinformation Detectors","summary":"  Nowadays, misinformation articles, especially multimodal ones, are widely\nspread on social media platforms and cause serious negative effects. To control\ntheir propagation, Multimodal Misinformation Detection (MMD) becomes an active\ntopic in the community to automatically identify misinformation. Previous MMD\nmethods focus on supervising detectors by collecting offline data. However, in\nreal-world scenarios, new events always continually emerge, making MMD models\ntrained on offline data consistently outdated and ineffective. To address this\nissue, training MMD models under online data streams is an alternative,\ninducing an emerging task named continual MMD. Unfortunately, it is hindered by\ntwo major challenges. First, training on new data consistently decreases the\ndetection performance on past data, named past knowledge forgetting. Second,\nthe social environment constantly evolves over time, affecting the\ngeneralization on future data. To alleviate these challenges, we propose to\nremember past knowledge by isolating interference between event-specific\nparameters with a Dirichlet process-based mixture-of-expert structure, and\nanticipate future environmental distributions by learning a continuous-time\ndynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.\nExtensive experiments demonstrate that DAEDCMD can consistently and\nsignificantly outperform the compared methods, including six MMD baselines and\nthree continual learning methods.\n","authors":["Bing Wang","Ximing Li","Mengzhe Ye","Changchun Li","Bo Fu","Jianfeng Qu","Lin Yuanbo Wu"],"pdf_url":"https://arxiv.org/pdf/2507.05939v1.pdf","comment":"Accepted by ACM MM 2025. 10 pages, 6 figures. Code:\n  https://github.com/wangbing1416/DAEDCMD"},{"id":"http://arxiv.org/abs/2507.05859v1","updated":"2025-07-08T10:39:32Z","published":"2025-07-08T10:39:32Z","title":"D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for\n  Free-Viewpoint Videos","summary":"  Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient\ncompression of dynamic 3D representations remains a major challenge. Recent\nadvances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have\nenabled high-fidelity scene modeling. However, existing methods often couple\nscene reconstruction with optimization-dependent coding, which limits\ngeneralizability. This paper presents Feedforward Compression of Dynamic\nGaussian Splatting (D-FCGS), a novel feedforward framework for compressing\ntemporally correlated Gaussian point cloud sequences. Our approach introduces a\nGroup-of-Frames (GoF) structure with I-P frame coding, where inter-frame\nmotions are extracted via sparse control points. The resulting motion tensors\nare compressed in a feedforward manner using a dual prior-aware entropy model\nthat combines hyperprior and spatial-temporal priors for accurate rate\nestimation. For reconstruction, we perform control-point-guided motion\ncompensation and employ a refinement network to enhance view-consistent\nfidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS\ngeneralizes across scenes without per-scene optimization. Experiments show that\nit matches the rate-distortion performance of optimization-based methods,\nachieving over 40 times compression in under 2 seconds while preserving visual\nquality across viewpoints. This work advances feedforward compression for\ndynamic 3DGS, paving the way for scalable FVV transmission and storage in\nimmersive applications.\n","authors":["Wenkang Zhang","Yan Zhao","Qiang Wang","Li Song","Zhengxue Cheng"],"pdf_url":"https://arxiv.org/pdf/2507.05859v1.pdf","comment":"12 pages, 9 figures, 8 tables"},{"id":"http://arxiv.org/abs/2506.20214v2","updated":"2025-07-08T07:46:39Z","published":"2025-06-25T07:57:09Z","title":"UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal\n  Understanding and Generation","summary":"  Unified multimodal large language models (MLLMs) have shown promise in\njointly advancing multimodal understanding and generation, with visual\ncodebooks discretizing images into tokens for autoregressive modeling. Existing\ncodebook-based methods either rely on small vocabularies (~16K entries) that\nlack fine-grained semantics or naively scale up, resulting in low token\nutilization and unstable training. We propose UniCode$^2$, a cascaded codebook\nframework enabling large-scale, semantically aligned, and stable visual\ntokenization. By clustering millions of SigLIP sequence embeddings, we build a\n500K-entry codebook that preserves vision-language alignment while expanding\ncapacity. Stability is ensured via a cascaded design: a frozen codebook anchors\nthe embedding space, and a trainable codebook refines task-specific semantics.\nThis decoupling promotes high utilization and robust learning. Moreover, the\nalignment of our visual tokens with textual semantics enables seamless\nintegration with pretrained diffusion decoders, supporting high-quality visual\nsynthesis with minimal adaptation. UniCode^2 delivers strong performance across\ndiverse benchmarks, demonstrating the viability of scaling visual token spaces\nwithout sacrificing stability, semantics, or modularity.\n","authors":["Yanzhe Chen","Huasong Zhong","Yan Li","Zhenheng Yang"],"pdf_url":"https://arxiv.org/pdf/2506.20214v2.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.05715v1","updated":"2025-07-08T06:58:24Z","published":"2025-07-08T06:58:24Z","title":"From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal\n  Collaborative Filtering Recommendation","summary":"  Most existing multimodal collaborative filtering recommendation (MCFRec)\nmethods rely heavily on ID features and multimodal content to enhance\nrecommendation performance. However, this paper reveals that ID features are\neffective but have limited benefits in multimodal collaborative filtering\nrecommendation. Therefore, this paper systematically deconstruct the pros and\ncons of ID features: (i) they provide initial embedding but lack semantic\nrichness, (ii) they provide a unique identifier for each user and item but\nhinder generalization to untrained data, and (iii) they assist in aligning and\nfusing multimodal features but may lead to representation shift. Based on these\ninsights, this paper proposes IDFREE, an ID-free multimodal collaborative\nFiltering REcommEndation baseline. IDFREE replaces ID features with multimodal\nfeatures and positional encodings to generate semantically meaningful ID-free\nembeddings. For ID-free multimodal collaborative filtering, it further proposes\nan adaptive similarity graph module to construct dynamic user-user and\nitem-item graphs based on multimodal features. Then, an augmented user-item\ngraph encoder is proposed to construct more effective user and item encoding.\nFinally, IDFREE achieves inter-multimodal alignment based on the contrastive\nlearning and uses Softmax loss as recommendation loss. Basic experiments on\nthree public datasets demonstrate that IDFREE outperforms existing ID-based\nMCFRec methods, achieving an average performance gain of 72.24% across standard\nmetrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended\nexperiments further validate our findings on the limitations of ID features in\nMCFRec. The code is released at https://github.com/G-H-Li/IDFREE.\n","authors":["Guohao Li","Li Jing","Jia Wu","Xuefei Li","Kai Zhu","Yue He"],"pdf_url":"https://arxiv.org/pdf/2507.05715v1.pdf","comment":"ACM MM'25 (Experimental supplementary version)"},{"id":"http://arxiv.org/abs/2503.12149v2","updated":"2025-07-08T03:29:45Z","published":"2025-03-15T14:10:25Z","title":"Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm\n  Perception in Large Vision-Language Models","summary":"  With the advent of large vision-language models (LVLMs) demonstrating\nincreasingly human-like abilities, a pivotal question emerges: do different\nLVLMs interpret multimodal sarcasm differently, and can a single model grasp\nsarcasm from multiple perspectives like humans? To explore this, we introduce\nan analytical framework using systematically designed prompts on existing\nmultimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409\nsamples, we examine interpretive variations within and across models, focusing\non confidence levels, alignment with dataset labels, and recognition of\nambiguous \"neutral\" cases. Our findings reveal notable discrepancies -- across\nLVLMs and within the same model under varied prompts. While\nclassification-oriented prompts yield higher internal consistency, models\ndiverge markedly when tasked with interpretive reasoning. These results\nchallenge binary labeling paradigms by highlighting sarcasm's subjectivity. We\nadvocate moving beyond rigid annotation schemes toward multi-perspective,\nuncertainty-aware modeling, offering deeper insights into multimodal sarcasm\ncomprehension. Our code and data are available at:\nhttps://github.com/CoderChen01/LVLMSarcasmAnalysis\n","authors":["Junjie Chen","Xuyang Liu","Subin Huang","Linfeng Zhang","Hang Yu"],"pdf_url":"https://arxiv.org/pdf/2503.12149v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05621v1","updated":"2025-07-08T03:04:08Z","published":"2025-07-08T03:04:08Z","title":"AdaptaGen: Domain-Specific Image Generation through Hierarchical\n  Semantic Optimization Framework","summary":"  Domain-specific image generation aims to produce high-quality visual content\nfor specialized fields while ensuring semantic accuracy and detail fidelity.\nHowever, existing methods exhibit two critical limitations: First, current\napproaches address prompt engineering and model adaptation separately,\noverlooking the inherent dependence between semantic understanding and visual\nrepresentation in specialized domains. Second, these techniques inadequately\nincorporate domain-specific semantic constraints during content synthesis,\nresulting in generation outcomes that exhibit hallucinations and semantic\ndeviations. To tackle these issues, we propose AdaptaGen, a hierarchical\nsemantic optimization framework that integrates matrix-based prompt\noptimization with multi-perspective understanding, capturing comprehensive\nsemantic relationships from both global and local perspectives. To mitigate\nhallucinations in specialized domains, we design a cross-modal adaptation\nmechanism, which, when combined with intelligent content synthesis, enables\npreserving core thematic elements while incorporating diverse details across\nimages. Additionally, we introduce a two-phase caption semantic transformation\nduring the generation phase. This approach maintains semantic coherence while\nenhancing visual diversity, ensuring the generated images adhere to\ndomain-specific constraints. Experimental results confirm our approach's\neffectiveness, with our framework achieving superior performance across 40\ncategories from diverse datasets using only 16 images per category,\ndemonstrating significant improvements in image quality, diversity, and\nsemantic consistency.\n","authors":["Suoxiang Zhang","Xiaxi Li","Hongrui Chang","Zhuoyan Hou","Guoxin Wu","Ronghua Ji"],"pdf_url":"https://arxiv.org/pdf/2507.05621v1.pdf","comment":null}]},"2025-07-07T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.05537v1","updated":"2025-07-07T23:21:20Z","published":"2025-07-07T23:21:20Z","title":"Information Needs and Practices Supported by ChatGPT","summary":"  This study considers ChatGPT as an information source, investigating the\ninformation needs that people come to ChatGPT with and the information\npractices that ChatGPT supports, through a qualitative content analysis of 205\nuser vignettes. The findings show that ChatGPT is used in a range of life\ndomains (home/family, work, leisure, etc.) and for a range of human needs\n(writing/editing, learning, simple programming tasks, etc.), constituting the\ninformation needs that people use ChatGPT to address. Related to these\ninformation needs, the findings show six categories of information practices\nthat ChatGPT supports: Writing, Deciding, Identifying, Ideating, Talking, and\nCritiquing. This work suggests that, in the AI age, information need should be\nconceptualized not just as a matter of \"getting questions answered\" or even\n\"making sense,\" but as skillfully coping in the world, a notion that includes\nboth understanding and action. This study leads to numerous opportunities for\nfuture work at the junction of generative AI and information needs, seeking,\nuse and experience.\n","authors":["Tim Gorichanaz"],"pdf_url":"https://arxiv.org/pdf/2507.05537v1.pdf","comment":"To be presented at the 2025 ASIS&T virtual satellite meeting,\n  December 2025"},{"id":"http://arxiv.org/abs/2409.04701v3","updated":"2025-07-07T17:49:51Z","published":"2024-09-07T03:54:46Z","title":"Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding\n  Models","summary":"  Many use cases require retrieving smaller portions of text, and dense\nvector-based retrieval systems often perform better with shorter text segments,\nas the semantics are less likely to be over-compressed in the embeddings.\nConsequently, practitioners often split text documents into smaller chunks and\nencode them separately. However, chunk embeddings created in this way can lose\ncontextual information from surrounding chunks, resulting in sub-optimal\nrepresentations. In this paper, we introduce a novel method called late\nchunking, which leverages long context embedding models to first embed all\ntokens of the long text, with chunking applied after the transformer model and\njust before mean pooling - hence the term late in its naming. The resulting\nchunk embeddings capture the full contextual information, leading to superior\nresults across various retrieval tasks. The method is generic enough to be\napplied to a wide range of long-context embedding models and works without\nadditional training. To further increase the effectiveness of late chunking, we\npropose a dedicated fine-tuning approach for embedding models.\n","authors":["Michael Günther","Isabelle Mohr","Daniel James Williams","Bo Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.04701v3.pdf","comment":"11 pages, 3rd draft"},{"id":"http://arxiv.org/abs/2506.18902v3","updated":"2025-07-07T17:41:02Z","published":"2025-06-23T17:59:55Z","title":"jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual\n  Retrieval","summary":"  We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding\nmodel that unifies text and image representations through a novel architecture\nsupporting both single-vector and multi-vector embeddings in the late\ninteraction style. The model incorporates task-specific Low-Rank Adaptation\n(LoRA) adapters to optimize performance across diverse retrieval scenarios,\nincluding query-document retrieval, semantic text similarity, and code search.\nComprehensive evaluations demonstrate that jina-embeddings-v4 achieves\nstate-of-the-art performance on both single-modal and cross-modal retrieval\ntasks, with particular strength in processing visually rich content such as\ntables, charts, diagrams, and mixed-media formats. To facilitate evaluation of\nthis capability, we also introduce Jina-VDR, a novel benchmark specifically\ndesigned for visually rich image retrieval.\n","authors":["Michael Günther","Saba Sturua","Mohammad Kalim Akram","Isabelle Mohr","Andrei Ungureanu","Bo Wang","Sedigheh Eslami","Scott Martens","Maximilian Werk","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2506.18902v3.pdf","comment":"22 pages, 1-10 main, 14-22 experimental results, benchmark tables"},{"id":"http://arxiv.org/abs/2507.05200v1","updated":"2025-07-07T17:01:17Z","published":"2025-07-07T17:01:17Z","title":"In-Context Learning as an Effective Estimator of Functional Correctness\n  of LLM-Generated Code","summary":"  When applying LLM-based code generation to software development projects that\nfollow a feature-driven or rapid application development approach, it becomes\nnecessary to estimate the functional correctness of the generated code in the\nabsence of test cases. Just as a user selects a relevant document from a ranked\nlist of retrieved ones, a software generation workflow requires a developer to\nchoose (and potentially refine) a generated solution from a ranked list of\nalternative solutions, ordered by their posterior likelihoods. This implies\nthat estimating the quality of a ranked list -- akin to estimating \"relevance\"\nfor query performance prediction (QPP) in IR -- is also crucial for generative\nsoftware development, where quality is defined in terms of \"functional\ncorrectness\". In this paper, we propose an in-context learning (ICL) based\napproach for code quality estimation. Our findings demonstrate that providing\nfew-shot examples of functionally correct code from a training set enhances the\nperformance of existing QPP approaches as well as a zero-shot-based approach\nfor code quality estimation.\n","authors":["Susmita Das","Madhusudan Ghosh","Priyanka Swami","Debasis Ganguly","Gul Calikli"],"pdf_url":"https://arxiv.org/pdf/2507.05200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01907v2","updated":"2025-07-07T16:14:00Z","published":"2025-05-03T19:24:40Z","title":"A Generalised and Adaptable Reinforcement Learning Stopping Method","summary":"  This paper presents a Technology Assisted Review (TAR) stopping approach\nbased on Reinforcement Learning (RL). Previous such approaches offered limited\ncontrol over stopping behaviour, such as fixing the target recall and tradeoff\nbetween preferring to maximise recall or cost. These limitations are overcome\nby introducing a novel RL environment, GRLStop, that allows a single model to\nbe applied to multiple target recalls, balances the recall/cost tradeoff and\nintegrates a classifier. Experiments were carried out on six benchmark datasets\n(CLEF e-Health datasets 2017-9, TREC Total Recall, TREC Legal and Reuters RCV1)\nat multiple target recall levels. Results showed that the proposed approach to\nbe effective compared to multiple baselines in addition to offering greater\nflexibility.\n","authors":["Reem Bin-Hezam","Mark Stevenson"],"pdf_url":"https://arxiv.org/pdf/2505.01907v2.pdf","comment":"Accepted by SIGIR2025, Figure 4 legend updated"},{"id":"http://arxiv.org/abs/2506.19794v2","updated":"2025-07-07T14:20:16Z","published":"2025-06-24T17:04:23Z","title":"Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study","summary":"  Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.\n","authors":["Yuqi Zhu","Yi Zhong","Jintian Zhang","Ziheng Zhang","Shuofei Qiao","Yujie Luo","Lun Du","Da Zheng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.19794v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2507.04995v1","updated":"2025-07-07T13:34:15Z","published":"2025-07-07T13:34:15Z","title":"Interest Networks (iNETs) for Cities: Cross-Platform Insights and Urban\n  Behavior Explanations","summary":"  Location-Based Social Networks (LBSNs) provide a rich foundation for modeling\nurban behavior through iNETs (Interest Networks), which capture how user\ninterests are distributed throughout urban spaces. This study compares iNETs\nacross platforms (Google Places and Foursquare) and spatial granularities,\nshowing that coarser levels reveal more consistent cross-platform patterns,\nwhile finer granularities expose subtle, platform-specific behaviors. Our\nanalysis finds that, in general, user interest is primarily shaped by\ngeographic proximity and venue similarity, while socioeconomic and political\ncontexts play a lesser role. Building on these insights, we develop a\nmulti-level, explainable recommendation system that predicts high-interest\nurban regions for different user types. The model adapts to behavior profiles\n-- such as explorers, who are driven by proximity, and returners, who prefer\nfamiliar venues -- and provides natural-language explanations using explainable\nAI (XAI) techniques. To support our approach, we introduce h3-cities, a tool\nfor multi-scale spatial analysis, and release a public demo for interactively\nexploring personalized urban recommendations. Our findings contribute to urban\nmobility research by providing scalable, context-aware, and interpretable\nrecommendation systems.\n","authors":["Gustavo H. Santos","Myriam Delgado","Thiago H. Silva"],"pdf_url":"https://arxiv.org/pdf/2507.04995v1.pdf","comment":"Accepted at ACM SIGKDD Conference on Knowledge Discovery and Data\n  Mining (KDD-UMC)"},{"id":"http://arxiv.org/abs/2504.20679v2","updated":"2025-07-07T11:58:36Z","published":"2025-04-29T12:00:33Z","title":"Are Information Retrieval Approaches Good at Harmonising Longitudinal\n  Survey Questions in Social Science?","summary":"  Automated detection of semantically equivalent questions in longitudinal\nsocial science surveys is crucial for long-term studies informing empirical\nresearch in the social, economic, and health sciences. Retrieving equivalent\nquestions faces dual challenges: inconsistent representation of theoretical\nconstructs (i.e. concept/sub-concept) across studies as well as between\nquestion and response options, and the evolution of vocabulary and structure in\nlongitudinal text. To address these challenges, our multi-disciplinary\ncollaboration of computer scientists and survey specialists presents a new\ninformation retrieval (IR) task of identifying concept (e.g. Housing, Job,\netc.) equivalence across question and response options to harmonise\nlongitudinal population studies. This paper investigates multiple unsupervised\napproaches on a survey dataset spanning 1946-2020, including probabilistic\nmodels, linear probing of language models, and pre-trained neural networks\nspecialised for IR. We show that IR-specialised neural models achieve the\nhighest overall performance with other approaches performing comparably.\nAdditionally, the re-ranking of the probabilistic model's results with neural\nmodels only introduces modest improvements of 0.07 at most in F1-score.\nQualitative post-hoc evaluation by survey specialists shows that models\ngenerally have a low sensitivity to questions with high lexical overlap,\nparticularly in cases where sub-concepts are mismatched. Altogether, our\nanalysis serves to further research on harmonising longitudinal studies in\nsocial science.\n","authors":["Wing Yan Li","Zeqiang Wang","Jon Johnson","Suparna De"],"pdf_url":"https://arxiv.org/pdf/2504.20679v2.pdf","comment":"Accepted at SIGIR 2025"},{"id":"http://arxiv.org/abs/2507.04888v1","updated":"2025-07-07T11:19:28Z","published":"2025-07-07T11:19:28Z","title":"SimLab: A Platform for Simulation-based Evaluation of Conversational\n  Information Access Systems","summary":"  Research on interactive and conversational information access systems,\nincluding search engines, recommender systems, and conversational assistants,\nhas been hindered by the difficulty in evaluating such systems with\nreproducible experiments. User simulation provides a promising solution, but\nthere is a lack of infrastructure and tooling to support this kind of\nevaluation. To facilitate simulation-based evaluation of conversational\ninformation access systems, we introduce SimLab, the first cloud-based platform\nto provide a centralized general solution for the community to benchmark both\nconversational systems and user simulators in a controlled and reproducible\nenvironment. We articulate requirements for such a platform and propose a\ngeneral infrastructure to address these requirements. We then present the\ndesign and implementation of an initial version of SimLab and showcase its\nfeatures with an initial evaluation task of conversational movie\nrecommendation, which is made publicly available. Furthermore, we discuss the\nsustainability of the platform and its future opportunities. This paper is a\ncall for the community to contribute to the platform to drive progress in the\nfield of conversational information access and user simulation.\n","authors":["Nolwenn Bernard","Sharath Chandra Etagi Suresh","Krisztian Balog","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2507.04888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05311v1","updated":"2025-07-07T09:48:09Z","published":"2025-07-07T09:48:09Z","title":"PLACE: Prompt Learning for Attributed Community Search","summary":"  In this paper, we propose PLACE (Prompt Learning for Attributed Community\nSearch), an innovative graph prompt learning framework for ACS. Enlightened by\nprompt-tuning in Natural Language Processing (NLP), where learnable prompt\ntokens are inserted to contextualize NLP queries, PLACE integrates structural\nand learnable prompt tokens into the graph as a query-dependent refinement\nmechanism, forming a prompt-augmented graph. Within this prompt-augmented graph\nstructure, the learned prompt tokens serve as a bridge that strengthens\nconnections between graph nodes for the query, enabling the GNN to more\neffectively identify patterns of structural cohesiveness and attribute\nsimilarity related to the specific query. We employ an alternating training\nparadigm to optimize both the prompt parameters and the GNN jointly. Moreover,\nwe design a divide-and-conquer strategy to enhance scalability, supporting the\nmodel to handle million-scale graphs. Extensive experiments on 9 real-world\ngraphs demonstrate the effectiveness of PLACE for three types of ACS queries,\nwhere PLACE achieves higher F1 scores by 22% compared to the state-of-the-arts\non average.\n","authors":["Shuheng Fang","Kangfei Zhao","Rener Zhang","Yu Rong","Jeffrey Xu Yu"],"pdf_url":"https://arxiv.org/pdf/2507.05311v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2507.06258v1","updated":"2025-07-07T09:40:16Z","published":"2025-07-07T09:40:16Z","title":"Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender\n  Systems","summary":"  Federated recommender systems (FedRec) have emerged as a promising solution\nfor delivering personalized recommendations while safeguarding user privacy.\nHowever, recent studies have demonstrated their vulnerability to poisoning\nattacks. Existing attacks typically target the entire user group, which\ncompromises stealth and increases the risk of detection. In contrast,\nreal-world adversaries may prefer to prompt target items to specific user\nsubgroups, such as recommending health supplements to elderly users. Motivated\nby this gap, we introduce Spattack, the first targeted poisoning attack\ndesigned to manipulate recommendations for specific user subgroups in the\nfederated setting. Specifically, Spattack adopts a two-stage\napproximation-and-promotion strategy, which first simulates user embeddings of\ntarget/non-target subgroups and then prompts target items to the target\nsubgroups. To enhance the approximation stage, we push the inter-group\nembeddings away based on contrastive learning and augment the target group's\nrelevant item set based on clustering. To enhance the promotion stage, we\nfurther propose to adaptively tune the optimization weights between target and\nnon-target subgroups. Besides, an embedding alignment strategy is proposed to\nalign the embeddings between the target items and the relevant items. We\nconduct comprehensive experiments on three real-world datasets, comparing\nSpattack against seven state-of-the-art poisoning attacks and seven\nrepresentative defense mechanisms. Experimental results demonstrate that\nSpattack consistently achieves strong manipulation performance on the specific\nuser subgroup, while incurring minimal impact on non-target users, even when\nonly 0.1\\% of users are malicious. Moreover, Spattack maintains competitive\noverall recommendation performance and exhibits strong resilience against\nexisting mainstream defenses.\n","authors":["Bo Yan","Yurong Hao","Dingqi Liu","Huabin Sun","Pengpeng Qiao","Wei Yang Bryan Lim","Yang Cao","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2507.06258v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2507.04820v1","updated":"2025-07-07T09:38:43Z","published":"2025-07-07T09:38:43Z","title":"Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking\n  Distillation","summary":"  While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is\none of the most effective zero-shot document ranking methods, it has a\nquadratic computational complexity with respect to the number of documents to\nbe ranked, as it requires an enumeration over all possible document pairs.\nConsequently, the outstanding ranking performance of PRP has remained\nunreachable for most real-world ranking applications.\n  In this work, we propose to harness the effectiveness of PRP through pairwise\ndistillation. Specifically, we distill a pointwise student ranker from pairwise\nteacher labels generated by PRP, resulting in an efficient student model that\nretains the performance of PRP with substantially lower computational costs.\nFurthermore, we find that the distillation process can be made\nsample-efficient: with only 2% of pairs, we are able to obtain the same\nperformance as using all pairs for teacher labels. Thus, our novel approach\nprovides a solution to harness the ranking performance of PRP without incurring\nhigh computational costs during both distillation and serving.\n","authors":["Junru Wu","Le Yan","Zhen Qin","Honglei Zhuang","Paul Suganthan G. C.","Tianqi Liu","Zhe Dong","Xuanhui Wang","Harrie Oosterhuis"],"pdf_url":"https://arxiv.org/pdf/2507.04820v1.pdf","comment":"ReNeuIR 2025 (at SIGIR 2025) - 4th Workshop on Reaching Efficiency in\n  Neural Information Retrieval, July 17, 2025, Padua, Italy"},{"id":"http://arxiv.org/abs/2507.04733v1","updated":"2025-07-07T07:58:15Z","published":"2025-07-07T07:58:15Z","title":"\"This Suits You the Best\": Query Focused Comparative Explainable\n  Summarization","summary":"  Product recommendations inherently involve comparisons, yet traditional\nopinion summarization often fails to provide holistic comparative insights. We\npropose the novel task of generating Query-Focused Comparative Explainable\nSummaries (QF-CES) using Multi-Source Opinion Summarization (M-OS). To address\nthe lack of query-focused recommendation datasets, we introduce MS-Q2P,\ncomprising 7,500 queries mapped to 22,500 recommended products with metadata.\nWe leverage Large Language Models (LLMs) to generate tabular comparative\nsummaries with query-specific explanations. Our approach is personalized,\nprivacy-preserving, recommendation engine-agnostic, and category-agnostic. M-OS\nas an intermediate step reduces inference latency approximately by 40% compared\nto the direct input approach (DIA), which processes raw data directly. We\nevaluate open-source and proprietary LLMs for generating and assessing QF-CES.\nExtensive evaluations using QF-CES-PROMPT across 5 dimensions (clarity,\nfaithfulness, informativeness, format adherence, and query relevance) showed an\naverage Spearman correlation of 0.74 with human judgments, indicating its\npotential for QF-CES evaluation.\n","authors":["Arnav Attri","Anuj Attri","Pushpak Bhattacharyya","Suman Banerjee","Amey Patil","Muthusamy Chelliah","Nikesh Garera"],"pdf_url":"https://arxiv.org/pdf/2507.04733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04651v1","updated":"2025-07-07T04:09:45Z","published":"2025-07-07T04:09:45Z","title":"FindRec: Stein-Guided Entropic Flow for Multi-Modal Sequential\n  Recommendation","summary":"  Modern recommendation systems face significant challenges in processing\nmultimodal sequential data, particularly in temporal dynamics modeling and\ninformation flow coordination. Traditional approaches struggle with\ndistribution discrepancies between heterogeneous features and noise\ninterference in multimodal signals. We propose \\textbf{FindRec}~\n(\\textbf{F}lexible unified \\textbf{in}formation \\textbf{d}isentanglement for\nmulti-modal sequential \\textbf{Rec}ommendation), introducing a novel\n\"information flow-control-output\" paradigm. The framework features two key\ninnovations: (1) A Stein kernel-based Integrated Information Coordination\nModule (IICM) that theoretically guarantees distribution consistency between\nmultimodal features and ID streams, and (2) A cross-modal expert routing\nmechanism that adaptively filters and combines multimodal features based on\ntheir contextual relevance. Our approach leverages multi-head subspace\ndecomposition for routing stability and RBF-Stein gradient for unbiased\ndistribution alignment, enhanced by linear-complexity Mamba layers for\nefficient temporal modeling. Extensive experiments on three real-world datasets\ndemonstrate FindRec's superior performance over state-of-the-art baselines,\nparticularly in handling long sequences and noisy multimodal inputs. Our\nframework achieves both improved recommendation accuracy and enhanced model\ninterpretability through its modular design. The implementation code is\navailable anonymously online for easy\nreproducibility~\\footnote{https://github.com/Applied-Machine-Learning-Lab/FindRec}.\n","authors":["Maolin Wang","Yutian Xiao","Binhao Wang","Sheng Zhang","Shanshan Ye","Wanyu Wang","Hongzhi Yin","Ruocheng Guo","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2507.04651v1.pdf","comment":"Accepted by KDD 2025"},{"id":"http://arxiv.org/abs/2507.04626v1","updated":"2025-07-07T03:08:28Z","published":"2025-07-07T03:08:28Z","title":"Heterogeneous User Modeling for LLM-based Recommendation","summary":"  Leveraging Large Language Models (LLMs) for recommendation has demonstrated\nnotable success in various domains, showcasing their potential for open-domain\nrecommendation. A key challenge to advancing open-domain recommendation lies in\neffectively modeling user preferences from users' heterogeneous behaviors\nacross multiple domains. Existing approaches, including ID-based and\nsemantic-based modeling, struggle with poor generalization, an inability to\ncompress noisy interactions effectively, and the domain seesaw phenomenon. To\naddress these challenges, we propose a Heterogeneous User Modeling (HUM)\nmethod, which incorporates a compression enhancer and a robustness enhancer for\nLLM-based recommendation. The compression enhancer uses a customized prompt to\ncompress heterogeneous behaviors into a tailored token, while a masking\nmechanism enhances cross-domain knowledge extraction and understanding. The\nrobustness enhancer introduces a domain importance score to mitigate the domain\nseesaw phenomenon by guiding domain optimization. Extensive experiments on\nheterogeneous datasets validate that HUM effectively models user heterogeneity\nby achieving both high efficacy and robustness, leading to superior performance\nin open-domain recommendation.\n","authors":["Honghui Bao","Wenjie Wang","Xinyu Lin","Fengbin Zhu","Teng Sun","Fuli Feng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2507.04626v1.pdf","comment":"Accepted by RecSys 2025"},{"id":"http://arxiv.org/abs/2406.17378v4","updated":"2025-07-07T02:58:42Z","published":"2024-06-25T08:55:12Z","title":"A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns\n  Well with The Key Tokens","summary":"  Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nLLM-based embedder, the obtained text embedding will be able to be aligned with\nthe key tokens in the input text. We first fully analyze this phenomenon on\neight LLM-based embedders and show that this phenomenon is universal and is not\naffected by model architecture, training strategy, and embedding method. With a\ndeeper analysis, we find that the main change in embedding space between these\nembedders and their LLM backbones is in the first principal component. By\nadjusting the first principal component, we can align text embedding with the\nkey tokens. Finally, we give several examples to demonstrate the vast\napplication potential of this finding: (1) we propose a simple and practical\nsparse retrieval method based on the aligned tokens, which can achieve 80% of\nthe dense retrieval effect of the same model while reducing the computation\nsignificantly; (2) we show that our findings provide a novel perspective to\nhelp understand novel technologies (e.g., instruction-following embedding) and\nfuzzy concepts (e.g., semantic relatedness vs. similarity) in this field.\n","authors":["Zhijie Nie","Richong Zhang","Zhanyu Wu"],"pdf_url":"https://arxiv.org/pdf/2406.17378v4.pdf","comment":"ACL2025 Oral"},{"id":"http://arxiv.org/abs/2507.04623v1","updated":"2025-07-07T02:50:04Z","published":"2025-07-07T02:50:04Z","title":"Hierarchical Intent-guided Optimization with Pluggable LLM-Driven\n  Semantics for Session-based Recommendation","summary":"  Session-based Recommendation (SBR) aims to predict the next item a user will\nlikely engage with, using their interaction sequence within an anonymous\nsession. Existing SBR models often focus only on single-session information,\nignoring inter-session relationships and valuable cross-session insights. Some\nmethods try to include inter-session data but struggle with noise and\nirrelevant information, reducing performance. Additionally, most models rely on\nitem ID co-occurrence and overlook rich semantic details, limiting their\nability to capture fine-grained item features. To address these challenges, we\npropose a novel hierarchical intent-guided optimization approach with pluggable\nLLM-driven semantic learning for session-based recommendations, called HIPHOP.\nFirst, we introduce a pluggable embedding module based on large language models\n(LLMs) to generate high-quality semantic representations, enhancing item\nembeddings. Second, HIPHOP utilizes graph neural networks (GNNs) to model item\ntransition relationships and incorporates a dynamic multi-intent capturing\nmodule to address users' diverse interests within a session. Additionally, we\ndesign a hierarchical inter-session similarity learning module, guided by user\nintent, to capture global and local session relationships, effectively\nexploring users' long-term and short-term interests. To mitigate noise, an\nintent-guided denoising strategy is applied during inter-session learning.\nFinally, we enhance the model's discriminative capability by using contrastive\nlearning to optimize session representations. Experiments on multiple datasets\nshow that HIPHOP significantly outperforms existing methods, demonstrating its\neffectiveness in improving recommendation quality. Our code is available:\nhttps://github.com/hjx159/HIPHOP.\n","authors":["Jinpeng Chen","Jianxiang He","Huan Li","Senzhang Wang","Yuan Cao","Kaimin Wei","Zhenye Yang","Ye Ji"],"pdf_url":"https://arxiv.org/pdf/2507.04623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05301v1","updated":"2025-07-07T02:17:57Z","published":"2025-07-07T02:17:57Z","title":"News Source Citing Patterns in AI Search Systems","summary":"  AI-powered search systems are emerging as new information gatekeepers,\nfundamentally transforming how users access news and information. Despite their\ngrowing influence, the citation patterns of these systems remain poorly\nunderstood. We address this gap by analyzing data from the AI Search Arena, a\nhead-to-head evaluation platform for AI search systems. The dataset comprises\nover 24,000 conversations and 65,000 responses from models across three major\nproviders: OpenAI, Perplexity, and Google. Among the over 366,000 citations\nembedded in these responses, 9% reference news sources. We find that while\nmodels from different providers cite distinct news sources, they exhibit shared\npatterns in citation behavior. News citations concentrate heavily among a small\nnumber of outlets and display a pronounced liberal bias, though low-credibility\nsources are rarely cited. User preference analysis reveals that neither the\npolitical leaning nor the quality of cited news sources significantly\ninfluences user satisfaction. These findings reveal significant challenges in\ncurrent AI search systems and have important implications for their design and\ngovernance.\n","authors":["Kai-Cheng Yang"],"pdf_url":"https://arxiv.org/pdf/2507.05301v1.pdf","comment":"15 pages, 7 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.05227v1","updated":"2025-07-07T17:37:01Z","published":"2025-07-07T17:37:01Z","title":"NavigScene: Bridging Local Perception and Global Navigation for\n  Beyond-Visual-Range Autonomous Driving","summary":"  Autonomous driving systems have made significant advances in Q&A, perception,\nprediction, and planning based on local visual information, yet they struggle\nto incorporate broader navigational context that human drivers routinely\nutilize. We address this critical gap between local sensor data and global\nnavigation information by proposing NavigScene, an auxiliary navigation-guided\nnatural language dataset that simulates a human-like driving environment within\nautonomous driving systems. Moreover, we develop three complementary paradigms\nto leverage NavigScene: (1) Navigation-guided Reasoning, which enhances\nvision-language models by incorporating navigation context into the prompting\napproach; (2) Navigation-guided Preference Optimization, a reinforcement\nlearning method that extends Direct Preference Optimization to improve\nvision-language model responses by establishing preferences for\nnavigation-relevant summarized information; and (3) Navigation-guided\nVision-Language-Action model, which integrates navigation guidance and\nvision-language models with conventional driving models through feature fusion.\nExtensive experiments demonstrate that our approaches significantly improve\nperformance across perception, prediction, planning, and question-answering\ntasks by enabling reasoning capabilities beyond visual range and improving\ngeneralization to diverse driving scenarios. This work represents a significant\nstep toward more comprehensive autonomous driving systems capable of navigating\ncomplex, unfamiliar environments with greater reliability and safety.\n","authors":["Qucheng Peng","Chen Bai","Guoxiang Zhang","Bo Xu","Xiaotong Liu","Xiaoyin Zheng","Chen Chen","Cheng Lu"],"pdf_url":"https://arxiv.org/pdf/2507.05227v1.pdf","comment":"Accepted by ACM Multimedia 2025"},{"id":"http://arxiv.org/abs/2507.05113v1","updated":"2025-07-07T15:29:26Z","published":"2025-07-07T15:29:26Z","title":"CLIP-Guided Backdoor Defense through Entropy-Based Poisoned Dataset\n  Separation","summary":"  Deep Neural Networks (DNNs) are susceptible to backdoor attacks, where\nadversaries poison training data to implant backdoor into the victim model.\nCurrent backdoor defenses on poisoned data often suffer from high computational\ncosts or low effectiveness against advanced attacks like clean-label and\nclean-image backdoors. To address them, we introduce CLIP-Guided backdoor\nDefense (CGD), an efficient and effective method that mitigates various\nbackdoor attacks. CGD utilizes a publicly accessible CLIP model to identify\ninputs that are likely to be clean or poisoned. It then retrains the model with\nthese inputs, using CLIP's logits as a guidance to effectively neutralize the\nbackdoor. Experiments on 4 datasets and 11 attack types demonstrate that CGD\nreduces attack success rates (ASRs) to below 1% while maintaining clean\naccuracy (CA) with a maximum drop of only 0.3%, outperforming existing\ndefenses. Additionally, we show that clean-data-based defenses can be adapted\nto poisoned data using CGD. Also, CGD exhibits strong robustness, maintaining\nlow ASRs even when employing a weaker CLIP model or when CLIP itself is\ncompromised by a backdoor. These findings underscore CGD's exceptional\nefficiency, effectiveness, and applicability for real-world backdoor defense\nscenarios. Code: https://github.com/binyxu/CGD.\n","authors":["Binyan Xu","Fan Yang","Xilin Dai","Di Tang","Kehuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.05113v1.pdf","comment":"15 pages, 9 figures, 15 tables. To appear in the Proceedings of the\n  32nd ACM International Conference on Multimedia (MM '25)"},{"id":"http://arxiv.org/abs/2503.24164v2","updated":"2025-07-07T14:41:48Z","published":"2025-03-31T14:46:34Z","title":"SVLA: A Unified Speech-Vision-Language Assistant with Multimodal\n  Reasoning and Speech Generation","summary":"  Large vision and language models show strong performance in tasks like image\ncaptioning, visual question answering, and retrieval. However, challenges\nremain in integrating speech, text, and vision into a unified model, especially\nfor spoken tasks. Speech generation methods vary (some produce speech\ndirectly), others through text (but their impact on quality is unclear).\nEvaluation often relies on automatic speech recognition, which may introduce\nbias. We propose SVLA, a unified speech vision language model based on a\ntransformer architecture that handles multimodal inputs and outputs. We train\nit on 38.2 million speech text image examples, including 64.1 hours of\nsynthetic speech. We also introduce Speech VQA Accuracy, a new metric for\nevaluating spoken responses. SVLA improves multimodal understanding and\ngeneration by better combining speech, vision, and language.\n","authors":["Ngoc Dung Huynh","Mohamed Reda Bouadjenek","Imran Razzak","Hakim Hacid","Sunil Aryal"],"pdf_url":"https://arxiv.org/pdf/2503.24164v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2507.04959v1","updated":"2025-07-07T13:01:50Z","published":"2025-07-07T13:01:50Z","title":"Hear-Your-Click: Interactive Video-to-Audio Generation via Object-aware\n  Contrastive Audio-Visual Fine-tuning","summary":"  Video-to-audio (V2A) generation shows great potential in fields such as film\nproduction. Despite significant advances, current V2A methods, which rely on\nglobal video information, struggle with complex scenes and often fail to\ngenerate audio tailored to specific objects or regions in the videos. To\naddress these limitations, we introduce Hear-Your-Click, an interactive V2A\nframework that enables users to generate sounds for specific objects in the\nvideos by simply clicking on the frame. To achieve this, we propose\nObject-aware Contrastive Audio-Visual Fine-tuning (OCAV) with a Mask-guided\nVisual Encoder (MVE) to obtain object-level visual features aligned with\ncorresponding audio segments. Furthermore, we tailor two data augmentation\nstrategies: Random Video Stitching (RVS) and Mask-guided Loudness Modulation\n(MLM), aimed at enhancing the model's sensitivity to the segmented objects. To\neffectively measure the audio-visual correspondence, we design a new evaluation\nmetric, the CAV score, for evaluation. Extensive experiments demonstrate that\nour framework offers more precise control and improved generation performance\nacross various metrics. Project Page:\nhttps://github.com/SynapGrid/Hear-Your-Click\n","authors":["Yingshan Liang","Keyu Fan","Zhicheng Du","Yiran Wang","Qingyang Shi","Xinyu Zhang","Jiasheng Lu","Peiwu Qin"],"pdf_url":"https://arxiv.org/pdf/2507.04959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04958v1","updated":"2025-07-07T13:01:06Z","published":"2025-07-07T13:01:06Z","title":"Boosting Temporal Sentence Grounding via Causal Inference","summary":"  Temporal Sentence Grounding (TSG) aims to identify relevant moments in an\nuntrimmed video that semantically correspond to a given textual query. Despite\nexisting studies having made substantial progress, they often overlook the\nissue of spurious correlations between video and textual queries. These\nspurious correlations arise from two primary factors: (1) inherent biases in\nthe textual data, such as frequent co-occurrences of specific verbs or phrases,\nand (2) the model's tendency to overfit to salient or repetitive patterns in\nvideo content. Such biases mislead the model into associating textual cues with\nincorrect visual moments, resulting in unreliable predictions and poor\ngeneralization to out-of-distribution examples. To overcome these limitations,\nwe propose a novel TSG framework, causal intervention and counterfactual\nreasoning that utilizes causal inference to eliminate spurious correlations and\nenhance the model's robustness. Specifically, we first formulate the TSG task\nfrom a causal perspective with a structural causal model. Then, to address\nunobserved confounders reflecting textual biases toward specific verbs or\nphrases, a textual causal intervention is proposed, utilizing do-calculus to\nestimate the causal effects. Furthermore, visual counterfactual reasoning is\nperformed by constructing a counterfactual scenario that focuses solely on\nvideo features, excluding the query and fused multi-modal features. This allows\nus to debias the model by isolating and removing the influence of the video\nfrom the overall effect. Experiments on public datasets demonstrate the\nsuperiority of the proposed method. The code is available at\nhttps://github.com/Tangkfan/CICR.\n","authors":["Kefan Tang","Lihuo He","Jisheng Dang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2507.04958v1.pdf","comment":"Accepted by ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.04955v1","updated":"2025-07-07T12:56:20Z","published":"2025-07-07T12:56:20Z","title":"EXPOTION: Facial Expression and Motion Control for Multimodal Music\n  Generation","summary":"  We propose Expotion (Facial Expression and Motion Control for Multimodal\nMusic Generation), a generative model leveraging multimodal visual controls -\nspecifically, human facial expressions and upper-body motion - as well as text\nprompts to produce expressive and temporally accurate music. We adopt\nparameter-efficient fine-tuning (PEFT) on the pretrained text-to-music\ngeneration model, enabling fine-grained adaptation to the multimodal controls\nusing a small dataset. To ensure precise synchronization between video and\nmusic, we introduce a temporal smoothing strategy to align multiple modalities.\nExperiments demonstrate that integrating visual features alongside textual\ndescriptions enhances the overall quality of generated music in terms of\nmusicality, creativity, beat-tempo consistency, temporal alignment with the\nvideo, and text adherence, surpassing both proposed baselines and existing\nstate-of-the-art video-to-music generation models. Additionally, we introduce a\nnovel dataset consisting of 7 hours of synchronized video recordings capturing\nexpressive facial and upper-body gestures aligned with corresponding music,\nproviding significant potential for future research in multimodal and\ninteractive music generation.\n","authors":["Fathinah Izzati","Xinyue Li","Gus Xia"],"pdf_url":"https://arxiv.org/pdf/2507.04955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03897v5","updated":"2025-07-07T10:40:56Z","published":"2025-02-06T09:18:30Z","title":"UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video\n  Generation","summary":"  With the rise of diffusion models, audio-video generation has been\nrevolutionized. However, most existing methods rely on separate modules for\neach modality, with limited exploration of unified generative architectures. In\naddition, many are confined to a single task and small-scale datasets. To\novercome these limitations, we introduce UniForm, a unified multi-task\ndiffusion transformer that generates both audio and visual modalities in a\nshared latent space. By using a unified denoising network, UniForm captures the\ninherent correlations between sound and vision. Additionally, we propose\ntask-specific noise schemes and task tokens, enabling the model to support\nmultiple tasks with a single set of parameters, including video-to-audio,\naudio-to-video and text-to-audio-video generation. Furthermore, by leveraging\nlarge language models and a large-scale text-audio-video combined dataset,\nUniForm achieves greater generative diversity than prior approaches.\nExperiments show that UniForm achieves performance close to the\nstate-of-the-art single-task models across three generation tasks, with\ngenerated content that is not only highly aligned with real-world data\ndistributions but also enables more diverse and fine-grained generation.\n","authors":["Lei Zhao","Linfeng Feng","Dongxu Ge","Rujin Chen","Fangqiu Yi","Chi Zhang","Xiao-Lei Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2502.03897v5.pdf","comment":"Our demos are available at https://uniform-t2av.github.io/"},{"id":"http://arxiv.org/abs/2507.04776v1","updated":"2025-07-07T08:52:06Z","published":"2025-07-07T08:52:06Z","title":"Improving BERT for Symbolic Music Understanding Using Token Denoising\n  and Pianoroll Prediction","summary":"  We propose a pre-trained BERT-like model for symbolic music understanding\nthat achieves competitive performance across a wide range of downstream tasks.\nTo achieve this target, we design two novel pre-training objectives, namely\ntoken correction and pianoroll prediction. First, we sample a portion of note\ntokens and corrupt them with a limited amount of noise, and then train the\nmodel to denoise the corrupted tokens; second, we also train the model to\npredict bar-level and local pianoroll-derived representations from the\ncorrupted note tokens. We argue that these objectives guide the model to better\nlearn specific musical knowledge such as pitch intervals. For evaluation, we\npropose a benchmark that incorporates 12 downstream tasks ranging from chord\nestimation to symbolic genre classification. Results confirm the effectiveness\nof the proposed pre-training objectives on downstream tasks.\n","authors":["Jun-You Wang","Li Su"],"pdf_url":"https://arxiv.org/pdf/2507.04776v1.pdf","comment":"Accepted at ISMIR 2025"},{"id":"http://arxiv.org/abs/2507.04758v1","updated":"2025-07-07T08:32:53Z","published":"2025-07-07T08:32:53Z","title":"Music2Palette: Emotion-aligned Color Palette Generation via Cross-Modal\n  Representation Learning","summary":"  Emotion alignment between music and palettes is crucial for effective\nmultimedia content, yet misalignment creates confusion that weakens the\nintended message. However, existing methods often generate only a single\ndominant color, missing emotion variation. Others rely on indirect mappings\nthrough text or images, resulting in the loss of crucial emotion details. To\naddress these challenges, we present Music2Palette, a novel method for\nemotion-aligned color palette generation via cross-modal representation\nlearning. We first construct MuCED, a dataset of 2,634 expert-validated\nmusic-palette pairs aligned through Russell-based emotion vectors. To directly\ntranslate music into palettes, we propose a cross-modal representation learning\nframework with a music encoder and color decoder. We further propose a\nmulti-objective optimization approach that jointly enhances emotion alignment,\ncolor diversity, and palette coherence. Extensive experiments demonstrate that\nour method outperforms current methods in interpreting music emotion and\ngenerating attractive and diverse color palettes. Our approach enables\napplications like music-driven image recoloring, video generating, and data\nvisualization, bridging the gap between auditory and visual emotion\nexperiences.\n","authors":["Jiayun Hu","Yueyi He","Tianyi Liang","Changbo Wang","Chenhui Li"],"pdf_url":"https://arxiv.org/pdf/2507.04758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14396v2","updated":"2025-07-07T03:39:01Z","published":"2025-06-17T10:51:34Z","title":"Manipulated Regions Localization For Partially Deepfake Audio: A Survey","summary":"  With the development of audio deepfake techniques, attacks with partially\ndeepfake audio are beginning to rise. Compared to fully deepfake, it is much\nharder to be identified by the detector due to the partially cryptic\nmanipulation, resulting in higher security risks. Although some studies have\nbeen launched, there is no comprehensive review to systematically introduce the\ncurrent situations and development trends for addressing this issue. Thus, in\nthis survey, we are the first to outline a systematic introduction for\npartially deepfake audio manipulated region localization tasks, including the\nfundamentals, branches of existing methods, current limitations and potential\ntrends, providing a revealing insight into this scope.\n","authors":["Jiayi He","Jiangyan Yi","Jianhua Tao","Siding Zeng","Hao Gu"],"pdf_url":"https://arxiv.org/pdf/2506.14396v2.pdf","comment":"Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence"}]},"2025-07-06T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2503.14800v2","updated":"2025-07-06T21:41:20Z","published":"2025-03-19T00:24:01Z","title":"Long Context Modeling with Ranked Memory-Augmented Retrieval","summary":"  Effective long-term memory management is crucial for language models handling\nextended contexts. We introduce a novel framework that dynamically ranks memory\nentries based on relevance. Unlike previous works, our model introduces a novel\nrelevance scoring and a pointwise re-ranking model for key-value embeddings,\ninspired by learning-to-rank techniques in information retrieval. Enhanced\nRanked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on\nstandard benchmarks.\n","authors":["Ghadir Alselwi","Hao Xue","Shoaib Jameel","Basem Suleiman","Hakim Hacid","Flora D. Salim","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2503.14800v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04410v1","updated":"2025-07-06T14:54:07Z","published":"2025-07-06T14:54:07Z","title":"Multimedia Verification Through Multi-Agent Deep Research Multimodal\n  Large Language Models","summary":"  This paper presents our submission to the ACMMM25 - Grand Challenge on\nMultimedia Verification. We developed a multi-agent verification system that\ncombines Multimodal Large Language Models (MLLMs) with specialized verification\ntools to detect multimedia misinformation. Our system operates through six\nstages: raw data processing, planning, information extraction, deep research,\nevidence collection, and report generation. The core Deep Researcher Agent\nemploys four tools: reverse image search, metadata analysis, fact-checking\ndatabases, and verified news processing that extracts spatial, temporal,\nattribution, and motivational context. We demonstrate our approach on a\nchallenge dataset sample involving complex multimedia content. Our system\nsuccessfully verified content authenticity, extracted precise geolocation and\ntiming information, and traced source attribution across multiple platforms,\neffectively addressing real-world multimedia verification scenarios.\n","authors":["Huy Hoan Le","Van Sy Thinh Nguyen","Thi Le Chi Dang","Vo Thanh Khang Nguyen","Truong Thanh Hung Nguyen","Hung Cao"],"pdf_url":"https://arxiv.org/pdf/2507.04410v1.pdf","comment":"33rd ACM International Conference on Multimedia (MM'25) Grand\n  Challenge on Multimedia Verification"},{"id":"http://arxiv.org/abs/2501.17788v3","updated":"2025-07-06T10:07:26Z","published":"2025-01-29T17:26:47Z","title":"WARP: An Efficient Engine for Multi-Vector Retrieval","summary":"  Multi-vector retrieval methods such as ColBERT and its recent variant, the\nConteXtualized Token Retriever (XTR), offer high accuracy but face efficiency\nchallenges at scale. To address this, we present WARP, a retrieval engine that\nsubstantially improves the efficiency of retrievers trained with the XTR\nobjective through three key innovations: (1) WARP$_\\text{SELECT}$ for dynamic\nsimilarity imputation; (2) implicit decompression, avoiding costly vector\nreconstruction during retrieval; and (3) a two-stage reduction process for\nefficient score aggregation. Combined with highly-optimized C++ kernels, our\nsystem reduces end-to-end latency compared to XTR's reference implementation by\n41x, and achieves a 3x speedup over the ColBERTv2/PLAID engine, while\npreserving retrieval quality.\n","authors":["Jan Luca Scheerer","Matei Zaharia","Christopher Potts","Gustavo Alonso","Omar Khattab"],"pdf_url":"https://arxiv.org/pdf/2501.17788v3.pdf","comment":"Accepted at SIGIR 2025"},{"id":"http://arxiv.org/abs/2506.20854v2","updated":"2025-07-06T09:59:04Z","published":"2025-06-25T22:00:12Z","title":"Towards Two-Stage Counterfactual Learning to Rank","summary":"  Counterfactual learning to rank (CLTR) aims to learn a ranking policy from\nuser interactions while correcting for the inherent biases in interaction data,\nsuch as position bias. Existing CLTR methods assume a single ranking policy\nthat selects top-K ranking from the entire document candidate set. In\nreal-world applications, the candidate document set is on the order of\nmillions, making a single-stage ranking policy impractical. In order to scale\nto millions of documents, real-world ranking systems are designed in a\ntwo-stage fashion, with a candidate generator followed by a ranker. The\nexisting CLTR method for a two-stage offline ranking system only considers the\ntop-1 ranking set-up and only focuses on training the candidate generator, with\nthe ranker fixed. A CLTR method for training both the ranker and candidate\ngenerator jointly is missing from the existing literature. In this paper, we\npropose a two-stage CLTR estimator that considers the interaction between the\ntwo stages and estimates the joint value of the two policies offline. In\naddition, we propose a novel joint optimization method to train the candidate\nand ranker policies, respectively. To the best of our knowledge, we are the\nfirst to propose a CLTR estimator and learning method for two-stage ranking.\nExperimental results on a semi-synthetic benchmark demonstrate the\neffectiveness of the proposed joint CLTR method over baselines.\n","authors":["Shashank Gupta","Yiming Liao","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2506.20854v2.pdf","comment":"Accepted at ICTIR 2025 (co-located with SIGIR 2025)"},{"id":"http://arxiv.org/abs/2507.04294v1","updated":"2025-07-06T08:39:26Z","published":"2025-07-06T08:39:26Z","title":"BiFair: A Fairness-aware Training Framework for LLM-enhanced Recommender\n  Systems via Bi-level Optimization","summary":"  Large Language Model-enhanced Recommender Systems (LLM-enhanced RSs) have\nemerged as a powerful approach to improving recommendation quality by\nleveraging LLMs to generate item representations. Despite these advancements,\nthe integration of LLMs raises severe fairness concerns. Existing studies\nreveal that LLM-based RSs exhibit greater unfairness than traditional RSs, yet\nfairness issues in LLM-enhanced RSs remain largely unexplored. In this paper,\nour empirical study reveals that while LLM-enhanced RSs improve fairness across\nitem groups, a significant fairness gap persists. Further enhancement remains\nchallenging due to the architectural differences and varying sources of\nunfairness inherent in LLM-enhanced RSs. To bridge this gap, we first decompose\nunfairness into i) \\textit{prior unfairness} in LLM-generated representations\nand ii) \\textit{training unfairness} in recommendation models. Then, we propose\nBiFair, a bi-level optimization-based fairness-aware training framework\ndesigned to mitigate both prior and training unfairness simultaneously. BiFair\noptimizes two sets of learnable parameters: LLM-generated representations and a\ntrainable projector in the recommendation model, using a two-level nested\noptimization process. Additionally, we introduce an adaptive inter-group\nbalancing mechanism, leveraging multi-objective optimization principles to\ndynamically balance fairness across item groups. Extensive experiments on three\nreal-world datasets demonstrate that BiFair significantly mitigates unfairness\nand outperforms previous state-of-the-art methods.\n","authors":["Jiaming Zhang","Yuyuan Li","Yiqun Xu","Li Zhang","Xiaohua Feng","Zhifei Ren","Chaochao Chen"],"pdf_url":"https://arxiv.org/pdf/2507.04294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04230v1","updated":"2025-07-06T03:40:54Z","published":"2025-07-06T03:40:54Z","title":"High-Resolution Sustain Pedal Depth Estimation from Piano Audio Across\n  Room Acoustics","summary":"  Piano sustain pedal detection has previously been approached as a binary\non/off classification task, limiting its application in real-world piano\nperformance scenarios where pedal depth significantly influences musical\nexpression. This paper presents a novel approach for high-resolution estimation\nthat predicts continuous pedal depth values. We introduce a Transformer-based\narchitecture that not only matches state-of-the-art performance on the\ntraditional binary classification task but also achieves high accuracy in\ncontinuous pedal depth estimation. Furthermore, by estimating continuous\nvalues, our model provides musically meaningful predictions for sustain pedal\nusage, whereas baseline models struggle to capture such nuanced expressions\nwith their binary detection approach. Additionally, this paper investigates the\ninfluence of room acoustics on sustain pedal estimation using a synthetic\ndataset that includes varied acoustic conditions. We train our model with\ndifferent combinations of room settings and test it in an unseen new\nenvironment using a \"leave-one-out\" approach. Our findings show that the two\nbaseline models and ours are not robust to unseen room conditions. Statistical\nanalysis further confirms that reverberation influences model predictions and\nintroduces an overestimation bias.\n","authors":["Kun Fang","Hanwen Zhang","Ziyu Wang","Ichiro Fujinaga"],"pdf_url":"https://arxiv.org/pdf/2507.04230v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.04377v1","updated":"2025-07-06T12:50:07Z","published":"2025-07-06T12:50:07Z","title":"Multi-Modal Semantic Parsing for the Interpretation of Tombstone\n  Inscriptions","summary":"  Tombstones are historically and culturally rich artifacts, encapsulating\nindividual lives, community memory, historical narratives and artistic\nexpression. Yet, many tombstones today face significant preservation\nchallenges, including physical erosion, vandalism, environmental degradation,\nand political shifts. In this paper, we introduce a novel multi-modal framework\nfor tombstones digitization, aiming to improve the interpretation, organization\nand retrieval of tombstone content. Our approach leverages vision-language\nmodels (VLMs) to translate tombstone images into structured Tombstone Meaning\nRepresentations (TMRs), capturing both image and text information. To further\nenrich semantic parsing, we incorporate retrieval-augmented generation (RAG)\nfor integrate externally dependent elements such as toponyms, occupation codes,\nand ontological concepts. Compared to traditional OCR-based pipelines, our\nmethod improves parsing accuracy from an F1 score of 36.1 to 89.5. We\nadditionally evaluate the model's robustness across diverse linguistic and\ncultural inscriptions, and simulate physical degradation through image fusion\nto assess performance under noisy or damaged conditions. Our work represents\nthe first attempt to formalize tombstone understanding using large\nvision-language models, presenting implications for heritage preservation.\n","authors":["Xiao Zhang","Johan Bos"],"pdf_url":"https://arxiv.org/pdf/2507.04377v1.pdf","comment":"Accepted by ACMMM 2025"},{"id":"http://arxiv.org/abs/2310.13103v2","updated":"2025-07-06T11:50:10Z","published":"2023-10-19T19:01:26Z","title":"AVTENet: A Human-Cognition-Inspired Audio-Visual Transformer-Based\n  Ensemble Network for Video Deepfake Detection","summary":"  The recent proliferation of hyper-realistic deepfake videos has drawn\nattention to the threat of audio and visual forgeries. Most previous studies on\ndetecting artificial intelligence-generated fake videos only utilize visual\nmodality or audio modality. While some methods exploit audio and visual\nmodalities to detect forged videos, they have not been comprehensively\nevaluated on multimodal datasets of deepfake videos involving acoustic and\nvisual manipulations, and are mostly based on convolutional neural networks\nwith low detection accuracy. Considering that human cognition instinctively\nintegrates multisensory information including audio and visual cues to perceive\nand interpret content and the success of transformer in various fields, this\nstudy introduces the audio-visual transformer-based ensemble network (AVTENet).\nThis innovative framework tackles the complexities of deepfake technology by\nintegrating both acoustic and visual manipulations to enhance the accuracy of\nvideo forgery detection. Specifically, the proposed model integrates several\npurely transformer-based variants that capture video, audio, and audio-visual\nsalient cues to reach a consensus in prediction. For evaluation, we use the\nrecently released benchmark multimodal audio-video FakeAVCeleb dataset. For a\ndetailed analysis, we evaluate AVTENet, its variants, and several existing\nmethods on multiple test sets of the FakeAVCeleb dataset. Experimental results\nshow that the proposed model outperforms all existing methods and achieves\nstate-of-the-art performance on Testset-I and Testset-II of the FakeAVCeleb\ndataset. We also compare AVTENet against humans in detecting video forgery. The\nresults show that AVTENet significantly outperforms humans.\n","authors":["Ammarah Hashmi","Sahibzada Adil Shahzad","Chia-Wen Lin","Yu Tsao","Hsin-Min Wang"],"pdf_url":"https://arxiv.org/pdf/2310.13103v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18151v2","updated":"2025-07-06T04:44:02Z","published":"2024-10-23T03:11:01Z","title":"Music102: An $D_{12}$-equivariant transformer for chord progression\n  accompaniment","summary":"  We present Music102, an advanced model aimed at enhancing chord progression\naccompaniment through a $D_{12}$-equivariant transformer. Inspired by group\ntheory and symbolic music structures, Music102 leverages musical symmetry--such\nas transposition and reflection operations--integrating these properties into\nthe transformer architecture. By encoding prior music knowledge, the model\nmaintains equivariance across both melody and chord sequences. The POP909\ndataset was employed to train and evaluate Music102, revealing significant\nimprovements over the non-equivariant Music101 prototype Music101 in both\nweighted loss and exact accuracy metrics, despite using fewer parameters. This\nwork showcases the adaptability of self-attention mechanisms and layer\nnormalization to the discrete musical domain, addressing challenges in\ncomputational music analysis. With its stable and flexible neural framework,\nMusic102 sets the stage for further exploration in equivariant music generation\nand computational composition tools, bridging mathematical theory with\npractical music performance.\n","authors":["Weiliang Luo"],"pdf_url":"https://arxiv.org/pdf/2410.18151v2.pdf","comment":"10 pages, 3 figures"}]},"2025-07-05T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.04182v1","updated":"2025-07-05T22:38:10Z","published":"2025-07-05T22:38:10Z","title":"Navigating Speech Recording Collections with AI-Generated Illustrations","summary":"  Although the amount of available spoken content is steadily increasing,\nextracting information and knowledge from speech recordings remains\nchallenging. Beyond enhancing traditional information retrieval methods such as\nspeech search and keyword spotting, novel approaches for navigating and\nsearching spoken content need to be explored and developed. In this paper, we\npropose a novel navigational method for speech archives that leverages recent\nadvances in language and multimodal generative models. We demonstrate our\napproach with a Web application that organizes data into a structured format\nusing interactive mind maps and image generation tools. The system is\nimplemented using the TED-LIUM~3 dataset, which comprises over 2,000 speech\ntranscripts and audio files of TED Talks. Initial user tests using a System\nUsability Scale (SUS) questionnaire indicate the application's potential to\nsimplify the exploration of large speech collections.\n","authors":["Sirina Håland","Trond Karlsen Strøm","Petra Galuščáková"],"pdf_url":"https://arxiv.org/pdf/2507.04182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04174v1","updated":"2025-07-05T22:06:42Z","published":"2025-07-05T22:06:42Z","title":"Cloud Digital Forensic Readiness: An Open Source Approach to Law\n  Enforcement Request Management","summary":"  Cloud Forensics presents a multi-jurisdictional challenge that may undermines\nthe success of digital forensic investigations (DFIs). The growing volumes of\ndomiciled and foreign law enforcement (LE) requests, the latency and complexity\nof formal channels for crossborder data access are challenging issues. In this\npaper, we first discuss major Cloud Service Providers (CSPs) transparency\nreports and law enforcement guidelines, then propose an abstract architecture\nfor a Cloud Law Enforcement Requests Management System (CLERMS). A proof of\nconcept of the proposed solution is developed, deployed and validated by two\nrealistic scenarios, in addition to an economic estimation of its associated\ncosts. Based on available open source components, our solution is for the\nbenefit of both CSPs and Cloud Service Consumers (CSCs), and aims to enhance\nthe due Cloud Digital Forensic Readiness (CDFR).\n","authors":["Abdellah Akilal","M-Tahar Kechadi"],"pdf_url":"https://arxiv.org/pdf/2507.04174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01297v2","updated":"2025-07-05T21:33:54Z","published":"2025-07-02T02:35:47Z","title":"Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive\n  Benchmarks","summary":"  Retrieval-augmented Generation (RAG) has primarily been studied in limited\nsettings, such as factoid question answering; more challenging,\nreasoning-intensive benchmarks have seen limited success from minimal RAG. In\nthis work, we challenge this prevailing view on established,\nreasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We\nidentify a key missing component in prior work: a usable, web-scale datastore\naligned with the breadth of pretraining data. To this end, we introduce\nCompactDS: a diverse, high-quality, web-scale datastore that achieves high\nretrieval accuracy and subsecond latency on a single-node. The key insights are\n(1) most web content can be filtered out without sacrificing coverage, and a\ncompact, high-quality subset is sufficient; and (2) combining in-memory\napproximate nearest neighbor (ANN) retrieval and on-disk exact search balances\nspeed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves\nconsistent accuracy improvements across all benchmarks and model sizes\n(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,\nand 19% on MATH. No single data source suffices alone, highlighting the\nimportance of diversity of sources (web crawls, curated math, academic papers,\ntextbooks). Finally, we show that our carefully designed in-house datastore\nmatches or outperforms web search engines such as Google Search, as well as\nrecently proposed, complex agent-based RAG systems--all while maintaining\nsimplicity, reproducibility, and self-containment. We release CompactDS and our\nretrieval pipeline, supporting future research exploring retrieval-based AI\nsystems.\n","authors":["Xinxi Lyu","Michael Duan","Rulin Shao","Pang Wei Koh","Sewon Min"],"pdf_url":"https://arxiv.org/pdf/2507.01297v2.pdf","comment":"33 pages, 2 figures, 27 tables"},{"id":"http://arxiv.org/abs/2507.05295v1","updated":"2025-07-05T21:16:02Z","published":"2025-07-05T21:16:02Z","title":"Enhancing Learning Path Recommendation via Multi-task Learning","summary":"  Personalized learning is a student-centered educational approach that adapts\ncontent, pace, and assessment to meet each learner's unique needs. As the key\ntechnique to implement the personalized learning, learning path recommendation\nsequentially recommends personalized learning items such as lectures and\nexercises. Advances in deep learning, particularly deep reinforcement learning,\nhave made modeling such recommendations more practical and effective. This\npaper proposes a multi-task LSTM model that enhances learning path\nrecommendation by leveraging shared information across tasks. The approach\nreframes learning path recommendation as a sequence-to-sequence (Seq2Seq)\nprediction problem, generating personalized learning paths from a learner's\nhistorical interactions. The model uses a shared LSTM layer to capture common\nfeatures for both learning path recommendation and deep knowledge tracing,\nalong with task-specific LSTM layers for each objective. To avoid redundant\nrecommendations, a non-repeat loss penalizes repeated items within the\nrecommended learning path. Experiments on the ASSIST09 dataset show that the\nproposed model significantly outperforms baseline methods for the learning path\nrecommendation.\n","authors":["Afsana Nasrin","Lijun Qian","Pamela Obiomon","Xishuang Dong"],"pdf_url":"https://arxiv.org/pdf/2507.05295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04072v1","updated":"2025-07-05T15:32:41Z","published":"2025-07-05T15:32:41Z","title":"CTR-Guided Generative Query Suggestion in Conversational Search","summary":"  Generating effective query suggestions in conversational search requires\naligning model outputs with user preferences, which is challenging due to\nsparse and noisy click signals. We propose GQS, a generative framework that\nintegrates click modeling and preference optimization to enhance real-world\nuser engagement. GQS consists of three key components: (1) a Multi-Source CTR\nModeling module that captures diverse contextual signals to estimate\nfine-grained click-through rates; (2) a Diversity-Aware Preference Alignment\nstrategy using CTR-weighted Direct Preference Optimization (DPO), which\nbalances relevance and semantic diversity; and (3) a CTR-Calibrated Iterative\nOptimization process that jointly refines the CTR and generation models across\ntraining rounds. Experiments on two real-world tasks demonstrate that GQS\noutperforms strong baselines in CTR, relevance, and diversity.\n","authors":["Erxue Min","Hsiu-Yuan Huang","Xihong Yang","Min Yang","Xin Jia","Yunfang Wu","Hengyi Cai","Junfeng Wang","Shuaiqiang Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2507.04072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06335v2","updated":"2025-07-05T13:39:26Z","published":"2025-05-31T13:59:44Z","title":"FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in\n  Finance-Specific Deployment of Large Language Models","summary":"  In natural language processing (NLP), the focus has shifted from encoder-only\ntiny language models like BERT to decoder-only large language models(LLMs) such\nas GPT-3. However, LLMs' practical application in the financial sector has\nrevealed three limitations: (1) LLMs often perform worse than fine-tuned BERT\non discriminative tasks despite costing much higher computational resources,\nsuch as market sentiment analysis in financial reports; (2) Application on\ngenerative tasks heavily relies on retrieval augmented generation (RAG) methods\nto provide current and specialized information, with general retrievers showing\nsuboptimal performance on domain-specific retrieval tasks; (3) There are\nadditional inadequacies in other feature-based scenarios, such as topic\nmodeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained\non a high-quality, financial-specific corpus of 32b tokens. This represents the\nlargest known Chinese financial pretraining corpus for models of this parameter\nsize. As a better backbone, FinBERT2 can bridge the gap in the\nfinancial-specific deployment of LLMs through the following achievements: (1)\nDiscriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT\nvariants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five\nfinancial classification tasks. (2) Contrastive fine-tuned models\n(Fin-Retrievers) outperform both open-source (e.g., +6.8\\% avg improvement over\nBGE-base-zh) and proprietary (e.g., +4.2\\% avg improvement over OpenAI's\ntext-embedding-3-large) embedders across five financial retrieval tasks; (3)\nBuilding on FinBERT2 variants, we construct the Fin-TopicModel, which enables\nsuperior clustering and topic representation for financial titles. Our work\nrevisits financial BERT models through comparative analysis with contemporary\nLLMs and offers practical insights for effectively utilizing FinBERT in the\nLLMs era.\n","authors":["Xuan Xu","Fufang Wen","Beilin Chu","Zhibing Fu","Qinhong Lin","Jiaqi Liu","Binjie Fei","Yu Li","Linna Zhou","Zhongliang Yang"],"pdf_url":"https://arxiv.org/pdf/2506.06335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10208v3","updated":"2025-07-05T12:59:17Z","published":"2025-04-14T13:21:29Z","title":"From Prompting to Alignment: A Generative Framework for Query\n  Recommendation","summary":"  In modern search systems, search engines often suggest relevant queries to\nusers through various panels or components, helping refine their information\nneeds. Traditionally, these recommendations heavily rely on historical search\nlogs to build models, which suffer from cold-start or long-tail issues.\nFurthermore, tasks such as query suggestion, completion or clarification are\nstudied separately by specific design, which lacks generalizability and hinders\nadaptation to novel applications. Despite recent attempts to explore the use of\nLLMs for query recommendation, these methods mainly rely on the inherent\nknowledge of LLMs or external sources like few-shot examples, retrieved\ndocuments, or knowledge bases, neglecting the importance of the calibration and\nalignment with user feedback, thus limiting their practical utility. To address\nthese challenges, we first propose a general Generative Query Recommendation\n(GQR) framework that aligns LLM-based query generation with user preference.\nSpecifically, we unify diverse query recommendation tasks by a universal prompt\nframework, leveraging the instruct-following capability of LLMs for effective\ngeneration. Secondly, we align LLMs with user feedback via presenting a\nCTR-alignment framework, which involves training a query-wise CTR predictor as\na process reward model and employing list-wise preference alignment to maximize\nthe click probability of the generated query list. Furthermore, recognizing the\ninconsistency between LLM knowledge and proactive search intents arising from\nthe separation of user-initiated queries from models, we align LLMs with user\ninitiative via retrieving co-occurrence queries as side information when\nhistorical logs are available.\n","authors":["Erxue Min","Hsiu-Yuan Huang","Xihong Yang","Min Yang","Xin Jia","Yunfang Wu","Hengyi Cai","Junfeng Wang","Shuaiqiang Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2504.10208v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03836v2","updated":"2025-07-05T12:37:33Z","published":"2025-05-04T20:35:15Z","title":"Explainable Coarse-to-Fine Ancient Manuscript Duplicates Discovery","summary":"  Ancient manuscripts are the primary source of ancient linguistic corpora.\nHowever, many ancient manuscripts exhibit duplications due to unintentional\nrepeated publication or deliberate forgery. The Dead Sea Scrolls, for example,\ninclude counterfeit fragments, whereas Oracle Bones (OB) contain both\nrepublished materials and fabricated specimens. Identifying ancient manuscript\nduplicates is of great significance for both archaeological curation and\nancient history study. In this work, we design a progressive OB duplicate\ndiscovery framework that combines unsupervised low-level keypoints matching\nwith high-level text-centric content-based matching to refine and rank the\ncandidate OB duplicates with semantic awareness and interpretability. We\ncompare our model with state-of-the-art content-based image retrieval and image\nmatching methods, showing that our model yields comparable recall performance\nand the highest simplified mean reciprocal rank scores for both Top-5 and\nTop-15 retrieval results, and with significantly accelerated computation\nefficiency. We have discovered over 60 pairs of new OB duplicates in real-world\ndeployment, which were missed by domain experts for decades. Code, model and\nreal-world results are available at: https://github.com/cszhangLMU/OBD-Finder/.\n","authors":["Chongsheng Zhang","Shuwen Wu","Yingqi Chen","Yi Men","Gaojuan Fan","Matthias Aßenmacher","Christian Heumann","João Gama"],"pdf_url":"https://arxiv.org/pdf/2505.03836v2.pdf","comment":"Explainable Coarse-to-Fine Ancient Manuscript Duplicates Discovery,\n  with Oracle Bones as a Case Study"},{"id":"http://arxiv.org/abs/2507.04000v1","updated":"2025-07-05T10:57:29Z","published":"2025-07-05T10:57:29Z","title":"Leveraging Multimodal Data and Side Users for Diffusion Cross-Domain\n  Recommendation","summary":"  Cross-domain recommendation (CDR) aims to address the persistent cold-start\nproblem in Recommender Systems. Current CDR research concentrates on\ntransferring cold-start users' information from the auxiliary domain to the\ntarget domain. However, these systems face two main issues: the\nunderutilization of multimodal data, which hinders effective cross-domain\nalignment, and the neglect of side users who interact solely within the target\ndomain, leading to inadequate learning of the target domain's vector space\ndistribution. To address these issues, we propose a model leveraging Multimodal\ndata and Side users for diffusion Cross-domain recommendation (MuSiC). We first\nemploy a multimodal large language model to extract item multimodal features\nand leverage a large language model to uncover user features using prompt\nlearning without fine-tuning. Secondly, we propose the cross-domain diffusion\nmodule to learn the generation of feature vectors in the target domain. This\napproach involves learning feature distribution from side users and\nunderstanding the patterns in cross-domain transformation through overlapping\nusers. Subsequently, the trained diffusion module is used to generate feature\nvectors for cold-start users in the target domain, enabling the completion of\ncross-domain recommendation tasks. Finally, our experimental evaluation of the\nAmazon dataset confirms that MuSiC achieves state-of-the-art performance,\nsignificantly outperforming all selected baselines. Our code is available:\nhttps://anonymous.4open.science/r/MuSiC-310A/.\n","authors":["Fan Zhang","Jinpeng Chen","Huan Li","Senzhang Wang","Yuan Cao","Kaimin Wei","JianXiang He","Feifei Kou","Jinqing Wang"],"pdf_url":"https://arxiv.org/pdf/2507.04000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05288v1","updated":"2025-07-05T09:52:21Z","published":"2025-07-05T09:52:21Z","title":"A Survey on Proactive Defense Strategies Against Misinformation in Large\n  Language Models","summary":"  The widespread deployment of large language models (LLMs) across critical\ndomains has amplified the societal risks posed by algorithmically generated\nmisinformation. Unlike traditional false content, LLM-generated misinformation\ncan be self-reinforcing, highly plausible, and capable of rapid propagation\nacross multiple languages, which traditional detection methods fail to mitigate\neffectively. This paper introduces a proactive defense paradigm, shifting from\npassive post hoc detection to anticipatory mitigation strategies. We propose a\nThree Pillars framework: (1) Knowledge Credibility, fortifying the integrity of\ntraining and deployed data; (2) Inference Reliability, embedding\nself-corrective mechanisms during reasoning; and (3) Input Robustness,\nenhancing the resilience of model interfaces against adversarial attacks.\nThrough a comprehensive survey of existing techniques and a comparative\nmeta-analysis, we demonstrate that proactive defense strategies offer up to\n63\\% improvement over conventional methods in misinformation prevention,\ndespite non-trivial computational overhead and generalization challenges. We\nargue that future research should focus on co-designing robust knowledge\nfoundations, reasoning certification, and attack-resistant interfaces to ensure\nLLMs can effectively counter misinformation across varied domains.\n","authors":["Shuliang Liu","Hongyi Liu","Aiwei Liu","Bingchen Duan","Qi Zheng","Yibo Yan","He Geng","Peijie Jiang","Jia Liu","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2507.05288v1.pdf","comment":"Accepted by ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2507.03958v1","updated":"2025-07-05T08:50:29Z","published":"2025-07-05T08:50:29Z","title":"A Comparative Study of Specialized LLMs as Dense Retrievers","summary":"  While large language models (LLMs) are increasingly deployed as dense\nretrievers, the impact of their domain-specific specialization on retrieval\neffectiveness remains underexplored. This investigation systematically examines\nhow task-specific adaptations in LLMs influence their retrieval capabilities,\nan essential step toward developing unified retrievers capable of handling\ntext, code, images, and multimodal content. We conduct extensive experiments\nwith eight Qwen2.5 7B LLMs, including base, instruction-tuned,\ncode/math-specialized, long reasoning, and vision-language models across\nzero-shot retrieval settings and the supervised setting. For the zero-shot\nretrieval settings, we consider text retrieval from the BEIR benchmark and code\nretrieval from the CoIR benchmark. Further, to evaluate supervised performance,\nall LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical\nspecialization and the long reasoning capability cause consistent degradation\nin three settings, indicating conflicts between mathematical reasoning and\nsemantic matching. The vision-language model and code-specialized LLMs\ndemonstrate superior zero-shot performance compared to other LLMs, even\nsurpassing BM25 on the code retrieval task, and maintain comparable performance\nto base LLMs in supervised settings. These findings suggest promising\ndirections for the unified retrieval task leveraging cross-domain and\ncross-modal fusion.\n","authors":["Hengran Zhang","Keping Bi","Jiafeng Guo"],"pdf_url":"https://arxiv.org/pdf/2507.03958v1.pdf","comment":"Accepted by CCIR25 and published by Springer LNCS or LNAI"},{"id":"http://arxiv.org/abs/2507.03945v1","updated":"2025-07-05T08:08:38Z","published":"2025-07-05T08:08:38Z","title":"Function-based Labels for Complementary Recommendation: Definition,\n  Annotation, and LLM-as-a-Judge","summary":"  Complementary recommendations enhance the user experience by suggesting items\nthat are frequently purchased together while serving different functions from\nthe query item. Inferring or evaluating whether two items have a complementary\nrelationship requires complementary relationship labels; however, defining\nthese labels is challenging because of the inherent ambiguity of such\nrelationships. Complementary labels based on user historical behavior logs\nattempt to capture these relationships, but often produce inconsistent and\nunreliable results. Recent efforts have introduced large language models (LLMs)\nto infer these relationships. However, these approaches provide a binary\nclassification without a nuanced understanding of complementary relationships.\nIn this study, we address these challenges by introducing Function-Based Labels\n(FBLs), a novel definition of complementary relationships independent of user\npurchase logs and the opaque decision processes of LLMs. We constructed a\nhuman-annotated FBLs dataset comprising 2,759 item pairs and demonstrated that\nit covered possible item relationships and minimized ambiguity. We then\nevaluated whether some machine learning (ML) methods using annotated FBLs could\naccurately infer labels for unseen item pairs, and whether LLM-generated\ncomplementary labels align with human perception. Our results demonstrate that\neven with limited data, ML models, such as logistic regression and SVM achieve\nhigh macro-F1 scores (approximately 0.82). Furthermore, LLMs, such as\ngpt-4o-mini, demonstrated high consistency (0.989) and classification accuracy\n(0.849) under the detailed definition of FBLs, indicating their potential as\neffective annotators that mimic human judgment. Overall, our study presents\nFBLs as a clear definition of complementary relationships, enabling more\naccurate inferences and automated labeling of complementary recommendations.\n","authors":["Chihiro Yamasaki","Kai Sugahara","Yuma Nagi","Kazushi Okamoto"],"pdf_url":"https://arxiv.org/pdf/2507.03945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03895v1","updated":"2025-07-05T04:22:42Z","published":"2025-07-05T04:22:42Z","title":"TayFCS: Towards Light Feature Combination Selection for Deep Recommender\n  Systems","summary":"  Feature interaction modeling is crucial for deep recommendation models. A\ncommon and effective approach is to construct explicit feature combinations to\nenhance model performance. However, in practice, only a small fraction of these\ncombinations are truly informative. Thus it is essential to select useful\nfeature combinations to reduce noise and manage memory consumption. While\nfeature selection methods have been extensively studied, they are typically\nlimited to selecting individual features. Extending these methods for\nhigh-order feature combination selection presents a significant challenge due\nto the exponential growth in time complexity when evaluating feature\ncombinations one by one. In this paper, we propose $\\textbf{TayFCS}$, a\nlightweight feature combination selection method that significantly improves\nmodel performance. Specifically, we propose the Taylor Expansion Scorer\n(TayScorer) module for field-wise Taylor expansion on the base model. Instead\nof evaluating all potential feature combinations' importance by repeatedly\nrunning experiments with feature adding and removal, this scorer only needs to\napproximate the importance based on their sub-components' gradients. This can\nbe simply computed with one backward pass based on a trained recommendation\nmodel. To further reduce information redundancy among feature combinations and\ntheir sub-components, we introduce Logistic Regression Elimination (LRE), which\nestimates the corresponding information gain based on the model prediction\nperformance. Experimental results on three benchmark datasets validate both the\neffectiveness and efficiency of our approach. Furthermore, online A/B test\nresults demonstrate its practical applicability and commercial value.\n","authors":["Xianquan Wang","Zhaocheng Du","Jieming Zhu","Chuhan Wu","Qinglin Jia","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2507.03895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03861v1","updated":"2025-07-05T02:20:15Z","published":"2025-07-05T02:20:15Z","title":"Continual Recommender Systems","summary":"  Modern recommender systems operate in uniquely dynamic settings: user\ninterests, item pools, and popularity trends shift continuously, and models\nmust adapt in real time without forgetting past preferences. While existing\ntutorials on continual or lifelong learning cover broad machine learning\ndomains (e.g., vision and graphs), they do not address recommendation-specific\ndemands-such as balancing stability and plasticity per user, handling\ncold-start items, and optimizing recommendation metrics under streaming\nfeedback. This tutorial aims to make a timely contribution by filling that gap.\nWe begin by reviewing the background and problem settings, followed by a\ncomprehensive overview of existing approaches. We then highlight recent efforts\nto apply continual learning to practical deployment environments, such as\nresource-constrained systems and sequential interaction settings. Finally, we\ndiscuss open challenges and future research directions. We expect this tutorial\nto benefit researchers and practitioners in recommender systems, data mining,\nAI, and information retrieval across academia and industry.\n","authors":["Hyunsik Yoo","SeongKu Kang","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2507.03861v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2501.16780v2","updated":"2025-07-05T14:55:44Z","published":"2025-01-28T08:05:22Z","title":"AVE Speech: A Comprehensive Multi-Modal Dataset for Speech Recognition\n  Integrating Audio, Visual, and Electromyographic Signals","summary":"  The global aging population faces considerable challenges, particularly in\ncommunication, due to the prevalence of hearing and speech impairments. To\naddress these, we introduce the AVE speech, a comprehensive multi-modal dataset\nfor speech recognition tasks. The dataset includes a 100-sentence Mandarin\ncorpus with audio signals, lip-region video recordings, and six-channel\nelectromyography (EMG) data, collected from 100 participants. Each subject read\nthe entire corpus ten times, with each sentence averaging approximately two\nseconds in duration, resulting in over 55 hours of multi-modal speech data per\nmodality. Experiments demonstrate that combining these modalities significantly\nimproves recognition performance, particularly in cross-subject and high-noise\nenvironments. To our knowledge, this is the first publicly available\nsentence-level dataset integrating these three modalities for large-scale\nMandarin speech recognition. We expect this dataset to drive advancements in\nboth acoustic and non-acoustic speech recognition research, enhancing\ncross-modal learning and human-machine interaction.\n","authors":["Dongliang Zhou","Yakun Zhang","Jinghan Wu","Xingyu Zhang","Liang Xie","Erwei Yin"],"pdf_url":"https://arxiv.org/pdf/2501.16780v2.pdf","comment":"The paper has been accepted by IEEE Transactions on Human-Machine\n  Systems"},{"id":"http://arxiv.org/abs/2507.04061v1","updated":"2025-07-05T14:53:32Z","published":"2025-07-05T14:53:32Z","title":"Consistent and Invariant Generalization Learning for Short-video\n  Misinformation Detection","summary":"  Short-video misinformation detection has attracted wide attention in the\nmulti-modal domain, aiming to accurately identify the misinformation in the\nvideo format accompanied by the corresponding audio. Despite significant\nadvancements, current models in this field, trained on particular domains\n(source domains), often exhibit unsatisfactory performance on unseen domains\n(target domains) due to domain gaps. To effectively realize such domain\ngeneralization on the short-video misinformation detection task, we propose\ndeep insights into the characteristics of different domains: (1) The detection\non various domains may mainly rely on different modalities (i.e., mainly\nfocusing on videos or audios). To enhance domain generalization, it is crucial\nto achieve optimal model performance on all modalities simultaneously. (2) For\nsome domains focusing on cross-modal joint fraud, a comprehensive analysis\nrelying on cross-modal fusion is necessary. However, domain biases located in\neach modality (especially in each frame of videos) will be accumulated in this\nfusion process, which may seriously damage the final identification of\nmisinformation. To address these issues, we propose a new DOmain generalization\nmodel via ConsisTency and invariance learning for shORt-video misinformation\ndetection (named DOCTOR), which contains two characteristic modules: (1) We\ninvolve the cross-modal feature interpolation to map multiple modalities into a\nshared space and the interpolation distillation to synchronize multi-modal\nlearning; (2) We design the diffusion model to add noise to retain core\nfeatures of multi modal and enhance domain invariant features through\ncross-modal guided denoising. Extensive experiments demonstrate the\neffectiveness of our proposed DOCTOR model. Our code is public available at\nhttps://github.com/ghh1125/DOCTOR.\n","authors":["Hanghui Guo","Weijie Shi","Mengze Li","Juncheng Li","Hao Chen","Yue Cui","Jiajie Xu","Jia Zhu","Jiawei Shen","Zhangze Chen","Sirui Han"],"pdf_url":"https://arxiv.org/pdf/2507.04061v1.pdf","comment":"Accepted to ACM MM 2025,15 pages, 16figures"},{"id":"http://arxiv.org/abs/2507.03902v1","updated":"2025-07-05T04:53:50Z","published":"2025-07-05T04:53:50Z","title":"The shortcomings of video conferencing technology, methods for revealing\n  them, and emerging XR solutions","summary":"  Video conferencing has become a central part of our daily lives, thanks to\nthe COVID-19 pandemic. Unfortunately, so have its many limitations, resulting\nin poor support for communicative and social behavior and ultimately, Zoom\nfatigue. New technologies will be required to address these limitations,\nincluding many drawn from mixed reality (XR). In this paper, our goals are to\nequip and encourage future researchers to develop and test such technologies.\nToward this end, we first survey research on the shortcomings of video\nconferencing systems, as defined before and after the pandemic. We then\nconsider the methods that research uses to evaluate support for communicative\nbehavior, and argue that those same methods should be employed in identifying,\nimproving, and validating promising video conferencing technologies. Next, we\nsurvey emerging XR solutions to video conferencing's limitations, most off\nwhich do not employ head-mounted displays.\n","authors":["Dani Paul Hove","Benjamin Watson"],"pdf_url":"https://arxiv.org/pdf/2507.03902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03868v1","updated":"2025-07-05T02:44:38Z","published":"2025-07-05T02:44:38Z","title":"From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented\n  Learning in STEM","summary":"  In AI-facilitated teaching, leveraging various query styles to interpret\nabstract educational content is crucial for delivering effective and accessible\nlearning experiences. However, existing retrieval systems predominantly focus\non natural text-image matching and lack the capacity to address the diversity\nand ambiguity inherent in real-world educational scenarios. To address this\nlimitation, we develop a lightweight and efficient multi-modal retrieval\nmodule, named Uni-Retrieval, which extracts query-style prototypes and\ndynamically matches them with tokens from a continually updated Prompt Bank.\nThis Prompt Bank encodes and stores domain-specific knowledge by leveraging a\nMixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to\nenhance Uni-Retrieval's capability to accommodate unseen query types at test\ntime. To enable natural language educational content generation, we integrate\nthe original Uni-Retrieval with a compact instruction-tuned language model,\nforming a complete retrieval-augmented generation pipeline named Uni-RAG. Given\na style-conditioned query, Uni-RAG first retrieves relevant educational\nmaterials and then generates human-readable explanations, feedback, or\ninstructional content aligned with the learning objective. Experimental results\non SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline\nretrieval and RAG systems in both retrieval accuracy and generation quality,\nwhile maintaining low computational cost. Our framework provides a scalable,\npedagogically grounded solution for intelligent educational systems, bridging\nretrieval and generation to support personalized, explainable, and efficient\nlearning assistance across diverse STEM scenarios.\n","authors":["Xinyi Wu","Yanhao Jia","Luwei Xiao","Shuai Zhao","Fengkuang Chiang","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2507.03868v1.pdf","comment":null}]},"2025-07-04T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.05285v1","updated":"2025-07-04T21:41:43Z","published":"2025-07-04T21:41:43Z","title":"Beyond classical and contemporary models: a transformative ai framework\n  for student dropout prediction in distance learning using rag, prompt\n  engineering, and cross-modal fusion","summary":"  Student dropout in distance learning remains a critical challenge, with\nprofound societal and economic consequences. While classical machine learning\nmodels leverage structured socio-demographic and behavioral data, they often\nfail to capture the nuanced emotional and contextual factors embedded in\nunstructured student interactions. This paper introduces a transformative AI\nframework that redefines dropout prediction through three synergistic\ninnovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment\nanalysis, prompt engineering to decode academic stressors, and cross-modal\nattention fusion to dynamically align textual, behavioral, and\nsocio-demographic insights. By grounding sentiment analysis in a curated\nknowledge base of pedagogical content, our RAG-enhanced BERT model interprets\nstudent comments with unprecedented contextual relevance, while optimized\nprompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload\nanxiety\"). A cross-modal attention layer then fuses these insights with\ntemporal engagement patterns, creating holistic risk profiles. Evaluated on a\nlongitudinal dataset of 4 423 students, the framework achieves 89% accuracy and\nan F1-score of 0.88, outperforming conventional models by 7% and reducing false\nnegatives by 21%. Beyond prediction, the system generates interpretable\ninterventions by retrieving contextually aligned strategies (e.g., mentorship\nprograms for isolated learners). This work bridges the gap between predictive\nanalytics and actionable pedagogy, offering a scalable solution to mitigate\ndropout risks in global education systems\n","authors":["Miloud Mihoubi","Meriem Zerkouk","Belkacem Chikhaoui"],"pdf_url":"https://arxiv.org/pdf/2507.05285v1.pdf","comment":"10 pages, 5 figures, 5 tables. Submitted to the 38th Canadian\n  Conference on Artificial Intelligence (Canadian AI 2025)"},{"id":"http://arxiv.org/abs/2409.11598v4","updated":"2025-07-04T20:56:35Z","published":"2024-09-17T23:10:04Z","title":"Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented\n  Generation","summary":"  Despite the central role of retrieval in retrieval-augmented generation (RAG)\nsystems, much of the existing research on RAG overlooks the well-established\nfield of fair ranking and fails to account for the interests of all\nstakeholders involved. In this paper, we conduct the first systematic\nevaluation of RAG systems that integrate fairness-aware rankings, addressing\nboth ranking fairness and attribution fairness, which ensures equitable\nexposure of the sources cited in the generated content. Our evaluation focuses\non measuring item-side fairness, specifically the fair exposure of relevant\nitems retrieved by RAG systems, and investigates how this fairness impacts both\nthe effectiveness of the systems and the attribution of sources in the\ngenerated output that users ultimately see. By experimenting with twelve RAG\nmodels across seven distinct tasks, we show that incorporating fairness-aware\nretrieval often maintains or even enhances both ranking quality and generation\nquality, countering the common belief that fairness compromises system\nperformance. Additionally, we demonstrate that fair retrieval practices lead to\nmore balanced attribution in the final responses, ensuring that the generator\nfairly cites the sources it relies on. Our findings underscore the importance\nof item-side fairness in retrieval and generation, laying the foundation for\nresponsible and equitable RAG systems and guiding future research in fair\nranking and attribution.\n","authors":["To Eun Kim","Fernando Diaz"],"pdf_url":"https://arxiv.org/pdf/2409.11598v4.pdf","comment":"ICTIR 2025 (Oral); NeurIPS 2024 AFME Workshop (Spotlight)"},{"id":"http://arxiv.org/abs/2502.17776v2","updated":"2025-07-04T20:45:42Z","published":"2025-02-25T02:11:42Z","title":"Tip of the Tongue Query Elicitation for Simulated Evaluation","summary":"  Tip-of-the-tongue (TOT) search occurs when a user struggles to recall a\nspecific identifier, such as a document title. While common, existing search\nsystems often fail to effectively support TOT scenarios. Research on TOT\nretrieval is further constrained by the challenge of collecting queries, as\ncurrent approaches rely heavily on community question-answering (CQA) websites,\nleading to labor-intensive evaluation and domain bias. To overcome these\nlimitations, we introduce two methods for eliciting TOT queries - leveraging\nlarge language models (LLMs) and human participants - to facilitate simulated\nevaluations of TOT retrieval systems. Our LLM-based TOT user simulator\ngenerates synthetic TOT queries at scale, achieving high correlations with how\nCQA-based TOT queries rank TOT retrieval systems when tested in the Movie\ndomain. Additionally, these synthetic queries exhibit high linguistic\nsimilarity to CQA-derived queries. For human-elicited queries, we developed an\ninterface that uses visual stimuli to place participants in a TOT state,\nenabling the collection of natural queries. In the Movie domain, system rank\ncorrelation and linguistic similarity analyses confirm that human-elicited\nqueries are both effective and closely resemble CQA-based queries. These\napproaches reduce reliance on CQA-based data collection while expanding\ncoverage to underrepresented domains, such as Landmark and Person. LLM-elicited\nqueries for the Movie, Landmark, and Person domains have been released as test\nqueries in the TREC 2024 TOT track, with human-elicited queries scheduled for\ninclusion in the TREC 2025 TOT track. Additionally, we provide source code for\nsynthetic query generation and the human query collection interface, along with\ncurated visual stimuli used for eliciting TOT queries.\n","authors":["Yifan He","To Eun Kim","Fernando Diaz","Jaime Arguello","Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2502.17776v2.pdf","comment":"SIGIR 2025"},{"id":"http://arxiv.org/abs/2507.03789v1","updated":"2025-07-04T19:50:01Z","published":"2025-07-04T19:50:01Z","title":"Efficient and Effective Query Context-Aware Learning-to-Rank Model for\n  Sequential Recommendation","summary":"  Modern sequential recommender systems commonly use transformer-based models\nfor next-item prediction. While these models demonstrate a strong balance\nbetween efficiency and quality, integrating interleaving features - such as the\nquery context (e.g., browse category) under which next-item interactions occur\n- poses challenges. Effectively capturing query context is crucial for refining\nranking relevance and enhancing user engagement, as it provides valuable\nsignals about user intent within a session. Unlike an item's features, query\ncontext is not temporally aligned with the item sequence, making its\nincorporation into transformers challenging and error-prone. This paper\nanalyzes different strategies for incorporating query context into transformers\ntrained with a causal language modeling procedure as a case study. We propose a\nnew method that effectively fuses the item sequence with query context within\nthe attention mechanism. Through extensive offline and online experiments on a\nlarge-scale online platform and open datasets, we present evidence that our\nproposed method is an effective approach for integrating query context to\nimprove model ranking quality in terms of relevance and diversity.\n","authors":["Andrii Dzhoha","Alisa Mironenko","Vladimir Vlasov","Maarten Versteegh","Marjan Celikik"],"pdf_url":"https://arxiv.org/pdf/2507.03789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03761v1","updated":"2025-07-04T18:17:52Z","published":"2025-07-04T18:17:52Z","title":"Ranking-based Fusion Algorithms for Extreme Multi-label Text\n  Classification (XMTC)","summary":"  In the context of Extreme Multi-label Text Classification (XMTC), where\nlabels are assigned to text instances from a large label space, the long-tail\ndistribution of labels presents a significant challenge. Labels can be broadly\ncategorized into frequent, high-coverage \\textbf{head labels} and infrequent,\nlow-coverage \\textbf{tail labels}, complicating the task of balancing\neffectiveness across all labels. To address this, combining predictions from\nmultiple retrieval methods, such as sparse retrievers (e.g., BM25) and dense\nretrievers (e.g., fine-tuned BERT), offers a promising solution. The fusion of\n\\textit{sparse} and \\textit{dense} retrievers is motivated by the complementary\nranking characteristics of these methods. Sparse retrievers compute relevance\nscores based on high-dimensional, bag-of-words representations, while dense\nretrievers utilize approximate nearest neighbor (ANN) algorithms on dense text\nand label embeddings within a shared embedding space. Rank-based fusion\nalgorithms leverage these differences by combining the precise matching\ncapabilities of sparse retrievers with the semantic richness of dense\nretrievers, thereby producing a final ranking that improves the effectiveness\nacross both head and tail labels.\n","authors":["Celso França","Gestefane Rabbi","Thiago Salles","Washington Cunha","Leonardo Rocha","Marcos André Gonçalves"],"pdf_url":"https://arxiv.org/pdf/2507.03761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03726v1","updated":"2025-07-04T17:28:33Z","published":"2025-07-04T17:28:33Z","title":"Agent-Based Detection and Resolution of Incompleteness and Ambiguity in\n  Interactions with Large Language Models","summary":"  Many of us now treat LLMs as modern-day oracles asking it almost any kind of\nquestion. However, consulting an LLM does not have to be a single turn\nactivity. But long multi-turn interactions can get tedious if it is simply to\nclarify contextual information that can be arrived at through reasoning. In\nthis paper, we examine the use of agent-based architecture to bolster LLM-based\nQuestion-Answering systems with additional reasoning capabilities. We examine\nthe automatic resolution of potential incompleteness or ambiguities in\nquestions by transducers implemented using LLM-based agents. We focus on\nseveral benchmark datasets that are known to contain questions with these\ndeficiencies to varying degrees. We equip different LLMs (GPT-3.5-Turbo and\nLlama-4-Scout) with agents that act as specialists in detecting and resolving\ndeficiencies of incompleteness and ambiguity. The agents are implemented as\nzero-shot ReAct agents. Rather than producing an answer in a single step, the\nmodel now decides between 3 actions a) classify b) resolve c) answer. Action a)\ndecides if the question is incomplete, ambiguous, or normal. Action b)\ndetermines if any deficiencies identified can be resolved. Action c) answers\nthe resolved form of the question. We compare the use of LLMs with and without\nthe use of agents with these components. Our results show benefits of agents\nwith transducer 1) A shortening of the length of interactions with human 2) An\nimprovement in the answer quality and 3) Explainable resolution of deficiencies\nin the question. On the negative side we find while it may result in additional\nLLM invocations and in some cases, increased latency. But on tested datasets,\nthe benefits outweigh the costs except when questions already have sufficient\ncontext. Suggesting the agent-based approach could be a useful mechanism to\nharness the power of LLMs to develop more robust QA systems.\n","authors":["Riya Naik","Ashwin Srinivasan","Swati Agarwal","Estrid He"],"pdf_url":"https://arxiv.org/pdf/2507.03726v1.pdf","comment":"14 pages. arXiv admin note: text overlap with arXiv:2503.17936"},{"id":"http://arxiv.org/abs/2408.11611v4","updated":"2025-07-04T14:36:47Z","published":"2024-08-21T13:39:21Z","title":"DTN: Deep Multiple Task-specific Feature Interactions Network for\n  Multi-Task Recommendation","summary":"  Neural-based multi-task learning (MTL) has been successfully applied to many\nrecommendation applications. However, these MTL models (e.g., MMoE, PLE) did\nnot consider feature interaction during the optimization, which is crucial for\ncapturing complex high-order features and has been widely used in ranking\nmodels for real-world recommender systems. Moreover, through feature importance\nanalysis across various tasks in MTL, we have observed an interesting\ndivergence phenomenon that the same feature can have significantly different\nimportance across different tasks in MTL. To address these issues, we propose\nDeep Multiple Task-specific Feature Interactions Network (DTN) with a novel\nmodel structure design. DTN introduces multiple diversified task-specific\nfeature interaction methods and task-sensitive network in MTL networks,\nenabling the model to learn task-specific diversified feature interaction\nrepresentations, which improves the efficiency of joint representation learning\nin a general setup. We applied DTN to our company's real-world E-commerce\nrecommendation dataset, which consisted of over 6.3 billion samples, the\nresults demonstrated that DTN significantly outperformed state-of-the-art MTL\nmodels. Moreover, during online evaluation of DTN in a large-scale E-commerce\nrecommender system, we observed a 3.28% in clicks, a 3.10% increase in orders\nand a 2.70% increase in GMV (Gross Merchandise Value) compared to the\nstate-of-the-art MTL models. Finally, extensive offline experiments conducted\non public benchmark datasets demonstrate that DTN can be applied to various\nscenarios beyond recommendations, enhancing the performance of ranking models.\n","authors":["Yaowen Bi","Yuteng Lian","Jie Cui","Jun Liu","Peijian Wang","Guanghui Li","Xuejun Chen","Jinglin Zhao","Hao Wen","Jing Zhang","Zhaoqi Zhang","Wenzhuo Song","Yang Sun","Weiwei Zhang","Mingchen Cai","Jian Dong","Guanxing Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.11611v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03568v1","updated":"2025-07-04T13:25:51Z","published":"2025-07-04T13:25:51Z","title":"GENPLUGIN: A Plug-and-Play Framework for Long-Tail Generative\n  Recommendation with Exposure Bias Mitigation","summary":"  Generative recommendation (GenRec) offers LLM integration, reduced embedding\ncosts, and eliminates per-candidate scoring, attracting great attention.\nDespite its promising performance, this study reveals that it suffers from\ngeneration exposure bias and poor long-tail item generalization, two critical\nlimitations overlooked by prior works on GenRec. To address these, we propose\nGENPLUGIN, a plug-and-play framework featuring a dual-encoder, shared-decoder\narchitecture. During pre-training, it aligns language and ID views via\ncontrastive learning, harmonizing item representations across two complementary\nviews. Besides, GENPLUGIN uses a novel training strategy that probabilistically\nsubstitutes ground-truth item ID tokens with predictions from the\nlanguage-semantics encoder, alleviating exposure bias. To improve long-tail\ngenerative recommendation, we propose a retrieval-based data augmentation\nmechanism. It fine-tunes the decoder of GENPLUGIN to endow GENPLUGIN with the\nability to use relevant users w.r.t. contexts or collaborative information to\naugment the generation of item ID tokens in long-tail recommendation scenarios.\nWe have plugged GENPLUGIN into several representative GenRec models and the\nextensive experiments demonstrate that GENPLUGIN can notably mitigate\ngeneration exposure bias during item ID generation while significantly\nimproving the quality of long-tail item recommendation.\n","authors":["Kun Yang","Siyao Zheng","Tianyi Li","Xiaodong Li","Hui Li"],"pdf_url":"https://arxiv.org/pdf/2507.03568v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.03556v1","updated":"2025-07-04T13:09:08Z","published":"2025-07-04T13:09:08Z","title":"A Multistakeholder Approach to Value-Driven Co-Design of Recommender\n  System Evaluation Metrics in Digital Archives","summary":"  This paper presents the first multistakeholder approach for translating\ndiverse stakeholder values into an evaluation metric setup for Recommender\nSystems (RecSys) in digital archives. While commercial platforms mainly rely on\nengagement metrics, cultural heritage domains require frameworks that balance\ncompeting priorities among archivists, platform owners, researchers, and other\nstakeholders. To address this challenge, we conducted high-profile focus groups\n(5 groups x 5 persons) with upstream, provider, system, consumer, and\ndownstream stakeholders, identifying value priorities across critical\ndimensions: visibility/representation, expertise adaptation, and\ntransparency/trust. Our analysis shows that stakeholder concerns naturally\nalign with four sequential research funnel stages: discovery, interaction,\nintegration, and impact. The resulting framework addresses domain-specific\nchallenges including collection representation imbalances, non-linear research\npatterns, and tensions between specialized expertise and broader accessibility.\nWe propose tailored metrics for each stage in this research journey, such as\nresearch path quality for discovery, contextual appropriateness for\ninteraction, metadata-weighted relevance for integration, and cross-stakeholder\nvalue alignment for impact assessment. Our contributions extend beyond digital\narchives to the broader RecSys community, offering transferable evaluation\napproaches for domains where value emerges through sustained engagement rather\nthan immediate consumption.\n","authors":["Florian Atzenhofer-Baumgartner","Georg Vogeler","Dominik Kowald"],"pdf_url":"https://arxiv.org/pdf/2507.03556v1.pdf","comment":"Accepted at RecSys'25"},{"id":"http://arxiv.org/abs/2507.03503v1","updated":"2025-07-04T11:56:11Z","published":"2025-07-04T11:56:11Z","title":"Exploring the Effect of Context-Awareness and Popularity Calibration on\n  Popularity Bias in POI Recommendations","summary":"  Point-of-interest (POI) recommender systems help users discover relevant\nlocations, but their effectiveness is often compromised by popularity bias,\nwhich disadvantages less popular, yet potentially meaningful places. This paper\naddresses this challenge by evaluating the effectiveness of context-aware\nmodels and calibrated popularity techniques as strategies for mitigating\npopularity bias. Using four real-world POI datasets (Brightkite, Foursquare,\nGowalla, and Yelp), we analyze the individual and combined effects of these\napproaches on recommendation accuracy and popularity bias. Our results reveal\nthat context-aware models cannot be considered a uniform solution, as the\nmodels studied exhibit divergent impacts on accuracy and bias. In contrast,\ncalibration techniques can effectively align recommendation popularity with\nuser preferences, provided there is a careful balance between accuracy and bias\nmitigation. Notably, the combination of calibration and context-awareness\nyields recommendations that balance accuracy and close alignment with the\nusers' popularity profiles, i.e., popularity calibration.\n","authors":["Andrea Forster","Simone Kopeinik","Denic Helic","Stefan Thalmann","Dominik Kowald"],"pdf_url":"https://arxiv.org/pdf/2507.03503v1.pdf","comment":"Accepted at RecSys 2025"},{"id":"http://arxiv.org/abs/2507.03479v1","updated":"2025-07-04T11:07:20Z","published":"2025-07-04T11:07:20Z","title":"Explainable Information Retrieval in the Audit Domain","summary":"  Conversational agents such as Microsoft Copilot and Google Gemini assist\nusers with complex search tasks but often generate misleading or fabricated\nreferences. This undermines trust, particularly in high-stakes domains such as\nmedicine and finance. Explainable information retrieval (XIR) aims to address\nthis by making search results more transparent and interpretable. While most\nXIR research is domain-agnostic, this paper focuses on auditing -- a critical\nyet underexplored area. We argue that XIR systems can support auditors in\ncompleting their complex task. We outline key challenges and future research\ndirections to advance XIR in this domain.\n","authors":["Alexander Frummet","Emanuel Slany","Jonas Amling","Moritz Lang","Stephan Scheele"],"pdf_url":"https://arxiv.org/pdf/2507.03479v1.pdf","comment":"Extended abstract accepted at the Workshop on Explainability in\n  Information Retrieval (WExIR), co-located with SIGIR 2025"},{"id":"http://arxiv.org/abs/2507.05282v1","updated":"2025-07-04T10:49:37Z","published":"2025-07-04T10:49:37Z","title":"Exploring LLM Capabilities in Extracting DCAT-Compatible Metadata for\n  Data Cataloging","summary":"  Efficient data exploration is crucial as data becomes increasingly important\nfor accelerating processes, improving forecasts and developing new business\nmodels. Data consumers often spend 25-98 % of their time searching for suitable\ndata due to the exponential growth, heterogeneity and distribution of data.\nData catalogs can support and accelerate data exploration by using metadata to\nanswer user queries. However, as metadata creation and maintenance is often a\nmanual process, it is time-consuming and requires expertise. This study\ninvestigates whether LLMs can automate metadata maintenance of text-based data\nand generate high-quality DCAT-compatible metadata. We tested zero-shot and\nfew-shot prompting strategies with LLMs from different vendors for generating\nmetadata such as titles and keywords, along with a fine-tuned model for\nclassification. Our results show that LLMs can generate metadata comparable to\nhuman-created content, particularly on tasks that require advanced semantic\nunderstanding. Larger models outperformed smaller ones, and fine-tuning\nsignificantly improves classification accuracy, while few-shot prompting yields\nbetter results in most cases. Although LLMs offer a faster and reliable way to\ncreate metadata, a successful application requires careful consideration of\ntask-specific criteria and domain context.\n","authors":["Lennart Busch","Daniel Tebernum","Gissel Velarde"],"pdf_url":"https://arxiv.org/pdf/2507.05282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23250v2","updated":"2025-07-04T08:06:20Z","published":"2025-05-29T08:55:39Z","title":"Deep Retrieval at CheckThat! 2025: Identifying Scientific Papers from\n  Implicit Social Media Mentions via Hybrid Retrieval and Re-Ranking","summary":"  We present the methodology and results of the Deep Retrieval team for subtask\n4b of the CLEF CheckThat! 2025 competition, which focuses on retrieving\nrelevant scientific literature for given social media posts. To address this\ntask, we propose a hybrid retrieval pipeline that combines lexical precision,\nsemantic generalization, and deep contextual re-ranking, enabling robust\nretrieval that bridges the informal-to-formal language gap. Specifically, we\ncombine BM25-based keyword matching with a FAISS vector store using a\nfine-tuned INF-Retriever-v1 model for dense semantic retrieval. BM25 returns\nthe top 30 candidates, and semantic search yields 100 candidates, which are\nthen merged and re-ranked via a large language model (LLM)-based cross-encoder.\n  Our approach achieves a mean reciprocal rank at 5 (MRR@5) of 76.46% on the\ndevelopment set and 66.43% on the hidden test set, securing the 1st position on\nthe development leaderboard and ranking 3rd on the test leaderboard (out of 31\nteams), with a relative performance gap of only 2 percentage points compared to\nthe top-ranked system. We achieve this strong performance by running\nopen-source models locally and without external training data, highlighting the\neffectiveness of a carefully designed and fine-tuned retrieval pipeline.\n","authors":["Pascal J. Sager","Ashwini Kamaraj","Benjamin F. Grewe","Thilo Stadelmann"],"pdf_url":"https://arxiv.org/pdf/2505.23250v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03280v1","updated":"2025-07-04T03:56:04Z","published":"2025-07-04T03:56:04Z","title":"Modeling Item-Level Dynamic Variability with Residual Diffusion for\n  Bundle Recommendation","summary":"  Existing solutions for bundle recommendation(BR) have achieved remarkable\neffectiveness for predicting the user's preference for prebuilt bundles.\nHowever, bundle-item(B-I) affiliation will vary dynamically in real scenarios.\nFor example, a bundle themed as 'casual outfit', may add 'hat' or remove\n'watch' due to factors such as seasonal variations, changes in user pes or\ninventory adjustments. Our empirical study demonstrates that the performance of\nmainstream BR models will fluctuate or even decline regarding item-level\nvariability. This paper makes the first attempt to referencaddress the above\nproblem and proposes a novel Residual Diffusion for Bundle\nRecommendation(RDiffBR) as a model-agnostic generative framework which can\nassist a BR model in adapting this scenario. During the initial training of the\nBR model, RDiffBR employs a residual diffusion model to process the item-level\nbundle embeddings which are generated by BR model to represent bundle theme via\na forward-reverse process. In the inference stage, RDiffBR reverses item-level\nbundle embeddings obtained by the well-trained bundle model under B-I\nvariability scenarios to generate the effective item-level bundle embeddings.\nIn particular, the residual connection in our residual approximator\nsignificantly enhances item-level bundle embeddings generation ability of BR\nmodels. Experiments on six BR models and four public datasets from different\ndomains show that RDiffBR improves the performance of Recall and NDCG of\nbackbone BR models by up to 23%, while only increased training time about\n4%.Codes and datasets are available at\nhttps://anonymous.4open.science/r/RDiffBR.\n","authors":["Dong Zhang","Lin Li","Ming Li","Xiaohui Tao","Meng Sun","Jimmy Xiangji Huang"],"pdf_url":"https://arxiv.org/pdf/2507.03280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06162v2","updated":"2025-07-04T03:51:55Z","published":"2025-06-06T15:27:23Z","title":"Recommender systems, stigmergy, and the tyranny of popularity","summary":"  Scientific recommender systems, such as Google Scholar and Web of Science,\nare essential tools for discovery. Search algorithms that power work through\nstigmergy, a collective intelligence mechanism that surfaces useful paths\nthrough repeated engagement. While generally effective, this \"rich-get-richer\"\ndynamic results in a small number of high-profile papers that dominate\nvisibility. This essay argues argue that these algorithm over-reliance on\npopularity fosters intellectual homogeneity and exacerbates structural\ninequities, stifling innovative and diverse perspectives critical for\nscientific progress. We propose an overhaul of search platforms to incorporate\nuser-specific calibration, allowing researchers to manually adjust the weights\nof factors like popularity, recency, and relevance. We also advise platform\ndevelopers on how text embeddings and LLMs could be implemented in ways that\nincrease user autonomy. While our suggestions are particularly pertinent to\naligning recommender systems with scientific values, these ideas are broadly\napplicable to information access systems in general. Designing platforms that\nincrease user autonomy is an important step toward more robust and dynamic\ninformation\n","authors":["Zackary Okun Dunivin","Paul E. Smaldino"],"pdf_url":"https://arxiv.org/pdf/2506.06162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.20963v2","updated":"2025-07-04T01:31:36Z","published":"2025-06-26T03:01:33Z","title":"EraRAG: Efficient and Incremental Retrieval Augmented Generation for\n  Growing Corpora","summary":"  Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large\nlanguage models (LLMs) by structuring retrieval over an external corpus.\nHowever, existing approaches typically assume a static corpus, requiring\nexpensive full-graph reconstruction whenever new documents arrive, limiting\ntheir scalability in dynamic, evolving environments. To address these\nlimitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework\nthat supports efficient and scalable dynamic updates. Our method leverages\nhyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the\noriginal corpus into hierarchical graph structures, enabling efficient and\nlocalized insertions of new data without disrupting the existing topology. The\ndesign eliminates the need for retraining or costly recomputation while\npreserving high retrieval accuracy and low latency. Experiments on large-scale\nbenchmarks demonstrate that EraRag achieves up to an order of magnitude\nreduction in update time and token consumption compared to existing Graph-RAG\nsystems, while providing superior accuracy performance. This work offers a\npractical path forward for RAG systems that must operate over continually\ngrowing corpora, bridging the gap between retrieval efficiency and\nadaptability. Our code and data are available at\nhttps://github.com/EverM0re/EraRAG-Official.\n","authors":["Fangyuan Zhang","Zhengjun Huang","Yingli Zhou","Qintian Guo","Zhixun Li","Wensheng Luo","Di Jiang","Yixiang Fang","Xiaofang Zhou"],"pdf_url":"https://arxiv.org/pdf/2506.20963v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.15272v2","updated":"2025-07-04T00:35:42Z","published":"2024-10-20T04:05:18Z","title":"Performance-Driven QUBO for Recommender Systems on Quantum Annealers","summary":"  We propose Counterfactual Analysis Quadratic Unconstrained Binary\nOptimization (CAQUBO) to solve QUBO problems for feature selection in\nrecommender systems. CAQUBO leverages counterfactual analysis to measure the\nimpact of individual features and feature combinations on model performance and\nemploys the measurements to construct the coefficient matrix for a quantum\nannealer to select the optimal feature combinations for recommender systems,\nthereby improving their final recommendation performance. By establishing\nexplicit connections between features and the recommendation performance, the\nproposed approach demonstrates superior performance compared to the\nstate-of-the-art quantum annealing methods. Extensive experiments indicate that\nintegrating quantum computing with counterfactual analysis holds great promise\nfor addressing these challenges.\n","authors":["Jiayang Niu","Jie Li","Ke Deng","Mark Sanderson","Nicola Ferro","Yongli Ren"],"pdf_url":"https://arxiv.org/pdf/2410.15272v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2501.04511v2","updated":"2025-07-04T21:18:16Z","published":"2025-01-08T13:58:07Z","title":"Multichannel Steganography: A Provably Secure Hybrid Steganographic\n  Model for Secure Communication","summary":"  Secure covert communication in hostile environments requires simultaneously\nachieving invisibility, provable security guarantees, and robustness against\ninformed adversaries. This paper presents a novel hybrid steganographic\nframework that unites cover synthesis and cover modification within a unified\nmultichannel protocol. A secret-seeded PRNG drives a lightweight Markov-chain\ngenerator to produce contextually plausible cover parameters, which are then\nmasked with the payload and dispersed across independent channels. The masked\nbit-vector is imperceptibly embedded into conventional media via a\nvariance-aware least-significant-bit algorithm, ensuring that statistical\nproperties remain within natural bounds. We formalize a multichannel adversary\nmodel (MC-ATTACK) and prove that, under standard security assumptions, the\nadversary's distinguishing advantage is negligible, thereby guaranteeing both\nconfidentiality and integrity. Empirical results corroborate these claims:\nlocal-variance-guided embedding yields near-lossless extraction (mean BER\n$<5\\times10^{-3}$, correlation $>0.99$) with minimal perceptual distortion\n(PSNR $\\approx100$,dB, SSIM $>0.99$), while key-based masking drives extraction\nsuccess to zero (BER $\\approx0.5$) for a fully informed adversary. Comparative\nanalysis demonstrates that purely distortion-free or invertible schemes fail\nunder the same threat model, underscoring the necessity of hybrid designs. The\nproposed approach advances high-assurance steganography by delivering an\nefficient, provably secure covert channel suitable for deployment in\nhigh-surveillance networks.\n","authors":["Obinna Omego","Michal Bosy"],"pdf_url":"https://arxiv.org/pdf/2501.04511v2.pdf","comment":"22 pages, 15 figures, 4 algorithms. This version is a preprint\n  uploaded to arXiv"},{"id":"http://arxiv.org/abs/2507.03797v1","updated":"2025-07-04T20:30:51Z","published":"2025-07-04T20:30:51Z","title":"Assessing the Viability of Wave Field Synthesis in VR-Based Cognitive\n  Research","summary":"  This paper investigates the viability of Wave Field Synthesis (WFS) for\nenhancing auditory immersion in VR-based cognitive research. While Virtual\nReality (VR) offers significant advantages for studying human perception and\nbehavior, auditory cues are often underutilized. WFS, an advanced audio\nrendering technique, can create highly realistic and spatially accurate\nsoundscapes, potentially increasing ecological validity. This study evaluates\nWFS by implementing a sample experiment where participants localize static and\nmoving sound sources in both a WFS-rendered environment and a conventional\nstereo headphone setup. The research explores the impact of virtual\nenvironments, sound types, and durations on localization accuracy and search\nbehavior. Findings indicate that while stereo setups can achieve higher\naccuracy, WFS provides a more natural and intuitive auditory experience,\nparticularly for directional cues. The study also highlights limitations of\ncurrent WFS systems, such as the lack of height localization, occlusion\nsimulation, and user-dependent optimization, which affect performance,\nespecially for centrally located sound sources. Despite these challenges, WFS\nshows promise for specialized auditory perception research, particularly for\ncomplex soundscapes where directional information is paramount.\n","authors":["Benjamin Kahl"],"pdf_url":"https://arxiv.org/pdf/2507.03797v1.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2507.03434v1","updated":"2025-07-04T09:45:11Z","published":"2025-07-04T09:45:11Z","title":"Unlearning the Noisy Correspondence Makes CLIP More Robust","summary":"  The data appetite for Vision-Language Models (VLMs) has continuously scaled\nup from the early millions to billions today, which faces an untenable\ntrade-off with data quality and inevitably introduces Noisy Correspondence (NC)\nsamples. Undoubtedly, such semantically unrelated data significantly impairs\nthe performance of VLMs. Previous efforts mainly address this challenge by\nestimating refined alignment for more precise guidance. However, such\nresource-intensive pipelines that train VLMs from scratch struggle to meet\nrealistic data demands. In this paper, we present a brand new perspective that\nseeks to directly eliminate the harmful effects of NC in pre-trained VLMs.\nSpecifically, we propose NCU, a Noisy Correspondence Unlearning fine-tuning\nframework that efficiently enhances VLMs' robustness by forgetting learned\nnoisy knowledge. The key to NCU is learning the hardest negative information,\nwhich can provide explicit unlearning direction for both false positives and\nfalse negatives. Such twin goals unlearning process can be formalized into one\nunified optimal transport objective for fast fine-tuning. We validate our\napproach with the prevailing CLIP model over various downstream tasks.\nRemarkably, NCU surpasses the robust pre-trained method on zero-shot transfer\nwhile with lower computational overhead. The code will be released upon\nacceptance.\n","authors":["Haochen Han","Alex Jinpeng Wang","Peijun Ye","Fangming Liu"],"pdf_url":"https://arxiv.org/pdf/2507.03434v1.pdf","comment":"ICCV 2025"},{"id":"http://arxiv.org/abs/2507.03286v1","updated":"2025-07-04T04:33:52Z","published":"2025-07-04T04:33:52Z","title":"Gaze and Glow: Exploring Editing Processes on Social Media through\n  Interactive Exhibition","summary":"  We present Gaze and Glow, an interactive installation that reveals the\noften-invisible efforts of social media editing. Through narrative personas,\nexperimental videos, and sensor-based interactions, the installation explores\nhow audience attention shapes users' editing practices and emotional\nexperiences. Deployed in a two-month public exhibition, Gaze and Glow engaged\nviewers and elicited responses. Reflexive thematic analysis of audience\nfeedback highlights how making editing visible prompts new reflections on\nauthenticity, agency, and performativity. We discuss implications for designing\ninteractive systems that support selective memory, user-controlled visibility,\nand critical engagement with everyday digital self-presentation.\n","authors":["Yang Hong","Jie-Yi Feng","Yi-Chun Yao","I-Hsuan Cho","Yu-Ting Lin","Ying-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2507.03286v1.pdf","comment":"6 pages, 6 figures, to be published in DIS 2025 (Provocations and\n  Works in Progress)"}]},"2025-07-11T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.11767v2","updated":"2025-07-11T17:59:56Z","published":"2024-11-18T17:46:32Z","title":"Drowning in Documents: Consequences of Scaling Reranker Inference","summary":"  Rerankers, typically cross-encoders, are computationally intensive but are\nfrequently used because they are widely assumed to outperform cheaper initial\nIR systems. We challenge this assumption by measuring reranker performance for\nfull retrieval, not just re-scoring first-stage retrieval. To provide a more\nrobust evaluation, we prioritize strong first-stage retrieval using modern\ndense embeddings and test rerankers on a variety of carefully chosen,\nchallenging tasks, including internally curated datasets to avoid\ncontamination, and out-of-domain ones. Our empirical results reveal a\nsurprising trend: the best existing rerankers provide initial improvements when\nscoring progressively more documents, but their effectiveness gradually\ndeclines and can even degrade quality beyond a certain limit. We hope that our\nfindings will spur future research to improve reranking.\n","authors":["Mathew Jacob","Erik Lindgren","Matei Zaharia","Michael Carbin","Omar Khattab","Andrew Drozdov"],"pdf_url":"https://arxiv.org/pdf/2411.11767v2.pdf","comment":"Accepted to ReNeuIR 2025 Workshop at SIGIR 2025 Conference"},{"id":"http://arxiv.org/abs/2507.08800v1","updated":"2025-07-11T17:59:40Z","published":"2025-07-11T17:59:40Z","title":"NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models","summary":"  We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.\n","authors":["Luke Rivard","Sun Sun","Hongyu Guo","Wenhu Chen","Yuntian Deng"],"pdf_url":"https://arxiv.org/pdf/2507.08800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08799v1","updated":"2025-07-11T17:59:36Z","published":"2025-07-11T17:59:36Z","title":"KV Cache Steering for Inducing Reasoning in Small Language Models","summary":"  We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.\n","authors":["Max Belitsky","Dawid J. Kopiczko","Michael Dorkenwald","M. Jehanzeb Mirza","Cees G. M. Snoek","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2507.08799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08794v1","updated":"2025-07-11T17:55:22Z","published":"2025-07-11T17:55:22Z","title":"One Token to Fool LLM-as-a-Judge","summary":"  Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM.\n","authors":["Yulai Zhao","Haolin Liu","Dian Yu","S. Y. Kung","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2507.08794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13959v2","updated":"2025-07-11T17:42:10Z","published":"2025-04-16T23:12:30Z","title":"AI Safety Should Prioritize the Future of Work","summary":"  Current efforts in AI safety prioritize filtering harmful content, preventing\nmanipulation of human behavior, and eliminating existential risks in\ncybersecurity or biosecurity. While pressing, this narrow focus overlooks\ncritical human-centric considerations that shape the long-term trajectory of a\nsociety. In this position paper, we identify the risks of overlooking the\nimpact of AI on the future of work and recommend comprehensive transition\nsupport towards the evolution of meaningful labor with human agency. Through\nthe lens of economic theories, we highlight the intertemporal impacts of AI on\nhuman livelihood and the structural changes in labor markets that exacerbate\nincome inequality. Additionally, the closed-source approach of major\nstakeholders in AI development resembles rent-seeking behavior through\nexploiting resources, breeding mediocrity in creative labor, and monopolizing\ninnovation. To address this, we argue in favor of a robust international\ncopyright anatomy supported by implementing collective licensing that ensures\nfair compensation mechanisms for using data to train AI models. We strongly\nrecommend a pro-worker framework of global AI governance to enhance shared\nprosperity and economic justice while reducing technical debt.\n","authors":["Sanchaita Hazra","Bodhisattwa Prasad Majumder","Tuhin Chakrabarty"],"pdf_url":"https://arxiv.org/pdf/2504.13959v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08771v1","updated":"2025-07-11T17:28:56Z","published":"2025-07-11T17:28:56Z","title":"BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity","summary":"  To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).\n","authors":["Chenyang Song","Weilin Zhao","Xu Han","Chaojun Xiao","Yingfa Chen","Yuxuan Li","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2507.08771v1.pdf","comment":"21 pages, 7 figures, 15 tables"},{"id":"http://arxiv.org/abs/2507.08768v1","updated":"2025-07-11T17:27:11Z","published":"2025-07-11T17:27:11Z","title":"On Barriers to Archival Audio Processing","summary":"  In this study, we leverage a unique UNESCO collection of mid-20th century\nradio recordings to probe the robustness of modern off-the-shelf language\nidentification (LID) and speaker recognition (SR) methods, especially with\nrespect to the impact of multilingual speakers and cross-age recordings. Our\nfindings suggest that LID systems, such as Whisper, are increasingly adept at\nhandling second-language and accented speech. However, speaker embeddings\nremain a fragile component of speech processing pipelines that is prone to\nbiases related to the channel, age, and language. Issues which will need to be\novercome should archives aim to employ SR methods for speaker indexing.\n","authors":["Peter Sullivan","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2507.08768v1.pdf","comment":"Update with Acknowledgements of ICNSLP 2025 paper"},{"id":"http://arxiv.org/abs/2401.02984v3","updated":"2025-07-11T16:38:18Z","published":"2024-01-01T17:35:52Z","title":"Large Language Models in Mental Health Care: a Scoping Review","summary":"  Objectieve:This review aims to deliver a comprehensive analysis of Large\nLanguage Models (LLMs) utilization in mental health care, evaluating their\neffectiveness, identifying challenges, and exploring their potential for future\napplication. Materials and Methods: A systematic search was performed across\nmultiple databases including PubMed, Web of Science, Google Scholar, arXiv,\nmedRxiv, and PsyArXiv in November 2023. The review includes all types of\noriginal research, regardless of peer-review status, published or disseminated\nbetween October 1, 2019, and December 2, 2023. Studies were included without\nlanguage restrictions if they employed LLMs developed after T5 and directly\ninvestigated research questions within mental health care settings. Results:\nOut of an initial 313 articles, 34 were selected based on their relevance to\nLLMs applications in mental health care and the rigor of their reported\noutcomes. The review identified various LLMs applications in mental health\ncare, including diagnostics, therapy, and enhancing patient engagement. Key\nchallenges highlighted were related to data availability and reliability, the\nnuanced handling of mental states, and effective evaluation methods. While LLMs\nshowed promise in improving accuracy and accessibility, significant gaps in\nclinical applicability and ethical considerations were noted. Conclusion: LLMs\nhold substantial promise for enhancing mental health care. For their full\npotential to be realized, emphasis must be placed on developing robust\ndatasets, development and evaluation frameworks, ethical guidelines, and\ninterdisciplinary collaborations to address current limitations.\n","authors":["Yining Hua","Fenglin Liu","Kailai Yang","Zehan Li","Hongbin Na","Yi-han Sheu","Peilin Zhou","Lauren V. Moran","Sophia Ananiadou","David A. Clifton","Andrew Beam","John Torous"],"pdf_url":"https://arxiv.org/pdf/2401.02984v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17256v4","updated":"2025-07-11T16:36:46Z","published":"2024-01-30T18:48:37Z","title":"Weak-to-Strong Jailbreaking on Large Language Models","summary":"  Large language models (LLMs) are vulnerable to jailbreak attacks - resulting\nin harmful, unethical, or biased text generations. However, existing\njailbreaking methods are computationally costly. In this paper, we propose the\nweak-to-strong jailbreaking attack, an efficient inference time attack for\naligned LLMs to produce harmful text. Our key intuition is based on the\nobservation that jailbroken and aligned models only differ in their initial\ndecoding distributions. The weak-to-strong attack's key technical insight is\nusing two smaller models (a safe and an unsafe one) to adversarially modify a\nsignificantly larger safe model's decoding probabilities. We evaluate the\nweak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The\nresults show our method can increase the misalignment rate to over 99% on two\ndatasets with just one forward pass per example. Our study exposes an urgent\nsafety issue that needs to be addressed when aligning LLMs. As an initial\nattempt, we propose a defense strategy to protect against such attacks, but\ncreating more advanced defenses remains challenging. The code for replicating\nthe method is available at https://github.com/XuandongZhao/weak-to-strong\n","authors":["Xuandong Zhao","Xianjun Yang","Tianyu Pang","Chao Du","Lei Li","Yu-Xiang Wang","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2401.17256v4.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2507.08719v1","updated":"2025-07-11T16:19:53Z","published":"2025-07-11T16:19:53Z","title":"Multilingual Multimodal Software Developer for Code Generation","summary":"  The rapid advancement of Large Language Models (LLMs) has significantly\nimproved code generation, yet most models remain text-only, neglecting crucial\nvisual aids like diagrams and flowcharts used in real-world software\ndevelopment. To bridge this gap, we introduce MM-Coder, a Multilingual\nMultimodal software developer. MM-Coder integrates visual design inputs-Unified\nModeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with\ntextual instructions to enhance code generation accuracy and architectural\nalignment. To enable this, we developed MMc-Instruct, a diverse multimodal\ninstruction-tuning dataset including visual-workflow-based code generation,\nallowing MM-Coder to synthesize textual and graphical information like human\ndevelopers, distinct from prior work on narrow tasks. Furthermore, we introduce\nMMEval, a new benchmark for evaluating multimodal code generation, addressing\nexisting text-only limitations. Our evaluations using MMEval highlight\nsignificant remaining challenges for models in precise visual information\ncapture, instruction following, and advanced programming knowledge. Our work\naims to revolutionize industrial programming by enabling LLMs to interpret and\nimplement complex specifications conveyed through both text and visual designs.\n","authors":["Linzheng Chai","Jian Yang","Shukai Liu","Wei Zhang","Liran Wang","Ke Jin","Tao Sun","Congnan Liu","Chenchen Zhang","Hualei Zhu","Jiaheng Liu","Xianjie Wu","Ge Zhang","Tianyu Liu","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2507.08719v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2507.08704v1","updated":"2025-07-11T15:57:37Z","published":"2025-07-11T15:57:37Z","title":"KG-Attention: Knowledge Graph-Guided Attention at Test-Time via\n  Bidirectional Information Aggregation","summary":"  Knowledge graphs (KGs) play a critical role in enhancing large language\nmodels (LLMs) by introducing structured and grounded knowledge into the\nlearning process. However, most existing KG-enhanced approaches rely on\nparameter-intensive fine-tuning, which risks catastrophic forgetting and\ndegrades the pretrained model's generalization. Moreover, they exhibit limited\nadaptability to real-time knowledge updates due to their static integration\nframeworks. To address these issues, we introduce the first test-time\nKG-augmented framework for LLMs, built around a dedicated knowledge\ngraph-guided attention (KGA) module that enables dynamic knowledge fusion\nwithout any parameter updates. The proposed KGA module augments the standard\nself-attention mechanism with two synergistic pathways: outward and inward\naggregation. Specifically, the outward pathway dynamically integrates external\nknowledge into input representations via input-driven KG fusion. This inward\naggregation complements the outward pathway by refining input representations\nthrough KG-guided filtering, suppressing task-irrelevant signals and amplifying\nknowledge-relevant patterns. Importantly, while the outward pathway handles\nknowledge fusion, the inward path selects the most relevant triples and feeds\nthem back into the fusion process, forming a closed-loop enhancement mechanism.\nBy synergistically combining these two pathways, the proposed method supports\nreal-time knowledge fusion exclusively at test-time, without any parameter\nmodification. Extensive experiments on five benchmarks verify the comparable\nknowledge fusion performance of KGA.\n","authors":["Songlin Zhai","Guilin Qi","Yuan Meng"],"pdf_url":"https://arxiv.org/pdf/2507.08704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00927v2","updated":"2025-07-11T15:49:30Z","published":"2025-04-01T15:59:32Z","title":"Multi-Token Attention","summary":"  Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial.\n","authors":["Olga Golovneva","Tianlu Wang","Jason Weston","Sainbayar Sukhbaatar"],"pdf_url":"https://arxiv.org/pdf/2504.00927v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08665v1","updated":"2025-07-11T15:05:06Z","published":"2025-07-11T15:05:06Z","title":"KELPS: A Framework for Verified Multi-Language Autoformalization via\n  Semantic-Syntactic Alignment","summary":"  Modern large language models (LLMs) show promising progress in formalizing\ninformal mathematics into machine-verifiable theorems. However, these methods\nstill face bottlenecks due to the limited quantity and quality of multilingual\nparallel corpora. In this paper, we propose a novel neuro-symbolic framework\nKELPS (Knowledge-Equation based Logical Processing System) to address these\nproblems. KELPS is an iterative framework for translating, synthesizing, and\nfiltering informal data into multiple formal languages (Lean, Coq, and\nIsabelle). First, we translate natural language into Knowledge Equations (KEs),\na novel language that we designed, theoretically grounded in assertional logic.\nNext, we convert them to target languages through rigorously defined rules that\npreserve both syntactic structure and semantic meaning. This process yielded a\nparallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic\naccuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3\n(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are\navailable in the supplementary materials.\n","authors":["Jiyao Zhang","Chengli Zhong","Hui Xu","Qige Li","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.08665v1.pdf","comment":"Accepted by the ICML 2025 AI4MATH Workshop. 22 pages, 16 figures, 2\n  tables"},{"id":"http://arxiv.org/abs/2507.08660v1","updated":"2025-07-11T15:00:32Z","published":"2025-07-11T15:00:32Z","title":"The Impact of Automatic Speech Transcription on Speaker Attribution","summary":"  Speaker attribution from speech transcripts is the task of identifying a\nspeaker from the transcript of their speech based on patterns in their language\nuse. This task is especially useful when the audio is unavailable (e.g.\ndeleted) or unreliable (e.g. anonymized speech). Prior work in this area has\nprimarily focused on the feasibility of attributing speakers using transcripts\nproduced by human annotators. However, in real-world settings, one often only\nhas more errorful transcripts produced by automatic speech recognition (ASR)\nsystems. In this paper, we conduct what is, to our knowledge, the first\ncomprehensive study of the impact of automatic transcription on speaker\nattribution performance. In particular, we study the extent to which speaker\nattribution performance degrades in the face of transcription errors, as well\nas how properties of the ASR system impact attribution. We find that\nattribution is surprisingly resilient to word-level transcription errors and\nthat the objective of recovering the true transcript is minimally correlated\nwith attribution performance. Overall, our findings suggest that speaker\nattribution on more errorful transcripts produced by ASR is as good, if not\nbetter, than attribution based on human-transcribed data, possibly because ASR\ntranscription errors can capture speaker-specific features revealing of speaker\nidentity.\n","authors":["Cristina Aggazzotti","Matthew Wiesner","Elizabeth Allyn Smith","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2507.08660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07257v2","updated":"2025-07-11T14:43:29Z","published":"2025-07-09T20:03:30Z","title":"Open Source Planning & Control System with Language Agents for\n  Autonomous Scientific Discovery","summary":"  We present a multi-agent system for automation of scientific research tasks,\ncmbagent (https://github.com/CMBAgents/cmbagent). The system is formed by about\n30 Large Language Model (LLM) agents and implements a Planning & Control\nstrategy to orchestrate the agentic workflow, with no human-in-the-loop at any\npoint. Each agent specializes in a different task (performing retrieval on\nscientific papers and codebases, writing code, interpreting results, critiquing\nthe output of other agents) and the system is able to execute code locally. We\nsuccessfully apply cmbagent to carry out a PhD level cosmology task (the\nmeasurement of cosmological parameters using supernova data) and evaluate its\nperformance on two benchmark sets, finding superior performance over\nstate-of-the-art LLMs. The source code is available on GitHub, demonstration\nvideos are also available, and the system is deployed on HuggingFace and will\nbe available on the cloud.\n","authors":["Licong Xu","Milind Sarkar","Anto I. Lonappan","Íñigo Zubeldia","Pablo Villanueva-Domingo","Santiago Casas","Christian Fidler","Chetana Amancharla","Ujjwal Tiwari","Adrian Bayer","Chadi Ait Ekioui","Miles Cranmer","Adrian Dimitrov","James Fergusson","Kahaan Gandhi","Sven Krippendorf","Andrew Laverick","Julien Lesgourgues","Antony Lewis","Thomas Meier","Blake Sherwin","Kristen Surrao","Francisco Villaescusa-Navarro","Chi Wang","Xueqing Xu","Boris Bolliet"],"pdf_url":"https://arxiv.org/pdf/2507.07257v2.pdf","comment":"Accepted contribution to the ICML 2025 Workshop on Machine Learning\n  for Astrophysics. Code: https://github.com/CMBAgents/cmbagent Videos:\n  https://www.youtube.com/@cmbagent HuggingFace:\n  https://huggingface.co/spaces/astropilot-ai/cmbagent Cloud:\n  https://cmbagent.cloud"},{"id":"http://arxiv.org/abs/2507.08637v1","updated":"2025-07-11T14:40:40Z","published":"2025-07-11T14:40:40Z","title":"Scaling Attention to Very Long Sequences in Linear Time with\n  Wavelet-Enhanced Random Spectral Attention (WERSA)","summary":"  Transformer models are computationally costly on long sequences since regular\nattention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced\nRandom Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time\ncomplexity that is pivotal to enable successful long-sequence processing\nwithout the performance trade-off. WERSA merges content-adaptive random\nspectral features together with multi-resolution Haar wavelets and learnable\nparameters to selectively attend to informative scales of data while preserving\nlinear efficiency.\n  Large-scale comparisons \\textbf{on single GPU} and across various benchmarks\n(vision, NLP, hierarchical reasoning) and various attention mechanisms (like\nMultiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,\nWaveformer), reveal uniform advantages of WERSA. It achieves best accuracy in\nall tests. On ArXiv classification, WERSA improves accuracy over vanilla\nattention by 1.2\\% (86.2\\% vs 85.0\\%) while cutting training time by 81\\% (296s\nvs 1554s) and FLOPS by 73.4\\% (26.2G vs 98.4G). Significantly, WERSA excels\nwhere vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy\nsequences, it achieves best accuracy (79.1\\%) and AUC (0.979) among viable\nmethods, operating on data that gives Out-Of-Memory errors to quadratic methods\nwhile being \\textbf{twice as fast} as Waveformer, its next-best competitor.\n  By significantly reducing computational loads without compromising accuracy,\nWERSA makes possible more practical, more affordable, long-context models, in\nparticular on low-resource hardware, for more sustainable and more scalable AI\ndevelopment.\n","authors":["Vincenzo Dentamaro"],"pdf_url":"https://arxiv.org/pdf/2507.08637v1.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.08054v2","updated":"2025-07-11T14:40:10Z","published":"2024-08-15T09:48:45Z","title":"Text2BIM: Generating Building Models Using a Large Language Model-based\n  Multi-Agent Framework","summary":"  The conventional BIM authoring process typically requires designers to master\ncomplex and tedious modeling commands in order to materialize their design\nintentions within BIM authoring tools. This additional cognitive burden\ncomplicates the design process and hinders the adoption of BIM and model-based\ndesign in the AEC (Architecture, Engineering, and Construction) industry. To\nfacilitate the expression of design intentions more intuitively, we propose\nText2BIM, an LLM-based multi-agent framework that can generate 3D building\nmodels from natural language instructions. This framework orchestrates multiple\nLLM agents to collaborate and reason, transforming textual user input into\nimperative code that invokes the BIM authoring tool's APIs, thereby generating\neditable BIM models with internal layouts, external envelopes, and semantic\ninformation directly in the software. Furthermore, a rule-based model checker\nis introduced into the agentic workflow, utilizing predefined domain knowledge\nto guide the LLM agents in resolving issues within the generated models and\niteratively improving model quality. Extensive experiments were conducted to\ncompare and analyze the performance of three different LLMs under the proposed\nframework. The evaluation results demonstrate that our approach can effectively\ngenerate high-quality, structurally rational building models that are aligned\nwith the abstract concepts specified by user input. Finally, an interactive\nsoftware prototype was developed to integrate the framework into the BIM\nauthoring software Vectorworks, showcasing the potential of modeling by\nchatting. The code is available at: https://github.com/dcy0577/Text2BIM\n","authors":["Changyu Du","Sebastian Esser","Stavros Nousias","André Borrmann"],"pdf_url":"https://arxiv.org/pdf/2408.08054v2.pdf","comment":"Journal of Computing in Civil Engineering"},{"id":"http://arxiv.org/abs/2505.00467v2","updated":"2025-07-11T14:39:47Z","published":"2025-05-01T11:43:27Z","title":"Red Teaming Large Language Models for Healthcare","summary":"  We present the design process and findings of the pre-conference workshop at\nthe Machine Learning for Healthcare Conference (2024) entitled Red Teaming\nLarge Language Models for Healthcare, which took place on August 15, 2024.\nConference participants, comprising a mix of computational and clinical\nexpertise, attempted to discover vulnerabilities -- realistic clinical prompts\nfor which a large language model (LLM) outputs a response that could cause\nclinical harm. Red-teaming with clinicians enables the identification of LLM\nvulnerabilities that may not be recognised by LLM developers lacking clinical\nexpertise. We report the vulnerabilities found, categorise them, and present\nthe results of a replication study assessing the vulnerabilities across all\nLLMs provided.\n","authors":["Vahid Balazadeh","Michael Cooper","David Pellow","Atousa Assadi","Jennifer Bell","Mark Coatsworth","Kaivalya Deshpande","Jim Fackler","Gabriel Funingana","Spencer Gable-Cook","Anirudh Gangadhar","Abhishek Jaiswal","Sumanth Kaja","Christopher Khoury","Amrit Krishnan","Randy Lin","Kaden McKeen","Sara Naimimohasses","Khashayar Namdar","Aviraj Newatia","Allan Pang","Anshul Pattoo","Sameer Peesapati","Diana Prepelita","Bogdana Rakova","Saba Sadatamin","Rafael Schulman","Ajay Shah","Syed Azhar Shah","Syed Ahmar Shah","Babak Taati","Balagopal Unnikrishnan","Iñigo Urteaga","Stephanie Williams","Rahul G Krishnan"],"pdf_url":"https://arxiv.org/pdf/2505.00467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08621v1","updated":"2025-07-11T14:23:40Z","published":"2025-07-11T14:23:40Z","title":"A comprehensive study of LLM-based argument classification: from LLAMA\n  through GPT-4o to Deepseek-R1","summary":"  Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings.\n","authors":["Marcin Pietroń","Rafał Olszowski","Jakub Gomułka","Filip Gampel","Andrzej Tomski"],"pdf_url":"https://arxiv.org/pdf/2507.08621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08846v2","updated":"2025-07-11T14:20:23Z","published":"2025-06-10T14:34:36Z","title":"Addressing Pitfalls in Auditing Practices of Automatic Speech\n  Recognition Technologies: A Case Study of People with Aphasia","summary":"  Automatic Speech Recognition (ASR) has transformed daily tasks from video\ntranscription to workplace hiring. ASR systems' growing use warrants robust and\nstandardized auditing approaches to ensure automated transcriptions of high and\nequitable quality. This is especially critical for people with speech and\nlanguage disorders (such as aphasia) who may disproportionately depend on ASR\nsystems to navigate everyday life. In this work, we identify three pitfalls in\nexisting standard ASR auditing procedures, and demonstrate how addressing them\nimpacts audit results via a case study of six popular ASR systems' performance\nfor aphasia speakers. First, audits often adhere to a single method of text\nstandardization during data pre-processing, which (a) masks variability in ASR\nperformance from applying different standardization methods, and (b) may not be\nconsistent with how users - especially those from marginalized speech\ncommunities - would want their transcriptions to be standardized. Second,\naudits often display high-level demographic findings without further\nconsidering performance disparities among (a) more nuanced demographic\nsubgroups, and (b) relevant covariates capturing acoustic information from the\ninput audio. Third, audits often rely on a single gold-standard metric -- the\nWord Error Rate -- which does not fully capture the extent of errors arising\nfrom generative AI models, such as transcription hallucinations. We propose a\nmore holistic auditing framework that accounts for these three pitfalls, and\nexemplify its results in our case study, finding consistently worse ASR\nperformance for aphasia speakers relative to a control group. We call on\npractitioners to implement these robust ASR auditing practices that remain\nflexible to the rapidly changing ASR landscape.\n","authors":["Katelyn Xiaoying Mei","Anna Seo Gyeong Choi","Hilke Schellmann","Mona Sloane","Allison Koenecke"],"pdf_url":"https://arxiv.org/pdf/2506.08846v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08606v1","updated":"2025-07-11T14:00:56Z","published":"2025-07-11T14:00:56Z","title":"DocPolarBERT: A Pre-trained Model for Document Understanding with\n  Relative Polar Coordinate Encoding of Layout Structures","summary":"  We introduce DocPolarBERT, a layout-aware BERT model for document\nunderstanding that eliminates the need for absolute 2D positional embeddings.\nWe extend self-attention to take into account text block positions in relative\npolar coordinate system rather than the Cartesian one. Despite being\npre-trained on a dataset more than six times smaller than the widely used\nIIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results\ndemonstrate that a carefully designed attention mechanism can compensate for\nreduced pre-training data, offering an efficient and effective alternative for\ndocument understanding.\n","authors":["Benno Uthayasooriyar","Antoine Ly","Franck Vermet","Caio Corro"],"pdf_url":"https://arxiv.org/pdf/2507.08606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07248v2","updated":"2025-07-11T13:39:47Z","published":"2025-07-09T19:38:58Z","title":"Medical Red Teaming Protocol of Language Models: On the Importance of\n  User Perspectives in Healthcare Settings","summary":"  As the performance of large language models (LLMs) continues to advance,\ntheir adoption is expanding across a wide range of domains, including the\nmedical field. The integration of LLMs into medical applications raises\ncritical safety concerns, particularly due to their use by users with diverse\nroles, e.g. patients and clinicians, and the potential for model's outputs to\ndirectly affect human health. Despite the domain-specific capabilities of\nmedical LLMs, prior safety evaluations have largely focused only on general\nsafety benchmarks. In this paper, we introduce a safety evaluation protocol\ntailored to the medical domain in both patient user and clinician user\nperspectives, alongside general safety assessments and quantitatively analyze\nthe safety of medical LLMs. We bridge a gap in the literature by building the\nPatientSafetyBench containing 466 samples over 5 critical categories to measure\nsafety from the perspective of the patient. We apply our red-teaming protocols\non the MediPhi model collection as a case study. To our knowledge, this is the\nfirst work to define safety evaluation criteria for medical LLMs through\ntargeted red-teaming taking three different points of view - patient,\nclinician, and general user - establishing a foundation for safer deployment in\nmedical domains.\n","authors":["Jean-Philippe Corbeil","Minseon Kim","Alessandro Sordoni","Francois Beaulieu","Paul Vozila"],"pdf_url":"https://arxiv.org/pdf/2507.07248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08575v1","updated":"2025-07-11T13:23:25Z","published":"2025-07-11T13:23:25Z","title":"Large Multi-modal Model Cartographic Map Comprehension for Textual\n  Locality Georeferencing","summary":"  Millions of biological sample records collected in the last few centuries\narchived in natural history collections are un-georeferenced. Georeferencing\ncomplex locality descriptions associated with these collection samples is a\nhighly labour-intensive task collection agencies struggle with. None of the\nexisting automated methods exploit maps that are an essential tool for\ngeoreferencing complex relations. We present preliminary experiments and\nresults of a novel method that exploits multi-modal capabilities of recent\nLarge Multi-Modal Models (LMM). This method enables the model to visually\ncontextualize spatial relations it reads in the locality description. We use a\ngrid-based approach to adapt these auto-regressive models for this task in a\nzero-shot setting. Our experiments conducted on a small manually annotated\ndataset show impressive results for our approach ($\\sim$1 km Average distance\nerror) compared to uni-modal georeferencing with Large Language Models and\nexisting georeferencing tools. The paper also discusses the findings of the\nexperiments in light of an LMM's ability to comprehend fine-grained maps.\nMotivated by these results, a practical framework is proposed to integrate this\nmethod into a georeferencing workflow.\n","authors":["Kalana Wijegunarathna","Kristin Stock","Christopher B. Jones"],"pdf_url":"https://arxiv.org/pdf/2507.08575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.12463v3","updated":"2025-07-11T12:49:25Z","published":"2023-01-29T15:22:10Z","title":"Comparing Spoken Languages using Paninian System of Sounds and Finite\n  State Machines","summary":"  The study of spoken languages comprises phonology, morphology, and grammar.\nThe languages can be classified as root languages, inflectional languages, and\nstem languages. In addition, languages continually change over time and space\nby picking isoglosses, as speakers move from region to/through region. All\nthese factors lead to the formation of vocabulary, which has\ncommonality/similarity across languages as well as distinct and subtle\ndifferences among them. Comparison of vocabularies across languages and\ndetailed analysis has led to the hypothesis of language families. In\nparticular, in the view of Western linguists, Vedic Sanskrit is a daughter\nlanguage, part of the Indo-Iranian branch of the Indo-European Language family,\nand Dravidian Languages belong to an entirely different family. These and such\nconclusions are reexamined in this paper. Based on our study and analysis, we\npropose an Ecosystem Model for Linguistic Development with Sanskrit at the\ncore, in place of the widely accepted family tree model. To that end, we\nleverage the Paninian system of sounds to construct a phonetic map. Then we\nrepresent words across languages as state transitions on the phonetic map and\nconstruct corresponding Morphological Finite Automata (MFA) that accept groups\nof words. Regardless of whether the contribution of this paper is significant\nor minor, it is an important step in challenging policy-driven research that\nhas plagued this field.\n","authors":["Shreekanth M Prabhu","Abhisek Midya"],"pdf_url":"https://arxiv.org/pdf/2301.12463v3.pdf","comment":"63 Pages, 20 Figures, 27 Tables"},{"id":"http://arxiv.org/abs/2507.08538v1","updated":"2025-07-11T12:38:02Z","published":"2025-07-11T12:38:02Z","title":"The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on\n  Multilingual Benchmarks","summary":"  To ensure equitable access to the benefits of large language models (LLMs),\nit is essential to evaluate their capabilities across the world's languages. We\nintroduce the AI Language Proficiency Monitor, a comprehensive multilingual\nbenchmark that systematically assesses LLM performance across up to 200\nlanguages, with a particular focus on low-resource languages. Our benchmark\naggregates diverse tasks including translation, question answering, math, and\nreasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We\nprovide an open-source, auto-updating leaderboard and dashboard that supports\nresearchers, developers, and policymakers in identifying strengths and gaps in\nmodel performance. In addition to ranking models, the platform offers\ndescriptive insights such as a global proficiency map and trends over time. By\ncomplementing and extending prior multilingual benchmarks, our work aims to\nfoster transparency, inclusivity, and progress in multilingual AI. The system\nis available at\nhttps://huggingface.co/spaces/fair-forward/evals-for-every-language.\n","authors":["David Pomerenke","Jonas Nothnagel","Simon Ostermann"],"pdf_url":"https://arxiv.org/pdf/2507.08538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08529v1","updated":"2025-07-11T12:26:19Z","published":"2025-07-11T12:26:19Z","title":"A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge\n  Graph Fusion Framework for Rare Disease Diagnosis","summary":"  Despite advances from medical large language models in healthcare,\nrare-disease diagnosis remains hampered by insufficient\nknowledge-representation depth, limited concept understanding, and constrained\nclinical reasoning. We propose a framework that couples multi-granularity\nsparse activation of medical concepts with a hierarchical knowledge graph. Four\ncomplementary matching algorithms, diversity control, and a five-level fallback\nstrategy enable precise concept activation, while a three-layer knowledge graph\n(taxonomy, clinical features, instances) provides structured, up-to-date\ncontext. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,\nROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89\napproaching the 0.90 clinical threshold. Expert evaluation confirms\nimprovements in information quality, reasoning, and professional expression,\nsuggesting our approach shortens the \"diagnostic odyssey\" for rare-disease\npatients.\n","authors":["Mingda Zhang","Na Zhao","Jianglong Qin","Guoyu Ye","Ruixiang Tang"],"pdf_url":"https://arxiv.org/pdf/2507.08529v1.pdf","comment":"10 pages,3 figures"},{"id":"http://arxiv.org/abs/2407.10657v4","updated":"2025-07-11T12:24:45Z","published":"2024-07-15T12:16:33Z","title":"An Empirical Study of Validating Synthetic Data for Formula Generation","summary":"  Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.\n","authors":["Usneek Singh","José Cambronero","Sumit Gulwani","Aditya Kanade","Anirudh Khatry","Vu Le","Mukul Singh","Gust Verbruggen"],"pdf_url":"https://arxiv.org/pdf/2407.10657v4.pdf","comment":"Accepted at Findings of NAACL"},{"id":"http://arxiv.org/abs/2503.11924v2","updated":"2025-07-11T12:13:04Z","published":"2025-03-14T23:47:46Z","title":"REGEN: A Dataset and Benchmarks with Natural Language Critiques and\n  Narratives","summary":"  This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative\nNarratives), designed to benchmark the conversational capabilities of\nrecommender Large Language Models (LLMs), addressing the limitations of\nexisting datasets that primarily focus on sequential item prediction. REGEN\nextends the Amazon Product Reviews dataset by inpainting two key natural\nlanguage features: (1) user critiques, representing user \"steering\" queries\nthat lead to the selection of a subsequent item, and (2) narratives, rich\ntextual outputs associated with each recommended item taking into account prior\ncontext. The narratives include product endorsements, purchase explanations,\nand summaries of user preferences.\n  Further, we establish an end-to-end modeling benchmark for the task of\nconversational recommendation, where models are trained to generate both\nrecommendations and corresponding narratives conditioned on user history (items\nand critiques). For this joint task, we introduce a modeling framework LUMEN\n(LLM-based Unified Multi-task Model with Critiques, Recommendations, and\nNarratives) which uses an LLM as a backbone for critiquing, retrieval and\ngeneration. We also evaluate the dataset's quality using standard auto-rating\ntechniques and benchmark it by training both traditional and LLM-based\nrecommender models. Our results demonstrate that incorporating critiques\nenhances recommendation quality by enabling the recommender to learn language\nunderstanding and integrate it with recommendation signals. Furthermore, LLMs\ntrained on our dataset effectively generate both recommendations and contextual\nnarratives, achieving performance comparable to state-of-the-art recommenders\nand language models.\n","authors":["Kun Su","Krishna Sayana","Hubert Pham","James Pine","Yuri Vasilevski","Raghavendra Vasudeva","Marialena Kyriakidi","Liam Hebert","Ambarish Jash","Anushya Subbiah","Sukhdeep Sodhi"],"pdf_url":"https://arxiv.org/pdf/2503.11924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18246v2","updated":"2025-07-11T11:43:08Z","published":"2025-04-25T10:46:56Z","title":"One-Pass to Reason: Token Duplication and Block-Sparse Mask for\n  Efficient Fine-Tuning on Multi-Turn Reasoning","summary":"  Fine-tuning Large Language Models (LLMs) on multi-turn reasoning datasets\nrequires N (number of turns) separate forward passes per conversation due to\nreasoning token visibility constraints, as reasoning tokens for a turn are\ndiscarded in subsequent turns. We propose duplicating response tokens along\nwith a custom attention mask to enable single-pass processing of entire\nconversations. We prove our method produces identical losses to the N-pass\napproach while reducing time complexity from $O\\bigl(N^{3}\\bigl)$ to\n$O\\bigl(N^{2}\\bigl)$ and maintaining the same memory complexity for a\ntransformer based model. Our approach achieves significant training speedup\nwhile preserving accuracy. Our implementation is available online\n(https://github.com/devrev/One-Pass-to-Reason).\n","authors":["Ritesh Goru","Shanay Mehta","Prateek Jain"],"pdf_url":"https://arxiv.org/pdf/2504.18246v2.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.08499v1","updated":"2025-07-11T11:21:18Z","published":"2025-07-11T11:21:18Z","title":"PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for\n  Cross-Lingual Multi-Emotion Detection in Short Texts","summary":"  This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection (Track A), which focuses on multi-label emotion\ndetection in short texts. We propose a feature-centric framework that\ndynamically adapts document representations and learning algorithms to optimize\nlanguage-specific performance. Our study evaluates three key components:\ndocument representation, dimensionality reduction, and model training in 28\nlanguages, highlighting five for detailed analysis. The results show that\nTF-IDF remains highly effective for low-resource languages, while contextual\nembeddings like FastText and transformer-based document representations, such\nas those produced by Sentence-BERT, exhibit language-specific strengths.\nPrincipal Component Analysis (PCA) reduces training time without compromising\nperformance, particularly benefiting FastText and neural models such as\nMulti-Layer Perceptrons (MLP). Computational efficiency analysis underscores\nthe trade-off between model complexity and processing cost. Our framework\nprovides a scalable solution for multilingual emotion detection, addressing the\nchallenges of linguistic diversity and resource constraints.\n","authors":["Ziyi Huang","Xia Cui"],"pdf_url":"https://arxiv.org/pdf/2507.08499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08498v1","updated":"2025-07-11T11:20:39Z","published":"2025-07-11T11:20:39Z","title":"Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop","summary":"  Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic\nmodel used for uncovering abstract topics within document collections. In this\npaper, we explore the effectiveness of augmenting topic models with Large\nLanguage Models (LLMs) through integration into two key phases: Initialization\nand Post-Correction. Since the LDA is highly dependent on the quality of its\ninitialization, we conduct extensive experiments on the LLM-guided topic\nclustering for initializing the Gibbs sampling algorithm. Interestingly, the\nexperimental results reveal that while the proposed initialization strategy\nimproves the early iterations of LDA, it has no effect on the convergence and\nyields the worst performance compared to the baselines. The LLM-enabled\npost-correction, on the other hand, achieved a promising improvement of 5.86%\nin the coherence evaluation. These results highlight the practical benefits of\nthe LLM-in-the-loop approach and challenge the belief that LLMs are always the\nsuperior text mining alternative.\n","authors":["Mengze Hong","Chen Jason Zhang","Di Jiang"],"pdf_url":"https://arxiv.org/pdf/2507.08498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08496v1","updated":"2025-07-11T11:18:49Z","published":"2025-07-11T11:18:49Z","title":"LLaPa: A Vision-Language Model Framework for Counterfactual-Aware\n  Procedural Planning","summary":"  While large language models (LLMs) have advanced procedural planning for\nembodied AI systems through strong reasoning abilities, the integration of\nmultimodal inputs and counterfactual reasoning remains underexplored. To tackle\nthese challenges, we introduce LLaPa, a vision-language model framework\ndesigned for multimodal procedural planning. LLaPa generates executable action\nsequences from textual task descriptions and visual environmental images using\nvision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary\nmodules to improve procedural planning. The first module, the Task-Environment\nReranker (TER), leverages task-oriented segmentation to create a task-sensitive\nfeature space, aligning textual descriptions with visual environments and\nemphasizing critical regions for procedural execution. The second module, the\nCounterfactual Activities Retriever (CAR), identifies and emphasizes potential\ncounterfactual conditions, enhancing the model's reasoning capability in\ncounterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED\nbenchmarks demonstrate that LLaPa generates higher-quality plans with superior\nLCS and correctness, outperforming advanced models. The code and models are\navailable https://github.com/sunshibo1234/LLaPa.\n","authors":["Shibo Sun","Xue Li","Donglin Di","Mingjie Wei","Lanshun Nie","Wei-Nan Zhang","Dechen Zhan","Yang Song","Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2507.08496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08491v1","updated":"2025-07-11T11:16:01Z","published":"2025-07-11T11:16:01Z","title":"A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation\n  using clembench","summary":"  There are currently two main paradigms for evaluating large language models\n(LLMs), reference-based evaluation and preference-based evaluation. The first,\ncarried over from the evaluation of machine learning models in general, relies\non pre-defined task instances, for which reference task executions are\navailable. The second, best exemplified by the LM-arena, relies on (often\nself-selected) users bringing their own intents to a site that routes these to\nseveral models in parallel, among whose responses the user then selects their\nmost preferred one. The former paradigm hence excels at control over what is\ntested, while the latter comes with higher ecological validity, testing actual\nuse cases interactively. Recently, a third complementary paradigm has emerged\nthat combines some of the strengths of these approaches, offering control over\nmulti-turn, reference-free, repeatable interactions, while stressing\ngoal-directedness: dialogue game based evaluation. While the utility of this\napproach has been shown by several projects, its adoption has been held back by\nthe lack of a mature, easily re-usable implementation. In this paper, we\npresent clembench, which has been in continuous development since 2023 and has\nin its latest release been optimized for ease of general use. We describe how\nit can be used to benchmark one's own models (using a provided set of benchmark\ngame instances in English), as well as how easily the benchmark itself can be\nextended with new, tailor-made targeted tests.\n","authors":["David Schlangen","Sherzod Hakimov","Jonathan Jordan","Philipp Sadler"],"pdf_url":"https://arxiv.org/pdf/2507.08491v1.pdf","comment":"All code required to run the benchmark, as well as extensive\n  documentation, is available at https://github.com/clembench/clembench"},{"id":"http://arxiv.org/abs/2507.08487v1","updated":"2025-07-11T11:05:27Z","published":"2025-07-11T11:05:27Z","title":"Enhancing Essay Cohesion Assessment: A Novel Item Response Theory\n  Approach","summary":"  Essays are considered a valuable mechanism for evaluating learning outcomes\nin writing. Textual cohesion is an essential characteristic of a text, as it\nfacilitates the establishment of meaning between its parts. Automatically\nscoring cohesion in essays presents a challenge in the field of educational\nartificial intelligence. The machine learning algorithms used to evaluate texts\ngenerally do not consider the individual characteristics of the instances that\ncomprise the analysed corpus. In this meaning, item response theory can be\nadapted to the context of machine learning, characterising the ability,\ndifficulty and discrimination of the models used. This work proposes and\nanalyses the performance of a cohesion score prediction approach based on item\nresponse theory to adjust the scores generated by machine learning models. In\nthis study, the corpus selected for the experiments consisted of the extended\nEssay-BR, which includes 6,563 essays in the style of the National High School\nExam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235\nessays written by 5th to 9th grade students from public schools. We extracted\n325 linguistic features and treated the problem as a machine learning\nregression task. The experimental results indicate that the proposed approach\noutperforms conventional machine learning models and ensemble methods in\nseveral evaluation metrics. This research explores a potential approach for\nimproving the automatic evaluation of cohesion in educational essays.\n","authors":["Bruno Alexandre Rosa","Hilário Oliveira","Luiz Rodrigues","Eduardo Araujo Oliveira","Rafael Ferreira Mello"],"pdf_url":"https://arxiv.org/pdf/2507.08487v1.pdf","comment":"24 pages, 4 tables"},{"id":"http://arxiv.org/abs/2507.06261v2","updated":"2025-07-11T11:03:21Z","published":"2025-07-07T17:36:04Z","title":"Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,\n  Long Context, and Next Generation Agentic Capabilities","summary":"  In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and\nGemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite\nmodels. Gemini 2.5 Pro is our most capable model yet, achieving SoTA\nperformance on frontier coding and reasoning benchmarks. In addition to its\nincredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that\nexcels at multimodal understanding and it is now able to process up to 3 hours\nof video content. Its unique combination of long context, multimodal and\nreasoning capabilities can be combined to unlock new agentic workflows. Gemini\n2.5 Flash provides excellent reasoning abilities at a fraction of the compute\nand latency requirements and Gemini 2.0 Flash and Flash-Lite provide high\nperformance at low latency and cost. Taken together, the Gemini 2.X model\ngeneration spans the full Pareto frontier of model capability vs cost, allowing\nusers to explore the boundaries of what is possible with complex agentic\nproblem solving.\n","authors":["Gheorghe Comanici","Eric Bieber","Mike Schaekermann","Ice Pasupat","Noveen Sachdeva","Inderjit Dhillon","Marcel Blistein","Ori Ram","Dan Zhang","Evan Rosen","Luke Marris","Sam Petulla","Colin Gaffney","Asaf Aharoni","Nathan Lintz","Tiago Cardal Pais","Henrik Jacobsson","Idan Szpektor","Nan-Jiang Jiang","Krishna Haridasan","Ahmed Omran","Nikunj Saunshi","Dara Bahri","Gaurav Mishra","Eric Chu","Toby Boyd","Brad Hekman","Aaron Parisi","Chaoyi Zhang","Kornraphop Kawintiranon","Tania Bedrax-Weiss","Oliver Wang","Ya Xu","Ollie Purkiss","Uri Mendlovic","Ilaï Deutel","Nam Nguyen","Adam Langley","Flip Korn","Lucia Rossazza","Alexandre Ramé","Sagar Waghmare","Helen Miller","Vaishakh Keshava","Ying Jian","Xiaofan Zhang","Raluca Ada Popa","Kedar Dhamdhere","Blaž Bratanič","Kyuyeun Kim","Terry Koo","Ferran Alet","Yi-ting Chen","Arsha Nagrani","Hannah Muckenhirn","Zhiyuan Zhang","Corbin Quick","Filip Pavetić","Duc Dung Nguyen","Joao Carreira","Michael Elabd","Haroon Qureshi","Fabian Mentzer","Yao-Yuan Yang","Danielle Eisenbud","Anmol Gulati","Ellie Talius","Eric Ni","Sahra Ghalebikesabi","Edouard Yvinec","Alaa Saade","Thatcher Ulrich","Lorenzo Blanco","Dan A. Calian","Muhuan Huang","Aäron van den Oord","Naman Goyal","Terry Chen","Praynaa Rawlani","Christian Schallhart","Swachhand Lokhande","Xianghong Luo","Jyn Shan","Ceslee Montgomery","Victoria Krakovna","Federico Piccinini","Omer Barak","Jingyu Cui","Yiling Jia","Mikhail Dektiarev","Alexey Kolganov","Shiyu Huang","Zhe Chen","Xingyu Wang","Jessica Austin","Peter de Boursac","Evgeny Sluzhaev","Frank Ding","Huijian Li","Surya Bhupatiraju","Mohit Agarwal","Sławek Kwasiborski","Paramjit Sandhu","Patrick Siegler","Ahmet Iscen","Eyal Ben-David","Shiraz Butt","Miltos Allamanis","Seth Benjamin","Robert Busa-Fekete","Felix Hernandez-Campos","Sasha Goldshtein","Matt Dibb","Weiyang Zhang","Annie Marsden","Carey Radebaugh","Stephen Roller","Abhishek Nayyar","Jacob Austin","Tayfun Terzi","Bhargav Kanagal Shamanna","Pete Shaw","Aayush Singh","Florian Luisier","Artur Mendonça","Vaibhav Aggarwal","Larisa Markeeva","Claudio Fantacci","Sergey Brin","HyunJeong Choe","Guanyu Wang","Hartwig Adam","Avigail Dabush","Tatsuya Kiyono","Eyal Marcus","Jeremy Cole","Theophane Weber","Hongrae Lee","Ronny Huang","Alex Muzio","Leandro Kieliger","Maigo Le","Courtney Biles","Long Le","Archit Sharma","Chengrun Yang","Avery Lamp","Dave Dopson","Nate Hurley","Katrina Xinyi Xu","Zhihao Shan","Shuang Song","Jiewen Tan","Alexandre Senges","George Zhang","Chong You","Yennie Jun","David Raposo","Susanna Ricco","Xuan Yang","Weijie Chen","Prakhar Gupta","Arthur Szlam","Kevin Villela","Chun-Sung Ferng","Daniel Kasenberg","Chen Liang","Rui Zhu","Arunachalam Narayanaswamy","Florence Perot","Paul Pucciarelli","Anna Shekhawat","Alexey Stern","Rishikesh Ingale","Stefani Karp","Sanaz Bahargam","Adrian Goedeckemeyer","Jie Han","Sicheng Li","Andrea Tacchetti","Dian Yu","Abhishek Chakladar","Zhiying Zhang","Mona El Mahdy","Xu Gao","Dale Johnson","Samrat Phatale","AJ Piergiovanni","Hyeontaek Lim","Clement Farabet","Carl Lebsack","Theo Guidroz","John Blitzer","Nico Duduta","David Madras","Steve Li","Daniel von Dincklage","Xin Li","Mahdis Mahdieh","George Tucker","Ganesh Jawahar","Owen Xiao","Danny Tarlow","Robert Geirhos","Noam Velan","Daniel Vlasic","Kalesha Bullard","SK Park","Nishesh Gupta","Kellie Webster","Ayal Hitron","Jieming Mao","Julian Eisenschlos","Laurel Prince","Nina D'Souza","Kelvin Zheng","Sara Nasso","Gabriela Botea","Carl Doersch","Caglar Unlu","Chris Alberti","Alexey Svyatkovskiy","Ankita Goel","Krzysztof Choromanski","Pan-Pan Jiang","Richard Nguyen","Four Flynn","Daria Ćurko","Peter Chen","Nicholas Roth","Kieran Milan","Caleb Habtegebriel","Shashi Narayan","Michael Moffitt","Jake Marcus","Thomas Anthony","Brendan McMahan","Gowoon Cheon","Ruibo Liu","Megan Barnes","Lukasz Lew","Rebeca Santamaria-Fernandez","Mayank Upadhyay","Arjun Akula","Arnar Mar Hrafnkelsson","Alvaro Caceres","Andrew Bunner","Michal Sokolik","Subha Puttagunta","Lawrence Moore","Berivan Isik","Jay Hartford","Lawrence Chan","Pradeep Shenoy","Dan Holtmann-Rice","Jane Park","Fabio Viola","Alex Salcianu","Sujeevan Rajayogam","Ian Stewart-Binks","Zelin Wu","Richard Everett","Xi Xiong","Pierre-Antoine Manzagol","Gary Leung","Carl Saroufim","Bo Pang","Dawid Wegner","George Papamakarios","Jennimaria Palomaki","Helena Pankov","Guangda Lai","Guilherme Tubone","Shubin Zhao","Theofilos Strinopoulos","Seth Neel","Mingqiu Wang","Joe Kelley","Li Li","Pingmei Xu","Anitha Vijayakumar","Andrea D'olimpio","Omer Levy","Massimo Nicosia","Grigory Rozhdestvenskiy","Ni Lao","Sirui Xie","Yash Katariya","Jon Simon","Sanjiv Kumar","Florian Hartmann","Michael Kilgore","Jinhyuk Lee","Aroma Mahendru","Roman Ring","Tom Hennigan","Fiona Lang","Colin Cherry","David Steiner","Dawsen Hwang","Ray Smith","Pidong Wang","Jeremy Chen","Ming-Hsuan Yang","Sam Kwei","Philippe Schlattner","Donnie Kim","Ganesh Poomal Girirajan","Nikola Momchev","Ayushi Agarwal","Xingyi Zhou","Ilkin Safarli","Zachary Garrett","AJ Pierigiovanni","Sarthak Jauhari","Alif Raditya Rochman","Shikhar Vashishth","Quan Yuan","Christof Angermueller","Jon Blanton","Xinying Song","Nitesh Bharadwaj Gundavarapu","Thi Avrahami","Maxine Deines","Subhrajit Roy","Manish Gupta","Christopher Semturs","Shobha Vasudevan","Aditya Srikanth Veerubhotla","Shriya Sharma","Josh Jacob","Zhen Yang","Andreas Terzis","Dan Karliner","Auriel Wright","Tania Rojas-Esponda","Ashley Brown","Abhijit Guha Roy","Pawan Dogra","Andrei Kapishnikov","Peter Young","Wendy Kan","Vinodh Kumar Rajendran","Maria Ivanova","Salil Deshmukh","Chia-Hua Ho","Mike Kwong","Stav Ginzburg","Annie Louis","KP Sawhney","Slav Petrov","Jing Xie","Yunfei Bai","Georgi Stoyanov","Alex Fabrikant","Rajesh Jayaram","Yuqi Li","Joe Heyward","Justin Gilmer","Yaqing Wang","Radu Soricut","Luyang Liu","Qingnan Duan","Jamie Hayes","Maura O'Brien","Gaurav Singh Tomar","Sivan Eiger","Bahar Fatemi","Jeffrey Hui","Catarina Barros","Adaeze Chukwuka","Alena Butryna","Saksham Thakur","Austin Huang","Zhufeng Pan","Haotian Tang","Serkan Cabi","Tulsee Doshi","Michiel Bakker","Sumit Bagri","Ruy Ley-Wild","Adam Lelkes","Jennie Lees","Patrick Kane","David Greene","Shimu Wu","Jörg Bornschein","Gabriela Surita","Sarah Hodkinson","Fangtao Li","Chris Hidey","Sébastien Pereira","Sean Ammirati","Phillip Lippe","Adam Kraft","Pu Han","Sebastian Gerlach","Zifeng Wang","Liviu Panait","Feng Han","Brian Farris","Yingying Bi","Hannah DeBalsi","Miaosen Wang","Gladys Tyen","James Cohan","Susan Zhang","Jarred Barber","Da-Woon Chung","Jaeyoun Kim","Markus Kunesch","Steven Pecht","Nami Akazawa","Abe Friesen","James Lyon","Ali Eslami","Junru Wu","Jie Tan","Yue Song","Ravi Kumar","Chris Welty","Ilia Akolzin","Gena Gibson","Sean Augenstein","Arjun Pillai","Nancy Yuen","Du Phan","Xin Wang","Iain Barr","Heiga Zen","Nan Hua","Casper Liu","Jilei Jerry Wang","Tanuj Bhatia","Hao Xu","Oded Elyada","Pushmeet Kohli","Mirek Olšák","Ke Chen","Azalia Mirhoseini","Noam Shazeer","Shoshana Jakobovits","Maggie Tran","Nolan Ramsden","Tarun Bharti","Fred Alcober","Yunjie Li","Shilpa Shetty","Jing Chen","Dmitry Kalashnikov","Megha Nawhal","Sercan Arik","Hanwen Chen","Michiel Blokzijl","Shubham Gupta","James Rubin","Rigel Swavely","Sophie Bridgers","Ian Gemp","Chen Su","Arun Suggala","Juliette Pluto","Mary Cassin","Alain Vaucher","Kaiyang Ji","Jiahao Cai","Andrew Audibert","Animesh Sinha","David Tian","Efrat Farkash","Amy Hua","Jilin Chen","Duc-Hieu Tran","Edward Loper","Nicole Brichtova","Lara McConnaughey","Ballie Sandhu","Robert Leland","Doug DeCarlo","Andrew Over","James Huang","Xing Wu","Connie Fan","Eric Li","Yun Lei","Deepak Sharma","Cosmin Paduraru","Luo Yu","Matko Bošnjak","Phuong Dao","Min Choi","Sneha Kudugunta","Jakub Adamek","Carlos Guía","Ali Khodaei","Jie Feng","Wenjun Zeng","David Welling","Sandeep Tata","Christina Butterfield","Andrey Vlasov","Seliem El-Sayed","Swaroop Mishra","Tara Sainath","Shentao Yang","RJ Skerry-Ryan","Jeremy Shar","Robert Berry","Arunkumar Rajendran","Arun Kandoor","Andrea Burns","Deepali Jain","Tom Stone","Wonpyo Park","Shibo Wang","Albin Cassirer","Guohui Wang","Hayato Kobayashi","Sergey Rogulenko","Vineetha Govindaraj","Mikołaj Rybiński","Nadav Olmert","Colin Evans","Po-Sen Huang","Kelvin Xu","Premal Shah","Terry Thurk","Caitlin Sikora","Mu Cai","Jin Xie","Elahe Dabir","Saloni Shah","Norbert Kalb","Carrie Zhang","Shruthi Prabhakara","Amit Sabne","Artiom Myaskovsky","Vikas Raunak","Blanca Huergo","Behnam Neyshabur","Jon Clark","Ye Zhang","Shankar Krishnan","Eden Cohen","Dinesh Tewari","James Lottes","Yumeya Yamamori","Hui Elena Li","Mohamed Elhawaty","Ada Maksutaj Oflazer","Adrià Recasens","Sheryl Luo","Duy Nguyen","Taylor Bos","Kalyan Andra","Ana Salazar","Ed Chi","Jeongwoo Ko","Matt Ginsberg","Anders Andreassen","Anian Ruoss","Todor Davchev","Elnaz Davoodi","Chenxi Liu","Min Kim","Santiago Ontanon","Chi Ming To","Dawei Jia","Rosemary Ke","Jing Wang","Anna Korsun","Moran Ambar","Ilya Kornakov","Irene Giannoumis","Toni Creswell","Denny Zhou","Yi Su","Ishaan Watts","Aleksandr Zaks","Evgenii Eltyshev","Ziqiang Feng","Sidharth Mudgal","Alex Kaskasoli","Juliette Love","Kingshuk Dasgupta","Sam Shleifer","Richard Green","Sungyong Seo","Chansoo Lee","Dale Webster","Prakash Shroff","Ganna Raboshchuk","Isabel Leal","James Manyika","Sofia Erell","Daniel Murphy","Zhisheng Xiao","Anton Bulyenov","Julian Walker","Mark Collier","Matej Kastelic","Nelson George","Sushant Prakash","Sailesh Sidhwani","Alexey Frolov","Steven Hansen","Petko Georgiev","Tiberiu Sosea","Chris Apps","Aishwarya Kamath","David Reid","Emma Cooney","Charlotte Magister","Oriana Riva","Alec Go","Pu-Chin Chen","Sebastian Krause","Nir Levine","Marco Fornoni","Ilya Figotin","Nick Roy","Parsa Mahmoudieh","Vladimir Magay","Mukundan Madhavan","Jin Miao","Jianmo Ni","Yasuhisa Fujii","Ian Chou","George Scrivener","Zak Tsai","Siobhan Mcloughlin","Jeremy Selier","Sandra Lefdal","Jeffrey Zhao","Abhijit Karmarkar","Kushal Chauhan","Shivanker Goel","Zhaoyi Zhang","Vihan Jain","Parisa Haghani","Mostafa Dehghani","Jacob Scott","Erin Farnese","Anastasija Ilić","Steven Baker","Julia Pawar","Li Zhong","Josh Camp","Yoel Zeldes","Shravya Shetty","Anand Iyer","Vít Listík","Jiaxian Guo","Luming Tang","Mark Geller","Simon Bucher","Yifan Ding","Hongzhi Shi","Carrie Muir","Dominik Grewe","Ramy Eskander","Octavio Ponce","Boqing Gong","Derek Gasaway","Samira Khan","Umang Gupta","Angelos Filos","Weicheng Kuo","Klemen Kloboves","Jennifer Beattie","Christian Wright","Leon Li","Alicia Jin","Sandeep Mariserla","Miteyan Patel","Jens Heitkaemper","Dilip Krishnan","Vivek Sharma","David Bieber","Christian Frank","John Lambert","Paul Caron","Martin Polacek","Mai Giménez","Himadri Choudhury","Xing Yu","Sasan Tavakkol","Arun Ahuja","Franz Och","Rodolphe Jenatton","Wojtek Skut","Bryan Richter","David Gaddy","Andy Ly","Misha Bilenko","Megh Umekar","Ethan Liang","Martin Sevenich","Mandar Joshi","Hassan Mansoor","Rebecca Lin","Sumit Sanghai","Abhimanyu Singh","Xiaowei Li","Sudheendra Vijayanarasimhan","Zaheer Abbas","Yonatan Bitton","Hansa Srinivasan","Manish Reddy Vuyyuru","Alexander Frömmgen","Yanhua Sun","Ralph Leith","Alfonso Castaño","DJ Strouse","Le Yan","Austin Kyker","Satish Kambala","Mary Jasarevic","Thibault Sellam","Chao Jia","Alexander Pritzel","Raghavender R","Huizhong Chen","Natalie Clay","Sudeep Gandhe","Sean Kirmani","Sayna Ebrahimi","Hannah Kirkwood","Jonathan Mallinson","Chao Wang","Adnan Ozturel","Kuo Lin","Shyam Upadhyay","Vincent Cohen-Addad","Sean Purser-haskell","Yichong Xu","Ebrahim Songhori","Babi Seal","Alberto Magni","Almog Gueta","Tingting Zou","Guru Guruganesh","Thais Kagohara","Hung Nguyen","Khalid Salama","Alejandro Cruzado Ruiz","Justin Frye","Zhenkai Zhu","Matthias Lochbrunner","Simon Osindero","Wentao Yuan","Lisa Lee","Aman Prasad","Lam Nguyen Thiet","Daniele Calandriello","Victor Stone","Qixuan Feng","Han Ke","Maria Voitovich","Geta Sampemane","Lewis Chiang","Ling Wu","Alexander Bykovsky","Matt Young","Luke Vilnis","Ishita Dasgupta","Aditya Chawla","Qin Cao","Bowen Liang","Daniel Toyama","Szabolcs Payrits","Anca Stefanoiu","Dimitrios Vytiniotis","Ankesh Anand","Tianxiao Shen","Blagoj Mitrevski","Michael Tschannen","Sreenivas Gollapudi","Aishwarya P S","José Leal","Zhe Shen","Han Fu","Wei Wang","Arvind Kannan","Doron Kukliansky","Sergey Yaroshenko","Svetlana Grant","Umesh Telang","David Wood","Alexandra Chronopoulou","Alexandru Ţifrea","Tao Zhou","Tony Tu\\'ân Nguy\\~ên","Muge Ersoy","Anima Singh","Meiyan Xie","Emanuel Taropa","Woohyun Han","Eirikur Agustsson","Andrei Sozanschi","Hui Peng","Alex Chen","Yoel Drori","Efren Robles","Yang Gao","Xerxes Dotiwalla","Ying Chen","Anudhyan Boral","Alexei Bendebury","John Nham","Chris Tar","Luis Castro","Jiepu Jiang","Canoee Liu","Felix Halim","Jinoo Baek","Andy Wan","Jeremiah Liu","Yuan Cao","Shengyang Dai","Trilok Acharya","Ruoxi Sun","Fuzhao Xue","Saket Joshi","Morgane Lustman","Yongqin Xian","Rishabh Joshi","Deep Karkhanis","Nora Kassner","Jamie Hall","Xiangzhuo Ding","Gan Song","Gang Li","Chen Zhu","Yana Kulizhskaya","Bin Ni","Alexey Vlaskin","Solomon Demmessie","Lucio Dery","Salah Zaiem","Yanping Huang","Cindy Fan","Felix Gimeno","Ananth Balashankar","Koji Kojima","Hagai Taitelbaum","Maya Meng","Dero Gharibian","Sahil Singla","Wei Chen","Ambrose Slone","Guanjie Chen","Sujee Rajayogam","Max Schumacher","Suyog Kotecha","Rory Blevins","Qifei Wang","Mor Hazan Taege","Alex Morris","Xin Liu","Fayaz Jamil","Richard Zhang","Pratik Joshi","Ben Ingram","Tyler Liechty","Ahmed Eleryan","Scott Baird","Alex Grills","Gagan Bansal","Shan Han","Kiran Yalasangi","Shawn Xu","Majd Al Merey","Isabel Gao","Felix Weissenberger","Igor Karpov","Robert Riachi","Ankit Anand","Gautam Prasad","Kay Lamerigts","Reid Hayes","Jamie Rogers","Mandy Guo","Ashish Shenoy","Qiong Q Hu","Kyle He","Yuchen Liu","Polina Zablotskaia","Sagar Gubbi","Yifan Chang","Jay Pavagadhi","Kristian Kjems","Archita Vadali","Diego Machado","Yeqing Li","Renshen Wang","Dipankar Ghosh","Aahil Mehta","Dana Alon","George Polovets","Alessio Tonioni","Nate Kushman","Joel D'sa","Lin Zhuo","Allen Wu","Rohin Shah","John Youssef","Jiayu Ye","Justin Snyder","Karel Lenc","Senaka Buthpitiya","Matthew Tung","Jichuan Chang","Tao Chen","David Saxton","Jenny Lee","Lydia Lihui Zhang","James Qin","Prabakar Radhakrishnan","Maxwell Chen","Piotr Ambroszczyk","Metin Toksoz-Exley","Yan Zhong","Nitzan Katz","Brendan O'Donoghue","Tamara von Glehn","Adi Gerzi Rosenthal","Aga Świetlik","Xiaokai Zhao","Nick Fernando","Jinliang Wei","Jieru Mei","Sergei Vassilvitskii","Diego Cedillo","Pranjal Awasthi","Hui Zheng","Koray Kavukcuoglu","Itay Laish","Joseph Pagadora","Marc Brockschmidt","Christopher A. Choquette-Choo","Arunkumar Byravan","Yifeng Lu","Xu Chen","Mia Chen","Kenton Lee","Rama Pasumarthi","Sijal Bhatnagar","Aditya Shah","Qiyin Wu","Zhuoyuan Chen","Zack Nado","Bartek Perz","Zixuan Jiang","David Kao","Ganesh Mallya","Nino Vieillard","Lantao Mei","Sertan Girgin","Mandy Jordan","Yeongil Ko","Alekh Agarwal","Yaxin Liu","Yasemin Altun","Raoul de Liedekerke","Anastasios Kementsietsidis","Daiyi Peng","Dangyi Liu","Utku Evci","Peter Humphreys","Austin Tarango","Xiang Deng","Yoad Lewenberg","Kevin Aydin","Chengda Wu","Bhavishya Mittal","Tsendsuren Munkhdalai","Kleopatra Chatziprimou","Rodrigo Benenson","Uri First","Xiao Ma","Jinning Li","Armand Joulin","Hamish Tomlinson","Tingnan Zhang","Milad Nasr","Zhi Hong","Michaël Sander","Lisa Anne Hendricks","Anuj Sharma","Andrew Bolt","Eszter Vértes","Jiri Simsa","Tomer Levinboim","Olcan Sercinoglu","Divyansh Shukla","Austin Wu","Craig Swanson","Danny Vainstein","Fan Bu","Bo Wang","Ryan Julian","Charles Yoon","Sergei Lebedev","Antonious Girgis","Bernd Bandemer","David Du","Todd Wang","Xi Chen","Ying Xiao","Peggy Lu","Natalie Ha","Vlad Ionescu","Simon Rowe","Josip Matak","Federico Lebron","Andreas Steiner","Lalit Jain","Manaal Faruqui","Nicolas Lacasse","Georgie Evans","Neesha Subramaniam","Dean Reich","Giulia Vezzani","Aditya Pandey","Joe Stanton","Tianhao Zhou","Liam McCafferty","Henry Griffiths","Verena Rieser","Soheil Hassas Yeganeh","Eleftheria Briakou","Lu Huang","Zichuan Wei","Liangchen Luo","Erik Jue","Gabby Wang","Victor Cotruta","Myriam Khan","Jongbin Park","Qiuchen Guo","Peiran Li","Rong Rong","Diego Antognini","Anastasia Petrushkina","Chetan Tekur","Eli Collins","Parul Bhatia","Chester Kwak","Wenhu Chen","Arvind Neelakantan","Immanuel Odisho","Sheng Peng","Vincent Nallatamby","Vaibhav Tulsyan","Fabian Pedregosa","Peng Xu","Raymond Lin","Yulong Wang","Emma Wang","Sholto Douglas","Reut Tsarfaty","Elena Gribovskaya","Renga Aravamudhan","Manu Agarwal","Mara Finkelstein","Qiao Zhang","Elizabeth Cole","Phil Crone","Sarmishta Velury","Anil Das","Chris Sauer","Luyao Xu","Danfeng Qin","Chenjie Gu","Dror Marcus","CJ Zheng","Wouter Van Gansbeke","Sobhan Miryoosefi","Haitian Sun","YaGuang Li","Charlie Chen","Jae Yoo","Pavel Dubov","Alex Tomala","Adams Yu","Paweł Wesołowski","Alok Gunjan","Eddie Cao","Jiaming Luo","Nikhil Sethi","Arkadiusz Socala","Laura Graesser","Tomas Kocisky","Arturo BC","Minmin Chen","Edward Lee","Sophie Wang","Weize Kong","Qiantong Xu","Nilesh Tripuraneni","Yiming Li","Xinxin Yu","Allen Porter","Paul Voigtlaender","Biao Zhang","Arpi Vezer","Sarah York","Qing Wei","Geoffrey Cideron","Mark Kurzeja","Seungyeon Kim","Benny Li","Angéline Pouget","Hyo Lee","Kaspar Daugaard","Yang Li","Dave Uthus","Aditya Siddhant","Paul Cavallaro","Sriram Ganapathy","Maulik Shah","Rolf Jagerman","Jeff Stanway","Piermaria Mendolicchio","Li Xiao","Kayi Lee","Tara Thompson","Shubham Milind Phal","Jason Chase","Sun Jae Lee","Adrian N Reyes","Disha Shrivastava","Zhen Qin","Roykrong Sukkerd","Seth Odoom","Lior Madmoni","John Aslanides","Jonathan Herzig","Elena Pochernina","Sheng Zhang","Parker Barnes","Daisuke Ikeda","Qiujia Li","Shuo-yiin Chang","Shakir Mohamed","Jim Sproch","Richard Powell","Bidisha Samanta","Domagoj Ćevid","Anton Kovsharov","Shrestha Basu Mallick","Srinivas Tadepalli","Anne Zheng","Kareem Ayoub","Andreas Noever","Christian Reisswig","Zhuo Xu","Junhyuk Oh","Martin Matysiak","Tim Blyth","Shereen Ashraf","Julien Amelot","Boone Severson","Michele Bevilacqua","Motoki Sano","Ethan Dyer","Ofir Roval","Anu Sinha","Yin Zhong","Sagi Perel","Tea Sabolić","Johannes Mauerer","Willi Gierke","Mauro Verzetti","Rodrigo Cabrera","Alvin Abdagic","Steven Hemingray","Austin Stone","Jong Lee","Farooq Ahmad","Karthik Raman","Lior Shani","Jonathan Lai","Orhan Firat","Nathan Waters","Eric Ge","Mo Shomrat","Himanshu Gupta","Rajeev Aggarwal","Tom Hudson","Bill Jia","Simon Baumgartner","Palak Jain","Joe Kovac","Junehyuk Jung","Ante Žužul","Will Truong","Morteza Zadimoghaddam","Songyou Peng","Marco Liang","Rachel Sterneck","Balaji Lakshminarayanan","Machel Reid","Oliver Woodman","Tong Zhou","Jianling Wang","Vincent Coriou","Arjun Narayanan","Jay Hoover","Yenai Ma","Apoorv Jindal","Clayton Sanford","Doug Reid","Swaroop Ramaswamy","Alex Kurakin","Roland Zimmermann","Yana Lunts","Dragos Dena","Zalán Borsos","Vered Cohen","Shujian Zhang","Will Grathwohl","Robert Dadashi","Morgan Redshaw","Joshua Kessinger","Julian Odell","Silvano Bonacina","Zihang Dai","Grace Chen","Ayush Dubey","Pablo Sprechmann","Mantas Pajarskas","Wenxuan Zhou","Niharika Ahuja","Tara Thomas","Martin Nikoltchev","Matija Kecman","Bharath Mankalale","Andrey Ryabtsev","Jennifer She","Christian Walder","Jiaming Shen","Lu Li","Carolina Parada","Sheena Panthaplackel","Okwan Kwon","Matt Lawlor","Utsav Prabhu","Yannick Schroecker","Marc'aurelio Ranzato","Pete Blois","Iurii Kemaev","Ting Yu","Dmitry Lepikhin","Hao Xiong","Sahand Sharifzadeh","Oleaser Johnson","Jeremiah Willcock","Rui Yao","Greg Farquhar","Sujoy Basu","Hidetoshi Shimokawa","Nina Anderson","Haiguang Li","Khiem Pham","Yizhong Liang","Sebastian Borgeaud","Alexandre Moufarek","Hideto Kazawa","Blair Kutzman","Marcin Sieniek","Sara Smoot","Ruth Wang","Natalie Axelsson","Nova Fallen","Prasha Sundaram","Yuexiang Zhai","Varun Godbole","Petros Maniatis","Alek Wang","Ilia Shumailov","Santhosh Thangaraj","Remi Crocker","Nikita Gupta","Gang Wu","Phil Chen","Gellért Weisz","Celine Smith","Mojtaba Seyedhosseini","Boya Fang","Xiyang Luo","Roey Yogev","Zeynep Cankara","Andrew Hard","Helen Ran","Rahul Sukthankar","George Necula","Gaël Liu","Honglong Cai","Praseem Banzal","Daniel Keysers","Sanjay Ghemawat","Connie Tao","Emma Dunleavy","Aditi Chaudhary","Wei Li","Maciej Mikuła","Chen-Yu Lee","Tiziana Refice","Krishna Somandepalli","Alexandre Fréchette","Dan Bahir","John Karro","Keith Rush","Sarah Perrin","Bill Rosgen","Xiaomeng Yang","Clara Huiyi Hu","Mahmoud Alnahlawi","Justin Mao-Jones","Roopal Garg","Hoang Nguyen","Bat-Orgil Batsaikhan","Iñaki Iturrate","Anselm Levskaya","Avi Singh","Ashyana Kachra","Tony Lu","Denis Petek","Zheng Xu","Mark Graham","Lukas Zilka","Yael Karov","Marija Kostelac","Fangyu Liu","Yaohui Guo","Weiyue Wang","Bernd Bohnet","Emily Pitler","Tony Bruguier","Keisuke Kinoshita","Chrysovalantis Anastasiou","Nilpa Jha","Ting Liu","Jerome Connor","Phil Wallis","Philip Pham","Eric Bailey","Shixin Li","Heng-Tze Cheng","Sally Ma","Haiqiong Li","Akanksha Maurya","Kate Olszewska","Manfred Warmuth","Christy Koh","Dominik Paulus","Siddhartha Reddy Jonnalagadda","Enrique Piqueras","Ali Elqursh","Geoff Brown","Hadar Shemtov","Loren Maggiore","Fei Xia","Ryan Foley","Beka Westberg","George van den Driessche","Livio Baldini Soares","Arjun Kar","Michael Quinn","Siqi Zuo","Jialin Wu","Kyle Kastner","Anna Bortsova","Aijun Bai","Ales Mikhalap","Luowei Zhou","Jennifer Brennan","Vinay Ramasesh","Honglei Zhuang","John Maggs","Johan Schalkwyk","Yuntao Xu","Hui Huang","Andrew Howard","Sasha Brown","Linting Xue","Gloria Shen","Brian Albert","Neha Jha","Daniel Zheng","Varvara Krayvanova","Spurthi Amba Hombaiah","Olivier Lacombe","Gautam Vasudevan","Dan Graur","Tian Xie","Meet Gandhi","Bangju Wang","Dustin Zelle","Harman Singh","Dahun Kim","Sébastien Cevey","Victor Ungureanu","Natasha Noy","Fei Liu","Annie Xie","Fangxiaoyu Feng","Katerina Tsihlas","Daniel Formoso","Neera Vats","Quentin Wellens","Yinan Wang","Niket Kumar Bhumihar","Samrat Ghosh","Matt Hoffman","Tom Lieber","Oran Lang","Kush Bhatia","Tom Paine","Aroonalok Pyne","Ronny Votel","Madeleine Clare Elish","Benoit Schillings","Alex Panagopoulos","Haichuan Yang","Adam Raveret","Zohar Yahav","Shuang Liu","Dalia El Badawy","Nishant Agrawal","Mohammed Badawi","Mahdi Mirzazadeh","Carla Bromberg","Fan Ye","Chang Liu","Tatiana Sholokhova","George-Cristian Muraru","Gargi Balasubramaniam","Jonathan Malmaud","Alen Carin","Danilo Martins","Irina Jurenka","Pankil Botadra","Dave Lacey","Richa Singh","Mariano Schain","Dan Zheng","Isabelle Guyon","Victor Lavrenko","Seungji Lee","Xiang Zhou","Demis Hassabis","Jeshwanth Challagundla","Derek Cheng","Nikhil Mehta","Matthew Mauger","Michela Paganini","Pushkar Mishra","Kate Lee","Zhang Li","Lexi Baugher","Ondrej Skopek","Max Chang","Amir Zait","Gaurav Menghani","Lizzetth Bellot","Guangxing Han","Jean-Michel Sarr","Sharat Chikkerur","Himanshu Sahni","Rohan Anil","Arun Narayanan","Chandu Thekkath","Daniele Pighin","Hana Strejček","Marko Velic","Fred Bertsch","Manuel Tragut","Keran Rong","Alicia Parrish","Kai Bailey","Jiho Park","Isabela Albuquerque","Abhishek Bapna","Rajesh Venkataraman","Alec Kosik","Johannes Griesser","Zhiwei Deng","Alek Andreev","Qingyun Dou","Kevin Hui","Fanny Wei","Xiaobin Yu","Lei Shu","Avia Aharon","David Barker","Badih Ghazi","Sebastian Flennerhag","Chris Breaux","Yuchuan Liu","Matthew Bilotti","Josh Woodward","Uri Alon","Stephanie Winkler","Tzu-Kuo Huang","Kostas Andriopoulos","João Gabriel Oliveira","Penporn Koanantakool","Berkin Akin","Michael Wunder","Cicero Nogueira dos Santos","Mohammad Hossein Bateni","Lin Yang","Dan Horgan","Beer Changpinyo","Keyvan Amiri","Min Ma","Dayeong Lee","Lihao Liang","Anirudh Baddepudi","Tejasi Latkar","Raia Hadsell","Jun Xu","Hairong Mu","Michael Han","Aedan Pope","Snchit Grover","Frank Kim","Ankit Bhagatwala","Guan Sun","Yamini Bansal","Amir Globerson","Alireza Nazari","Samira Daruki","Hagen Soltau","Jane Labanowski","Laurent El Shafey","Matt Harvey","Yanif Ahmad","Elan Rosenfeld","William Kong","Etienne Pot","Yi-Xuan Tan","Aurora Wei","Victoria Langston","Marcel Prasetya","Petar Veličković","Richard Killam","Robin Strudel","Darren Ni","Zhenhai Zhu","Aaron Archer","Kavya Kopparapu","Lynn Nguyen","Emilio Parisotto","Hussain Masoom","Sravanti Addepalli","Jordan Grimstad","Hexiang Hu","Joss Moore","Avinatan Hassidim","Le Hou","Mukund Raghavachari","Jared Lichtarge","Adam R. Brown","Hilal Dib","Natalia Ponomareva","Justin Fu","Yujing Zhang","Altaf Rahman","Joana Iljazi","Edouard Leurent","Gabriel Dulac-Arnold","Cosmo Du","Chulayuth Asawaroengchai","Larry Jin","Ela Gruzewska","Ziwei Ji","Benigno Uria","Daniel De Freitas","Paul Barham","Lauren Beltrone","Víctor Campos","Jun Yan","Neel Kovelamudi","Arthur Nguyen","Elinor Davies","Zhichun Wu","Zoltan Egyed","Kristina Toutanova","Nithya Attaluri","Hongliang Fei","Peter Stys","Siddhartha Brahma","Martin Izzard","Siva Velusamy","Scott Lundberg","Vincent Zhuang","Kevin Sequeira","Adam Santoro","Ehsan Amid","Ophir Aharoni","Shuai Ye","Mukund Sundararajan","Lijun Yu","Yu-Cheng Ling","Stephen Spencer","Hugo Song","Josip Djolonga","Christo Kirov","Sonal Gupta","Alessandro Bissacco","Clemens Meyer","Mukul Bhutani","Andrew Dai","Weiyi Wang","Siqi Liu","Ashwin Sreevatsa","Qijun Tan","Maria Wang","Lucy Kim","Yicheng Wang","Alex Irpan","Yang Xiao","Stanislav Fort","Yifan He","Alex Gurney","Bryan Gale","Yue Ma","Monica Roy","Viorica Patraucean","Taylan Bilal","Golnaz Ghiasi","Anahita Hosseini","Melvin Johnson","Zhuowan Li","Yi Tay","Benjamin Beyret","Katie Millican","Josef Broder","Mayank Lunayach","Danny Swisher","Eugen Vušak","David Parkinson","MH Tessler","Adi Mayrav Gilady","Richard Song","Allan Dafoe","Yves Raimond","Masa Yamaguchi","Itay Karo","Elizabeth Nielsen","Kevin Kilgour","Mike Dusenberry","Rajiv Mathews","Jiho Choi","Siyuan Qiao","Harsh Mehta","Sahitya Potluri","Chris Knutsen","Jialu Liu","Tat Tan","Kuntal Sengupta","Keerthana Gopalakrishnan","Abodunrinwa Toki","Mencher Chiang","Mike Burrows","Grace Vesom","Zafarali Ahmed","Ilia Labzovsky","Siddharth Vashishtha","Preeti Singh","Ankur Sharma","Ada Ma","Jinyu Xie","Pranav Talluri","Hannah Forbes-Pollard","Aarush Selvan","Joel Wee","Loic Matthey","Tom Funkhouser","Parthasarathy Gopavarapu","Lev Proleev","Cheng Li","Matt Thomas","Kashyap Kolipaka","Zhipeng Jia","Ashwin Kakarla","Srinivas Sunkara","Joan Puigcerver","Suraj Satishkumar Sheth","Emily Graves","Chen Wang","Sadh MNM Khan","Kai Kang","Shyamal Buch","Fred Zhang","Omkar Savant","David Soergel","Kevin Lee","Linda Friso","Xuanyi Dong","Rahul Arya","Shreyas Chandrakaladharan","Connor Schenck","Greg Billock","Tejas Iyer","Anton Bakalov","Leslie Baker","Alex Ruiz","Angad Chandorkar","Trieu Trinh","Matt Miecnikowski","Yanqi Zhou","Yangsibo Huang","Jiazhong Nie","Ali Shah","Ashish Thapliyal","Sam Haves","Lun Wang","Uri Shaham","Patrick Morris-Suzuki","Soroush Radpour","Leonard Berrada","Thomas Strohmann","Chaochao Yan","Jingwei Shen","Sonam Goenka","Tris Warkentin","Petar Dević","Dan Belov","Albert Webson","Madhavi Yenugula","Puranjay Datta","Jerry Chang","Nimesh Ghelani","Aviral Kumar","Vincent Perot","Jessica Lo","Yang Song","Herman Schmit","Jianmin Chen","Vasilisa Bashlovkina","Xiaoyue Pan","Diana Mincu","Paul Roit","Isabel Edkins","Andy Davis","Yujia Li","Ben Horn","Xinjian Li","Pradeep Kumar S","Eric Doi","Wanzheng Zhu","Sri Gayatri Sundara Padmanabhan","Siddharth Verma","Jasmine Liu","Heng Chen","Mihajlo Velimirović","Malcolm Reynolds","Priyanka Agrawal","Nick Sukhanov","Abhinit Modi","Siddharth Goyal","John Palowitch","Nima Khajehnouri","Wing Lowe","David Klinghoffer","Sharon Silver","Vinh Tran","Candice Schumann","Francesco Piccinno","Xi Liu","Mario Lučić","Xiaochen Yang","Sandeep Kumar","Ajay Kannan","Ragha Kotikalapudi","Mudit Bansal","Fabian Fuchs","Mohammad Javad Hosseini","Abdelrahman Abdelhamed","Dawn Bloxwich","Tianhe Yu","Ruoxin Sang","Gregory Thornton","Karan Gill","Yuchi Liu","Virat Shejwalkar","Jason Lin","Zhipeng Yan","Kehang Han","Thomas Buschmann","Michael Pliskin","Zhi Xing","Susheel Tatineni","Junlin Zhang","Sissie Hsiao","Gavin Buttimore","Marcus Wu","Zefei Li","Geza Kovacs","Legg Yeung","Tao Huang","Aaron Cohen","Bethanie Brownfield","Averi Nowak","Mikel Rodriguez","Tianze Shi","Hado van Hasselt","Kevin Cen","Deepanway Ghoshal","Kushal Majmundar","Weiren Yu","Warren Weilun Chen","Danila Sinopalnikov","Hao Zhang","Vlado Galić","Di Lu","Zeyu Zheng","Maggie Song","Gary Wang","Gui Citovsky","Swapnil Gawde","Isaac Galatzer-Levy","David Silver","Ivana Balazevic","Dipanjan Das","Kingshuk Majumder","Yale Cong","Praneet Dutta","Dustin Tran","Hui Wan","Junwei Yuan","Daniel Eppens","Alanna Walton","Been Kim","Harry Ragan","James Cobon-Kerr","Lu Liu","Weijun Wang","Bryce Petrini","Jack Rae","Rakesh Shivanna","Yan Xiong","Chace Lee","Pauline Coquinot","Yiming Gu","Lisa Patel","Blake Hechtman","Aviel Boag","Orion Jankowski","Alex Wertheim","Alex Lee","Paul Covington","Hila Noga","Sam Sobell","Shanthal Vasanth","William Bono","Chirag Nagpal","Wei Fan","Xavier Garcia","Kedar Soparkar","Aybuke Turker","Nathan Howard","Sachit Menon","Yuankai Chen","Vikas Verma","Vladimir Pchelin","Harish Rajamani","Valentin Dalibard","Ana Ramalho","Yang Guo","Kartikeya Badola","Seojin Bang","Nathalie Rauschmayr","Julia Proskurnia","Sudeep Dasari","Xinyun Chen","Mikhail Sushkov","Anja Hauth","Pauline Sho","Abhinav Singh","Bilva Chandra","Allie Culp","Max Dylla","Olivier Bachem","James Besley","Heri Zhao","Timothy Lillicrap","Wei Wei","Wael Al Jishi","Ning Niu","Alban Rrustemi","Raphaël Lopez Kaufman","Ryan Poplin","Jewel Zhao","Minh Truong","Shikhar Bharadwaj","Ester Hlavnova","Eli Stickgold","Cordelia Schmid","Georgi Stephanov","Zhaoqi Leng","Frederick Liu","Léonard Hussenot","Shenil Dodhia","Juliana Vicente Franco","Lesley Katzen","Abhanshu Sharma","Sarah Cogan","Zuguang Yang","Aniket Ray","Sergi Caelles","Shen Yan","Ravin Kumar","Daniel Gillick","Renee Wong","Joshua Ainslie","Jonathan Hoech","Séb Arnold","Dan Abolafia","Anca Dragan","Ben Hora","Grace Hu","Alexey Guseynov","Yang Lu","Chas Leichner","Jinmeng Rao","Abhimanyu Goyal","Nagabhushan Baddi","Daniel Hernandez Diaz","Tim McConnell","Max Bain","Jake Abernethy","Qiqi Yan","Rylan Schaeffer","Paul Vicol","Will Thompson","Montse Gonzalez Arenas","Mathias Bellaiche","Pablo Barrio","Stefan Zinke","Riccardo Patana","Pulkit Mehta","JK Kearns","Avraham Ruderman","Scott Pollom","David D'Ambrosio","Cath Hope","Yang Yu","Andrea Gesmundo","Kuang-Huei Lee","Aviv Rosenberg","Yiqian Zhou","Yaoyiran Li","Drew Garmon","Yonghui Wu","Safeen Huda","Gil Fidel","Martin Baeuml","Jian Li","Phoebe Kirk","Rhys May","Tao Tu","Sara Mc Carthy","Toshiyuki Fukuzawa","Miranda Aperghis","Chih-Kuan Yeh","Toshihiro Yoshino","Bo Li","Austin Myers","Kaisheng Yao","Ben Limonchik","Changwan Ryu","Rohun Saxena","Alex Goldin","Ruizhe Zhao","Rocky Rhodes","Tao Zhu","Divya Tyam","Heidi Howard","Nathan Byrd","Hongxu Ma","Yan Wu","Ryan Mullins","Qingze Wang","Aida Amini","Sebastien Baur","Yiran Mao","Subhashini Venugopalan","Will Song","Wen Ding","Paul Collins","Sashank Reddi","Megan Shum","Andrei Rusu","Luisa Zintgraf","Kelvin Chan","Sheela Goenka","Mathieu Blondel","Michael Collins","Renke Pan","Marissa Giustina","Nikolai Chinaev","Christian Schuler","Ce Zheng","Jonas Valfridsson","Alyssa Loo","Alex Yakubovich","Jamie Smith","Tao Jiang","Rich Munoz","Gabriel Barcik","Rishabh Bansal","Mingyao Yang","Yilun Du","Pablo Duque","Mary Phuong","Alexandra Belias","Kunal Lad","Zeyu Liu","Tal Schuster","Karthik Duddu","Jieru Hu","Paige Kunkle","Matthew Watson","Jackson Tolins","Josh Smith","Denis Teplyashin","Garrett Bingham","Marvin Ritter","Marco Andreetto","Divya Pitta","Mohak Patel","Shashank Viswanadha","Trevor Strohman","Catalin Ionescu","Jincheng Luo","Yogesh Kalley","Jeremy Wiesner","Dan Deutsch","Derek Lockhart","Peter Choy","Rumen Dangovski","Chawin Sitawarin","Cat Graves","Tanya Lando","Joost van Amersfoort","Ndidi Elue","Zhouyuan Huo","Pooya Moradi","Jean Tarbouriech","Henryk Michalewski","Wenting Ye","Eunyoung Kim","Alex Druinsky","Florent Altché","Xinyi Chen","Artur Dwornik","Da-Cheng Juan","Rivka Moroshko","Horia Toma","Jarrod Kahn","Hai Qian","Maximilian Sieb","Irene Cai","Roman Goldenberg","Praneeth Netrapalli","Sindhu Raghuram","Yuan Gong","Lijie Fan","Evan Palmer","Yossi Matias","Valentin Gabeur","Shreya Pathak","Tom Ouyang","Don Metzler","Geoff Bacon","Srinivasan Venkatachary","Sridhar Thiagarajan","Alex Cullum","Eran Ofek","Vytenis Sakenas","Mohamed Hammad","Cesar Magalhaes","Mayank Daswani","Oscar Chang","Ashok Popat","Ruichao Li","Komal Jalan","Yanhan Hou","Josh Lipschultz","Antoine He","Wenhao Jia","Pier Giuseppe Sessa","Prateek Kolhar","William Wong","Sumeet Singh","Lukas Haas","Jay Whang","Hanna Klimczak-Plucińska","Georges Rotival","Grace Chung","Yiqing Hua","Anfal Siddiqui","Nicolas Serrano","Dongkai Chen","Billy Porter","Libin Bai","Keshav Shivam","Sho Arora","Partha Talukdar","Tom Cobley","Sangnie Bhardwaj","Evgeny Gladchenko","Simon Green","Kelvin Guu","Felix Fischer","Xiao Wu","Eric Wang","Achintya Singhal","Tatiana Matejovicova","James Martens","Hongji Li","Roma Patel","Elizabeth Kemp","Jiaqi Pan","Lily Wang","Blake JianHang Chen","Jean-Baptiste Alayrac","Navneet Potti","Erika Gemzer","Eugene Ie","Kay McKinney","Takaaki Saeki","Edward Chou","Pascal Lamblin","SQ Mah","Zach Fisher","Martin Chadwick","Jon Stritar","Obaid Sarvana","Andrew Hogue","Artem Shtefan","Hadi Hashemi","Yang Xu","Jindong Gu","Sharad Vikram","Chung-Ching Chang","Sabela Ramos","Logan Kilpatrick","Weijuan Xi","Jenny Brennan","Yinghao Sun","Abhishek Jindal","Ionel Gog","Dawn Chen","Felix Wu","Jason Lee","Sudhindra Kopalle","Srinadh Bhojanapalli","Oriol Vinyals","Natan Potikha","Burcu Karagol Ayan","Yuan Yuan","Michael Riley","Piotr Stanczyk","Sergey Kishchenko","Bing Wang","Dan Garrette","Antoine Yang","Vlad Feinberg","CJ Carey","Javad Azizi","Viral Shah","Erica Moreira","Chongyang Shi","Josh Feldman","Elizabeth Salesky","Thomas Lampe","Aneesh Pappu","Duhyeon Kim","Jonas Adler","Avi Caciularu","Brian Walker","Yunhan Xu","Yochai Blau","Dylan Scandinaro","Terry Huang","Sam El-Husseini","Abhishek Sinha","Lijie Ren","Taylor Tobin","Patrik Sundberg","Tim Sohn","Vikas Yadav","Mimi Ly","Emily Xue","Jing Xiong","Afzal Shama Soudagar","Sneha Mondal","Nikhil Khadke","Qingchun Ren","Ben Vargas","Stan Bileschi","Sarah Chakera","Cindy Wang","Boyu Wang","Yoni Halpern","Joe Jiang","Vikas Sindhwani","Petre Petrov","Pranavaraj Ponnuramu","Sanket Vaibhav Mehta","Yu Watanabe","Betty Chan","Matheus Wisniewski","Trang Pham","Jingwei Zhang","Conglong Li","Dario de Cesare","Art Khurshudov","Alex Vasiloff","Melissa Tan","Zoe Ashwood","Bobak Shahriari","Maryam Majzoubi","Garrett Tanzer","Olga Kozlova","Robin Alazard","James Lee-Thorp","Nguyet Minh Phu","Isaac Tian","Junwhan Ahn","Andy Crawford","Lauren Lax","Yuan Shangguan","Iftekhar Naim","David Ross","Oleksandr Ferludin","Tongfei Guo","Andrea Banino","Hubert Soyer","Xiaoen Ju","Dominika Rogozińska","Ishaan Malhi","Marcella Valentine","Daniel Balle","Apoorv Kulshreshtha","Maciej Kula","Yiwen Song","Sophia Austin","John Schultz","Roy Hirsch","Arthur Douillard","Apoorv Reddy","Michael Fink","Summer Yue","Khyatti Gupta","Adam Zhang","Norman Rink","Daniel McDuff","Lei Meng","András György","Yasaman Razeghi","Ricky Liang","Kazuki Osawa","Aviel Atias","Matan Eyal","Tyrone Hill","Nikolai Grigorev","Zhengdong Wang","Nitish Kulkarni","Rachel Soh","Ivan Lobov","Zachary Charles","Sid Lall","Kazuma Hashimoto","Ido Kessler","Victor Gomes","Zelda Mariet","Danny Driess","Alessandro Agostini","Canfer Akbulut","Jingcao Hu","Marissa Ikonomidis","Emily Caveness","Kartik Audhkhasi","Saurabh Agrawal","Ioana Bica","Evan Senter","Jayaram Mudigonda","Kelly Chen","Jingchen Ye","Xuanhui Wang","James Svensson","Philipp Fränken","Josh Newlan","Li Lao","Eva Schnider","Sami Alabed","Joseph Kready","Jesse Emond","Afief Halumi","Tim Zaman","Chengxi Ye","Naina Raisinghani","Vilobh Meshram","Bo Chang","Ankit Singh Rawat","Axel Stjerngren","Sergey Levi","Rui Wang","Xiangzhu Long","Mitchelle Rasquinha","Steven Hand","Aditi Mavalankar","Lauren Agubuzu","Sudeshna Roy","Junquan Chen","Jarek Wilkiewicz","Hao Zhou","Michal Jastrzebski","Qiong Hu","Agustin Dal Lago","Ramya Sree Boppana","Wei-Jen Ko","Jennifer Prendki","Yao Su","Zhi Li","Eliza Rutherford","Girish Ramchandra Rao","Ramona Comanescu","Adrià Puigdomènech","Qihang Chen","Dessie Petrova","Christine Chan","Vedrana Milutinovic","Felipe Tiengo Ferreira","Chin-Yi Cheng","Ming Zhang","Tapomay Dey","Sherry Yang","Ramesh Sampath","Quoc Le","Howard Zhou","Chu-Cheng Lin","Hoi Lam","Christine Kaeser-Chen","Kai Hui","Dean Hirsch","Tom Eccles","Basil Mustafa","Shruti Rijhwani","Morgane Rivière","Yuanzhong Xu","Junjie Wang","Xinyang Geng","Xiance Si","Arjun Khare","Cheolmin Kim","Vahab Mirrokni","Kamyu Lee","Khuslen Baatarsukh","Nathaniel Braun","Lisa Wang","Pallavi LV","Richard Tanburn","Yonghao Zhu","Fangda Li","Setareh Ariafar","Dan Goldberg","Ken Burke","Daniil Mirylenka","Meiqi Guo","Olaf Ronneberger","Hadas Natalie Vogel","Liqun Cheng","Nishita Shetty","Johnson Jia","Thomas Jimma","Corey Fry","Ted Xiao","Martin Sundermeyer","Ryan Burnell","Yannis Assael","Mario Pinto","JD Chen","Rohit Sathyanarayana","Donghyun Cho","Jing Lu","Rishabh Agarwal","Sugato Basu","Lucas Gonzalez","Dhruv Shah","Meng Wei","Dre Mahaarachchi","Rohan Agrawal","Tero Rissa","Yani Donchev","Ramiro Leal-Cavazos","Adrian Hutter","Markus Mircea","Alon Jacovi","Faruk Ahmed","Jiageng Zhang","Shuguang Hu","Bo-Juen Chen","Jonni Kanerva","Guillaume Desjardins","Andrew Lee","Nikos Parotsidis","Asier Mujika","Tobias Weyand","Jasper Snoek","Jo Chick","Kai Chen","Paul Chang","Ethan Mahintorabi","Zi Wang","Tolly Powell","Orgad Keller","Abhirut Gupta","Claire Sha","Kanav Garg","Nicolas Heess","Ágoston Weisz","Cassidy Hardin","Bartek Wydrowski","Ben Coleman","Karina Zainullina","Pankaj Joshi","Alessandro Epasto","Terry Spitz","Binbin Xiong","Kai Zhao","Arseniy Klimovskiy","Ivy Zheng","Johan Ferret","Itay Yona","Waleed Khawaja","Jean-Baptiste Lespiau","Maxim Krikun","Siamak Shakeri","Timothee Cour","Bonnie Li","Igor Krivokon","Dan Suh","Alex Hofer","Jad Al Abdallah","Nikita Putikhin","Oscar Akerlund","Silvio Lattanzi","Anurag Kumar","Shane Settle","Himanshu Srivastava","Folawiyo Campbell-Ajala","Edouard Rosseel","Mihai Dorin Istin","Nishanth Dikkala","Anand Rao","Nick Young","Kate Lin","Dhruva Bhaswar","Yiming Wang","Jaume Sanchez Elias","Kritika Muralidharan","James Keeling","Dayou Du","Siddharth Gopal","Gregory Dibb","Charles Blundell","Manolis Delakis","Jacky Liang","Marco Tulio Ribeiro","Georgi Karadzhov","Guillermo Garrido","Ankur Bapna","Jiawei Cao","Adam Sadovsky","Pouya Tafti","Arthur Guez","Coline Devin","Yixian Di","Jinwei Xing","Chuqiao Joyce Xu","Hanzhao Lin","Chun-Te Chu","Sameera Ponda","Wesley Helmholz","Fan Yang","Yue Gao","Sara Javanmardi","Wael Farhan","Alex Ramirez","Ricardo Figueira","Khe Chai Sim","Yuval Bahat","Ashwin Vaswani","Liangzhe Yuan","Gufeng Zhang","Leland Rechis","Hanjun Dai","Tayo Oguntebi","Alexandra Cordell","Eugénie Rives","Kaan Tekelioglu","Naveen Kumar","Bing Zhang","Aurick Zhou","Nikolay Savinov","Andrew Leach","Alex Tudor","Sanjay Ganapathy","Yanyan Zheng","Mirko Rossini","Vera Axelrod","Arnaud Autef","Yukun Zhu","Zheng Zheng","Mingda Zhang","Baochen Sun","Jie Ren","Nenad Tomasev","Nithish Kannen","Amer Sinha","Charles Chen","Louis O'Bryan","Alex Pak","Aditya Kusupati","Weel Yang","Deepak Ramachandran","Patrick Griffin","Seokhwan Kim","Philipp Neubeck","Craig Schiff","Tammo Spalink","Mingyang Ling","Arun Nair","Ga-Young Joung","Linda Deng","Avishkar Bhoopchand","Lora Aroyo","Tom Duerig","Jordan Griffith","Gabe Barth-Maron","Jake Ades","Alex Haig","Ankur Taly","Yunting Song","Paul Michel","Dave Orr","Dean Weesner","Corentin Tallec","Carrie Grimes Bostock","Paul Niemczyk","Andy Twigg","Mudit Verma","Rohith Vallu","Henry Wang","Marco Gelmi","Kiranbir Sodhia","Aleksandr Chuklin","Omer Goldman","Jasmine George","Liang Bai","Kelvin Zhang","Petar Sirkovic","Efrat Nehoran","Golan Pundak","Jiaqi Mu","Alice Chen","Alex Greve","Paulo Zacchello","David Amos","Heming Ge","Eric Noland","Colton Bishop","Jeffrey Dudek","Youhei Namiki","Elena Buchatskaya","Jing Li","Dorsa Sadigh","Masha Samsikova","Dan Malkin","Damien Vincent","Robert David","Rob Willoughby","Phoenix Meadowlark","Shawn Gao","Yan Li","Raj Apte","Amit Jhindal","Stein Xudong Lin","Alex Polozov","Zhicheng Wang","Tomas Mery","Anirudh GP","Varun Yerram","Sage Stevens","Tianqi Liu","Noah Fiedel","Charles Sutton","Matthew Johnson","Xiaodan Song","Kate Baumli","Nir Shabat","Muqthar Mohammad","Hao Liu","Marco Selvi","Yichao Zhou","Mehdi Hafezi Manshadi","Chu-ling Ko","Anthony Chen","Michael Bendersky","Jorge Gonzalez Mendez","Nisarg Kothari","Amir Zandieh","Yiling Huang","Daniel Andor","Ellie Pavlick","Idan Brusilovsky","Jitendra Harlalka","Sally Goldman","Andrew Lampinen","Guowang Li","Asahi Ushio","Somit Gupta","Lei Zhang","Chuyuan Kelly Fu","Madhavi Sewak","Timo Denk","Jed Borovik","Brendan Jou","Avital Zipori","Prateek Jain","Junwen Bai","Thang Luong","Jonathan Tompson","Alice Li","Li Liu","George Powell","Jiajun Shen","Alex Feng","Grishma Chole","Da Yu","Yinlam Chow","Tongxin Yin","Eric Malmi","Kefan Xiao","Yash Pande","Shachi Paul","Niccolò Dal Santo","Adil Dostmohamed","Sergio Guadarrama","Aaron Phillips","Thanumalayan Sankaranarayana Pillai","Gal Yona","Amin Ghafouri","Preethi Lahoti","Benjamin Lee","Dhruv Madeka","Eren Sezener","Simon Tokumine","Adrian Collister","Nicola De Cao","Richard Shin","Uday Kalra","Parker Beak","Emily Nottage","Ryo Nakashima","Ivan Jurin","Vikash Sehwag","Meenu Gaba","Junhao Zeng","Kevin R. McKee","Fernando Pereira","Tamar Yakar","Amayika Panda","Arka Dhar","Peilin Zhong","Daniel Sohn","Mark Brand","Lars Lowe Sjoesund","Viral Carpenter","Sharon Lin","Shantanu Thakoor","Marcus Wainwright","Ashwin Chaugule","Pranesh Srinivasan","Muye Zhu","Bernett Orlando","Jack Weber","Ayzaan Wahid","Gilles Baechler","Apurv Suman","Jovana Mitrović","Gabe Taubman","Honglin Yu","Helen King","Josh Dillon","Cathy Yip","Dhriti Varma","Tomas Izo","Levent Bolelli","Borja De Balle Pigem","Julia Di Trapani","Fotis Iliopoulos","Adam Paszke","Nishant Ranka","Joe Zou","Francesco Pongetti","Jed McGiffin","Alex Siegman","Rich Galt","Ross Hemsley","Goran Žužić","Victor Carbune","Tao Li","Myle Ott","Félix de Chaumont Quitry","David Vilar Torres","Yuri Chervonyi","Tomy Tsai","Prem Eruvbetine","Samuel Yang","Matthew Denton","Jake Walker","Slavica Andačić","Idan Heimlich Shtacher","Vittal Premachandran","Harshal Tushar Lehri","Cip Baetu","Damion Yates","Lampros Lamprou","Mariko Iinuma","Ioana Mihailescu","Ben Albrecht","Shachi Dave","Susie Sargsyan","Bryan Perozzi","Lucas Manning","Chiyuan Zhang","Denis Vnukov","Igor Mordatch","Raia Hadsell Wolfgang Macherey","Ryan Kappedal","Jim Stephan","Aditya Tripathi","Klaus Macherey","Jun Qian","Abhishek Bhowmick","Shekoofeh Azizi","Rémi Leblond","Shiva Mohan Reddy Garlapati","Timothy Knight","Matthew Wiethoff","Wei-Chih Hung","Anelia Angelova","Georgios Evangelopoulos","Pawel Janus","Dimitris Paparas","Matthew Rahtz","Ken Caluwaerts","Vivek Sampathkumar","Daniel Jarrett","Shadi Noghabi","Antoine Miech","Chak Yeung","Geoff Clark","Henry Prior","Fei Zheng","Jean Pouget-Abadie","Indro Bhattacharya","Kalpesh Krishna","Will Bishop","Zhe Yuan","Yunxiao Deng","Ashutosh Sathe","Kacper Krasowiak","Ciprian Chelba","Cho-Jui Hsieh","Kiran Vodrahalli","Buhuang Liu","Thomas Köppe","Amr Khalifa","Lubo Litchev","Pichi Charoenpanit","Reed Roberts","Sachin Yadav","Yasumasa Onoe","Desi Ivanov","Megha Mohabey","Vighnesh Birodkar","Nemanja Rakićević","Pierre Sermanet","Vaibhav Mehta","Krishan Subudhi","Travis Choma","Will Ng","Luheng He","Kathie Wang","Tasos Kementsietsidis","Shane Gu","Mansi Gupta","Andrew Nystrom","Mehran Kazemi","Timothy Chung","Nacho Cano","Nikhil Dhawan","Yufei Wang","Jiawei Xia","Trevor Yacovone","Eric Jia","Mingqing Chen","Simeon Ivanov","Ashrith Sheshan","Sid Dalmia","Paweł Stradomski","Pengcheng Yin","Salem Haykal","Congchao Wang","Dennis Duan","Neslihan Bulut","Greg Kochanski","Liam MacDermed","Namrata Godbole","Shitao Weng","Jingjing Chen","Rachana Fellinger","Ramin Mehran","Daniel Suo","Hisham Husain","Tong He","Kaushal Patel","Joshua Howland","Randall Parker","Kelvin Nguyen","Sharath Maddineni","Chris Rawles","Mina Khan","Shlomi Cohen-Ganor","Amol Mandhane","Xinyi Wu","Chenkai Kuang","Iulia Comşa","Ramya Ganeshan","Hanie Sedghi","Adam Bloniarz","Nuo Wang Pierse","Anton Briukhov","Petr Mitrichev","Anita Gergely","Serena Zhan","Allan Zhou","Nikita Saxena","Eva Lu","Josef Dean","Ashish Gupta","Nicolas Perez-Nieves","Renjie Wu","Cory McLean","Wei Liang","Disha Jindal","Anton Tsitsulin","Wenhao Yu","Kaiz Alarakyia","Tom Schaul","Piyush Patil","Peter Sung","Elijah Peake","Hongkun Yu","Feryal Behbahani","JD Co-Reyes","Alan Ansell","Sean Sun","Clara Barbu","Jonathan Lee","Seb Noury","James Allingham","Bilal Piot","Mohit Sharma","Christopher Yew","Ivan Korotkov","Bibo Xu","Demetra Brady","Goran Petrovic","Shibl Mourad","Claire Cui","Aditya Gupta","Parker Schuh","Saarthak Khanna","Anna Goldie","Abhinav Arora","Vadim Zubov","Amy Stuart","Mark Epstein","Yun Zhu","Jianqiao Liu","Yury Stuken","Ziyue Wang","Karolis Misiunas","Dee Guo","Ashleah Gill","Ale Hartman","Zaid Nabulsi","Aurko Roy","Aleksandra Faust","Jason Riesa","Ben Withbroe","Mengchao Wang","Marco Tagliasacchi","Andreea Marzoca","James Noraky","Serge Toropov","Malika Mehrotra","Bahram Raad","Sanja Deur","Steve Xu","Marianne Monteiro","Zhongru Wu","Yi Luan","Sam Ritter","Nick Li","Håvard Garnes","Yanzhang He","Martin Zlocha","Jifan Zhu","Matteo Hessel","Will Wu","Spandana Raj Babbula","Chizu Kawamoto","Yuanzhen Li","Mehadi Hassen","Yan Wang","Brian Wieder","James Freedman","Yin Zhang","Xinyi Bai","Tianli Yu","David Reitter","XiangHai Sheng","Mateo Wirth","Aditya Kini","Dima Damen","Mingcen Gao","Rachel Hornung","Michael Voznesensky","Brian Roark","Adhi Kuncoro","Yuxiang Zhou","Rushin Shah","Anthony Brohan","Kuangyuan Chen","James Wendt","David Rim","Paul Kishan Rubenstein","Jonathan Halcrow","Michelle Liu","Ty Geri","Yunhsuan Sung","Jane Shapiro","Shaan Bijwadia","Chris Duvarney","Christina Sorokin","Paul Natsev","Reeve Ingle","Pramod Gupta","Young Maeng","Ndaba Ndebele","Kexin Zhu","Valentin Anklin","Katherine Lee","Yuan Liu","Yaroslav Akulov","Shaleen Gupta","Guolong Su","Flavien Prost","Tianlin Liu","Vitaly Kovalev","Pol Moreno","Martin Scholz","Sam Redmond","Zongwei Zhou","Alex Castro-Ros","André Susano Pinto","Dia Kharrat","Michal Yarom","Rachel Saputro","Jannis Bulian","Ben Caine","Ji Liu","Abbas Abdolmaleki","Shariq Iqbal","Tautvydas Misiunas","Mikhail Sirotenko","Shefali Garg","Guy Bensky","Huan Gui","Xuezhi Wang","Raphael Koster","Mike Bernico","Da Huang","Romal Thoppilan","Trevor Cohn","Ben Golan","Wenlei Zhou","Andrew Rosenberg","Markus Freitag","Tynan Gangwani","Vincent Tsang","Anand Shukla","Xiaoqi Ren","Minh Giang","Chi Zou","Andre Elisseeff","Charline Le Lan","Dheeru Dua","Shuba Lall","Pranav Shyam","Frankie Garcia","Sarah Nguyen","Michael Guzman","AJ Maschinot","Marcello Maggioni","Ming-Wei Chang","Karol Gregor","Lotte Weerts","Kumaran Venkatesan","Bogdan Damoc","Leon Liu","Jan Wassenberg","Lewis Ho","Becca Roelofs","Majid Hadian","François-Xavier Aubet","Yu Liang","Sami Lachgar","Danny Karmon","Yong Cheng","Amelio Vázquez-Reina","Angie Chen","Zhuyun Dai","Andy Brock","Shubham Agrawal","Chenxi Pang","Peter Garst","Mariella Sanchez-Vargas","Ivor Rendulic","Aditya Ayyar","Andrija Ražnatović","Olivia Ma","Roopali Vij","Neha Sharma","Ashwin Balakrishna","Bingyuan Liu","Ian Mackinnon","Sorin Baltateanu","Petra Poklukar","Gabriel Ibagon","Colin Ji","Hongyang Jiao","Isaac Noble","Wojciech Stokowiec","Zhihao Li","Jeff Dean","David Lindner","Mark Omernick","Kristen Chiafullo","Mason Dimarco","Vitor Rodrigues","Vittorio Selo","Garrett Honke","Xintian Cindy Wu","Wei He","Adam Hillier","Anhad Mohananey","Vihari Piratla","Chang Ye","Chase Malik","Sebastian Riedel","Samuel Albanie","Zi Yang","Kenny Vassigh","Maria Bauza","Sheng Li","Yiqing Tao","Nevan Wichers","Andrii Maksai","Abe Ittycheriah","Ross Mcilroy","Bryan Seybold","Noah Goodman","Romina Datta","Steven M. Hernandez","Tian Shi","Yony Kochinski","Anna Bulanova","Ken Franko","Mikita Sazanovich","Nicholas FitzGerald","Praneeth Kacham","Shubha Srinivas Raghvendra","Vincent Hellendoorn","Alexander Grushetsky","Julian Salazar","Angeliki Lazaridou","Jason Chang","Jan-Thorsten Peter","Sushant Kafle","Yann Dauphin","Abhishek Rao","Filippo Graziano","Izhak Shafran","Yuguo Liao","Tianli Ding","Geng Yan","Grace Chu","Zhao Fu","Vincent Roulet","Gabriel Rasskin","Duncan Williams","Shahar Drath","Alex Mossin","Raphael Hoffmann","Jordi Orbay","Francesco Bertolini","Hila Sheftel","Justin Chiu","Siyang Xue","Yuheng Kuang","Ferjad Naeem","Swaroop Nath","Nana Nti","Phil Culliton","Kashyap Krishnakumar","Michael Isard","Pei Sun","Ayan Chakrabarti","Nathan Clement","Regev Cohen","Arissa Wongpanich","GS Oh","Ashwin Murthy","Hao Zheng","Jessica Hamrick","Oskar Bunyan","Suhas Ganesh","Nitish Gupta","Roy Frostig","John Wieting","Yury Malkov","Pierre Marcenac","Zhixin Lucas Lai","Xiaodan Tang","Mohammad Saleh","Fedir Zubach","Chinmay Kulkarni","Huanjie Zhou","Vicky Zayats","Nan Ding","Anshuman Tripathi","Arijit Pramanik","Patrik Zochbauer","Harish Ganapathy","Vedant Misra","Zach Behrman","Hugo Vallet","Mingyang Zhang","Mukund Sridhar","Ye Jin","Mohammad Babaeizadeh","Siim Põder","Megha Goel","Divya Jain","Tajwar Nasir","Shubham Mittal","Tim Dozat","Diego Ardila","Aliaksei Severyn","Fabio Pardo","Sammy Jerome","Siyang Qin","Louis Rouillard","Amir Yazdanbakhsh","Zizhao Zhang","Shivani Agrawal","Kaushik Shivakumar","Caden Lu","Praveen Kallakuri","Rachita Chhaparia","Kanishka Rao","Charles Kwong","Asya Fadeeva","Shitij Nigam","Yan Virin","Yuan Zhang","Balaji Venkatraman","Beliz Gunel","Marc Wilson","Huiyu Wang","Abhinav Gupta","Xiaowei Xu","Adrien Ali Taïga","Kareem Mohamed","Doug Fritz","Daniel Rodriguez","Zoubin Ghahramani","Harry Askham","Lior Belenki","James Zhao","Rahul Gupta","Krzysztof Jastrzębski","Takahiro Kosakai","Kaan Katircioglu","Jon Schneider","Rina Panigrahy","Konstantinos Bousmalis","Peter Grabowski","Prajit Ramachandran","Chaitra Hegde","Mihaela Rosca","Angelo Scorza Scarpati","Kyriakos Axiotis","Ying Xu","Zach Gleicher","Assaf Hurwitz Michaely","Mandar Sharma","Sanil Jain","Christoph Hirnschall","Tal Marian","Xuhui Jia","Kevin Mather","Kilol Gupta","Linhai Qiu","Nigamaa Nayakanti","Lucian Ionita","Steven Zheng","Lucia Loher","Kurt Shuster","Igor Petrovski","Roshan Sharma","Rahma Chaabouni","Angel Yeh","James An","Arushi Gupta","Steven Schwarcz","Seher Ellis","Sam Conway-Rahman","Javier Snaider","Alex Zhai","James Atwood","Daniel Golovin","Liqian Peng","Te I","Vivian Xia","Salvatore Scellato","Mahan Malihi","Arthur Bražinskas","Vlad-Doru Ion","Younghoon Jun","James Swirhun","Soroosh Mariooryad","Jiao Sun","Steve Chien","Rey Coaguila","Ariel Brand","Yi Gao","Tom Kwiatkowski","Roee Aharoni","Cheng-Chun Lee","Mislav Žanić","Yichi Zhang","Dan Ethier","Vitaly Nikolaev","Pranav Nair","Yoav Ben Shalom","Hen Fitoussi","Jai Gupta","Hongbin Liu","Dee Cattle","Tolga Bolukbasi","Ben Murdoch","Fantine Huot","Yin Li","Chris Hahn"],"pdf_url":"https://arxiv.org/pdf/2507.06261v2.pdf","comment":"72 pages, 17 figures"},{"id":"http://arxiv.org/abs/2507.08477v1","updated":"2025-07-11T10:38:51Z","published":"2025-07-11T10:38:51Z","title":"ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual\n  Speech Recognition","summary":"  The deep integration of large language models and automatic speech\nrecognition systems has become a promising research direction with high\npractical value. To address the overfitting issue commonly observed in Low-Rank\nAdaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work\nproposes an innovative training paradigm Iterative LoRA Training (ILT) in\ncombination with an Iterative Pseudo Labeling strategy, effectively enhancing\nthe theoretical upper bound of model performance. Based on Whisper-large-v3 and\nQwen2-Audio, we conduct systematic experiments using a three-stage training\nprocess: Focus Training, Feed Back Training, and Fix Training. Experimental\nresults demonstrate the effectiveness of the proposed method. Furthermore, the\nMegaAIS research team applied this technique in the Interspeech 2025\nMultilingual Conversational Speech Language Modeling Challenge (MLC-SLM),\nachieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2\n(Speech Separation and Recognition Task), showcasing the practical feasibility\nand strong application potential of our approach.\n","authors":["Qingliang Meng","Hao Wu","Wei Liang","Wei Xu","Qing Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.08477v1.pdf","comment":"Accepted By Interspeech 2025 MLC-SLM workshop as a Research Paper"},{"id":"http://arxiv.org/abs/2507.06892v3","updated":"2025-07-11T10:32:34Z","published":"2025-07-09T14:29:45Z","title":"Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model","summary":"  Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.\n","authors":["Jing Liang","Hongyao Tang","Yi Ma","Jinyi Liu","Yan Zheng","Shuyue Hu","Lei Bai","Jianye Hao"],"pdf_url":"https://arxiv.org/pdf/2507.06892v3.pdf","comment":"Preliminary version, v3, added the missing name of x-axis in the left\n  part of Fig.1 and corrected a wrong number in Fig.3. Project page:\n  https://anitaleungxx.github.io/ReMix"},{"id":"http://arxiv.org/abs/2507.08468v1","updated":"2025-07-11T10:19:56Z","published":"2025-07-11T10:19:56Z","title":"Using Large Language Models for Legal Decision-Making in Austrian\n  Value-Added Tax Law: An Experimental Study","summary":"  This paper provides an experimental evaluation of the capability of large\nlanguage models (LLMs) to assist in legal decision-making within the framework\nof Austrian and European Union value-added tax (VAT) law. In tax consulting\npractice, clients often describe cases in natural language, making LLMs a prime\ncandidate for supporting automated decision-making and reducing the workload of\ntax professionals. Given the requirement for legally grounded and\nwell-justified analyses, the propensity of LLMs to hallucinate presents a\nconsiderable challenge. The experiments focus on two common methods for\nenhancing LLM performance: fine-tuning and retrieval-augmented generation\n(RAG). In this study, these methods are applied on both textbook cases and\nreal-world cases from a tax consulting firm to systematically determine the\nbest configurations of LLM-based systems and assess the legal-reasoning\ncapabilities of LLMs. The findings highlight the potential of using LLMs to\nsupport tax consultants by automating routine tasks and providing initial\nanalyses, although current prototypes are not ready for full automation due to\nthe sensitivity of the legal domain. The findings indicate that LLMs, when\nproperly configured, can effectively support tax professionals in VAT tasks and\nprovide legally grounded justifications for decisions. However, limitations\nremain regarding the handling of implicit client knowledge and context-specific\ndocumentation, underscoring the need for future integration of structured\nbackground information.\n","authors":["Marina Luketina","Andrea Benkel","Christoph G. Schuetz"],"pdf_url":"https://arxiv.org/pdf/2507.08468v1.pdf","comment":"26 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2507.08459v1","updated":"2025-07-11T10:02:21Z","published":"2025-07-11T10:02:21Z","title":"Diagnosing Failures in Large Language Models' Answers: Integrating Error\n  Attribution into Evaluation Framework","summary":"  With the widespread application of Large Language Models (LLMs) in various\ntasks, the mainstream LLM platforms generate massive user-model interactions\ndaily. In order to efficiently analyze the performance of models and diagnose\nfailures in their answers, it is essential to develop an automated framework to\nsystematically categorize and attribute errors. However, existing evaluation\nmodels lack error attribution capability. In this work, we establish a\ncomprehensive Misattribution Framework with 6 primary and 15 secondary\ncategories to facilitate in-depth analysis. Based on this framework, we present\nAttriData, a dataset specifically designed for error attribution, encompassing\nmisattribution, along with the corresponding scores and feedback. We also\npropose MisAttributionLLM, a fine-tuned model on AttriData, which is the first\ngeneral-purpose judge model capable of simultaneously generating score,\nmisattribution, and feedback. Extensive experiments and analyses are conducted\nto confirm the effectiveness and robustness of our proposed method.\n","authors":["Zishan Xu","Shuyi Xie","Qingsong Lv","Shupei Xiao","Linlin Song","Sui Wenjuan","Fan Lin"],"pdf_url":"https://arxiv.org/pdf/2507.08459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08440v1","updated":"2025-07-11T09:31:10Z","published":"2025-07-11T09:31:10Z","title":"Finding Common Ground: Using Large Language Models to Detect Agreement\n  in Multi-Agent Decision Conferences","summary":"  Decision conferences are structured, collaborative meetings that bring\ntogether experts from various fields to address complex issues and reach a\nconsensus on recommendations for future actions or policies. These conferences\noften rely on facilitated discussions to ensure productive dialogue and\ncollective agreement. Recently, Large Language Models (LLMs) have shown\nsignificant promise in simulating real-world scenarios, particularly through\ncollaborative multi-agent systems that mimic group interactions. In this work,\nwe present a novel LLM-based multi-agent system designed to simulate decision\nconferences, specifically focusing on detecting agreement among the participant\nagents. To achieve this, we evaluate six distinct LLMs on two tasks: stance\ndetection, which identifies the position an agent takes on a given issue, and\nstance polarity detection, which identifies the sentiment as positive,\nnegative, or neutral. These models are further assessed within the multi-agent\nsystem to determine their effectiveness in complex simulations. Our results\nindicate that LLMs can reliably detect agreement even in dynamic and nuanced\ndebates. Incorporating an agreement-detection agent within the system can also\nimprove the efficiency of group debates and enhance the overall quality and\ncoherence of deliberations, making them comparable to real-world decision\nconferences regarding outcome and decision-making. These findings demonstrate\nthe potential for LLM-based multi-agent systems to simulate group\ndecision-making processes. They also highlight that such systems could be\ninstrumental in supporting decision-making with expert elicitation workshops\nacross various domains.\n","authors":["Selina Heller","Mohamed Ibrahim","David Antony Selby","Sebastian Vollmer"],"pdf_url":"https://arxiv.org/pdf/2507.08440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08432v1","updated":"2025-07-11T09:18:41Z","published":"2025-07-11T09:18:41Z","title":"xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models","summary":"  Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.\n","authors":["Gustavo Correa Publio","José Emilio Labra Gayo"],"pdf_url":"https://arxiv.org/pdf/2507.08432v1.pdf","comment":"Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25"},{"id":"http://arxiv.org/abs/2111.14003v2","updated":"2025-07-11T09:13:57Z","published":"2021-11-27T23:19:49Z","title":"Answer Generation for Questions With Multiple Information Sources in\n  E-Commerce","summary":"  Automatic question answering is an important yet challenging task in\nE-commerce given the millions of questions posted by users about the product\nthat they are interested in purchasing. Hence, there is a great demand for\nautomatic answer generation systems that provide quick responses using related\ninformation about the product. There are three sources of knowledge available\nfor answering a user posted query, they are reviews, duplicate or similar\nquestions, and specifications. Effectively utilizing these information sources\nwill greatly aid us in answering complex questions. However, there are two main\nchallenges present in exploiting these sources: (i) The presence of irrelevant\ninformation and (ii) the presence of ambiguity of sentiment present in reviews\nand similar questions. Through this work we propose a novel pipeline (MSQAP)\nthat utilizes the rich information present in the aforementioned sources by\nseparately performing relevancy and ambiguity prediction before generating a\nresponse.\n  Experimental results show that our relevancy prediction model (BERT-QA)\noutperforms all other variants and has an improvement of 12.36% in F1 score\ncompared to the BERT-base baseline. Our generation model (T5-QA) outperforms\nthe baselines in all content preservation metrics such as BLEU, ROUGE and has\nan average improvement of 35.02% in ROUGE and 198.75% in BLEU compared to the\nhighest performing baseline (HSSC-q). Human evaluation of our pipeline shows us\nthat our method has an overall improvement in accuracy of 30.7% over the\ngeneration model (T5-QA), resulting in our full pipeline-based approach (MSQAP)\nproviding more accurate answers. To the best of our knowledge, this is the\nfirst work in the e-commerce domain that automatically generates natural\nlanguage answers combining the information present in diverse sources such as\nspecifications, similar questions, and reviews data.\n","authors":["Anand A. Rajasekar","Nikesh Garera"],"pdf_url":"https://arxiv.org/pdf/2111.14003v2.pdf","comment":"7 pages, 10 tables, 1 figure"},{"id":"http://arxiv.org/abs/2507.08427v1","updated":"2025-07-11T09:13:29Z","published":"2025-07-11T09:13:29Z","title":"ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through\n  Logical Rule-Guided Chains","summary":"  Current knowledge editing methods for large language models (LLMs) struggle\nto maintain logical consistency when propagating ripple effects to associated\nfacts. We propose ChainEdit, a framework that synergizes knowledge\ngraph-derived logical rules with LLM logical reasoning capabilities to enable\nsystematic chain updates. By automatically extracting logical patterns from\nstructured knowledge bases and aligning them with LLMs' internal logics,\nChainEdit dynamically generates and edits logically connected knowledge\nclusters. Experiments demonstrate an improvement of more than 30% in logical\ngeneralization over baselines while preserving editing reliability and\nspecificity. We further address evaluation biases in existing benchmarks\nthrough knowledge-aware protocols that disentangle external dependencies. This\nwork establishes new state-of-the-art performance on ripple effect while\nensuring internal logical consistency after knowledge editing.\n","authors":["Zilu Dong","Xiangqing Shen","Zinong Yang","Rui Xia"],"pdf_url":"https://arxiv.org/pdf/2507.08427v1.pdf","comment":"Accepted to ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2507.08425v1","updated":"2025-07-11T09:11:18Z","published":"2025-07-11T09:11:18Z","title":"A Survey of Large Language Models in Discipline-specific Research:\n  Challenges, Methods and Opportunities","summary":"  Large Language Models (LLMs) have demonstrated their transformative potential\nacross numerous disciplinary studies, reshaping the existing research\nmethodologies and fostering interdisciplinary collaboration. However, a\nsystematic understanding of their integration into diverse disciplines remains\nunderexplored. This survey paper provides a comprehensive overview of the\napplication of LLMs in interdisciplinary studies, categorising research efforts\nfrom both a technical perspective and with regard to their applicability. From\na technical standpoint, key methodologies such as supervised fine-tuning,\nretrieval-augmented generation, agent-based approaches, and tool-use\nintegration are examined, which enhance the adaptability and effectiveness of\nLLMs in discipline-specific contexts. From the perspective of their\napplicability, this paper explores how LLMs are contributing to various\ndisciplines including mathematics, physics, chemistry, biology, and the\nhumanities and social sciences, demonstrating their role in discipline-specific\ntasks. The prevailing challenges are critically examined and the promising\nresearch directions are highlighted alongside the recent advances in LLMs. By\nproviding a comprehensive overview of the technical developments and\napplications in this field, this survey aims to serve as an invaluable resource\nfor the researchers who are navigating the complex landscape of LLMs in the\ncontext of interdisciplinary studies.\n","authors":["Lu Xiang","Yang Zhao","Yaping Zhang","Chengqing Zong"],"pdf_url":"https://arxiv.org/pdf/2507.08425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13857v4","updated":"2025-07-11T08:59:48Z","published":"2025-03-18T03:14:23Z","title":"Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations","summary":"  Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate incorporation of preprint articles\nduring the appraisal phase of systematic reviews, supporting researchers in\nmore effective utilization of preprint resources.\n","authors":["Rui Yang","Jiayi Tong","Haoyuan Wang","Hui Huang","Ziyang Hu","Peiyu Li","Nan Liu","Christopher J. Lindsell","Michael J. Pencina","Yong Chen","Chuan Hong"],"pdf_url":"https://arxiv.org/pdf/2503.13857v4.pdf","comment":"30 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.14192v5","updated":"2025-07-11T08:32:31Z","published":"2024-04-22T14:01:09Z","title":"Swap distance minimization beyond entropy minimization in word order\n  variation","summary":"  Consider a linguistic structure formed by $n$ elements, for instance,\nsubject, direct object and verb ($n=3$) or subject, direct object, indirect\nobject and verb ($n=4$). We investigate whether the frequency of the $n!$\npossible orders is constrained by two principles. First, entropy minimization,\na principle that has been suggested to shape natural communication systems at\ndistinct levels of organization. Second, swap distance minimization, namely a\npreference for word orders that require fewer swaps of adjacent elements to be\nproduced from a source order. We present average swap distance, a novel score\nfor research on swap distance minimization. We find strong evidence of pressure\nfor entropy minimization and swap distance minimization with respect to a die\nrolling experiment in distinct linguistic structures with $n=3$ or $n=4$.\nEvidence with respect to a Polya urn process is strong for $n=4$ but weaker for\n$n=3$. We still find evidence consistent with the action of swap distance\nminimization when word order frequencies are shuffled, indicating that swap\ndistance minimization effects are beyond pressure to reduce word order entropy.\n","authors":["Víctor Franco-Sánchez","Arnau Martí-Llobet","Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2404.14192v5.pdf","comment":"Reorganization with technical appendices; minor corrections; in press\n  in the Journal of Quantitative Linguistics"},{"id":"http://arxiv.org/abs/2507.07930v2","updated":"2025-07-11T08:22:41Z","published":"2025-07-10T17:09:21Z","title":"Probing Experts' Perspectives on AI-Assisted Public Speaking Training","summary":"  Background: Public speaking is a vital professional skill, yet it remains a\nsource of significant anxiety for many individuals. Traditional training relies\nheavily on expert coaching, but recent advances in AI has led to novel types of\ncommercial automated public speaking feedback tools. However, most research has\nfocused on prototypes rather than commercial applications, and little is known\nabout how public speaking experts perceive these tools.\n  Objectives: This study aims to evaluate expert opinions on the efficacy and\ndesign of commercial AI-based public speaking training tools and to propose\nguidelines for their improvement.\n  Methods: The research involved 16 semi-structured interviews and 2 focus\ngroups with public speaking experts. Participants discussed their views on\ncurrent commercial tools, their potential integration into traditional\ncoaching, and suggestions for enhancing these systems.\n  Results and Conclusions: Experts acknowledged the value of AI tools in\nhandling repetitive, technical aspects of training, allowing coaches to focus\non higher-level skills. However they found key issues in current tools,\nemphasising the need for personalised, understandable, carefully selected\nfeedback and clear instructional design. Overall, they supported a hybrid model\ncombining traditional coaching with AI-supported exercises.\n","authors":["Nesrine Fourati","Alisa Barkar","Marion Dragée","Liv Danthon-Lefebvre","Mathieu Chollet"],"pdf_url":"https://arxiv.org/pdf/2507.07930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05788v2","updated":"2025-07-11T08:00:51Z","published":"2025-07-08T08:50:47Z","title":"Flippi: End To End GenAI Assistant for E-Commerce","summary":"  The emergence of conversational assistants has fundamentally reshaped user\ninteractions with digital platforms. This paper introduces Flippi-a\ncutting-edge, end-to-end conversational assistant powered by large language\nmodels (LLMs) and tailored for the e-commerce sector. Flippi addresses the\nchallenges posed by the vast and often overwhelming product landscape, enabling\ncustomers to discover products more efficiently through natural language\ndialogue. By accommodating both objective and subjective user requirements,\nFlippi delivers a personalized shopping experience that surpasses traditional\nsearch methods. This paper details how Flippi interprets customer queries to\nprovide precise product information, leveraging advanced NLP techniques such as\nQuery Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),\nNamed Entity Recognition (NER), and Context Reduction. Flippi's unique\ncapability to identify and present the most attractive offers on an e-commerce\nsite is also explored, demonstrating how it empowers users to make\ncost-effective decisions. Additionally, the paper discusses Flippi's\ncomparative analysis features, which help users make informed choices by\ncontrasting product features, prices, and other relevant attributes. The\nsystem's robust architecture is outlined, emphasizing its adaptability for\nintegration across various e-commerce platforms and the technological choices\nunderpinning its performance and accuracy. Finally, a comprehensive evaluation\nframework is presented, covering performance metrics, user satisfaction, and\nthe impact on customer engagement and conversion rates. By bridging the\nconvenience of online shopping with the personalized assistance traditionally\nfound in physical stores, Flippi sets a new standard for customer satisfaction\nand engagement in the digital marketplace.\n","authors":["Anand A. Rajasekar","Praveen Tangarajan","Anjali Nainani","Amogh Batwal","Vinay Rao Dandin","Anusua Trivedi","Ozan Ersoy"],"pdf_url":"https://arxiv.org/pdf/2507.05788v2.pdf","comment":"10 pages, 2 figures, 7 tables"},{"id":"http://arxiv.org/abs/2506.14123v2","updated":"2025-07-11T07:43:41Z","published":"2025-06-17T02:37:04Z","title":"Sampling from Your Language Model One Byte at a Time","summary":"  Tokenization is used almost universally by modern language models, enabling\nefficient text representation using multi-byte or multi-character tokens.\nHowever, prior work has shown that tokenization can introduce distortion into\nthe model's generations, an issue known as the Prompt Boundary Problem (PBP).\nFor example, users are often advised not to end their prompts with a space\nbecause it prevents the model from including the space as part of the next\ntoken. While this heuristic is effective in English, the underlying PBP\ncontinues to affect languages such as Chinese as well as code generation, where\ntokens often do not line up with word and syntactic boundaries. In this work,\nwe present an inference-time method to convert any autoregressive LM with a BPE\ntokenizer into a character-level or byte-level LM. Our method efficiently\nsolves the PBP and is also able to unify the vocabularies of language models\nwith different tokenizers, allowing one to ensemble LMs with different\ntokenizers at inference time or transfer the post-training from one model to\nanother using proxy-tuning. We demonstrate in experiments that the ensemble and\nproxy-tuned models outperform their constituents on downstream evals. Code is\navailable at https://github.com/SewoongLab/byte-sampler .\n","authors":["Jonathan Hayase","Alisa Liu","Noah A. Smith","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2506.14123v2.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.03897v3","updated":"2025-07-11T07:40:01Z","published":"2024-06-06T09:36:14Z","title":"HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew","summary":"  While large language models (LLMs) excel in various natural language tasks in\nEnglish, their performance in lower-resourced languages like Hebrew, especially\nfor generative tasks such as abstractive summarization, remains unclear. The\nhigh morphological richness in Hebrew adds further challenges due to the\nambiguity in sentence comprehension and the complexities in meaning\nconstruction. In this paper, we address this resource and evaluation gap by\nintroducing HeSum, a novel benchmark specifically designed for abstractive text\nsummarization in Modern Hebrew. HeSum consists of 10,000 article-summary pairs\nsourced from Hebrew news websites written by professionals. Linguistic analysis\nconfirms HeSum's high abstractness and unique morphological challenges. We show\nthat HeSum presents distinct difficulties for contemporary state-of-the-art\nLLMs, establishing it as a valuable testbed for generative language technology\nin Hebrew, and MRLs generative challenges in general.\n","authors":["Tzuf Paz-Argaman","Itai Mondshine","Asaf Achi Mordechai","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2406.03897v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18865v3","updated":"2025-07-11T07:37:47Z","published":"2024-04-29T16:52:57Z","title":"Truth-value judgment in language models: 'truth directions' are context\n  sensitive","summary":"  Recent work has demonstrated that the latent spaces of large language models\n(LLMs) contain directions predictive of the truth of sentences. Multiple\nmethods recover such directions and build probes that are described as\nuncovering a model's \"knowledge\" or \"beliefs\". We investigate this phenomenon,\nlooking closely at the impact of context on the probes. Our experiments\nestablish where in the LLM the probe's predictions are (most) sensitive to the\npresence of related sentences, and how to best characterize this kind of\nsensitivity. We do so by measuring different types of consistency errors that\noccur after probing an LLM whose inputs consist of hypotheses preceded by\n(negated) supporting and contradicting sentences. We also perform a causal\nintervention experiment, investigating whether moving the representation of a\npremise along these truth-value directions influences the position of an\nentailed or contradicted sentence along that same direction. We find that the\nprobes we test are generally context sensitive, but that contexts which should\nnot affect the truth often still impact the probe outputs. Our experiments show\nthat the type of errors depend on the layer, the model, and the kind of data.\nFinally, our results suggest that truth-value directions are causal mediators\nin the inference process that incorporates in-context information.\n","authors":["Stefan F. Schouten","Peter Bloem","Ilia Markov","Piek Vossen"],"pdf_url":"https://arxiv.org/pdf/2404.18865v3.pdf","comment":"COLM 2025"},{"id":"http://arxiv.org/abs/2507.08371v1","updated":"2025-07-11T07:34:34Z","published":"2025-07-11T07:34:34Z","title":"The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can\n  Improve Factuality","summary":"  Language models are prone to hallucination - generating text that is\nfactually incorrect. Finetuning models on high-quality factual information can\npotentially reduce hallucination, but concerns remain; obtaining factual gold\ndata can be expensive and training on correct but unfamiliar data may\npotentially lead to even more downstream hallucination. What data should\npractitioners finetune on to mitigate hallucinations in language models? In\nthis work, we study the relationship between the factuality of finetuning data\nand the prevalence of hallucinations in long-form generation tasks.\nCounterintuitively, we find that finetuning on factual gold data is not as\nhelpful as finetuning on model-generated data that models believe to be\nfactual. Next, we evaluate filtering strategies applied on both factual gold\ndata and model-generated data, and find that finetuning on model-generated data\nthat is filtered by models' own internal judgments often leads to better\noverall factuality compared to other configurations: training on gold data\nfiltered by models' judgments, training on gold data alone, or training on\nmodel-generated data that is supported by gold data. These factuality\nimprovements transfer across three domains we study, suggesting that a models'\nown beliefs can provide a powerful signal for factuality.\n","authors":["Benjamin Newman","Abhilasha Ravichander","Jaehun Jung","Rui Xin","Hamish Ivison","Yegor Kuznetsov","Pang Wei Koh","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2507.08371v1.pdf","comment":"29 pages, 4 figures, 16 tables"},{"id":"http://arxiv.org/abs/2507.08350v1","updated":"2025-07-11T06:53:46Z","published":"2025-07-11T06:53:46Z","title":"Exploring Design of Multi-Agent LLM Dialogues for Research Ideation","summary":"  Large language models (LLMs) are increasingly used to support creative tasks\nsuch as research idea generation. While recent work has shown that structured\ndialogues between LLMs can improve the novelty and feasibility of generated\nideas, the optimal design of such interactions remains unclear. In this study,\nwe conduct a comprehensive analysis of multi-agent LLM dialogues for scientific\nideation. We compare different configurations of agent roles, number of agents,\nand dialogue depth to understand how these factors influence the novelty and\nfeasibility of generated ideas. Our experimental setup includes settings where\none agent generates ideas and another critiques them, enabling iterative\nimprovement. Our results show that enlarging the agent cohort, deepening the\ninteraction depth, and broadening agent persona heterogeneity each enrich the\ndiversity of generated ideas. Moreover, specifically increasing critic-side\ndiversity within the ideation-critique-revision loop further boosts the\nfeasibility of the final proposals. Our findings offer practical guidelines for\nbuilding effective multi-agent LLM systems for scientific ideation. Our code is\navailable at https://github.com/g6000/MultiAgent-Research-Ideator.\n","authors":["Keisuke Ueda","Wataru Hirota","Takuto Asakura","Takahiro Omi","Kosuke Takahashi","Kosuke Arima","Tatsuya Ishigaki"],"pdf_url":"https://arxiv.org/pdf/2507.08350v1.pdf","comment":"16 pages, 1 figure, appendix. Accepted to SIGDIAL 2025"},{"id":"http://arxiv.org/abs/2507.08342v1","updated":"2025-07-11T06:44:52Z","published":"2025-07-11T06:44:52Z","title":"Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for\n  Multilingual Abstractive Summarization","summary":"  Automatic n-gram based metrics such as ROUGE are widely used for evaluating\ngenerative tasks such as summarization. While these metrics are considered\nindicative (even if imperfect) of human evaluation for English, their\nsuitability for other languages remains unclear. To address this, we\nsystematically assess evaluation metrics for generation both n-gram-based and\nneural based to evaluate their effectiveness across languages and tasks.\nSpecifically, we design a large-scale evaluation suite across eight languages\nfrom four typological families: agglutinative, isolating, low-fusional, and\nhigh-fusional, spanning both low- and high-resource settings, to analyze their\ncorrelation with human judgments. Our findings highlight the sensitivity of\nevaluation metrics to the language type. For example, in fusional languages,\nn-gram-based metrics show lower correlation with human assessments compared to\nisolating and agglutinative languages. We also demonstrate that proper\ntokenization can significantly mitigate this issue for morphologically rich\nfusional languages, sometimes even reversing negative trends. Additionally, we\nshow that neural-based metrics specifically trained for evaluation, such as\nCOMET, consistently outperform other neural metrics and better correlate with\nhuman judgments in low-resource languages. Overall, our analysis highlights the\nlimitations of n-gram metrics for fusional languages and advocates for greater\ninvestment in neural-based metrics trained for evaluation tasks.\n","authors":["Itai Mondshine","Tzuf Paz-Argaman","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2507.08342v1.pdf","comment":"ACL 2025 Main"},{"id":"http://arxiv.org/abs/2507.08339v1","updated":"2025-07-11T06:37:44Z","published":"2025-07-11T06:37:44Z","title":"What Factors Affect LLMs and RLLMs in Financial Question Answering?","summary":"  Recently, the development of large language models (LLMs) and reasoning large\nlanguage models (RLLMs) have gained considerable attention from many\nresearchers. RLLMs enhance the reasoning capabilities of LLMs through Long\nChain-of-Thought (Long CoT) processes, significantly improving the performance\nof LLMs in addressing complex problems. However, there are few works that\nsystematically explore what methods can fully unlock the performance of LLMs\nand RLLMs within the financial domain. To investigate the impact of various\nmethods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the\neffects of prompting methods, agentic frameworks, and multilingual alignment\nmethods on financial question-answering tasks. Our research findings indicate:\n(1) Current prompting methods and agent frameworks enhance the performance of\nLLMs in financial question answering by simulating Long CoT; (2) RLLMs possess\ninherent Long CoT capabilities, which limits the effectiveness of conventional\nmethods in further enhancing their performance; (3) Current advanced\nmultilingual alignment methods primarily improve the multilingual performance\nof LLMs by extending the reasoning length, which yields minimal benefits for\nRLLMs. We hope that this study can serve as an important reference for LLMs and\nRLLMs in the field of financial question answering.\n","authors":["Peng Wang","Xuesi Hu","Jiageng Wu","Yuntao Zou","Qiancheng Zhang","Dagang Li"],"pdf_url":"https://arxiv.org/pdf/2507.08339v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2507.08336v1","updated":"2025-07-11T06:28:35Z","published":"2025-07-11T06:28:35Z","title":"Distillation versus Contrastive Learning: How to Train Your Rerankers","summary":"  Training text rerankers is crucial for information retrieval. Two primary\nstrategies are widely used: contrastive learning (optimizing directly on\nground-truth labels) and knowledge distillation (transferring knowledge from a\nlarger reranker). While both have been studied in the literature, a clear\ncomparison of their effectiveness for training cross-encoder rerankers under\npractical conditions is needed.\n  This paper empirically compares these strategies by training rerankers of\ndifferent sizes and architectures using both methods on the same data, with a\nstrong contrastive learning model acting as the distillation teacher. Our\nresults show that knowledge distillation generally yields better in-domain and\nout-of-domain ranking performance than contrastive learning when distilling\nfrom a larger teacher model. This finding is consistent across student model\nsizes and architectures. However, distilling from a teacher of the same\ncapacity does not provide the same advantage, particularly for out-of-domain\ntasks. These findings offer practical guidance for choosing a training strategy\nbased on available teacher models. Therefore, we recommend using knowledge\ndistillation to train smaller rerankers if a larger, more powerful teacher is\naccessible; in its absence, contrastive learning provides a strong and more\nreliable alternative otherwise.\n","authors":["Zhichao Xu","Zhiqi Huang","Shengyao Zhuang","Ashim Gupta","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2507.08336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08335v1","updated":"2025-07-11T06:27:42Z","published":"2025-07-11T06:27:42Z","title":"MK2 at PBIG Competition: A Prompt Generation Solution","summary":"  The Patent-Based Idea Generation task asks systems to turn real patents into\nproduct ideas viable within three years. We propose MK2, a prompt-centric\npipeline: Gemini 2.5 drafts and iteratively edits a prompt, grafting useful\nfragments from weaker outputs; GPT-4.1 then uses this prompt to create one idea\nper patent, and an Elo loop judged by Qwen3-8B selects the best prompt-all\nwithout extra training data. Across three domains, two evaluator types, and six\ncriteria, MK2 topped the automatic leaderboard and won 25 of 36 tests. Only the\nmaterials-chemistry track lagged, indicating the need for deeper domain\ngrounding; yet, the results show that lightweight prompt engineering has\nalready delivered competitive, commercially relevant ideation from patents.\n","authors":["Yuzheng Xu","Tosho Hirasawa","Seiya Kawano","Shota Kato","Tadashi Kozuno"],"pdf_url":"https://arxiv.org/pdf/2507.08335v1.pdf","comment":"9 pages, to appear in the 2nd Workshop on Agent AI for Scenario\n  Planning (AGENTSCEN 2025)"},{"id":"http://arxiv.org/abs/2411.01077v4","updated":"2025-07-11T05:39:08Z","published":"2024-11-01T23:18:32Z","title":"Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection","summary":"  Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted output, posing a potential threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This alters the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the unsafe prediction rate, bypassing existing\nsafeguards.\n","authors":["Zhipeng Wei","Yuqi Liu","N. Benjamin Erichson"],"pdf_url":"https://arxiv.org/pdf/2411.01077v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08325v1","updated":"2025-07-11T05:31:35Z","published":"2025-07-11T05:31:35Z","title":"CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template\n  Generation","summary":"  In e-commerce private-domain channels such as instant messaging and e-mail,\nmerchants engage customers directly as part of their Customer Relationship\nManagement (CRM) programmes to drive retention and conversion. While a few top\nperformers excel at crafting outbound messages, most merchants struggle to\nwrite persuasive copy because they lack both expertise and scalable tools. We\nintroduce CRMAgent, a multi-agent system built on large language models (LLMs)\nthat generates high-quality message templates and actionable writing guidance\nthrough three complementary modes. First, group-based learning enables the\nagent to learn from a merchant's own top-performing messages within the same\naudience segment and rewrite low-performing ones. Second,\nretrieval-and-adaptation fetches templates that share the same audience segment\nand exhibit high similarity in voucher type and product category, learns their\nsuccessful patterns, and adapts them to the current campaign. Third, a\nrule-based fallback provides a lightweight zero-shot rewrite when no suitable\nreferences are available. Extensive experiments show that CRMAgent consistently\noutperforms merchants' original templates, delivering significant gains in both\naudience-match and marketing-effectiveness metrics.\n","authors":["Yinzhu Quan","Xinrui Li","Ying Chen"],"pdf_url":"https://arxiv.org/pdf/2507.08325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08893v2","updated":"2025-07-11T05:27:37Z","published":"2025-03-11T21:12:48Z","title":"EvalTree: Profiling Language Model Weaknesses via Hierarchical\n  Capability Trees","summary":"  An ideal model evaluation should achieve two goals: identifying where the\nmodel fails and providing actionable improvement guidance. Toward these goals\nfor language model (LM) evaluations, we formulate the problem of generating a\nweakness profile, a set of weaknesses expressed in natural language, given an\nLM's performance on every individual instance in a benchmark. We introduce a\nsuite of quantitative assessments to compare different weakness profiling\nmethods. We also introduce a weakness profiling method EvalTree. EvalTree\nconstructs a capability tree where each node represents a capability described\nin natural language and is linked to a subset of benchmark instances that\nspecifically evaluate this capability; it then extracts nodes where the LM\nperforms poorly to generate a weakness profile. On the MATH and WildChat\nbenchmarks, we show that EvalTree outperforms baseline weakness profiling\nmethods by identifying weaknesses more precisely and comprehensively. Weakness\nprofiling further enables weakness-guided data collection, and training data\ncollection guided by EvalTree-identified weaknesses improves LM performance\nmore than other data collection strategies. We also show how EvalTree exposes\nflaws in Chatbot Arena's human-voter-based evaluation practice. To facilitate\nfuture work, we provide an interface that allows practitioners to interactively\nexplore the capability trees built by EvalTree.\n","authors":["Zhiyuan Zeng","Yizhong Wang","Hannaneh Hajishirzi","Pang Wei Koh"],"pdf_url":"https://arxiv.org/pdf/2503.08893v2.pdf","comment":"COLM 2025"},{"id":"http://arxiv.org/abs/2507.08309v1","updated":"2025-07-11T05:02:06Z","published":"2025-07-11T05:02:06Z","title":"Improving MLLM's Document Image Machine Translation via Synchronously\n  Self-reviewing Its OCR Proficiency","summary":"  Multimodal Large Language Models (MLLMs) have shown strong performance in\ndocument image tasks, especially Optical Character Recognition (OCR). However,\nthey struggle with Document Image Machine Translation (DIMT), which requires\nhandling both cross-modal and cross-lingual challenges. Previous efforts to\nenhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT\ndataset often result in the forgetting of the model's existing monolingual\nabilities, such as OCR. To address these challenges, we introduce a novel\nfine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR\nproficiency, inspired by the concept \"Bilingual Cognitive Advantage\".\nSpecifically, SSR prompts the model to generate OCR text before producing\ntranslation text, which allows the model to leverage its strong monolingual OCR\nability while learning to translate text across languages. Comprehensive\nexperiments demonstrate the proposed SSR learning helps mitigate catastrophic\nforgetting, improving the generalization ability of MLLMs on both OCR and DIMT\ntasks.\n","authors":["Yupu Liang","Yaping Zhang","Zhiyang Zhang","Zhiyuan Chen","Yang Zhao","Lu Xiang","Chengqing Zong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.08309v1.pdf","comment":"Accepted by ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2507.08306v1","updated":"2025-07-11T04:44:07Z","published":"2025-07-11T04:44:07Z","title":"M2-Reasoning: Empowering MLLMs with Unified General and Spatial\n  Reasoning","summary":"  Recent advancements in Multimodal Large Language Models (MLLMs), particularly\nthrough Reinforcement Learning with Verifiable Rewards (RLVR), have\nsignificantly enhanced their reasoning abilities. However, a critical gap\npersists: these models struggle with dynamic spatial interactions, a capability\nessential for real-world applications. To bridge this gap, we introduce\nM2-Reasoning-7B, a model designed to excel in both general and spatial\nreasoning. Our approach integrates two key innovations: (1) a novel data\npipeline that generates 294.2K high-quality data samples (168K for cold-start\nfine-tuning and 126.2K for RLVR), which feature logically coherent reasoning\ntrajectories and have undergone comprehensive assessment; and (2) a dynamic\nmulti-task training strategy with step-wise optimization to mitigate conflicts\nbetween data, and task-specific rewards for delivering tailored incentive\nsignals. This combination of curated data and advanced training allows\nM2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,\nshowcasing superior performance in both general and spatial reasoning domains.\n","authors":["Inclusion AI"," :","Fudong Wang","Jiajia Liu","Jingdong Chen","Jun Zhou","Kaixiang Ji","Lixiang Ru","Qingpei Guo","Ruobing Zheng","Tianqi Li","Yi Yuan","Yifan Mao","Yuting Xiao","Ziping Ma"],"pdf_url":"https://arxiv.org/pdf/2507.08306v1.pdf","comment":"31pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.14023v5","updated":"2025-07-11T04:26:11Z","published":"2024-06-20T06:42:08Z","title":"Evaluating Implicit Bias in Large Language Models by Attacking From a\n  Psychometric Perspective","summary":"  As large language models (LLMs) become an important way of information\naccess, there have been increasing concerns that LLMs may intensify the spread\nof unethical content, including implicit bias that hurts certain populations\nwithout explicit harmful words. In this paper, we conduct a rigorous evaluation\nof LLMs' implicit bias towards certain demographics by attacking them from a\npsychometric perspective to elicit agreements to biased viewpoints. Inspired by\npsychometric principles in cognitive and social psychology, we propose three\nattack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the\ncorresponding attack instructions, we built two benchmarks: (1) a bilingual\ndataset with biased statements covering four bias types (2.7K instances) for\nextensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning\nnine common bias types (12.7K instances) for comprehensive evaluation.\nExtensive evaluation of popular commercial and open-source LLMs shows that our\nmethods can elicit LLMs' inner bias more effectively than competitive\nbaselines. Our attack methodology and benchmarks offer an effective means of\nassessing the ethical risks of LLMs, driving progress toward greater\naccountability in their development. Our code, data, and benchmarks are\navailable at https://yuchenwen1.github.io/ImplicitBiasEvaluation/.\n","authors":["Yuchen Wen","Keping Bi","Wei Chen","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.14023v5.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2507.08297v1","updated":"2025-07-11T04:07:10Z","published":"2025-07-11T04:07:10Z","title":"KAT-V1: Kwai-AutoThink Technical Report","summary":"  We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model\ndeveloped to address the overthinking problem in reasoning-intensive tasks,\nwhere an automatic thinking training paradigm is proposed to dynamically switch\nbetween reasoning and non-reasoning modes based on task complexity.\nSpecifically, first, we construct the dual-regime dataset based on a novel\ntagging pipeline and a multi-agent synthesis strategy, and then we apply\nMulti-Token Prediction (MTP)-enhanced knowledge distillation, enabling\nefficient and fine-grained reasoning transfer with minimal pretraining cost.\nBesides, we implement a cold-start initialization strategy that introduces\nmode-selection priors using majority-vote signals and intent-aware prompting.\nFinally, we propose Step-SRPO, a reinforcement learning algorithm that\nincorporates intermediate supervision into the GRPO framework, offering\nstructured guidance over both reasoning-mode selection and response accuracy.\nExtensive experiments across multiple benchmarks demonstrate that KAT\nconsistently matches or even outperforms current state-of-the-art models,\nincluding DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of\nreasoning-intensive tasks while reducing token usage by up to approximately\n30\\%. Beyond academic evaluation, KAT has been successfully deployed in\nKwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world\ndevelopment workflows with high accuracy, efficiency, and controllable\nreasoning behaviors. Moreover, we are actively training a 200B\nMixture-of-Experts (MoE) with 40B activation parameters, where the early-stage\nresults already demonstrate promising improvements in performance and\nefficiency, further showing the scalability of the AutoThink paradigm.\n","authors":["Zizheng Zhan","Ken Deng","Huaixi Tang","Wen Xiang","Kun Wu","Weihao Li","Wenqiang Zhu","Jingxuan Xu","Lecheng Huang","Zongxian Feng","Shaojie Wang","Shangpeng Yan","Jiaheng Liu","Zhongyuan Peng","Zuchen Gao","Haoyang Huang","Ziqi Zhan","Yanan Wu","Yuanxing Zhang","Jian Yang","Guang Chen","Haotian Zhang","Bin Chen","Bing Yu"],"pdf_url":"https://arxiv.org/pdf/2507.08297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01163v2","updated":"2025-07-11T03:56:05Z","published":"2025-03-03T04:24:04Z","title":"Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers","summary":"  Prompt optimization aims to search for effective prompts that enhance the\nperformance of large language models (LLMs). Although existing prompt\noptimization methods have discovered effective prompts, they often differ from\nsophisticated prompts carefully designed by human experts. Prompt design\nstrategies, representing best practices for improving prompt performance, can\nbe key to improving prompt optimization. Recently, a method termed the\nAutonomous Prompt Engineering Toolbox (APET) has incorporated various prompt\ndesign strategies into the prompt optimization process. In APET, the LLM is\nneeded to implicitly select and apply the appropriate strategies because prompt\ndesign strategies can have negative effects. This implicit selection may be\nsuboptimal due to the limited optimization capabilities of LLMs. This paper\nintroduces Optimizing Prompts with sTrategy Selection (OPTS), which implements\nexplicit selection mechanisms for prompt design. We propose three mechanisms,\nincluding a Thompson sampling-based approach, and integrate them into\nEvoPrompt, a well-known prompt optimizer. Experiments optimizing prompts for\ntwo LLMs, Llama-3-8B-Instruct and GPT-4o mini, were conducted using BIG-Bench\nHard. Our results show that the selection of prompt design strategies improves\nthe performance of EvoPrompt, and the Thompson sampling-based mechanism\nachieves the best overall results. Our experimental code is provided at\nhttps://github.com/shiralab/OPTS .\n","authors":["Rin Ashizawa","Yoichi Hirose","Nozomu Yoshinari","Kento Uchida","Shinichi Shirakawa"],"pdf_url":"https://arxiv.org/pdf/2503.01163v2.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2507.08284v1","updated":"2025-07-11T03:17:58Z","published":"2025-07-11T03:17:58Z","title":"Lightweight Safety Guardrails via Synthetic Data and RL-guided\n  Adversarial Training","summary":"  We introduce a lightweight yet highly effective safety guardrail framework\nfor language models, demonstrating that small-scale language models can\nachieve, and even surpass, the performance of larger counterparts in content\nmoderation tasks. This is accomplished through high-fidelity synthetic data\ngeneration and adversarial training. The synthetic data generation process\nbegins with human-curated seed data, which undergoes query augmentation and\nparaphrasing to create diverse and contextually rich examples. This augmented\ndata is then subjected to multiple rounds of curation, ensuring high fidelity\nand relevance. Inspired by recent advances in the Generative Adversarial\nNetwork (GAN) architecture, our adversarial training employs reinforcement\nlearning to guide a generator that produces challenging synthetic examples.\nThese examples are used to fine-tune the safety classifier, enhancing its\nability to detect and mitigate harmful content. Additionally, we incorporate\nstrategies from recent research on efficient LLM training, leveraging the\ncapabilities of smaller models to improve the performance of larger generative\nmodels. With iterative adversarial training and the generation of diverse,\nhigh-quality synthetic data, our framework enables small language models (SLMs)\nto serve as robust safety guardrails. This approach not only reduces\ncomputational overhead but also enhances resilience against adversarial\nattacks, offering a scalable and efficient solution for content moderation in\nAI systems.\n","authors":["Aleksei Ilin","Gor Matevosyan","Xueying Ma","Vladimir Eremin","Suhaa Dada","Muqun Li","Riyaaz Shaik","Haluk Noyan Tokgozoglu"],"pdf_url":"https://arxiv.org/pdf/2507.08284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01403v2","updated":"2025-07-11T02:49:25Z","published":"2025-04-02T06:40:09Z","title":"Generative Retrieval and Alignment Model: A New Paradigm for E-commerce\n  Retrieval","summary":"  Traditional sparse and dense retrieval methods struggle to leverage general\nworld knowledge and often fail to capture the nuanced features of queries and\nproducts. With the advent of large language models (LLMs), industrial search\nsystems have started to employ LLMs to generate identifiers for product\nretrieval. Commonly used identifiers include (1) static/semantic IDs and (2)\nproduct term sets. The first approach requires creating a product ID system\nfrom scratch, missing out on the world knowledge embedded within LLMs. While\nthe second approach leverages this general knowledge, the significant\ndifference in word distribution between queries and products means that\nproduct-based identifiers often do not align well with user search queries,\nleading to missed product recalls. Furthermore, when queries contain numerous\nattributes, these algorithms generate a large number of identifiers, making it\ndifficult to assess their quality, which results in low overall recall\nefficiency.\n  To address these challenges, this paper introduces a novel e-commerce\nretrieval paradigm: the Generative Retrieval and Alignment Model (GRAM). GRAM\nemploys joint training on text information from both queries and products to\ngenerate shared text identifier codes, effectively bridging the gap between\nqueries and products. This approach not only enhances the connection between\nqueries and products but also improves inference efficiency. The model uses a\nco-alignment strategy to generate codes optimized for maximizing retrieval\nefficiency. Additionally, it introduces a query-product scoring mechanism to\ncompare product values across different codes, further boosting retrieval\nefficiency. Extensive offline and online A/B testing demonstrates that GRAM\nsignificantly outperforms traditional models and the latest generative\nretrieval models, confirming its effectiveness and practicality.\n","authors":["Ming Pang","Chunyuan Yuan","Xiaoyu He","Zheng Fang","Donghao Xie","Fanyi Qu","Xue Jiang","Changping Peng","Zhangang Lin","Ching Law","Jingping Shao"],"pdf_url":"https://arxiv.org/pdf/2504.01403v2.pdf","comment":"Accepted by WWW2025"},{"id":"http://arxiv.org/abs/2507.07505v2","updated":"2025-07-11T02:03:49Z","published":"2025-07-10T07:50:52Z","title":"Hallucination Stations: On Some Basic Limitations of Transformer-Based\n  Language Models","summary":"  With widespread adoption of transformer-based language models in AI, there is\nsignificant interest in the limits of LLMs capabilities, specifically so-called\nhallucinations, occurrences in which LLMs provide spurious, factually incorrect\nor nonsensical information when prompted on certain subjects. Furthermore,\nthere is growing interest in agentic uses of LLMs - that is, using LLMs to\ncreate agents that act autonomously or semi-autonomously to carry out various\ntasks, including tasks with applications in the real world. This makes it\nimportant to understand the types of tasks LLMs can and cannot perform. We\nexplore this topic from the perspective of the computational complexity of LLM\ninference. We show that LLMs are incapable of carrying out computational and\nagentic tasks beyond a certain complexity, and further that LLMs are incapable\nof verifying the accuracy of tasks beyond a certain complexity. We present\nexamples of both, then discuss some consequences of this work.\n","authors":["Varin Sikka","Vishal Sikka"],"pdf_url":"https://arxiv.org/pdf/2507.07505v2.pdf","comment":"6 pages; to be submitted to AAAI-26 after reviews"},{"id":"http://arxiv.org/abs/2405.19715v3","updated":"2025-07-11T01:33:18Z","published":"2024-05-30T05:49:38Z","title":"SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths","summary":"  Speculative decoding reduces the inference latency of a target large language\nmodel via utilizing a smaller and faster draft model. Its performance depends\non a hyperparameter K -- the candidate length, i.e., the number of candidate\ntokens for the target model to verify in each round. However, previous methods\noften use simple heuristics to choose K, which may result in sub-optimal\nperformance. We study the choice of the candidate length K and formulate it as\na Markov Decision Process. We theoretically show that the optimal policy of\nthis Markov decision process takes the form of a threshold policy, i.e., the\ncurrent speculation should stop and be verified when the probability of getting\na rejection exceeds a threshold value. Motivated by this theory, we propose\nSpecDec++, an enhanced version of speculative decoding that adaptively\ndetermines the candidate length on the fly. We augment the draft model with a\ntrained acceptance prediction head to predict the conditional acceptance\nprobability of the candidate tokens. SpecDec++ will stop the current\nspeculation when the predicted probability that at least one token gets\nrejected exceeds a threshold. We implement SpecDec++ and apply it to the\nllama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup\non the Alpaca dataset (7.2% improvement over the baseline speculative\ndecoding). On the GSM8K and HumanEval datasets, our method achieves a 2.26x\nspeedup (9.4% improvement) and 2.23x speedup (11.1% improvement), respectively.\nThe code of this paper is available at\nhttps://github.com/Kaffaljidhmah2/SpecDec_pp.\n","authors":["Kaixuan Huang","Xudong Guo","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2405.19715v3.pdf","comment":"Accepted to COLM 2025"},{"id":"http://arxiv.org/abs/2507.08241v1","updated":"2025-07-11T01:11:06Z","published":"2025-07-11T01:11:06Z","title":"Exploring Gender Differences in Chronic Pain Discussions on Reddit","summary":"  Pain is an inherent part of human existence, manifesting as both physical and\nemotional experiences, and can be categorized as either acute or chronic. Over\nthe years, extensive research has been conducted to understand the causes of\npain and explore potential treatments, with contributions from various\nscientific disciplines. However, earlier studies often overlooked the role of\ngender in pain experiences. In this study, we utilized Natural Language\nProcessing (NLP) to analyze and gain deeper insights into individuals' pain\nexperiences, with a particular focus on gender differences. We successfully\nclassified posts into male and female corpora using the Hidden Attribute\nModel-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by\naggregating posts based on usernames. Our analysis revealed linguistic\ndifferences between genders, with female posts tending to be more emotionally\nfocused. Additionally, the study highlighted that conditions such as migraine\nand sinusitis are more prevalent among females and explored how pain medication\naffects individuals differently based on gender.\n","authors":["Ancita Maria Andrade","Tanvi Banerjee","Ramakrishna Mundugar"],"pdf_url":"https://arxiv.org/pdf/2507.08241v1.pdf","comment":"This is an extended version of the short paper accepted at ASONAM\n  2025"},{"id":"http://arxiv.org/abs/2402.08830v2","updated":"2025-07-11T00:42:38Z","published":"2024-02-13T22:22:51Z","title":"Sequence graphs realizations and ambiguity in language models","summary":"  Several popular language models represent local contexts in an input text $x$\nas bags of words. Such representations are naturally encoded by a sequence\ngraph whose vertices are the distinct words occurring in $x$, with edges\nrepresenting the (ordered) co-occurrence of two words within a sliding window\nof size $w$. However, this compressed representation is not generally\nbijective: some may be ambiguous, admitting several realizations as a sequence,\nwhile others may not admit any realization. In this paper, we study the\nrealizability and ambiguity of sequence graphs from a combinatorial and\nalgorithmic point of view. We consider the existence and enumeration of\nrealizations of a sequence graph under multiple settings: window size $w$,\npresence/absence of graph orientation, and presence/absence of weights\n(multiplicities). When $w=2$, we provide polynomial time algorithms for\nrealizability and enumeration in all cases except the undirected/weighted\nsetting, where we show the $\\#$P-hardness of enumeration. For $w \\ge 3$, we\nprove the hardness of all variants, even when $w$ is considered as a constant,\nwith the notable exception of the undirected unweighted case for which we\npropose XP algorithms for both problems, tight due to a corresponding\n$W[1]-$hardness result. We conclude with an integer program formulation to\nsolve the realizability problem, and a dynamic programming algorithm to solve\nthe enumeration problem in instances of moderate sizes. This work leaves open\nthe membership to NP of both problems, a non-trivial question due to the\nexistence of minimum realizations having size exponential on the instance\nencoding.\n","authors":["Sammy Khalife","Yann Ponty","Laurent Bulteau"],"pdf_url":"https://arxiv.org/pdf/2402.08830v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08232v1","updated":"2025-07-11T00:36:57Z","published":"2025-07-11T00:36:57Z","title":"Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?","summary":"  Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.\n","authors":["KV Aditya Srivatsa","Kaushal Kumar Maurya","Ekaterina Kochmar"],"pdf_url":"https://arxiv.org/pdf/2507.08232v1.pdf","comment":"Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA), co-located with ACL 2025"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2507.08801v1","updated":"2025-07-11T17:59:42Z","published":"2025-07-11T17:59:42Z","title":"Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective","summary":"  Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.\n","authors":["Hangjie Yuan","Weihua Chen","Jun Cen","Hu Yu","Jingyun Liang","Shuning Chang","Zhihui Lin","Tao Feng","Pengwei Liu","Jiazheng Xing","Hao Luo","Jiasheng Tang","Fan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2507.08801v1.pdf","comment":"Code and Models: https://github.com/alibaba-damo-academy/Lumos"},{"id":"http://arxiv.org/abs/2507.08800v1","updated":"2025-07-11T17:59:40Z","published":"2025-07-11T17:59:40Z","title":"NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models","summary":"  We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.\n","authors":["Luke Rivard","Sun Sun","Hongyu Guo","Wenhu Chen","Yuntian Deng"],"pdf_url":"https://arxiv.org/pdf/2507.08800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17414v2","updated":"2025-07-11T17:58:23Z","published":"2025-02-24T18:47:54Z","title":"X-Dancer: Expressive Music to Human Dance Video Generation","summary":"  We present X-Dancer, a novel zero-shot music-driven image animation pipeline\nthat creates diverse and long-range lifelike human dance videos from a single\nstatic image. As its core, we introduce a unified transformer-diffusion\nframework, featuring an autoregressive transformer model that synthesize\nextended and music-synchronized token sequences for 2D body, head and hands\nposes, which then guide a diffusion model to produce coherent and realistic\ndance video frames. Unlike traditional methods that primarily generate human\nmotion in 3D, X-Dancer addresses data limitations and enhances scalability by\nmodeling a wide spectrum of 2D dance motions, capturing their nuanced alignment\nwith musical beats through readily available monocular videos. To achieve this,\nwe first build a spatially compositional token representation from 2D human\npose labels associated with keypoint confidences, encoding both large\narticulated body movements (e.g., upper and lower body) and fine-grained\nmotions (e.g., head and hands). We then design a music-to-motion transformer\nmodel that autoregressively generates music-aligned dance pose token sequences,\nincorporating global attention to both musical style and prior motion context.\nFinally we leverage a diffusion backbone to animate the reference image with\nthese synthesized pose tokens through AdaIN, forming a fully differentiable\nend-to-end framework. Experimental results demonstrate that X-Dancer is able to\nproduce both diverse and characterized dance videos, substantially\noutperforming state-of-the-art methods in term of diversity, expressiveness and\nrealism. Code and model will be available for research purposes.\n","authors":["Zeyuan Chen","Hongyi Xu","Guoxian Song","You Xie","Chenxu Zhang","Xin Chen","Chao Wang","Di Chang","Linjie Luo"],"pdf_url":"https://arxiv.org/pdf/2502.17414v2.pdf","comment":"ICCV 2025. Project Page: https://zeyuan-chen.com/X-Dancer/"},{"id":"http://arxiv.org/abs/2507.08776v1","updated":"2025-07-11T17:38:52Z","published":"2025-07-11T17:38:52Z","title":"CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive\n  Neural Rendering","summary":"  This paper proposes a neural rendering approach that represents a scene as\n\"compressed light-field tokens (CLiFTs)\", retaining rich appearance and\ngeometric information of a scene. CLiFT enables compute-efficient rendering by\ncompressed tokens, while being capable of changing the number of tokens to\nrepresent a scene or render a novel view with one trained network. Concretely,\ngiven a set of images, multi-view encoder tokenizes the images with the camera\nposes. Latent-space K-means selects a reduced set of rays as cluster centroids\nusing the tokens. The multi-view ``condenser'' compresses the information of\nall the tokens into the centroid tokens to construct CLiFTs. At test time,\ngiven a target view and a compute budget (i.e., the number of CLiFTs), the\nsystem collects the specified number of nearby tokens and synthesizes a novel\nview using a compute-adaptive renderer. Extensive experiments on RealEstate10K\nand DL3DV datasets quantitatively and qualitatively validate our approach,\nachieving significant data reduction with comparable rendering quality and the\nhighest overall rendering score, while providing trade-offs of data size,\nrendering quality, and rendering speed.\n","authors":["Zhengqing Wang","Yuefan Wu","Jiacheng Chen","Fuyang Zhang","Yasutaka Furukawa"],"pdf_url":"https://arxiv.org/pdf/2507.08776v1.pdf","comment":"Project page: https://c-lift.github.io"},{"id":"http://arxiv.org/abs/2507.08772v1","updated":"2025-07-11T17:33:18Z","published":"2025-07-11T17:33:18Z","title":"From One to More: Contextual Part Latents for 3D Generation","summary":"  Recent advances in 3D generation have transitioned from multi-view 2D\nrendering approaches to 3D-native latent diffusion frameworks that exploit\ngeometric priors in ground truth data. Despite progress, three key limitations\npersist: (1) Single-latent representations fail to capture complex multi-part\ngeometries, causing detail degradation; (2) Holistic latent coding neglects\npart independence and interrelationships critical for compositional design; (3)\nGlobal conditioning mechanisms lack fine-grained controllability. Inspired by\nhuman 3D design workflows, we propose CoPart - a part-aware diffusion framework\nthat decomposes 3D objects into contextual part latents for coherent multi-part\ngeneration. This paradigm offers three advantages: i) Reduces encoding\ncomplexity through part decomposition; ii) Enables explicit part relationship\nmodeling; iii) Supports part-level conditioning. We further develop a mutual\nguidance strategy to fine-tune pre-trained diffusion models for joint part\nlatent denoising, ensuring both geometric coherence and foundation model\npriors. To enable large-scale training, we construct Partverse - a novel 3D\npart dataset derived from Objaverse through automated mesh segmentation and\nhuman-verified annotations. Extensive experiments demonstrate CoPart's superior\ncapabilities in part-level editing, articulated object generation, and scene\ncomposition with unprecedented controllability.\n","authors":["Shaocong Dong","Lihe Ding","Xiao Chen","Yaokun Li","Yuxin Wang","Yucheng Wang","Qi Wang","Jaehyeok Kim","Chenjian Gao","Zhanpeng Huang","Zibin Wang","Tianfan Xue","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2507.08772v1.pdf","comment":"Project page: https://hkdsc.github.io/project/copart"},{"id":"http://arxiv.org/abs/2507.08766v1","updated":"2025-07-11T17:26:06Z","published":"2025-07-11T17:26:06Z","title":"A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for\n  MNIST Classification","summary":"  This study presents a hybrid model for classifying handwritten digits in the\nMNIST dataset, combining convolutional neural networks (CNNs) with a multi-well\nHopfield network. The approach employs a CNN to extract high-dimensional\nfeatures from input images, which are then clustered into class-specific\nprototypes using k-means clustering. These prototypes serve as attractors in a\nmulti-well energy landscape, where a Hopfield network performs classification\nby minimizing an energy function that balances feature similarity and class\nassignment.The model's design enables robust handling of intraclass\nvariability, such as diverse handwriting styles, while providing an\ninterpretable framework through its energy-based decision process. Through\nsystematic optimization of the CNN architecture and the number of wells, the\nmodel achieves a high test accuracy of 99.2% on 10,000 MNIST images,\ndemonstrating its effectiveness for image classification tasks. The findings\nhighlight the critical role of deep feature extraction and sufficient prototype\ncoverage in achieving high performance, with potential for broader applications\nin pattern recognition.\n","authors":["Ahmed Farooq"],"pdf_url":"https://arxiv.org/pdf/2507.08766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08765v1","updated":"2025-07-11T17:21:06Z","published":"2025-07-11T17:21:06Z","title":"Compress Any Segment Anything Model (SAM)","summary":"  Due to the excellent performance in yielding high-quality, zero-shot\nsegmentation, Segment Anything Model (SAM) and its variants have been widely\napplied in diverse scenarios such as healthcare and intelligent manufacturing.\nTherefore, effectively compressing SAMs has become an increasingly pressing\npractical need. In this study, we propose Birkhoff, a novel data-free\ncompression algorithm for SAM and its variants. Unlike quantization, pruning,\ndistillation, and other compression methods, Birkhoff embodies versatility\nacross model types, agility in deployment, faithfulness to the original model,\nand compactness in model size. Specifically, Birkhoff introduces a novel\ncompression algorithm: Hyper-Compression, whose core principle is to find a\ndense trajectory to turn a high-dimensional parameter vector into a\nlow-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer\noperator, HyperLinear, to fuse decompression and matrix multiplication to\nsignificantly accelerate inference of the compressed SAMs. Extensive\nexperiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff\nperforms consistently and competitively in compression time, compression ratio,\npost-compression performance, and inference speed. For example, Birkhoff can\nachieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance\ndrop without using any fine-tuning data. Moreover, the compression is finished\nwithin 60 seconds for all models.\n","authors":["Juntong Fan","Zhiwei Hao","Jianqiang Shen","Shang-Ling Jui","Yi Zhang","Jing-Xiao Liao","Feng-Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2507.08765v1.pdf","comment":"13 pages, 6 tables, 8 figures"},{"id":"http://arxiv.org/abs/2507.08743v1","updated":"2025-07-11T16:45:59Z","published":"2025-07-11T16:45:59Z","title":"Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane\n  Geometry Detection","summary":"  Digital Twins (DT) have the potential to transform traffic management and\noperations by creating dynamic, virtual representations of transportation\nsystems that sense conditions, analyze operations, and support decision-making.\nA key component for DT of the transportation system is dynamic roadway geometry\nsensing. However, existing approaches often rely on static maps or costly\nsensors, limiting scalability and adaptability. Additionally, large-scale DTs\nthat collect and analyze data from multiple sources face challenges in privacy,\ncommunication, and computational efficiency. To address these challenges, we\nintroduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated\nTwin), a unified framework that combines real-time lane detection, DT\nsynchronization, and federated meta-learning. At the core of Geo-ORBIT is\nGeoLane, a lightweight lane detection model that learns lane geometries from\nvehicle trajectory data using roadside cameras. We extend this model through\nMeta-GeoLane, which learns to personalize detection parameters for local\nentities, and FedMeta-GeoLane, a federated learning strategy that ensures\nscalable and privacy-preserving adaptation across roadside deployments. Our\nsystem is integrated with CARLA and SUMO to create a high-fidelity DT that\nrenders highway scenarios and captures traffic flows in real-time. Extensive\nexperiments across diverse urban scenes show that FedMeta-GeoLane consistently\noutperforms baseline and meta-learning approaches, achieving lower geometric\nerror and stronger generalization to unseen locations while drastically\nreducing communication overhead. This work lays the foundation for flexible,\ncontext-aware infrastructure modeling in DTs. The framework is publicly\navailable at https://github.com/raynbowy23/FedMeta-GeoLane.git.\n","authors":["Rei Tamaru","Pei Li","Bin Ran"],"pdf_url":"https://arxiv.org/pdf/2507.08743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08741v1","updated":"2025-07-11T16:44:01Z","published":"2025-07-11T16:44:01Z","title":"HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing\n  Enabling Multi-Granularity Interpretation and Cross-Domain Transfer","summary":"  Hierarchical land cover and land use (LCLU) classification aims to assign\npixel-wise labels with multiple levels of semantic granularity to remote\nsensing (RS) imagery. However, existing deep learning-based methods face two\nmajor challenges: 1) They predominantly adopt a flat classification paradigm,\nwhich limits their ability to generate end-to-end multi-granularity\nhierarchical predictions aligned with tree-structured hierarchies used in\npractice. 2) Most cross-domain studies focus on performance degradation caused\nby sensor or scene variations, with limited attention to transferring LCLU\nmodels to cross-domain tasks with heterogeneous hierarchies (e.g., LCLU to crop\nclassification). These limitations hinder the flexibility and generalization of\nLCLU models in practical applications. To address these challenges, we propose\nHieraRS, a novel hierarchical interpretation paradigm that enables\nmulti-granularity predictions and supports the efficient transfer of LCLU\nmodels to cross-domain tasks with heterogeneous tree-structured hierarchies. We\nintroduce the Bidirectional Hierarchical Consistency Constraint Mechanism\n(BHCCM), which can be seamlessly integrated into mainstream flat classification\nmodels to generate hierarchical predictions, while improving both semantic\nconsistency and classification accuracy. Furthermore, we present TransLU, a\ndual-branch cross-domain transfer framework comprising two key components:\nCross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment\n(CDSA). TransLU supports dynamic category expansion and facilitates the\neffective adaptation of LCLU models to heterogeneous hierarchies. In addition,\nwe construct MM-5B, a large-scale multi-modal hierarchical land use dataset\nfeaturing pixel-wise annotations. The code and MM-5B dataset will be released\nat: https://github.com/AI-Tianlong/HieraRS.\n","authors":["Tianlong Ai","Tianzhu Liu","Haochen Jiang","Yanfeng Gu"],"pdf_url":"https://arxiv.org/pdf/2507.08741v1.pdf","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2402.19002v2","updated":"2025-07-11T16:39:37Z","published":"2024-02-29T09:53:19Z","title":"GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction","summary":"  Predicting the future trajectories of pedestrians on the road is an important\ntask for autonomous driving. The pedestrian trajectory prediction is affected\nby scene paths, pedestrian's intentions and decision-making, which is a\nmulti-modal problem. Most recent studies use past trajectories to predict a\nvariety of potential future trajectory distributions, which do not account for\nthe scene context and pedestrian targets. Instead of predicting the future\ntrajectory directly, we propose to use scene context and observed trajectory to\npredict the goal points first, and then reuse the goal points to predict the\nfuture trajectories. By leveraging the information from scene context and\nobserved trajectory, the uncertainty can be limited to a few target areas,\nwhich represent the \"goals\" of the pedestrians. In this paper, we propose\nGoalNet, a new trajectory prediction neural network based on the goal areas of\na pedestrian. Our network can predict both pedestrian's trajectories and\nbounding boxes. The overall model is efficient and modular, and its outputs can\nbe changed according to the usage scenario. Experimental results show that\nGoalNet significantly improves the previous state-of-the-art performance by\n48.7% on the JAAD and 40.8% on the PIE dataset.\n","authors":["Amar Fadillah","Ching-Lin Lee","Zhi-Xuan Wang","Kuan-Ting Lai"],"pdf_url":"https://arxiv.org/pdf/2402.19002v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20287v2","updated":"2025-07-11T16:39:34Z","published":"2025-03-26T07:30:58Z","title":"InsViE-1M: Effective Instruction-based Video Editing with Elaborate\n  Dataset Construction","summary":"  Instruction-based video editing allows effective and interactive editing of\nvideos using only instructions without extra inputs such as masks or\nattributes. However, collecting high-quality training triplets (source video,\nedited video, instruction) is a challenging task. Existing datasets mostly\nconsist of low-resolution, short duration, and limited amount of source videos\nwith unsatisfactory editing quality, limiting the performance of trained\nediting models. In this work, we present a high-quality Instruction-based Video\nEditing dataset with 1M triplets, namely InsViE-1M. We first curate\nhigh-resolution and high-quality source videos and images, then design an\neffective editing-filtering pipeline to construct high-quality editing triplets\nfor model training. For a source video, we generate multiple edited samples of\nits first frame with different intensities of classifier-free guidance, which\nare automatically filtered by GPT-4o with carefully crafted guidelines. The\nedited first frame is propagated to subsequent frames to produce the edited\nvideo, followed by another round of filtering for frame quality and motion\nevaluation. We also generate and filter a variety of video editing triplets\nfrom high-quality images. With the InsViE-1M dataset, we propose a multi-stage\nlearning strategy to train our InsViE model, progressively enhancing its\ninstruction following and editing ability. Extensive experiments demonstrate\nthe advantages of our InsViE-1M dataset and the trained model over\nstate-of-the-art works. Codes are available at\n\\href{https://github.com/langmanbusi/InsViE}{InsViE}.\n","authors":["Yuhui Wu","Liyi Chen","Ruibin Li","Shihao Wang","Chenxi Xie","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.20287v2.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.08735v1","updated":"2025-07-11T16:38:28Z","published":"2025-07-11T16:38:28Z","title":"Ensemble of Weak Spectral Total Variation Learners: a PET-CT Case Study","summary":"  Solving computer vision problems through machine learning, one often\nencounters lack of sufficient training data. To mitigate this we propose the\nuse of ensembles of weak learners based on spectral total-variation (STV)\nfeatures (Gilboa 2014). The features are related to nonlinear eigenfunctions of\nthe total-variation subgradient and can characterize well textures at various\nscales. It was shown (Burger et-al 2016) that, in the one-dimensional case,\northogonal features are generated, whereas in two-dimensions the features are\nempirically lowly correlated. Ensemble learning theory advocates the use of\nlowly correlated weak learners. We thus propose here to design ensembles using\nlearners based on STV features. To show the effectiveness of this paradigm we\nexamine a hard real-world medical imaging problem: the predictive value of\ncomputed tomography (CT) data for high uptake in positron emission tomography\n(PET) for patients suspected of skeletal metastases. The database consists of\n457 scans with 1524 unique pairs of registered CT and PET slices. Our approach\nis compared to deep-learning methods and to Radiomics features, showing STV\nlearners perform best (AUC=0.87), compared to neural nets (AUC=0.75) and\nRadiomics (AUC=0.79). We observe that fine STV scales in CT images are\nespecially indicative for the presence of high uptake in PET.\n","authors":["Anna Rosenberg","John Kennedy","Zohar Keidar","Yehoshua Y. Zeevi","Guy Gilboa"],"pdf_url":"https://arxiv.org/pdf/2507.08735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08729v1","updated":"2025-07-11T16:30:27Z","published":"2025-07-11T16:30:27Z","title":"RoundaboutHD: High-Resolution Real-World Urban Environment Benchmark for\n  Multi-Camera Vehicle Tracking","summary":"  The multi-camera vehicle tracking (MCVT) framework holds significant\npotential for smart city applications, including anomaly detection, traffic\ndensity estimation, and suspect vehicle tracking. However, current publicly\navailable datasets exhibit limitations, such as overly simplistic scenarios,\nlow-resolution footage, and insufficiently diverse conditions, creating a\nconsiderable gap between academic research and real-world scenario. To fill\nthis gap, we introduce RoundaboutHD, a comprehensive, high-resolution\nmulti-camera vehicle tracking benchmark dataset specifically designed to\nrepresent real-world roundabout scenarios. RoundaboutHD provides a total of 40\nminutes of labelled video footage captured by four non-overlapping,\nhigh-resolution (4K resolution, 15 fps) cameras. In total, 512 unique vehicle\nidentities are annotated across different camera views, offering rich\ncross-camera association data. RoundaboutHD offers temporal consistency video\nfootage and enhanced challenges, including increased occlusions and nonlinear\nmovement inside the roundabout. In addition to the full MCVT dataset, several\nsubsets are also available for object detection, single camera tracking, and\nimage-based vehicle re-identification (ReID) tasks. Vehicle model information\nand camera modelling/ geometry information are also included to support further\nanalysis. We provide baseline results for vehicle detection, single-camera\ntracking, image-based vehicle re-identification, and multi-camera tracking. The\ndataset and the evaluation code are publicly available at:\nhttps://github.com/siri-rouser/RoundaboutHD.git\n","authors":["Yuqiang Lin","Sam Lockyer","Mingxuan Sui","Li Gan","Florian Stanek","Markus Zarbock","Wenbin Li","Adrian Evans","Nic Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.08729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07802v2","updated":"2025-07-11T16:27:52Z","published":"2025-07-10T14:28:12Z","title":"Synergistic Prompting for Robust Visual Recognition with Missing\n  Modalities","summary":"  Large-scale multi-modal models have demonstrated remarkable performance\nacross various visual recognition tasks by leveraging extensive paired\nmulti-modal training data. However, in real-world applications, the presence of\nmissing or incomplete modality inputs often leads to significant performance\ndegradation. Recent research has focused on prompt-based strategies to tackle\nthis issue; however, existing methods are hindered by two major limitations:\n(1) static prompts lack the flexibility to adapt to varying missing-data\nconditions, and (2) basic prompt-tuning methods struggle to ensure reliable\nperformance when critical modalities are missing.To address these challenges,\nwe propose a novel Synergistic Prompting (SyP) framework for robust visual\nrecognition with missing modalities. The proposed SyP introduces two key\ninnovations: (I) a Dynamic Adapter, which computes adaptive scaling factors to\ndynamically generate prompts, replacing static parameters for flexible\nmulti-modal adaptation, and (II) a Synergistic Prompting Strategy, which\ncombines static and dynamic prompts to balance information across modalities,\nensuring robust reasoning even when key modalities are missing. The proposed\nSyP achieves significant performance improvements over existing approaches\nacross three widely-used visual recognition datasets, demonstrating robustness\nunder diverse missing rates and conditions. Extensive experiments and ablation\nstudies validate its effectiveness in handling missing modalities, highlighting\nits superior adaptability and reliability.\n","authors":["Zhihui Zhang","Luanyuan Dai","Qika Lin","Yunfeng Diao","Guangyin Jin","Yufei Guo","Jing Zhang","Xiaoshuai Hao"],"pdf_url":"https://arxiv.org/pdf/2507.07802v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08726v1","updated":"2025-07-11T16:26:31Z","published":"2025-07-11T16:26:31Z","title":"Learning human-to-robot handovers through 3D scene reconstruction","summary":"  Learning robot manipulation policies from raw, real-world image data requires\na large number of robot-action trials in the physical environment. Although\ntraining using simulations offers a cost-effective alternative, the visual\ndomain gap between simulation and robot workspace remains a major limitation.\nGaussian Splatting visual reconstruction methods have recently provided new\ndirections for robot manipulation by generating realistic environments. In this\npaper, we propose the first method for learning supervised-based robot\nhandovers solely from RGB images without the need of real-robot training or\nreal-robot data collection. The proposed policy learner, Human-to-Robot\nHandover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view\nGaussian Splatting reconstruction of human-to-robot handover scenes to generate\nrobot demonstrations containing image-action pairs captured with a camera\nmounted on the robot gripper. As a result, the simulated camera pose changes in\nthe reconstructed scene can be directly translated into gripper pose changes.\nWe train a robot policy on demonstrations collected with 16 household objects\nand {\\em directly} deploy this policy in the real environment. Experiments in\nboth Gaussian Splatting reconstructed scene and real-world human-to-robot\nhandover experiments demonstrate that H2RH-SGS serves as a new and effective\nrepresentation for the human-to-robot handover task.\n","authors":["Yuekun Wu","Yik Lung Pang","Andrea Cavallaro","Changjae Oh"],"pdf_url":"https://arxiv.org/pdf/2507.08726v1.pdf","comment":"8 pages, 6 figures, 2 table"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2411.11767v2","updated":"2025-07-11T17:59:56Z","published":"2024-11-18T17:46:32Z","title":"Drowning in Documents: Consequences of Scaling Reranker Inference","summary":"  Rerankers, typically cross-encoders, are computationally intensive but are\nfrequently used because they are widely assumed to outperform cheaper initial\nIR systems. We challenge this assumption by measuring reranker performance for\nfull retrieval, not just re-scoring first-stage retrieval. To provide a more\nrobust evaluation, we prioritize strong first-stage retrieval using modern\ndense embeddings and test rerankers on a variety of carefully chosen,\nchallenging tasks, including internally curated datasets to avoid\ncontamination, and out-of-domain ones. Our empirical results reveal a\nsurprising trend: the best existing rerankers provide initial improvements when\nscoring progressively more documents, but their effectiveness gradually\ndeclines and can even degrade quality beyond a certain limit. We hope that our\nfindings will spur future research to improve reranking.\n","authors":["Mathew Jacob","Erik Lindgren","Matei Zaharia","Michael Carbin","Omar Khattab","Andrew Drozdov"],"pdf_url":"https://arxiv.org/pdf/2411.11767v2.pdf","comment":"Accepted to ReNeuIR 2025 Workshop at SIGIR 2025 Conference"},{"id":"http://arxiv.org/abs/2507.08553v1","updated":"2025-07-11T12:56:17Z","published":"2025-07-11T12:56:17Z","title":"Digital gazetteers: review and prospects for place name knowledge bases","summary":"  Gazetteers typically store data on place names, place types and the\nassociated coordinates. They play an essential role in disambiguating place\nnames in online geographical information retrieval systems for navigation and\nmapping, detecting and disambiguating place names in text, and providing\ncoordinates. Currently there are many gazetteers in use derived from many\nsources, with no commonly accepted standard for encoding the data. Most\ngazetteers are also very limited in the extent to which they represent the\nmultiple facets of the named places yet they have potential to assist user\nsearch for locations with specific physical, commercial, social or cultural\ncharacteristics. With a view to understanding digital gazetteer technologies\nand advancing their future effectiveness for information retrieval, we provide\na review of data sources, components, software and data management\ntechnologies, data quality and volunteered data, and methods for matching\nsources that refer to the same real-world places. We highlight the need for\nfuture work on richer representation of named places, the temporal evolution of\nplace identity and location, and the development of more effective methods for\ndata integration.\n","authors":["Kalana Wijegunarathna","Kristin Stock","Christopher B. Jones"],"pdf_url":"https://arxiv.org/pdf/2507.08553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11924v2","updated":"2025-07-11T12:13:04Z","published":"2025-03-14T23:47:46Z","title":"REGEN: A Dataset and Benchmarks with Natural Language Critiques and\n  Narratives","summary":"  This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative\nNarratives), designed to benchmark the conversational capabilities of\nrecommender Large Language Models (LLMs), addressing the limitations of\nexisting datasets that primarily focus on sequential item prediction. REGEN\nextends the Amazon Product Reviews dataset by inpainting two key natural\nlanguage features: (1) user critiques, representing user \"steering\" queries\nthat lead to the selection of a subsequent item, and (2) narratives, rich\ntextual outputs associated with each recommended item taking into account prior\ncontext. The narratives include product endorsements, purchase explanations,\nand summaries of user preferences.\n  Further, we establish an end-to-end modeling benchmark for the task of\nconversational recommendation, where models are trained to generate both\nrecommendations and corresponding narratives conditioned on user history (items\nand critiques). For this joint task, we introduce a modeling framework LUMEN\n(LLM-based Unified Multi-task Model with Critiques, Recommendations, and\nNarratives) which uses an LLM as a backbone for critiquing, retrieval and\ngeneration. We also evaluate the dataset's quality using standard auto-rating\ntechniques and benchmark it by training both traditional and LLM-based\nrecommender models. Our results demonstrate that incorporating critiques\nenhances recommendation quality by enabling the recommender to learn language\nunderstanding and integrate it with recommendation signals. Furthermore, LLMs\ntrained on our dataset effectively generate both recommendations and contextual\nnarratives, achieving performance comparable to state-of-the-art recommenders\nand language models.\n","authors":["Kun Su","Krishna Sayana","Hubert Pham","James Pine","Yuri Vasilevski","Raghavendra Vasudeva","Marialena Kyriakidi","Liam Hebert","Ambarish Jash","Anushya Subbiah","Sukhdeep Sodhi"],"pdf_url":"https://arxiv.org/pdf/2503.11924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08480v1","updated":"2025-07-11T10:44:09Z","published":"2025-07-11T10:44:09Z","title":"Improving Korean-English Cross-Lingual Retrieval: A Data-Centric Study\n  of Language Composition and Model Merging","summary":"  With the increasing utilization of multilingual text information,\nCross-Lingual Information Retrieval (CLIR) has become a crucial research area.\nHowever, the impact of training data composition on both CLIR and Mono-Lingual\nInformation Retrieval (IR) performance remains under-explored. To\nsystematically investigate this data-centric aspect, we construct\nlinguistically parallel Korean-English datasets and train retrieval models with\nvarious language combinations. Our experiments reveal that the language\ncomposition of training data significantly influences IR performance,\nexhibiting important inter-lingual correlations: CLIR performance improves with\nspecific language pairs, while Mono-Lingual IR performance declines. Our work\ndemonstrates that Model Merging can effectively mitigate this trade-off,\nachieving strong CLIR results while preserving Mono-Lingual IR capabilities.\nOur findings underscore the effects of linguistic configuration of training\ndata on both CLIR and Mono-Lingual IR, and present Model Merging as a viable\nstrategy to optimize performance across these tasks.\n","authors":["Youngjoon Jang","Junyoung Son","Taemin Lee","Seongtae Hong","Heuiseok Lim"],"pdf_url":"https://arxiv.org/pdf/2507.08480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10381v3","updated":"2025-07-11T10:07:47Z","published":"2024-10-14T11:10:15Z","title":"Collaborative filtering based on nonnegative/binary matrix factorization","summary":"  Collaborative filtering generates recommendations by exploiting user-item\nsimilarities based on rating data, which often contains numerous unrated items.\nThis paper proposes a nonnegative/binary matrix factorization (NBMF) algorithm\nmodified for collaborative filtering and demonstrates that utilizing a\nlow-latency Ising machine in NBMF is advantageous in terms of computation time.\nWhile previous studies have primarily applied NBMF to dense data, such as\nimages, this study applies a modified NBMF to sparse data. Results show the\nbenefits of using a low-latency Ising machine to implement the proposed method.\n","authors":["Yukino Terui","Yuka Inoue","Yohei Hamakawa","Kosuke Tatsumura","Kazue Kudo"],"pdf_url":"https://arxiv.org/pdf/2410.10381v3.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.08445v1","updated":"2025-07-11T09:36:45Z","published":"2025-07-11T09:36:45Z","title":"CUE-RAG: Towards Accurate and Cost-Efficient Graph-Based RAG via\n  Multi-Partite Graph and Query-Driven Iterative Retrieval","summary":"  Despite the remarkable progress of Large Language Models (LLMs), their\nperformance in question answering (QA) remains limited by the lack of\ndomain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG)\naddresses this limitation by incorporating external information, often from\ngraph-structured data. However, existing graph-based RAG methods suffer from\npoor graph quality due to incomplete extraction and insufficient utilization of\nquery information during retrieval. To overcome these limitations, we propose\nCUE-RAG, a novel approach that introduces (1) a multi-partite graph index\nincorporates text Chunks, knowledge Units, and Entities to capture semantic\ncontent at multiple levels of granularity, (2) a hybrid extraction strategy\nthat reduces LLM token usage while still producing accurate and disambiguated\nknowledge units, and (3) Q-Iter, a query-driven iterative retrieval strategy\nthat enhances relevance through semantic search and constrained graph\ntraversal. Experiments on three QA benchmarks show that CUE-RAG significantly\noutperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy\nand 113.51% higher F1 score while reducing indexing costs by 72.58%.\nRemarkably, CUE-RAG matches or outperforms baselines even without using an LLM\nfor indexing. These results demonstrate the effectiveness and cost-efficiency\nof CUE-RAG in advancing graph-based RAG systems.\n","authors":["Yaodong Su","Yixiang Fang","Yingli Zhou","Quanqing Xu","Chuanhui Yang"],"pdf_url":"https://arxiv.org/pdf/2507.08445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08360v1","updated":"2025-07-11T07:23:08Z","published":"2025-07-11T07:23:08Z","title":"DS@GT at LongEval: Evaluating Temporal Performance in Web Search Systems\n  and Topics with Two-Stage Retrieval","summary":"  Information Retrieval (IR) models are often trained on static datasets,\nmaking them vulnerable to performance degradation as web content evolves. The\nDS@GT competition team participated in the Longitudinal Evaluation of Model\nPerformance (LongEval) lab at CLEF 2025, which evaluates IR systems across\ntemporally distributed web snapshots. Our analysis of the Qwant web dataset\nincludes exploratory data analysis with topic modeling over time. The two-phase\nretrieval system employs sparse keyword searches, utilizing query expansion and\ndocument reranking. Our best system achieves an average NDCG@10 of 0.296 across\nthe entire training and test dataset, with an overall best score of 0.395 on\n2023-05. The accompanying source code for this paper is at\nhttps://github.com/dsgt-arc/longeval-2025\n","authors":["Anthony Miyaguchi","Imran Afrulbasha","Aleksandar Pramov"],"pdf_url":"https://arxiv.org/pdf/2507.08360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08336v1","updated":"2025-07-11T06:28:35Z","published":"2025-07-11T06:28:35Z","title":"Distillation versus Contrastive Learning: How to Train Your Rerankers","summary":"  Training text rerankers is crucial for information retrieval. Two primary\nstrategies are widely used: contrastive learning (optimizing directly on\nground-truth labels) and knowledge distillation (transferring knowledge from a\nlarger reranker). While both have been studied in the literature, a clear\ncomparison of their effectiveness for training cross-encoder rerankers under\npractical conditions is needed.\n  This paper empirically compares these strategies by training rerankers of\ndifferent sizes and architectures using both methods on the same data, with a\nstrong contrastive learning model acting as the distillation teacher. Our\nresults show that knowledge distillation generally yields better in-domain and\nout-of-domain ranking performance than contrastive learning when distilling\nfrom a larger teacher model. This finding is consistent across student model\nsizes and architectures. However, distilling from a teacher of the same\ncapacity does not provide the same advantage, particularly for out-of-domain\ntasks. These findings offer practical guidance for choosing a training strategy\nbased on available teacher models. Therefore, we recommend using knowledge\ndistillation to train smaller rerankers if a larger, more powerful teacher is\naccessible; in its absence, contrastive learning provides a strong and more\nreliable alternative otherwise.\n","authors":["Zhichao Xu","Zhiqi Huang","Shengyao Zhuang","Ashim Gupta","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2507.08336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08322v1","updated":"2025-07-11T05:25:09Z","published":"2025-07-11T05:25:09Z","title":"Towards Efficient Quantity Retrieval from Text:an Approach via\n  Description Parsing and Weak Supervision","summary":"  Quantitative facts are continually generated by companies and governments,\nsupporting data-driven decision-making. While common facts are structured, many\nlong-tail quantitative facts remain buried in unstructured documents, making\nthem difficult to access. We propose the task of Quantity Retrieval: given a\ndescription of a quantitative fact, the system returns the relevant value and\nsupporting evidence. Understanding quantity semantics in context is essential\nfor this task. We introduce a framework based on description parsing that\nconverts text into structured (description, quantity) pairs for effective\nretrieval. To improve learning, we construct a large paraphrase dataset using\nweak supervision based on quantity co-occurrence. We evaluate our approach on a\nlarge corpus of financial annual reports and a newly annotated quantity\ndescription dataset. Our method significantly improves top-1 retrieval accuracy\nfrom 30.98 percent to 64.66 percent.\n","authors":["Yixuan Cao","Zhengrong Chen","Chengxuan Xia","Kun Wu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2507.08322v1.pdf","comment":"Extended version of the paper accepted in DEXA 2025"},{"id":"http://arxiv.org/abs/2504.01403v2","updated":"2025-07-11T02:49:25Z","published":"2025-04-02T06:40:09Z","title":"Generative Retrieval and Alignment Model: A New Paradigm for E-commerce\n  Retrieval","summary":"  Traditional sparse and dense retrieval methods struggle to leverage general\nworld knowledge and often fail to capture the nuanced features of queries and\nproducts. With the advent of large language models (LLMs), industrial search\nsystems have started to employ LLMs to generate identifiers for product\nretrieval. Commonly used identifiers include (1) static/semantic IDs and (2)\nproduct term sets. The first approach requires creating a product ID system\nfrom scratch, missing out on the world knowledge embedded within LLMs. While\nthe second approach leverages this general knowledge, the significant\ndifference in word distribution between queries and products means that\nproduct-based identifiers often do not align well with user search queries,\nleading to missed product recalls. Furthermore, when queries contain numerous\nattributes, these algorithms generate a large number of identifiers, making it\ndifficult to assess their quality, which results in low overall recall\nefficiency.\n  To address these challenges, this paper introduces a novel e-commerce\nretrieval paradigm: the Generative Retrieval and Alignment Model (GRAM). GRAM\nemploys joint training on text information from both queries and products to\ngenerate shared text identifier codes, effectively bridging the gap between\nqueries and products. This approach not only enhances the connection between\nqueries and products but also improves inference efficiency. The model uses a\nco-alignment strategy to generate codes optimized for maximizing retrieval\nefficiency. Additionally, it introduces a query-product scoring mechanism to\ncompare product values across different codes, further boosting retrieval\nefficiency. Extensive offline and online A/B testing demonstrates that GRAM\nsignificantly outperforms traditional models and the latest generative\nretrieval models, confirming its effectiveness and practicality.\n","authors":["Ming Pang","Chunyuan Yuan","Xiaoyu He","Zheng Fang","Donghao Xie","Fanyi Qu","Xue Jiang","Changping Peng","Zhangang Lin","Ching Law","Jingping Shao"],"pdf_url":"https://arxiv.org/pdf/2504.01403v2.pdf","comment":"Accepted by WWW2025"},{"id":"http://arxiv.org/abs/2507.08248v1","updated":"2025-07-11T01:21:21Z","published":"2025-07-11T01:21:21Z","title":"Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi\n  Classification","summary":"  Accurate identification of fungi species presents a unique challenge in\ncomputer vision due to fine-grained inter-species variation and high\nintra-species variation. This paper presents our approach for the FungiCLEF\n2025 competition, which focuses on few-shot fine-grained visual categorization\n(FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented\nwith multiple vision transformer models, data augmentation, weighted sampling,\nand incorporating textual information. We also explored generative AI models\nfor zero-shot classification using structured prompting but found them to\nsignificantly underperform relative to vision-based models. Our final model\noutperformed both competition baselines and highlighted the effectiveness of\ndomain specific pretraining and balanced sampling strategies. Our approach\nranked 35/74 on the private test set in post-completion evaluation, this\nsuggests additional work can be done on metadata selection and domain-adapted\nmulti-modal learning. Our code is available at\nhttps://github.com/dsgt-arc/fungiclef-2025.\n","authors":["Jason Kahei Tam","Murilo Gustineli","Anthony Miyaguchi"],"pdf_url":"https://arxiv.org/pdf/2507.08248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09068v1","updated":"2025-07-11T23:07:04Z","published":"2025-07-11T23:07:04Z","title":"Infinite Video Understanding","summary":"  The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.\n","authors":["Dell Zhang","Xiangyu Chen","Jixiang Luo","Mengxi Jia","Changzhi Sun","Ruilong Ren","Jingren Liu","Hao Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2507.09068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09055v1","updated":"2025-07-11T22:19:39Z","published":"2025-07-11T22:19:39Z","title":"Analysing Health Misinformation with Advanced Centrality Metrics in\n  Online Social Networks","summary":"  The rapid spread of health misinformation on online social networks (OSNs)\nduring global crises such as the COVID-19 pandemic poses challenges to public\nhealth, social stability, and institutional trust. Centrality metrics have long\nbeen pivotal in understanding the dynamics of information flow, particularly in\nthe context of health misinformation. However, the increasing complexity and\ndynamism of online networks, especially during crises, highlight the\nlimitations of these traditional approaches. This study introduces and compares\nthree novel centrality metrics: dynamic influence centrality (DIC), health\nmisinformation vulnerability centrality (MVC), and propagation centrality (PC).\nThese metrics incorporate temporal dynamics, susceptibility, and multilayered\nnetwork interactions. Using the FibVID dataset, we compared traditional and\nnovel metrics to identify influential nodes, propagation pathways, and\nmisinformation influencers. Traditional metrics identified 29 influential\nnodes, while the new metrics uncovered 24 unique nodes, resulting in 42\ncombined nodes, an increase of 44.83%. Baseline interventions reduced health\nmisinformation by 50%, while incorporating the new metrics increased this to\n62.5%, an improvement of 25%. To evaluate the broader applicability of the\nproposed metrics, we validated our framework on a second dataset, Monant\nMedical Misinformation, which covers a diverse range of health misinformation\ndiscussions beyond COVID-19. The results confirmed that the advanced metrics\ngeneralised successfully, identifying distinct influential actors not captured\nby traditional methods. In general, the findings suggest that a combination of\ntraditional and novel centrality measures offers a more robust and\ngeneralisable framework for understanding and mitigating the spread of health\nmisinformation in different online network contexts.\n","authors":["Mkululi Sikosana","Sean Maudsley-Barton","Oluwaseun Ajao"],"pdf_url":"https://arxiv.org/pdf/2507.09055v1.pdf","comment":"10 Pages, 2 figures, 3 tables, journal article in PLOS Digital Health\n  (2025)"},{"id":"http://arxiv.org/abs/2507.08945v1","updated":"2025-07-11T18:10:01Z","published":"2025-07-11T18:10:01Z","title":"GraphRunner: A Multi-Stage Framework for Efficient and Accurate\n  Graph-Based Retrieval","summary":"  Conventional Retrieval Augmented Generation (RAG) approaches are common in\ntext-based applications. However, they struggle with structured, interconnected\ndatasets like knowledge graphs, where understanding underlying relationships is\ncrucial for accurate retrieval. A common direction in graph-based retrieval\nemploys iterative, rule-based traversal guided by Large Language Models (LLMs).\nSuch existing iterative methods typically combine reasoning with single hop\ntraversal at each step, making them vulnerable to LLM reasoning errors and\nhallucinations that ultimately hinder the retrieval of relevant information.\n  To address these limitations, we propose GraphRunner, a novel graph-based\nretrieval framework that operates in three distinct stages: planning,\nverification, and execution. This introduces high-level traversal actions that\nenable multi-hop exploration in a single step. It also generates a holistic\ntraversal plan, which is verified against the graph structure and pre-defined\ntraversal actions, reducing reasoning errors and detecting hallucinations\nbefore execution. GraphRunner significantly reduces LLM reasoning errors and\ndetects hallucinations through validation. Our evaluation using the GRBench\ndataset shows that GraphRunner consistently outperforms existing approaches,\nachieving 10-50% performance improvements over the strongest baseline while\nreducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,\nmaking it significantly more robust and efficient for graph-based retrieval\ntasks.\n","authors":["Savini Kashmira","Jayanaka L. Dantanarayana","Krisztián Flautner","Lingjia Tang","Jason Mars"],"pdf_url":"https://arxiv.org/pdf/2507.08945v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2411.11767v2","updated":"2025-07-11T17:59:56Z","published":"2024-11-18T17:46:32Z","title":"Drowning in Documents: Consequences of Scaling Reranker Inference","summary":"  Rerankers, typically cross-encoders, are computationally intensive but are\nfrequently used because they are widely assumed to outperform cheaper initial\nIR systems. We challenge this assumption by measuring reranker performance for\nfull retrieval, not just re-scoring first-stage retrieval. To provide a more\nrobust evaluation, we prioritize strong first-stage retrieval using modern\ndense embeddings and test rerankers on a variety of carefully chosen,\nchallenging tasks, including internally curated datasets to avoid\ncontamination, and out-of-domain ones. Our empirical results reveal a\nsurprising trend: the best existing rerankers provide initial improvements when\nscoring progressively more documents, but their effectiveness gradually\ndeclines and can even degrade quality beyond a certain limit. We hope that our\nfindings will spur future research to improve reranking.\n","authors":["Mathew Jacob","Erik Lindgren","Matei Zaharia","Michael Carbin","Omar Khattab","Andrew Drozdov"],"pdf_url":"https://arxiv.org/pdf/2411.11767v2.pdf","comment":"Accepted to ReNeuIR 2025 Workshop at SIGIR 2025 Conference"},{"id":"http://arxiv.org/abs/2507.08802v1","updated":"2025-07-11T17:59:55Z","published":"2025-07-11T17:59:55Z","title":"The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for\n  Mechanistic Interpretability?","summary":"  The concept of causal abstraction got recently popularised to demystify the\nopaque decision-making processes of machine learning models; in short, a neural\nnetwork can be abstracted as a higher-level algorithm if there exists a\nfunction which allows us to map between them. Notably, most interpretability\npapers implement these maps as linear functions, motivated by the linear\nrepresentation hypothesis: the idea that features are encoded linearly in a\nmodel's representations. However, this linearity constraint is not required by\nthe definition of causal abstraction. In this work, we critically examine the\nconcept of causal abstraction by considering arbitrarily powerful alignment\nmaps. In particular, we prove that under reasonable assumptions, any neural\nnetwork can be mapped to any algorithm, rendering this unrestricted notion of\ncausal abstraction trivial and uninformative. We complement these theoretical\nfindings with empirical evidence, demonstrating that it is possible to\nperfectly map models to algorithms even when these models are incapable of\nsolving the actual task; e.g., on an experiment using randomly initialised\nlanguage models, our alignment maps reach 100% interchange-intervention\naccuracy on the indirect object identification task. This raises the non-linear\nrepresentation dilemma: if we lift the linearity constraint imposed to\nalignment maps in causal abstraction analyses, we are left with no principled\nway to balance the inherent trade-off between these maps' complexity and\naccuracy. Together, these results suggest an answer to our title's question:\ncausal abstraction is not enough for mechanistic interpretability, as it\nbecomes vacuous without assumptions about how models encode information.\nStudying the connection between this information-encoding assumption and causal\nabstraction should lead to exciting future work.\n","authors":["Denis Sutter","Julian Minder","Thomas Hofmann","Tiago Pimentel"],"pdf_url":"https://arxiv.org/pdf/2507.08802v1.pdf","comment":"42 pages, 17 figures, code available in\n  github.com/densutter/non-linear-representation-dilemma"},{"id":"http://arxiv.org/abs/2507.08800v1","updated":"2025-07-11T17:59:40Z","published":"2025-07-11T17:59:40Z","title":"NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models","summary":"  We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.\n","authors":["Luke Rivard","Sun Sun","Hongyu Guo","Wenhu Chen","Yuntian Deng"],"pdf_url":"https://arxiv.org/pdf/2507.08800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08796v1","updated":"2025-07-11T17:57:16Z","published":"2025-07-11T17:57:16Z","title":"Filter Equivariant Functions: A symmetric account of length-general\n  extrapolation on lists","summary":"  What should a function that extrapolates beyond known input/output examples\nlook like? This is a tricky question to answer in general, as any function\nmatching the outputs on those examples can in principle be a correct\nextrapolant. We argue that a \"good\" extrapolant should follow certain kinds of\nrules, and here we study a particularly appealing criterion for rule-following\nin list functions: that the function should behave predictably even when\ncertain elements are removed. In functional programming, a standard way to\nexpress such removal operations is by using a filter function. Accordingly, our\npaper introduces a new semantic class of functions -- the filter equivariant\nfunctions. We show that this class contains interesting examples, prove some\nbasic theorems about it, and relate it to the well-known class of map\nequivariant functions. We also present a geometric account of filter\nequivariants, showing how they correspond naturally to certain simplicial\nstructures. Our highlight result is the amalgamation algorithm, which\nconstructs any filter-equivariant function's output by first studying how it\nbehaves on sublists of the input, in a way that extrapolates perfectly.\n","authors":["Owen Lewis","Neil Ghani","Andrew Dudzik","Christos Perivolaropoulos","Razvan Pascanu","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2507.08796v1.pdf","comment":"18 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.08794v1","updated":"2025-07-11T17:55:22Z","published":"2025-07-11T17:55:22Z","title":"One Token to Fool LLM-as-a-Judge","summary":"  Generative reward models (also known as LLMs-as-judges), which use large\nlanguage models (LLMs) to evaluate answer quality, are increasingly adopted in\nreinforcement learning with verifiable rewards (RLVR). They are often preferred\nover rigid rule-based metrics, especially for complex reasoning tasks involving\nfree-form outputs. In this paradigm, an LLM is typically prompted to compare a\ncandidate answer against a ground-truth reference and assign a binary reward\nindicating correctness. Despite the seeming simplicity of this comparison task,\nwe find that generative reward models exhibit surprising vulnerabilities to\nsuperficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning\nopeners like \"Thought process:\" and \"Let's solve this problem step by step.\"\ncan often lead to false positive rewards. We demonstrate that this weakness is\nwidespread across LLMs, datasets, and prompt formats, posing a serious threat\nfor core algorithmic paradigms that rely on generative reward models, such as\nrejection sampling, preference optimization, and RLVR. To mitigate this issue,\nwe introduce a simple yet effective data augmentation strategy and train a new\ngenerative reward model with substantially improved robustness. Our findings\nhighlight the urgent need for more reliable LLM-based evaluation methods. We\nrelease our robust, general-domain reward model and its synthetic training data\nat https://huggingface.co/sarosavo/Master-RM and\nhttps://huggingface.co/datasets/sarosavo/Master-RM.\n","authors":["Yulai Zhao","Haolin Liu","Dian Yu","S. Y. Kung","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2507.08794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08793v1","updated":"2025-07-11T17:54:54Z","published":"2025-07-11T17:54:54Z","title":"Optimistic Exploration for Risk-Averse Constrained Reinforcement\n  Learning","summary":"  Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies\nthat minimise the likelihood of rare and catastrophic constraint violations\ncaused by an environment's inherent randomness. In general, risk-aversion leads\nto conservative exploration of the environment which typically results in\nconverging to sub-optimal policies that fail to adequately maximise reward or,\nin some cases, fail to achieve the goal. In this paper, we propose an\nexploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic\n(ORAC), which constructs an exploratory policy by maximising a local upper\nconfidence bound of the state-action reward value function whilst minimising a\nlocal lower confidence bound of the risk-averse state-action cost value\nfunction. Specifically, at each step, the weighting assigned to the cost value\nis increased or decreased if it exceeds or falls below the safety constraint\nvalue. This way the policy is encouraged to explore uncertain regions of the\nenvironment to discover high reward states whilst still satisfying the safety\nconstraints. Our experimental results demonstrate that the ORAC approach\nprevents convergence to sub-optimal policies and improves significantly the\nreward-cost trade-off in various continuous control tasks such as\nSafety-Gymnasium and a complex building energy management environment\nCityLearn.\n","authors":["James McCarthy","Radu Marinescu","Elizabeth Daly","Ivana Dusparic"],"pdf_url":"https://arxiv.org/pdf/2507.08793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.19703v2","updated":"2025-07-11T17:51:51Z","published":"2025-06-24T15:12:45Z","title":"Learning-aided Bigraph Matching Approach to Multi-Crew Restoration of\n  Damaged Power Networks Coupled with Road Transportation Networks","summary":"  The resilience of critical infrastructure networks (CINs) after disruptions,\nsuch as those caused by natural hazards, depends on both the speed of\nrestoration and the extent to which operational functionality can be regained.\nAllocating resources for restoration is a combinatorial optimal planning\nproblem that involves determining which crews will repair specific network\nnodes and in what order. This paper presents a novel graph-based formulation\nthat merges two interconnected graphs, representing crew and transportation\nnodes and power grid nodes, into a single heterogeneous graph. To enable\nefficient planning, graph reinforcement learning (GRL) is integrated with\nbigraph matching. GRL is utilized to design the incentive function for\nassigning crews to repair tasks based on the graph-abstracted state of the\nenvironment, ensuring generalization across damage scenarios. Two learning\ntechniques are employed: a graph neural network trained using Proximal Policy\nOptimization and another trained via Neuroevolution. The learned incentive\nfunctions inform a bipartite graph that links crews to repair tasks, enabling\nweighted maximum matching for crew-to-task allocations. An efficient simulation\nenvironment that pre-computes optimal node-to-node path plans is used to train\nthe proposed restoration planning methods. An IEEE 8500-bus power distribution\ntest network coupled with a 21 square km transportation network is used as the\ncase study, with scenarios varying in terms of numbers of damaged nodes,\ndepots, and crews. Results demonstrate the approach's generalizability and\nscalability across scenarios, with learned policies providing 3-fold better\nperformance than random policies, while also outperforming optimization-based\nsolutions in both computation time (by several orders of magnitude) and power\nrestored.\n","authors":["Nathan Maurer","Harshal Kaushik","Roshni Anna Jacob","Jie Zhang","Souma Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2506.19703v2.pdf","comment":"Accepted for presentation in proceedings of ASME IDETC 2025"},{"id":"http://arxiv.org/abs/2506.18247v2","updated":"2025-07-11T17:48:06Z","published":"2025-06-23T02:32:20Z","title":"Exploring Efficient Quantification of Modeling Uncertainties with\n  Differentiable Physics-Informed Machine Learning Architectures","summary":"  Quantifying and propagating modeling uncertainties is crucial for reliability\nanalysis, robust optimization, and other model-based algorithmic processes in\nengineering design and control. Now, physics-informed machine learning (PIML)\nmethods have emerged in recent years as a new alternative to traditional\ncomputational modeling and surrogate modeling methods, offering a balance\nbetween computing efficiency, modeling accuracy, and interpretability. However,\ntheir ability to predict and propagate modeling uncertainties remains mostly\nunexplored. In this paper, a promising class of auto-differentiable hybrid PIML\narchitectures that combine partial physics and neural networks or ANNs (for\ninput transformation or adaptive parameter estimation) is integrated with\nBayesian Neural networks (replacing the ANNs); this is done with the goal to\nexplore whether BNNs can successfully provision uncertainty propagation\ncapabilities in the PIML architectures as well, further supported by the\nauto-differentiability of these architectures. A two-stage training process is\nused to alleviate the challenges traditionally encountered in training\nprobabilistic ML models. The resulting BNN-integrated PIML architecture is\nevaluated on an analytical benchmark problem and flight experiments data for a\nfixed-wing RC aircraft, with prediction performance observed to be slightly\nworse or at par with purely data-driven ML and original PIML models. Moreover,\nMonte Carlo sampling of probabilistic BNN weights was found to be most\neffective in propagating uncertainty in the BNN-integrated PIML architectures.\n","authors":["Manaswin Oddiraju","Bharath Varma Penumatsa","Divyang Amin","Michael Piedmonte","Souma Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2506.18247v2.pdf","comment":"Accepted for presentation in proceedings of ASME IDETC 2025"},{"id":"http://arxiv.org/abs/2507.08784v1","updated":"2025-07-11T17:46:12Z","published":"2025-07-11T17:46:12Z","title":"Greedy Low-Rank Gradient Compression for Distributed Learning with\n  Convergence Guarantees","summary":"  Distributed optimization is pivotal for large-scale signal processing and\nmachine learning, yet communication overhead remains a major bottleneck.\nLow-rank gradient compression, in which the transmitted gradients are\napproximated by low-rank matrices to reduce communication, offers a promising\nremedy. Existing methods typically adopt either randomized or greedy\ncompression strategies: randomized approaches project gradients onto randomly\nchosen subspaces, introducing high variance and degrading empirical\nperformance; greedy methods select the most informative subspaces, achieving\nstrong empirical results but lacking convergence guarantees. To address this\ngap, we propose GreedyLore--the first Greedy Low-Rank gradient compression\nalgorithm for distributed learning with rigorous convergence guarantees.\nGreedyLore incorporates error feedback to correct the bias introduced by greedy\ncompression and introduces a semi-lazy subspace update that ensures the\ncompression operator remains contractive throughout all iterations. With these\ntechniques, we prove that GreedyLore achieves a convergence rate of\n$\\mathcal{O}(\\sigma/\\sqrt{NT} + 1/T)$ under standard optimizers such as MSGD\nand Adam--marking the first linear speedup convergence rate for low-rank\ngradient compression. Extensive experiments are conducted to validate our\ntheoretical findings.\n","authors":["Chuyan Chen","Yutong He","Pengrui Li","Weichen Jia","Kun Yuan"],"pdf_url":"https://arxiv.org/pdf/2507.08784v1.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.00615v2","updated":"2025-07-11T17:33:58Z","published":"2024-12-31T19:28:21Z","title":"Predicting Barge Presence and Quantity on Inland Waterways using Vessel\n  Tracking Data: A Machine Learning Approach","summary":"  This study presents a machine learning approach to predict the number of\nbarges transported by vessels on inland waterways using tracking data from the\nAutomatic Identification System (AIS). While AIS tracks the location of tug and\ntow vessels, it does not monitor the presence or number of barges transported\nby those vessels. Understanding the number and types of barges conveyed along\nriver segments, between ports, and at ports is crucial for estimating the\nquantities of freight transported on the nation's waterways. This insight is\nalso valuable for waterway management and infrastructure operations impacting\nareas such as targeted dredging operations, and data-driven resource\nallocation. Labeled sample data was generated using observations from traffic\ncameras located along key river segments and matched to AIS data records. A\nsample of 164 vessels representing up to 42 barge convoys per vessel was used\nfor model development. The methodology involved first predicting barge presence\nand then predicting barge quantity. Features derived from the AIS data included\nspeed measures, vessel characteristics, turning measures, and interaction\nterms. For predicting barge presence, the AdaBoost model achieved an F1 score\nof 0.932. For predicting barge quantity, the Random Forest combined with an\nAdaBoost ensemble model achieved an F1 score of 0.886. Bayesian optimization\nwas used for hyperparameter tuning. By advancing predictive modeling for inland\nwaterways, this study offers valuable insights for transportation planners and\norganizations, which require detailed knowledge of traffic volumes, including\nthe flow of commodities, their destinations, and the tonnage moving in and out\nof ports.\n","authors":["Geoffery Agorku","Sarah Hernandez","Maria Falquez","Subhadipto Poddar","Shihao Pang"],"pdf_url":"https://arxiv.org/pdf/2501.00615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08771v1","updated":"2025-07-11T17:28:56Z","published":"2025-07-11T17:28:56Z","title":"BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity","summary":"  To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).\n","authors":["Chenyang Song","Weilin Zhao","Xu Han","Chaojun Xiao","Yingfa Chen","Yuxuan Li","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2507.08771v1.pdf","comment":"21 pages, 7 figures, 15 tables"},{"id":"http://arxiv.org/abs/2507.08766v1","updated":"2025-07-11T17:26:06Z","published":"2025-07-11T17:26:06Z","title":"A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for\n  MNIST Classification","summary":"  This study presents a hybrid model for classifying handwritten digits in the\nMNIST dataset, combining convolutional neural networks (CNNs) with a multi-well\nHopfield network. The approach employs a CNN to extract high-dimensional\nfeatures from input images, which are then clustered into class-specific\nprototypes using k-means clustering. These prototypes serve as attractors in a\nmulti-well energy landscape, where a Hopfield network performs classification\nby minimizing an energy function that balances feature similarity and class\nassignment.The model's design enables robust handling of intraclass\nvariability, such as diverse handwriting styles, while providing an\ninterpretable framework through its energy-based decision process. Through\nsystematic optimization of the CNN architecture and the number of wells, the\nmodel achieves a high test accuracy of 99.2% on 10,000 MNIST images,\ndemonstrating its effectiveness for image classification tasks. The findings\nhighlight the critical role of deep feature extraction and sufficient prototype\ncoverage in achieving high performance, with potential for broader applications\nin pattern recognition.\n","authors":["Ahmed Farooq"],"pdf_url":"https://arxiv.org/pdf/2507.08766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02548v3","updated":"2025-07-11T17:24:48Z","published":"2024-10-03T14:53:10Z","title":"Local Flow Matching Generative Models","summary":"  Flow Matching (FM) is a simulation-free method for learning a continuous and\ninvertible flow to interpolate between two distributions, and in particular to\ngenerate data from noise. Inspired by the variational nature of the diffusion\nprocess as a gradient flow, we introduce a stepwise FM model called Local Flow\nMatching (LFM), which consecutively learns a sequence of FM sub-models, each\nmatching a diffusion process up to the time of the step size in the\ndata-to-noise direction. In each step, the two distributions to be interpolated\nby the sub-flow model are closer to each other than data vs. noise, and this\nenables the use of smaller models with faster training. This variational\nperspective also allows us to theoretically prove a generation guarantee of the\nproposed flow model in terms of the $\\chi^2$-divergence between the generated\nand true data distributions, utilizing the contraction property of the\ndiffusion process. In practice, the stepwise structure of LFM is natural to be\ndistilled and different distillation techniques can be adopted to speed up\ngeneration. We empirically demonstrate improved training efficiency and\ncompetitive generative performance of LFM compared to FM on the unconditional\ngeneration of tabular data and image datasets, and also on the conditional\ngeneration of robotic manipulation policies.\n","authors":["Chen Xu","Xiuyuan Cheng","Yao Xie"],"pdf_url":"https://arxiv.org/pdf/2410.02548v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08761v1","updated":"2025-07-11T17:16:02Z","published":"2025-07-11T17:16:02Z","title":"Penalizing Infeasible Actions and Reward Scaling in Reinforcement\n  Learning with Offline Data","summary":"  Reinforcement learning with offline data suffers from Q-value extrapolation\nerrors. To address this issue, we first demonstrate that linear extrapolation\nof the Q-function beyond the data range is particularly problematic. To\nmitigate this, we propose guiding the gradual decrease of Q-values outside the\ndata range, which is achieved through reward scaling with layer normalization\n(RS-LN) and a penalization mechanism for infeasible actions (PA). By combining\nRS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a\nrange of tasks, demonstrating superior performance compared to state-of-the-art\nalgorithms in both offline training and online fine-tuning on the D4RL\nbenchmark, with notable success in the challenging AntMaze Ultra task.\n","authors":["Jeonghye Kim","Yongjae Shin","Whiyoung Jung","Sunghoon Hong","Deunsol Yoon","Youngchul Sung","Kanghoon Lee","Woohyung Lim"],"pdf_url":"https://arxiv.org/pdf/2507.08761v1.pdf","comment":"Accepted to ICML2025"},{"id":"http://arxiv.org/abs/2501.19334v3","updated":"2025-07-11T17:08:45Z","published":"2025-01-31T17:34:53Z","title":"The Value of Prediction in Identifying the Worst-Off","summary":"  Machine learning is increasingly used in government programs to identify and\nsupport the most vulnerable individuals, prioritizing assistance for those at\ngreatest risk over optimizing aggregate outcomes. This paper examines the\nwelfare impacts of prediction in equity-driven contexts, and how they compare\nto other policy levers, such as expanding bureaucratic capacity. Through\nmathematical models and a real-world case study on long-term unemployment\namongst German residents, we develop a comprehensive understanding of the\nrelative effectiveness of prediction in surfacing the worst-off. Our findings\nprovide clear analytical frameworks and practical, data-driven tools that\nempower policymakers to make principled decisions when designing these systems.\n","authors":["Unai Fischer-Abaigar","Christoph Kern","Juan Carlos Perdomo"],"pdf_url":"https://arxiv.org/pdf/2501.19334v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08751v1","updated":"2025-07-11T17:02:33Z","published":"2025-07-11T17:02:33Z","title":"ML-Based Automata Simplification for Symbolic Accelerators","summary":"  Symbolic accelerators are increasingly used for symbolic data processing in\ndomains such as genomics, NLP, and cybersecurity. However, these accelerators\nface scalability issues due to excessive memory use and routing complexity,\nespecially when targeting a large set. We present AutoSlim, a machine\nlearning-based graph simplification framework designed to reduce the complexity\nof symbolic accelerators built on Non-deterministic Finite Automata (NFA)\ndeployed on FPGA-based overlays such as NAPOLY+. AutoSlim uses Random Forest\nclassification to prune low-impact transitions based on edge scores and\nstructural features, significantly reducing automata graph density while\npreserving semantic correctness. Unlike prior tools, AutoSlim targets automated\nscore-aware simplification with weighted transitions, enabling efficient\nranking-based sequence analysis. We evaluated data sets (1K to 64K nodes) in\nNAPOLY+ and conducted performance measurements including latency, throughput,\nand resource usage. AutoSlim achieves up to 40 percent reduction in FPGA LUTs\nand over 30 percent pruning in transitions, while scaling to graphs an order of\nmagnitude larger than existing benchmarks. Our results also demonstrate how\nhardware interconnection (fanout) heavily influences hardware cost and that\nAutoSlim's pruning mitigates resource blowup.\n","authors":["Tiffany Yu","Rye Stahle-Smith","Darssan Eswaramoorthi","Rasha Karakchi"],"pdf_url":"https://arxiv.org/pdf/2507.08751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01635v2","updated":"2025-07-11T17:00:27Z","published":"2025-06-02T13:12:02Z","title":"Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces","summary":"  Temporal alignment of multiple signals through time warping is crucial in\nmany fields, such as classification within speech recognition or robot motion\nlearning. Almost all related works are limited to data in Euclidean space.\nAlthough an attempt was made in 2011 to adapt this concept to unit quaternions,\na general extension to Riemannian manifolds remains absent. Given its\nimportance for numerous applications in robotics and beyond, we introduce\nRiemannian Time Warping (RTW). This novel approach efficiently aligns multiple\nsignals by considering the geometric structure of the Riemannian manifold in\nwhich the data is embedded. Extensive experiments on synthetic and real-world\ndata, including tests with an LBR iiwa robot, demonstrate that RTW consistently\noutperforms state-of-the-art baselines in both averaging and classification\ntasks.\n","authors":["Julian Richter","Christopher Erdös","Christian Scheurer","Jochen J. Steil","Niels Dehio"],"pdf_url":"https://arxiv.org/pdf/2506.01635v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08749v1","updated":"2025-07-11T16:59:27Z","published":"2025-07-11T16:59:27Z","title":"Modeling Partially Observed Nonlinear Dynamical Systems and Efficient\n  Data Assimilation via Discrete-Time Conditional Gaussian Koopman Network","summary":"  A discrete-time conditional Gaussian Koopman network (CGKN) is developed in\nthis work to learn surrogate models that can perform efficient state forecast\nand data assimilation (DA) for high-dimensional complex dynamical systems,\ne.g., systems governed by nonlinear partial differential equations (PDEs).\nFocusing on nonlinear partially observed systems that are common in many\nengineering and earth science applications, this work exploits Koopman\nembedding to discover a proper latent representation of the unobserved system\nstates, such that the dynamics of the latent states are conditional linear,\ni.e., linear with the given observed system states. The modeled system of the\nobserved and latent states then becomes a conditional Gaussian system, for\nwhich the posterior distribution of the latent states is Gaussian and can be\nefficiently evaluated via analytical formulae. The analytical formulae of DA\nfacilitate the incorporation of DA performance into the learning process of the\nmodeled system, which leads to a framework that unifies scientific machine\nlearning (SciML) and data assimilation. The performance of discrete-time CGKN\nis demonstrated on several canonical problems governed by nonlinear PDEs with\nintermittency and turbulent features, including the viscous Burgers' equation,\nthe Kuramoto-Sivashinsky equation, and the 2-D Navier-Stokes equations, with\nwhich we show that the discrete-time CGKN framework achieves comparable\nperformance as the state-of-the-art SciML methods in state forecast and\nprovides efficient and accurate DA results. The discrete-time CGKN framework\nalso serves as an example to illustrate unifying the development of SciML\nmodels and their other outer-loop applications such as design optimization,\ninverse problems, and optimal control.\n","authors":["Chuanqi Chen","Zhongrui Wang","Nan Chen","Jin-Long Wu"],"pdf_url":"https://arxiv.org/pdf/2507.08749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08746v1","updated":"2025-07-11T16:56:37Z","published":"2025-07-11T16:56:37Z","title":"Partitioned Hybrid Quantum Fourier Neural Operators for Scientific\n  Quantum Machine Learning","summary":"  We introduce the Partitioned Hybrid Quantum Fourier Neural Operator (PHQFNO),\na generalization of the Quantum Fourier Neural Operator (QFNO) for scientific\nmachine learning. PHQFNO partitions the Fourier operator computation across\nclassical and quantum resources, enabling tunable quantum-classical\nhybridization and distributed execution across quantum and classical devices.\nThe method extends QFNOs to higher dimensions and incorporates a\nmessage-passing framework to distribute data across different partitions. Input\ndata are encoded into quantum states using unary encoding, and quantum circuit\nparameters are optimized using a variational scheme. We implement PHQFNO using\nPennyLane with PyTorch integration and evaluate it on Burgers' equation,\nincompressible and compressible Navier-Stokes equations. We show that PHQFNO\nrecovers classical FNO accuracy. On incompressible Navier-Stokes, PHQFNO\nachieves higher accuracy than its classical counterparts. Finally, we perform a\nsensitivity analysis under input noise, confirming improved stability of PHQFNO\nover classical baselines.\n","authors":["Paolo Marcandelli","Yuanchun He","Stefano Mariani","Martina Siena","Stefano Markidis"],"pdf_url":"https://arxiv.org/pdf/2507.08746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08745v1","updated":"2025-07-11T16:55:25Z","published":"2025-07-11T16:55:25Z","title":"Hashing for Fast Pattern Set Selection","summary":"  Pattern set mining, which is the task of finding a good set of patterns\ninstead of all patterns, is a fundamental problem in data mining. Many\ndifferent definitions of what constitutes a good set have been proposed in\nrecent years. In this paper, we consider the reconstruction error as a proxy\nmeasure for the goodness of the set, and concentrate on the adjacent problem of\nhow to find a good set efficiently. We propose a method based on bottom-k\nhashing for efficiently selecting the set and extend the method for the common\ncase where the patterns might only appear in approximate form in the data. Our\napproach has applications in tiling databases, Boolean matrix factorization,\nand redescription mining, among others. We show that our hashing-based approach\nis significantly faster than the standard greedy algorithm while obtaining\nalmost equally good results in both synthetic and real-world data sets.\n","authors":["Maiju Karjalainen","Pauli Miettinen"],"pdf_url":"https://arxiv.org/pdf/2507.08745v1.pdf","comment":"17 pages, 5 figures, to appear at ECML-PKDD 2025"},{"id":"http://arxiv.org/abs/2507.03190v2","updated":"2025-07-11T16:55:14Z","published":"2025-07-03T21:45:17Z","title":"Discovering Algorithms with Computational Language Processing","summary":"  Algorithms are the engine for reproducible problem-solving. We present a\nframework automating algorithm discovery by conceptualizing them as sequences\nof operations, represented as tokens. These computational tokens are chained\nusing a grammar, enabling the formation of increasingly sophisticated\nprocedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement\nlearning (RL) explores token chaining and drives the creation of new tokens.\nThis methodology rediscovers, improves, and generates new algorithms that\nsubstantially outperform existing methods for strongly NP-hard combinatorial\noptimization problems and foundational quantum computing approaches such as\nGrover's and Quantum Approximate Optimization Algorithm. Operating at the\ncomputational rather than code-generation level, our framework produces\nalgorithms that can be tailored specifically to problem instances, not merely\nclasses.\n","authors":["Theo Bourdais","Abeynaya Gnanasekaran","Houman Owhadi","Tuhin Sahai"],"pdf_url":"https://arxiv.org/pdf/2507.03190v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2507.08738v1","updated":"2025-07-11T16:40:10Z","published":"2025-07-11T16:40:10Z","title":"Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy\n  Chaotic Time Series","summary":"  Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have\nshown promise in forecasting chaotic dynamical systems, such as the Lorenz-63\nmodel and El Nino-Southern Oscillation. However, their reliance on fixed\nnonlinearities - polynomial expansions in NVAR or random feature maps in RC -\nlimits their adaptability to high noise or real-world data. These methods also\nscale poorly in high-dimensional settings due to costly matrix inversion during\nreadout computation. We propose an adaptive NVAR model that combines\ndelay-embedded linear inputs with features generated by a shallow, learnable\nmulti-layer perceptron (MLP). The MLP and linear readout are jointly trained\nusing gradient-based optimization, enabling the model to learn data-driven\nnonlinearities while preserving a simple readout structure. Unlike standard\nNVAR, our approach avoids the need for an exhaustive and sensitive grid search\nover ridge and delay parameters. Instead, tuning is restricted to neural\nnetwork hyperparameters, improving scalability. Initial experiments on chaotic\nsystems tested under noise-free and synthetically noisy conditions showed that\nthe adaptive model outperformed the standard NVAR in predictive accuracy and\nshowed robust forecasting under noisy conditions with a lower observation\nfrequency.\n","authors":["Azimov Sherkhon","Susana Lopez-Moreno","Eric Dolores-Cuenca","Sieun Lee","Sangil Kim"],"pdf_url":"https://arxiv.org/pdf/2507.08738v1.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2507.08736v1","updated":"2025-07-11T16:38:40Z","published":"2025-07-11T16:38:40Z","title":"Catastrophic Forgetting Mitigation Through Plateau Phase Activity\n  Profiling","summary":"  Catastrophic forgetting in deep neural networks occurs when learning new\ntasks degrades performance on previously learned tasks due to knowledge\noverwriting. Among the approaches to mitigate this issue, regularization\ntechniques aim to identify and constrain \"important\" parameters to preserve\nprevious knowledge. In the highly nonconvex optimization landscape of deep\nlearning, we propose a novel perspective: tracking parameters during the final\ntraining plateau is more effective than monitoring them throughout the entire\ntraining process. We argue that parameters that exhibit higher activity\n(movement and variability) during this plateau reveal directions in the loss\nlandscape that are relatively flat, making them suitable for adaptation to new\ntasks while preserving knowledge from previous ones. Our comprehensive\nexperiments demonstrate that this approach achieves superior performance in\nbalancing catastrophic forgetting mitigation with strong performance on newly\nlearned tasks.\n","authors":["Idan Mashiach","Oren Glickman","Tom Tirer"],"pdf_url":"https://arxiv.org/pdf/2507.08736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06489v2","updated":"2025-07-11T16:26:05Z","published":"2025-06-06T19:29:13Z","title":"Alternating Gradient Flows: A Theory of Feature Learning in Two-layer\n  Neural Networks","summary":"  What features neural networks learn, and how, remains an open question. In\nthis paper, we introduce Alternating Gradient Flows (AGF), an algorithmic\nframework that describes the dynamics of feature learning in two-layer networks\ntrained from small initialization. Prior works have shown that gradient flow in\nthis regime exhibits a staircase-like loss curve, alternating between plateaus\nwhere neurons slowly align to useful directions and sharp drops where neurons\nrapidly grow in norm. AGF approximates this behavior as an alternating two-step\nprocess: maximizing a utility function over dormant neurons and minimizing a\ncost function over active ones. AGF begins with all neurons dormant. At each\nround, a dormant neuron activates, triggering the acquisition of a feature and\na drop in the loss. AGF quantifies the order, timing, and magnitude of these\ndrops, matching experiments across architectures. We show that AGF unifies and\nextends existing saddle-to-saddle analyses in fully connected linear networks\nand attention-only linear transformers, where the learned features are singular\nmodes and principal components, respectively. In diagonal linear networks, we\nprove AGF converges to gradient flow in the limit of vanishing initialization.\nApplying AGF to quadratic networks trained to perform modular addition, we give\nthe first complete characterization of the training dynamics, revealing that\nnetworks learn Fourier features in decreasing order of coefficient magnitude.\nAltogether, AGF offers a promising step towards understanding feature learning\nin neural networks.\n","authors":["Daniel Kunin","Giovanni Luca Marchetti","Feng Chen","Dhruva Karkada","James B. Simon","Michael R. DeWeese","Surya Ganguli","Nina Miolane"],"pdf_url":"https://arxiv.org/pdf/2506.06489v2.pdf","comment":"39 pages, 7 figures"},{"id":"http://arxiv.org/abs/2507.08721v1","updated":"2025-07-11T16:21:33Z","published":"2025-07-11T16:21:33Z","title":"Monitoring Risks in Test-Time Adaptation","summary":"  Encountering shifted data at test time is a ubiquitous challenge when\ndeploying predictive models. Test-time adaptation (TTA) methods address this\nissue by continuously adapting a deployed model using only unlabeled test data.\nWhile TTA can extend the model's lifespan, it is only a temporary solution.\nEventually the model might degrade to the point that it must be taken offline\nand retrained. To detect such points of ultimate failure, we propose pairing\nTTA with risk monitoring frameworks that track predictive performance and raise\nalerts when predefined performance criteria are violated. Specifically, we\nextend existing monitoring tools based on sequential testing with confidence\nsequences to accommodate scenarios in which the model is updated at test time\nand no test labels are available to estimate the performance metrics of\ninterest. Our extensions unlock the application of rigorous statistical risk\nmonitoring to TTA, and we demonstrate the effectiveness of our proposed TTA\nmonitoring framework across a representative set of datasets, distribution\nshift types, and TTA methods.\n","authors":["Mona Schirmer","Metod Jazbec","Christian A. Naesseth","Eric Nalisnick"],"pdf_url":"https://arxiv.org/pdf/2507.08721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08718v1","updated":"2025-07-11T16:19:45Z","published":"2025-07-11T16:19:45Z","title":"On the Effect of Regularization in Policy Mirror Descent","summary":"  Policy Mirror Descent (PMD) has emerged as a unifying framework in\nreinforcement learning (RL) by linking policy gradient methods with a\nfirst-order optimization method known as mirror descent. At its core, PMD\nincorporates two key regularization components: (i) a distance term that\nenforces a trust region for stable policy updates and (ii) an MDP regularizer\nthat augments the reward function to promote structure and robustness. While\nPMD has been extensively studied in theory, empirical investigations remain\nscarce. This work provides a large-scale empirical analysis of the interplay\nbetween these two regularization techniques, running over 500k training seeds\non small RL environments. Our results demonstrate that, although the two\nregularizers can partially substitute each other, their precise combination is\ncritical for achieving robust performance. These findings highlight the\npotential for advancing research on more robust algorithms in RL, particularly\nwith respect to hyperparameter sensitivity.\n","authors":["Jan Felix Kleuker","Aske Plaat","Thomas Moerland"],"pdf_url":"https://arxiv.org/pdf/2507.08718v1.pdf","comment":"Accepted at RLC"},{"id":"http://arxiv.org/abs/2504.19034v2","updated":"2025-07-11T16:07:37Z","published":"2025-04-26T22:00:42Z","title":"On learning functions over biological sequence space: relating Gaussian\n  process priors, regularization, and gauge fixing","summary":"  Mappings from biological sequences (DNA, RNA, protein) to quantitative\nmeasures of sequence functionality play an important role in contemporary\nbiology. We are interested in the related tasks of (i) inferring predictive\nsequence-to-function maps and (ii) decomposing sequence-function maps to\nelucidate the contributions of individual subsequences. Because each\nsequence-function map can be written as a weighted sum over subsequences in\nmultiple ways, meaningfully interpreting these weights requires \"gauge-fixing,\"\ni.e., defining a unique representation for each map. Recent work has\nestablished that most existing gauge-fixed representations arise as the unique\nsolutions to $L_2$-regularized regression in an overparameterized \"weight\nspace\" where the choice of regularizer defines the gauge. Here, we establish\nthe relationship between regularized regression in overparameterized weight\nspace and Gaussian process approaches that operate in \"function space,\" i.e.\nthe space of all real-valued functions on a finite set of sequences. We\ndisentangle how weight space regularizers both impose an implicit prior on the\nlearned function and restrict the optimal weights to a particular gauge. We\nalso show how to construct regularizers that correspond to arbitrary explicit\nGaussian process priors combined with a wide variety of gauges. Next, we derive\nthe distribution of gauge-fixed weights implied by the Gaussian process\nposterior and demonstrate that even for long sequences this distribution can be\nefficiently computed for product-kernel priors using a kernel trick. Finally,\nwe characterize the implicit function space priors associated with the most\ncommon weight space regularizers. Overall, our framework unifies and extends\nour ability to infer and interpret sequence-function relationships.\n","authors":["Samantha Petti","Carlos Martí-Gómez","Justin B. Kinney","Juannan Zhou","David M. McCandlish"],"pdf_url":"https://arxiv.org/pdf/2504.19034v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03366v2","updated":"2025-07-11T16:06:51Z","published":"2025-02-05T17:03:49Z","title":"Rethinking Approximate Gaussian Inference in Classification","summary":"  In classification tasks, softmax functions are ubiquitously used as output\nactivations to produce predictive probabilities. Such outputs only capture\naleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian\ninference methods have been proposed. We develop a common formalism to describe\nsuch methods, which we view as outputting Gaussian distributions over the logit\nspace. Predictives are then obtained as the expectations of the Gaussian\ndistributions pushed forward through the softmax. However, such softmax\nGaussian integrals cannot be solved analytically, and Monte Carlo (MC)\napproximations can be costly and noisy. We propose to replace the softmax\nactivation by element-wise normCDF or sigmoid, which allows for the accurate\nsampling-free approximation of predictives. This also enables the approximation\nof the Gaussian pushforwards by Dirichlet distributions with moment matching.\nThis approach entirely eliminates the runtime and memory overhead associated\nwith MC sampling. We evaluate it combined with several approximate Gaussian\ninference methods (Laplace, HET, SNGP) on large- and small-scale datasets\n(ImageNet, CIFAR-100, CIFAR-10), demonstrating improved uncertainty\nquantification capabilities compared to softmax MC sampling.\n","authors":["Bálint Mucsányi","Nathaël Da Costa","Philipp Hennig"],"pdf_url":"https://arxiv.org/pdf/2502.03366v2.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2507.08707v1","updated":"2025-07-11T16:05:18Z","published":"2025-07-11T16:05:18Z","title":"SPLASH! Sample-efficient Preference-based inverse reinforcement learning\n  for Long-horizon Adversarial tasks from Suboptimal Hierarchical\n  demonstrations","summary":"  Inverse Reinforcement Learning (IRL) presents a powerful paradigm for\nlearning complex robotic tasks from human demonstrations. However, most\napproaches make the assumption that expert demonstrations are available, which\nis often not the case. Those that allow for suboptimality in the demonstrations\nare not designed for long-horizon goals or adversarial tasks. Many desirable\nrobot capabilities fall into one or both of these categories, thus highlighting\na critical shortcoming in the ability of IRL to produce field-ready robotic\nagents. We introduce Sample-efficient Preference-based inverse reinforcement\nlearning for Long-horizon Adversarial tasks from Suboptimal Hierarchical\ndemonstrations (SPLASH), which advances the state-of-the-art in learning from\nsuboptimal demonstrations to long-horizon and adversarial settings. We\nempirically validate SPLASH on a maritime capture-the-flag task in simulation,\nand demonstrate real-world applicability with sim-to-real translation\nexperiments on autonomous unmanned surface vehicles. We show that our proposed\nmethods allow SPLASH to significantly outperform the state-of-the-art in reward\nlearning from suboptimal demonstrations.\n","authors":["Peter Crowley","Zachary Serlin","Tyler Paine","Makai Mann","Michael Benjamin","Calin Belta"],"pdf_url":"https://arxiv.org/pdf/2507.08707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09686v2","updated":"2025-07-11T16:03:51Z","published":"2024-11-14T18:53:51Z","title":"Conditional regression for the Nonlinear Single-Variable Model","summary":"  Regressing a function $F$ on $\\mathbb{R}^d$ without the statistical and\ncomputational curse of dimensionality requires special statistical models, for\nexample that impose geometric assumptions on the distribution of the data\n(e.g., that its support is low-dimensional), or strong smoothness assumptions\non $F$, or a special structure $F$. Among the latter, compositional models\n$F=f\\circ g$ with $g$ mapping to $\\mathbb{R}^r$ with $r\\ll d$ include classical\nsingle- and multi-index models, as well as neural networks. While the case\nwhere $g$ is linear is well-understood, less is known when $g$ is nonlinear,\nand in particular for which $g$'s the curse of dimensionality in estimating\n$F$, or both $f$ and $g$, may be circumvented. Here we consider a model\n$F(X):=f(\\Pi_\\gamma X)$ where\n$\\Pi_\\gamma:\\mathbb{R}^d\\to[0,\\textrm{len}_\\gamma]$ is the closest-point\nprojection onto the parameter of a regular curve $\\gamma:[0,\n\\textrm{len}_\\gamma]\\to\\mathbb{R}^d$, and $f:[0,\\textrm{len}_\\gamma]\\to\n\\mathbb{R}^1$. The input data $X$ is not low-dimensional: it can be as far from\n$\\gamma$ as the condition that $\\Pi_\\gamma(X)$ is well-defined allows. The\ndistribution $X$, the curve $\\gamma$ and the function $f$ are all unknown. This\nmodel is a natural nonlinear generalization of the single-index model,\ncorresponding to $\\gamma$ being a line. We propose a nonparametric estimator,\nbased on conditional regression, that under suitable assumptions, the strongest\nof which being that $f$ is coarsely monotone, achieves, up to log factors, the\n$\\textit{one-dimensional}$ optimal min-max rate for non-parametric regression,\nup to the level of noise in the observations, and be constructed in time\n$\\mathcal{O}(d^2 n\\log n)$. All the constants in the learning bounds, in the\nminimal number of samples required for our bounds to hold, and in the\ncomputational complexity are at most low-order polynomials in $d$.\n","authors":["Yantao Wu","Mauro Maggioni"],"pdf_url":"https://arxiv.org/pdf/2411.09686v2.pdf","comment":"57 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.14371v3","updated":"2025-07-11T15:57:22Z","published":"2024-12-18T22:12:28Z","title":"SEREP: Semantic Facial Expression Representation for Robust In-the-Wild\n  Capture and Retargeting","summary":"  Monocular facial performance capture in-the-wild is challenging due to varied\ncapture conditions, face shapes, and expressions. Most current methods rely on\nlinear 3D Morphable Models, which represent facial expressions independently of\nidentity at the vertex displacement level. We propose SEREP (Semantic\nExpression Representation), a model that disentangles expression from identity\nat the semantic level. We start by learning an expression representation from\nhigh-quality 3D data of unpaired facial expressions. Then, we train a model to\npredict expression from monocular images relying on a novel semi-supervised\nscheme using low quality synthetic data. In addition, we introduce MultiREX, a\nbenchmark addressing the lack of evaluation resources for the expression\ncapture task. Our experiments show that SEREP outperforms state-of-the-art\nmethods, capturing challenging expressions and transferring them to new\nidentities.\n","authors":["Arthur Josi","Luiz Gustavo Hafemann","Abdallah Dib","Emeline Got","Rafael M. O. Cruz","Marc-Andre Carbonneau"],"pdf_url":"https://arxiv.org/pdf/2412.14371v3.pdf","comment":"For our project page, see\n  https://ubisoft-laforge.github.io/character/serep/"},{"id":"http://arxiv.org/abs/2507.08697v1","updated":"2025-07-11T15:50:23Z","published":"2025-07-11T15:50:23Z","title":"Domain-Informed Operation Excellence of Gas Turbine System with Machine\n  Learning","summary":"  The domain-consistent adoption of artificial intelligence (AI) remains low in\nthermal power plants due to the black-box nature of AI algorithms and low\nrepresentation of domain knowledge in conventional data-centric analytics. In\nthis paper, we develop a MAhalanobis Distance-based OPTimization (MAD-OPT)\nframework that incorporates the Mahalanobis distance-based constraint to\nintroduce domain knowledge into data-centric analytics. The developed MAD-OPT\nframework is applied to maximize thermal efficiency and minimize turbine heat\nrate for a 395 MW capacity gas turbine system. We demonstrate that the MAD-OPT\nframework can estimate domain-informed optimal process conditions under\ndifferent ambient conditions, and the optimal solutions are found to be robust\nas evaluated by Monte Carlo simulations. We also apply the MAD-OPT framework to\nestimate optimal process conditions beyond the design power generation limit of\nthe gas turbine system, and have found comparable results with the actual data\nof the power plant. We demonstrate that implementing data-centric optimization\nanalytics without incorporating domain-informed constraints may provide\nineffective solutions that may not be implementable in the real operation of\nthe gas turbine system. This research advances the integration of the\ndata-driven domain knowledge into machine learning-powered analytics that\nenhances the domain-informed operation excellence and paves the way for safe AI\nadoption in thermal power systems.\n","authors":["Waqar Muhammad Ashraf","Amir H. Keshavarzzadeh","Abdulelah S. Alshehri","Abdulrahman bin Jumah","Ramit Debnath","Vivek Dua"],"pdf_url":"https://arxiv.org/pdf/2507.08697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05640v2","updated":"2025-07-11T15:46:42Z","published":"2025-07-08T03:36:40Z","title":"Learnable quantum spectral filters for hybrid graph neural networks","summary":"  In this paper, we describe a parameterized quantum circuit that can be\nconsidered as convolutional and pooling layers for graph neural networks. The\ncircuit incorporates the parameterized quantum Fourier circuit where the qubit\nconnections for the controlled gates derived from the Laplacian operator.\nSpecifically, we show that the eigenspace of the Laplacian operator of a graph\ncan be approximated by using QFT based circuit whose connections are determined\nfrom the adjacency matrix. For an $N\\times N$ Laplacian, this approach yields\nan approximate polynomial-depth circuit requiring only $n=log(N)$ qubits. These\ntypes of circuits can eliminate the expensive classical computations for\napproximating the learnable functions of the Laplacian through Chebyshev\npolynomial or Taylor expansions.\n  Using this circuit as a convolutional layer provides an $n-$ dimensional\nprobability vector that can be considered as the filtered and compressed graph\nsignal. Therefore, the circuit along with the measurement can be considered a\nvery efficient convolution plus pooling layer that transforms an\n$N$-dimensional signal input into $n-$dimensional signal with an exponential\ncompression. We then apply a classical neural network prediction head to the\noutput of the circuit to construct a complete graph neural network. Since the\ncircuit incorporates geometric structure through its graph connection-based\napproach, we present graph classification results for the benchmark datasets\nlisted in TUDataset library. Using only [1-100] learnable parameters for the\nquantum circuit and minimal classical layers (1000-5000 parameters) in a\ngeneric setting, the obtained results are comparable to and in some cases\nbetter than many of the baseline results, particularly for the cases when\ngeometric structure plays a significant role.\n","authors":["Ammar Daskin"],"pdf_url":"https://arxiv.org/pdf/2507.05640v2.pdf","comment":"The simulation code and results used for this paper is publicly\n  available at: https://github.com/adaskin/gnn-qsf"},{"id":"http://arxiv.org/abs/2507.08686v1","updated":"2025-07-11T15:37:24Z","published":"2025-07-11T15:37:24Z","title":"Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and\n  Distillation","summary":"  Overfitting in deep neural networks occurs less frequently than expected.\nThis is a puzzling observation, as theory predicts that greater model capacity\nshould eventually lead to overfitting -- yet this is rarely seen in practice.\nBut what if overfitting does occur, not globally, but in specific sub-regions\nof the data space? In this work, we introduce a novel score that measures the\nforgetting rate of deep models on validation data, capturing what we term local\noverfitting: a performance degradation confined to certain regions of the input\nspace. We demonstrate that local overfitting can arise even without\nconventional overfitting, and is closely linked to the double descent\nphenomenon.\n  Building on these insights, we introduce a two-stage approach that leverages\nthe training history of a single model to recover and retain forgotten\nknowledge: first, by aggregating checkpoints into an ensemble, and then by\ndistilling it into a single model of the original size, thus enhancing\nperformance without added inference cost.\n  Extensive experiments across multiple datasets, modern architectures, and\ntraining regimes validate the effectiveness of our approach. Notably, in the\npresence of label noise, our method -- Knowledge Fusion followed by Knowledge\nDistillation -- outperforms both the original model and independently trained\nensembles, achieving a rare win-win scenario: reduced training and inference\ncomplexity.\n","authors":["Uri Stern","Eli Corn","Daphna Weinshall"],"pdf_url":"https://arxiv.org/pdf/2507.08686v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2412.12968"},{"id":"http://arxiv.org/abs/2502.02582v2","updated":"2025-07-11T15:35:20Z","published":"2025-02-04T18:56:47Z","title":"Open Materials Generation with Stochastic Interpolants","summary":"  The discovery of new materials is essential for enabling technological\nadvancements. Computational approaches for predicting novel materials must\neffectively learn the manifold of stable crystal structures within an infinite\ndesign space. We introduce Open Materials Generation (OMatG), a unifying\nframework for the generative design and discovery of inorganic crystalline\nmaterials. OMatG employs stochastic interpolants (SI) to bridge an arbitrary\nbase distribution to the target distribution of inorganic crystals via a broad\nclass of tunable stochastic processes, encompassing both diffusion models and\nflow matching as special cases. In this work, we adapt the SI framework by\nintegrating an equivariant graph representation of crystal structures and\nextending it to account for periodic boundary conditions in unit cell\nrepresentations. Additionally, we couple the SI flow over spatial coordinates\nand lattice vectors with discrete flow matching for atomic species. We\nbenchmark OMatG's performance on two tasks: Crystal Structure Prediction (CSP)\nfor specified compositions, and 'de novo' generation (DNG) aimed at discovering\nstable, novel, and unique structures. In our ground-up implementation of OMatG,\nwe refine and extend both CSP and DNG metrics compared to previous works. OMatG\nestablishes a new state of the art in generative modeling for materials\ndiscovery, outperforming purely flow-based and diffusion-based implementations.\nThese results underscore the importance of designing flexible deep learning\nframeworks to accelerate progress in materials science. The OMatG code is\navailable at https://github.com/FERMat-ML/OMatG.\n","authors":["Philipp Hoellmer","Thomas Egg","Maya M. Martirossyan","Eric Fuemmeler","Zeren Shui","Amit Gupta","Pawan Prakash","Adrian Roitberg","Mingjie Liu","George Karypis","Mark Transtrum","Richard G. Hennig","Ellad B. Tadmor","Stefano Martiniani"],"pdf_url":"https://arxiv.org/pdf/2502.02582v2.pdf","comment":"Accepted at Forty-second International Conference on Machine Learning\n  (ICML): https://openreview.net/forum?id=gHGrzxFujU"},{"id":"http://arxiv.org/abs/2503.18114v2","updated":"2025-07-11T15:03:15Z","published":"2025-03-23T15:39:56Z","title":"Feature Learning beyond the Lazy-Rich Dichotomy: Insights from\n  Representational Geometry","summary":"  Integrating task-relevant information into neural representations is a\nfundamental ability of both biological and artificial intelligence systems.\nRecent theories have categorized learning into two regimes: the rich regime,\nwhere neural networks actively learn task-relevant features, and the lazy\nregime, where networks behave like random feature models. Yet this simple\nlazy-rich dichotomy overlooks a diverse underlying taxonomy of feature\nlearning, shaped by differences in learning algorithms, network architectures,\nand data properties. To address this gap, we introduce an analysis framework to\nstudy feature learning via the geometry of neural representations. Rather than\ninspecting individual learned features, we characterize how task-relevant\nrepresentational manifolds evolve throughout the learning process. We show, in\nboth theoretical and empirical settings, that as networks learn features,\ntask-relevant manifolds untangle, with changes in manifold geometry revealing\ndistinct learning stages and strategies beyond the lazy-rich dichotomy. This\nframework provides novel insights into feature learning across neuroscience and\nmachine learning, shedding light on structural inductive biases in neural\ncircuits and the mechanisms underlying out-of-distribution generalization.\n","authors":["Chi-Ning Chou","Hang Le","Yichen Wang","SueYeon Chung"],"pdf_url":"https://arxiv.org/pdf/2503.18114v2.pdf","comment":"This work was published in ICML 2025 and was selected for a spotlight\n  presentation"},{"id":"http://arxiv.org/abs/2507.08660v1","updated":"2025-07-11T15:00:32Z","published":"2025-07-11T15:00:32Z","title":"The Impact of Automatic Speech Transcription on Speaker Attribution","summary":"  Speaker attribution from speech transcripts is the task of identifying a\nspeaker from the transcript of their speech based on patterns in their language\nuse. This task is especially useful when the audio is unavailable (e.g.\ndeleted) or unreliable (e.g. anonymized speech). Prior work in this area has\nprimarily focused on the feasibility of attributing speakers using transcripts\nproduced by human annotators. However, in real-world settings, one often only\nhas more errorful transcripts produced by automatic speech recognition (ASR)\nsystems. In this paper, we conduct what is, to our knowledge, the first\ncomprehensive study of the impact of automatic transcription on speaker\nattribution performance. In particular, we study the extent to which speaker\nattribution performance degrades in the face of transcription errors, as well\nas how properties of the ASR system impact attribution. We find that\nattribution is surprisingly resilient to word-level transcription errors and\nthat the objective of recovering the true transcript is minimally correlated\nwith attribution performance. Overall, our findings suggest that speaker\nattribution on more errorful transcripts produced by ASR is as good, if not\nbetter, than attribution based on human-transcribed data, possibly because ASR\ntranscription errors can capture speaker-specific features revealing of speaker\nidentity.\n","authors":["Cristina Aggazzotti","Matthew Wiesner","Elizabeth Allyn Smith","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2507.08660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08653v1","updated":"2025-07-11T14:57:37Z","published":"2025-07-11T14:57:37Z","title":"Safe Deep Reinforcement Learning for Resource Allocation with Peak Age\n  of Information Violation Guarantees","summary":"  In Wireless Networked Control Systems (WNCSs), control and communication\nsystems must be co-designed due to their strong interdependence. This paper\npresents a novel optimization theory-based safe deep reinforcement learning\n(DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction\nwhile optimizing performance, for the first time in the literature. The\napproach minimizes power consumption under key constraints, including Peak Age\nof Information (PAoI) violation probability, transmit power, and schedulability\nin the finite blocklength regime. PAoI violation probability is uniquely\nderived by combining stochastic maximum allowable transfer interval (MATI) and\nmaximum allowable packet delay (MAD) constraints in a multi-sensor network. The\nframework consists of two stages: optimization theory and safe DRL. The first\nstage derives optimality conditions to establish mathematical relationships\namong variables, simplifying and decomposing the problem. The second stage\nemploys a safe DRL model where a teacher-student framework guides the DRL agent\n(student). The control mechanism (teacher) evaluates compliance with system\nconstraints and suggests the nearest feasible action when needed. Extensive\nsimulations show that the proposed framework outperforms rule-based and other\noptimization theory based DRL benchmarks, achieving faster convergence, higher\nrewards, and greater stability.\n","authors":["Berire Gunes Reyhan","Sinem Coleri"],"pdf_url":"https://arxiv.org/pdf/2507.08653v1.pdf","comment":"15 Pages, to be published in IEEE Transactions on Communications"},{"id":"http://arxiv.org/abs/2507.08637v1","updated":"2025-07-11T14:40:40Z","published":"2025-07-11T14:40:40Z","title":"Scaling Attention to Very Long Sequences in Linear Time with\n  Wavelet-Enhanced Random Spectral Attention (WERSA)","summary":"  Transformer models are computationally costly on long sequences since regular\nattention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced\nRandom Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time\ncomplexity that is pivotal to enable successful long-sequence processing\nwithout the performance trade-off. WERSA merges content-adaptive random\nspectral features together with multi-resolution Haar wavelets and learnable\nparameters to selectively attend to informative scales of data while preserving\nlinear efficiency.\n  Large-scale comparisons \\textbf{on single GPU} and across various benchmarks\n(vision, NLP, hierarchical reasoning) and various attention mechanisms (like\nMultiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,\nWaveformer), reveal uniform advantages of WERSA. It achieves best accuracy in\nall tests. On ArXiv classification, WERSA improves accuracy over vanilla\nattention by 1.2\\% (86.2\\% vs 85.0\\%) while cutting training time by 81\\% (296s\nvs 1554s) and FLOPS by 73.4\\% (26.2G vs 98.4G). Significantly, WERSA excels\nwhere vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy\nsequences, it achieves best accuracy (79.1\\%) and AUC (0.979) among viable\nmethods, operating on data that gives Out-Of-Memory errors to quadratic methods\nwhile being \\textbf{twice as fast} as Waveformer, its next-best competitor.\n  By significantly reducing computational loads without compromising accuracy,\nWERSA makes possible more practical, more affordable, long-context models, in\nparticular on low-resource hardware, for more sustainable and more scalable AI\ndevelopment.\n","authors":["Vincenzo Dentamaro"],"pdf_url":"https://arxiv.org/pdf/2507.08637v1.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2507.08623v1","updated":"2025-07-11T14:25:36Z","published":"2025-07-11T14:25:36Z","title":"Entangled Threats: A Unified Kill Chain Model for Quantum Machine\n  Learning Security","summary":"  Quantum Machine Learning (QML) systems inherit vulnerabilities from classical\nmachine learning while introducing new attack surfaces rooted in the physical\nand algorithmic layers of quantum computing. Despite a growing body of research\non individual attack vectors - ranging from adversarial poisoning and evasion\nto circuit-level backdoors, side-channel leakage, and model extraction - these\nthreats are often analyzed in isolation, with unrealistic assumptions about\nattacker capabilities and system environments. This fragmentation hampers the\ndevelopment of effective, holistic defense strategies. In this work, we argue\nthat QML security requires more structured modeling of the attack surface,\ncapturing not only individual techniques but also their relationships,\nprerequisites, and potential impact across the QML pipeline. We propose\nadapting kill chain models, widely used in classical IT and cybersecurity, to\nthe quantum machine learning context. Such models allow for structured\nreasoning about attacker objectives, capabilities, and possible multi-stage\nattack paths - spanning reconnaissance, initial access, manipulation,\npersistence, and exfiltration. Based on extensive literature analysis, we\npresent a detailed taxonomy of QML attack vectors mapped to corresponding\nstages in a quantum-aware kill chain framework that is inspired by the MITRE\nATLAS for classical machine learning. We highlight interdependencies between\nphysical-level threats (like side-channel leakage and crosstalk faults), data\nand algorithm manipulation (such as poisoning or circuit backdoors), and\nprivacy attacks (including model extraction and training data inference). This\nwork provides a foundation for more realistic threat modeling and proactive\nsecurity-in-depth design in the emerging field of quantum machine learning.\n","authors":["Pascal Debus","Maximilian Wendlinger","Kilian Tscharke","Daniel Herr","Cedric Brügmann","Daniel Ohl de Mello","Juris Ulmanis","Alexander Erhard","Arthur Schmidt","Fabian Petsch"],"pdf_url":"https://arxiv.org/pdf/2507.08623v1.pdf","comment":"Accepted for publication at IEEE International Conference on Quantum\n  Computing and Engineering (QCE) 2025"},{"id":"http://arxiv.org/abs/2503.08311v2","updated":"2025-07-11T14:23:16Z","published":"2025-03-11T11:21:35Z","title":"Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM\n  Inference","summary":"  Large language models have been widely adopted across different tasks, but\ntheir auto-regressive generation nature often leads to inefficient resource\nutilization during inference. While batching is commonly used to increase\nthroughput, performance gains plateau beyond a certain batch size, especially\nwith smaller models, a phenomenon that existing literature typically explains\nas a shift to the compute-bound regime. In this paper, through an in-depth\nGPU-level analysis, we reveal that large-batch inference remains memory-bound,\nwith most GPU compute capabilities underutilized due to DRAM bandwidth\nsaturation as the primary bottleneck. To address this, we propose a Batching\nConfiguration Advisor (BCA) that optimizes memory allocation, reducing GPU\nmemory requirements with minimal impact on throughput. The freed memory and\nunderutilized GPU compute capabilities can then be leveraged by concurrent\nworkloads. Specifically, we use model replication to improve serving throughput\nand GPU utilization. Our findings challenge conventional assumptions about LLM\ninference, offering new insights and practical strategies for improving\nresource utilization, particularly for smaller language models. The code is\npublicly available at\nhttps://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap.\n","authors":["Pol G. Recasens","Ferran Agullo","Yue Zhu","Chen Wang","Eun Kyung Lee","Olivier Tardieu","Jordi Torres","Josep Ll. Berral"],"pdf_url":"https://arxiv.org/pdf/2503.08311v2.pdf","comment":"Pol G. Recasens, Ferran Agullo: equal contribution. Paper accepted at\n  IEEE CLOUD 2025"},{"id":"http://arxiv.org/abs/2507.05550v2","updated":"2025-07-11T14:14:03Z","published":"2025-07-08T00:20:57Z","title":"A Malliavin calculus approach to score functions in diffusion generative\n  models","summary":"  Score-based diffusion generative models have recently emerged as a powerful\ntool for modelling complex data distributions. These models aim at learning the\nscore function, which defines a map from a known probability distribution to\nthe target data distribution via deterministic or stochastic differential\nequations (SDEs). The score function is typically estimated from data using a\nvariety of approximation techniques, such as denoising or sliced score\nmatching, Hyv\\\"arien's method, or Schr\\\"odinger bridges. In this paper, we\nderive an exact, closed form, expression for the score function for a broad\nclass of nonlinear diffusion generative models. Our approach combines modern\nstochastic analysis tools such as Malliavin derivatives and their adjoint\noperators (Skorokhod integrals or Malliavin Divergence) with a new Bismut-type\nformula. The resulting expression for the score function can be written\nentirely in terms of the first and second variation processes, with all\nMalliavin derivatives systematically eliminated, thereby enhancing its\npractical applicability. The theoretical framework presented in this work\noffers a principled foundation for advancing score estimation methods in\ngenerative modelling, enabling the design of new sampling algorithms for\ncomplex probability distributions. Our results can be extended to broader\nclasses of stochastic differential equations, opening new directions for the\ndevelopment of score-based diffusion generative models.\n","authors":["Ehsan Mirafzali","Frank Proske","Utkarsh Gupta","Daniele Venturi","Razvan Marinescu"],"pdf_url":"https://arxiv.org/pdf/2507.05550v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08617v1","updated":"2025-07-11T14:13:41Z","published":"2025-07-11T14:13:41Z","title":"Towards Collaborative Fairness in Federated Learning Under Imbalanced\n  Covariate Shift","summary":"  Collaborative fairness is a crucial challenge in federated learning. However,\nexisting approaches often overlook a practical yet complex form of\nheterogeneity: imbalanced covariate shift. We provide a theoretical analysis of\nthis setting, which motivates the design of FedAKD (Federated Asynchronous\nKnowledge Distillation)- simple yet effective approach that balances accurate\nprediction with collaborative fairness. FedAKD consists of client and server\nupdates. In the client update, we introduce a novel asynchronous knowledge\ndistillation strategy based on our preliminary analysis, which reveals that\nwhile correctly predicted samples exhibit similar feature distributions across\nclients, incorrectly predicted samples show significant variability. This\nsuggests that imbalanced covariate shift primarily arises from misclassified\nsamples. Leveraging this insight, our approach first applies traditional\nknowledge distillation to update client models while keeping the global model\nfixed. Next, we select correctly predicted high-confidence samples and update\nthe global model using these samples while keeping client models fixed. The\nserver update simply aggregates all client models. We further provide a\ntheoretical proof of FedAKD's convergence. Experimental results on public\ndatasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records\n(EHR) dataset demonstrate that FedAKD significantly improves collaborative\nfairness, enhances predictive accuracy, and fosters client participation even\nunder highly heterogeneous data distributions.\n","authors":["Tianrun Yu","Jiaqi Wang","Haoyu Wang","Mingquan Lin","Han Liu","Nelson S. Yee","Fenglong Ma"],"pdf_url":"https://arxiv.org/pdf/2507.08617v1.pdf","comment":"18 pages, accepted to the 31st ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD' 25), Toronto, Canada, August 3-7 2025"},{"id":"http://arxiv.org/abs/2507.08616v1","updated":"2025-07-11T14:13:22Z","published":"2025-07-11T14:13:22Z","title":"AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs","summary":"  Large-language models (LLMs) have demonstrated powerful problem-solving\ncapabilities, in particular when organized in multi-agent systems. However, the\nadvent of such systems also raises several questions on the ability of a\ncomplex network of agents to effectively self-organize and collaborate. While\nmeasuring performance on standard reasoning benchmarks indicates how well\nmulti-agent systems can solve reasoning tasks, it is unclear whether these\nsystems are able to leverage their topology effectively. Here, we propose\nAgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration\nfrom classical problems in distributed systems and graph theory, AgentsNet\nmeasures the ability of multi-agent systems to collaboratively form strategies\nfor problem-solving, self-organization, and effective communication given a\nnetwork topology. We evaluate a variety of baseline methods on AgentsNet\nincluding homogeneous networks of agents which first have to agree on basic\nprotocols for organization and communication. We find that some frontier LLMs\nare already demonstrating strong performance for small networks but begin to\nfall off once the size of the network scales. While existing multi-agent\nbenchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size\nand can scale with new generations of LLMs. As such, we also probe frontier\nmodels in a setup with up to 100 agents.\n","authors":["Florian Grötschla","Luis Müller","Jan Tönshoff","Mikhail Galkin","Bryan Perozzi"],"pdf_url":"https://arxiv.org/pdf/2507.08616v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2507.08610v1","updated":"2025-07-11T14:08:36Z","published":"2025-07-11T14:08:36Z","title":"Emergent Natural Language with Communication Games for Improving Image\n  Captioning Capabilities without Additional Data","summary":"  Image captioning is an important problem in developing various AI systems,\nand these tasks require large volumes of annotated images to train the models.\nSince all existing labelled datasets are already used for training the large\nVision Language Models (VLMs), it becomes challenging to improve the\nperformance of the same. Considering this, it is essential to consider the\nunsupervised image captioning performance, which remains relatively\nunder-explored. To that end, we propose LoGIC (Lewis Communication Game for\nImage Captioning), a Multi-agent Reinforcement Learning game. The proposed\nmethod consists of two agents, a 'speaker' and a 'listener', with the objective\nof learning a strategy for communicating in natural language. We train agents\nin the cooperative common-reward setting using the GRPO algorithm and show that\nimprovement in image captioning performance emerges as a consequence of the\nagents learning to play the game. We show that using pre-trained VLMs as the\n'speaker' and Large Language Model (LLM) for language understanding in the\n'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without\nadditional labels, a $2$ units advantage in absolute metrics compared to the\n$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the\n'speaker' with lightweight components: (i) a ViT for image perception and (ii)\na GPT2 language generation, and train them from scratch using LoGIC, obtaining\na $31$ BLEU score in the unsupervised setting, a $10$ points advantage over\nexisting unsupervised image-captioning methods.\n","authors":["Parag Dutta","Ambedkar Dukkipati"],"pdf_url":"https://arxiv.org/pdf/2507.08610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.23182v2","updated":"2025-07-11T14:05:02Z","published":"2025-06-29T10:50:46Z","title":"Attribution assignment for deep-generative sequence models enables\n  interpretability analysis using positive-only data","summary":"  Generative machine learning models offer a powerful framework for therapeutic\ndesign by efficiently exploring large spaces of biological sequences enriched\nfor desirable properties. Unlike supervised learning methods, which require\nboth positive and negative labeled data, generative models such as LSTMs can be\ntrained solely on positively labeled sequences, for example, high-affinity\nantibodies. This is particularly advantageous in biological settings where\nnegative data are scarce, unreliable, or biologically ill-defined. However, the\nlack of attribution methods for generative models has hindered the ability to\nextract interpretable biological insights from such models. To address this\ngap, we developed Generative Attribution Metric Analysis (GAMA), an attribution\nmethod for autoregressive generative models based on Integrated Gradients. We\nassessed GAMA using synthetic datasets with known ground truths to characterize\nits statistical behavior and validate its ability to recover biologically\nrelevant features. We further demonstrated the utility of GAMA by applying it\nto experimental antibody-antigen binding data. GAMA enables model\ninterpretability and the validation of generative sequence design strategies\nwithout the need for negative training data.\n","authors":["Robert Frank","Michael Widrich","Rahmad Akbar","Günter Klambauer","Geir Kjetil Sandve","Philippe A. Robert","Victor Greiff"],"pdf_url":"https://arxiv.org/pdf/2506.23182v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06897v2","updated":"2025-07-11T14:02:58Z","published":"2025-04-09T13:56:05Z","title":"MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs","summary":"  This paper presents MedSegFactory, a versatile medical synthesis framework\nthat generates high-quality paired medical images and segmentation masks across\nmodalities and tasks. It aims to serve as an unlimited data repository,\nsupplying image-mask pairs to enhance existing segmentation tools. The core of\nMedSegFactory is a dual-stream diffusion model, where one stream synthesizes\nmedical images and the other generates corresponding segmentation masks. To\nensure precise alignment between image-mask pairs, we introduce Joint\nCross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic\ncross-conditioning between streams. This bidirectional interaction allows both\nrepresentations to guide each other's generation, enhancing consistency between\ngenerated pairs. MedSegFactory unlocks on-demand generation of paired medical\nimages and segmentation masks through user-defined prompts that specify the\ntarget labels, imaging modalities, anatomical regions, and pathological\nconditions, facilitating scalable and high-quality data generation. This new\nparadigm of medical image synthesis enables seamless integration into diverse\nmedical imaging workflows, enhancing both efficiency and accuracy. Extensive\nexperiments show that MedSegFactory generates data of superior quality and\nusability, achieving competitive or state-of-the-art performance in 2D and 3D\nsegmentation tasks while addressing data scarcity and regulatory constraints.\n","authors":["Jiawei Mao","Yuhan Wang","Yucheng Tang","Daguang Xu","Kang Wang","Yang Yang","Zongwei Zhou","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.06897v2.pdf","comment":"12 pages, 8 figures, The project page can be accessed via\n  https://jwmao1.github.io/MedSegFactory_web"},{"id":"http://arxiv.org/abs/2507.08605v1","updated":"2025-07-11T14:00:43Z","published":"2025-07-11T14:00:43Z","title":"Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices\n  Across Punjab, India","summary":"  Rice cultivation consumes 24-30% of global freshwater, creating critical\nwater management challenges in major rice-producing regions. Sustainable\nirrigation practices like direct seeded rice (DSR) and alternate wetting and\ndrying (AWD) can reduce water use by 20-40% while maintaining yields, helping\nsecure long-term agricultural productivity as water scarcity intensifies - a\nkey component of the Zero Hunger Sustainable Development Goal. However, limited\ndata on adoption rates of these practices prevents evidence-based policymaking\nand targeted resource allocation. We developed a novel remote sensing framework\nto monitor sustainable water management practices at scale in Punjab, India - a\nregion facing severe groundwater depletion of 41.6 cm/year. To collect\nessential ground truth data, we partnered with the Nature Conservancy's\nPromoting Regenerative and No-burn Agriculture (PRANA) program, which trained\napproximately 1,400 farmers on water-saving techniques while documenting their\nfield-level practices. Using this data, we created a classification system with\nSentinel-1 satellite imagery that separates water management along sowing and\nirrigation dimensions. Our approach achieved a 78% F1-score in distinguishing\nDSR from traditional puddled transplanted rice without requiring prior\nknowledge of planting dates. We demonstrated scalability by mapping DSR\nadoption across approximately 3 million agricultural plots in Punjab, with\ndistrict-level predictions showing strong correlation (Pearson=0.77, RBO= 0.77)\nwith government records. This study provides policymakers with a powerful tool\nto track sustainable water management adoption, target interventions, and\nmeasure program impacts at scale.\n","authors":["Ando Shah","Rajveer Singh","Akram Zaytar","Girmaw Abebe Tadesse","Caleb Robinson","Negar Tafti","Stephen A. Wood","Rahul Dodhia","Juan M. Lavista Ferres"],"pdf_url":"https://arxiv.org/pdf/2507.08605v1.pdf","comment":"Dataset and code will be published shortly and links updated in v2"},{"id":"http://arxiv.org/abs/2507.07883v2","updated":"2025-07-11T13:57:39Z","published":"2025-07-10T16:06:02Z","title":"SAMO: A Lightweight Sharpness-Aware Approach for Multi-Task Optimization\n  with Joint Global-Local Perturbation","summary":"  Multi-task learning (MTL) enables a joint model to capture commonalities\nacross multiple tasks, reducing computation costs and improving data\nefficiency. However, a major challenge in MTL optimization is task conflicts,\nwhere the task gradients differ in direction or magnitude, limiting model\nperformance compared to single-task counterparts. Sharpness-aware minimization\n(SAM) minimizes task loss while simultaneously reducing the sharpness of the\nloss landscape. Our empirical observations show that SAM effectively mitigates\ntask conflicts in MTL. Motivated by these findings, we explore integrating SAM\ninto MTL but face two key challenges. While both the average loss gradient and\nindividual task gradients-referred to as global and local\ninformation-contribute to SAM, how to combine them remains unclear. Moreover,\ndirectly computing each task gradient introduces significant computational and\nmemory overheads. To address these challenges, we propose SAMO, a lightweight\n\\textbf{S}harpness-\\textbf{A}ware \\textbf{M}ulti-task \\textbf{O}ptimization\napproach, that leverages a joint global-local perturbation. The local\nperturbations are approximated using only forward passes and are layerwise\nnormalized to improve efficiency. Extensive experiments on a suite of\nmulti-task benchmarks demonstrate both the effectiveness and efficiency of our\nmethod. Code is available at https://github.com/OptMN-Lab/SAMO.\n","authors":["Hao Ban","Gokul Ram Subramani","Kaiyi Ji"],"pdf_url":"https://arxiv.org/pdf/2507.07883v2.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2507.08597v1","updated":"2025-07-11T13:47:07Z","published":"2025-07-11T13:47:07Z","title":"ADAPT: A Pseudo-labeling Approach to Combat Concept Drift in Malware\n  Detection","summary":"  Machine learning models are commonly used for malware classification;\nhowever, they suffer from performance degradation over time due to concept\ndrift. Adapting these models to changing data distributions requires frequent\nupdates, which rely on costly ground truth annotations. While active learning\ncan reduce the annotation burden, leveraging unlabeled data through\nsemi-supervised learning remains a relatively underexplored approach in the\ncontext of malware detection. In this research, we introduce \\texttt{ADAPT}, a\nnovel pseudo-labeling semi-supervised algorithm for addressing concept drift.\nOur model-agnostic method can be applied to various machine learning models,\nincluding neural networks and tree-based algorithms. We conduct extensive\nexperiments on five diverse malware detection datasets spanning Android,\nWindows, and PDF domains. The results demonstrate that our method consistently\noutperforms baseline models and competitive benchmarks. This work paves the way\nfor more effective adaptation of machine learning models to concept drift in\nmalware detection.\n","authors":["Md Tanvirul Alam","Aritran Piplai","Nidhi Rastogi"],"pdf_url":"https://arxiv.org/pdf/2507.08597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20289v2","updated":"2025-07-11T13:27:24Z","published":"2024-10-26T23:18:33Z","title":"On the Gaussian process limit of Bayesian Additive Regression Trees","summary":"  Bayesian Additive Regression Trees (BART) is a nonparametric Bayesian\nregression technique of rising fame. It is a sum-of-decision-trees model, and\nis in some sense the Bayesian version of boosting. In the limit of infinite\ntrees, it becomes equivalent to Gaussian process (GP) regression. This limit is\nknown but has not yet led to any useful analysis or application. For the first\ntime, I derive and compute the exact BART prior covariance function. With it I\nimplement the infinite trees limit of BART as GP regression. Through empirical\ntests, I show that this limit is worse than standard BART in a fixed\nconfiguration, but also that tuning its hyperparameters in the natural GP way\nmakes it competitive with BART. The advantage of using a GP surrogate of BART\nis the analytical likelihood, which simplifies model building and sidesteps the\ncomplex BART MCMC algorithm. More generally, this study opens new ways to\nunderstand and develop BART and GP regression. The implementation of BART as GP\nis available in the Python package lsqfitgp.\n","authors":["Giacomo Petrillo"],"pdf_url":"https://arxiv.org/pdf/2410.20289v2.pdf","comment":"Check out the software at https://github.com/Gattocrucco/lsqfitgp"},{"id":"http://arxiv.org/abs/2412.02482v4","updated":"2025-07-11T13:19:43Z","published":"2024-12-03T14:45:46Z","title":"What should a neuron aim for? Designing local objective functions based\n  on information theory","summary":"  In modern deep neural networks, the learning dynamics of the individual\nneurons is often obscure, as the networks are trained via global optimization.\nConversely, biological systems build on self-organized, local learning,\nachieving robustness and efficiency with limited global information. We here\nshow how self-organization between individual artificial neurons can be\nachieved by designing abstract bio-inspired local learning goals. These goals\nare parameterized using a recent extension of information theory, Partial\nInformation Decomposition (PID), which decomposes the information that a set of\ninformation sources holds about an outcome into unique, redundant and\nsynergistic contributions. Our framework enables neurons to locally shape the\nintegration of information from various input classes, i.e. feedforward,\nfeedback, and lateral, by selecting which of the three inputs should contribute\nuniquely, redundantly or synergistically to the output. This selection is\nexpressed as a weighted sum of PID terms, which, for a given problem, can be\ndirectly derived from intuitive reasoning or via numerical optimization,\noffering a window into understanding task-relevant local information\nprocessing. Achieving neuron-level interpretability while enabling strong\nperformance using local learning, our work advances a principled\ninformation-theoretic foundation for local learning strategies.\n","authors":["Andreas C. Schneider","Valentin Neuhaus","David A. Ehrlich","Abdullah Makkeh","Alexander S. Ecker","Viola Priesemann","Michael Wibral"],"pdf_url":"https://arxiv.org/pdf/2412.02482v4.pdf","comment":"Presented as an oral at ICLR 2025. Conference version:\n  https://openreview.net/forum?id=CLE09ESvul, 24 pages, 11 figures"},{"id":"http://arxiv.org/abs/2507.08567v1","updated":"2025-07-11T13:11:11Z","published":"2025-07-11T13:11:11Z","title":"AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient\n  Sequence Modeling","summary":"  We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a\nnovel recursive generalization of the encoder-only Transformer architecture,\nwhich achieves better perplexity than a standard Transformer and allows for the\ndynamic scaling of compute resources at test time. This simple, recursive\napproach is a complement to scaling large language model (LLM) performance\nthrough parameter and token counts. AbbIE performs its iterations in latent\nspace, but unlike latent reasoning models, does not require a specialized\ndataset or training protocol. We show that AbbIE upward generalizes (ability to\ngeneralize to arbitrary iteration lengths) at test time by only using 2\niterations during train time, far outperforming alternative iterative methods.\nAbbIE's ability to scale its computational expenditure based on the complexity\nof the task gives it an up to \\textbf{12\\%} improvement in zero-shot in-context\nlearning tasks versus other iterative and standard methods and up to 5\\%\nimprovement in language perplexity. The results from this study open a new\navenue to Transformer performance scaling. We perform all of our evaluations on\nmodel sizes up to 350M parameters.\n","authors":["Preslav Aleksandrov","Meghdad Kurmanji","Fernando Garcia Redondo","David O'Shea","William Shen","Alex Iacob","Lorenzo Sani","Xinchi Qiu","Nicola Cancedda","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2507.08567v1.pdf","comment":"14 pages and 6 figures. Submitted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2501.08202v2","updated":"2025-07-11T13:09:59Z","published":"2025-01-14T15:37:03Z","title":"Data-driven system identification using quadratic embeddings of\n  nonlinear dynamics","summary":"  We propose a novel data-driven method called QENDy (Quadratic Embedding of\nNonlinear Dynamics) that not only allows us to learn quadratic representations\nof highly nonlinear dynamical systems, but also to identify the governing\nequations. The approach is based on an embedding of the system into a\nhigher-dimensional feature space in which the dynamics become quadratic. Just\nlike SINDy (Sparse Identification of Nonlinear Dynamics), our method requires\ntrajectory data, time derivatives for the training data points, which can also\nbe estimated using finite difference approximations, and a set of preselected\nbasis functions, called dictionary. We illustrate the efficacy and accuracy of\nQENDy with the aid of various benchmark problems and compare its performance\nwith SINDy and a deep learning method for identifying quadratic embeddings.\nFurthermore, we analyze the convergence of QENDy and SINDy in the infinite data\nlimit, highlight their similarities and main differences, and compare the\nquadratic embedding with linearization techniques based on the Koopman\noperator.\n","authors":["Stefan Klus","Joel-Pascal Ntwali N'konzi"],"pdf_url":"https://arxiv.org/pdf/2501.08202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08563v1","updated":"2025-07-11T13:05:35Z","published":"2025-07-11T13:05:35Z","title":"STRAP: Spatial-Temporal Risk-Attentive Vehicle Trajectory Prediction for\n  Autonomous Driving","summary":"  Accurate vehicle trajectory prediction is essential for ensuring safety and\nefficiency in fully autonomous driving systems. While existing methods\nprimarily focus on modeling observed motion patterns and interactions with\nother vehicles, they often neglect the potential risks posed by the uncertain\nor aggressive behaviors of surrounding vehicles. In this paper, we propose a\nnovel spatial-temporal risk-attentive trajectory prediction framework that\nincorporates a risk potential field to assess perceived risks arising from\nbehaviors of nearby vehicles. The framework leverages a spatial-temporal\nencoder and a risk-attentive feature fusion decoder to embed the risk potential\nfield into the extracted spatial-temporal feature representations for\ntrajectory prediction. A risk-scaled loss function is further designed to\nimprove the prediction accuracy of high-risk scenarios, such as short relative\nspacing. Experiments on the widely used NGSIM and HighD datasets demonstrate\nthat our method reduces average prediction errors by 4.8% and 31.2%\nrespectively compared to state-of-the-art approaches, especially in high-risk\nscenarios. The proposed framework provides interpretable, risk-aware\npredictions, contributing to more robust decision-making for autonomous driving\nsystems.\n","authors":["Xinyi Ning","Zilin Bian","Kaan Ozbay","Semiha Ergan"],"pdf_url":"https://arxiv.org/pdf/2507.08563v1.pdf","comment":"6 pages, 3 figures, accepted at ITSC 2025"},{"id":"http://arxiv.org/abs/2504.10240v3","updated":"2025-07-11T12:58:59Z","published":"2025-04-14T14:02:09Z","title":"GNN-ACLP: Graph Neural Networks Based Analog Circuit Link Prediction","summary":"  Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in automating analog circuit design. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural\nNetworks (GNNs) based framework featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes\nfor Link Prediction) framework and achieve port-level accuracy in circuit link\nprediction. Second, we propose Netlist Babel Fish, a netlist format conversion\ntool leveraging retrieval-augmented generation (RAG) with a large language\nmodel (LLM) to enhance the compatibility of netlist formats. Finally, we\nconstruct SpiceNetlist, a comprehensive dataset that contains 775 annotated\ncircuits across 10 different component classes. Experiments demonstrate\naccuracy improvements of 16.08% on SpiceNetlist, 11.38% on Image2Net, and\n16.01% on Masala-CHAI compared to the baseline in intra-dataset evaluation,\nwhile maintaining accuracy from 92.05% to 99.07% in cross-dataset evaluation,\nexhibiting robust feature transfer capabilities.\n","authors":["Guanyuan Pan","Tiansheng Zhou","Bingtao Ma","Yaqi Wang","Jianxiang Zhao","Zhi Li","Yugui Lin","Pietro Lio","Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10240v3.pdf","comment":"Code and data will be made available on request. V3 Update: Add\n  Ablation Study and Discussion; Improve Introduction; Optimize Figures; Add\n  references"},{"id":"http://arxiv.org/abs/2503.04518v2","updated":"2025-07-11T12:53:49Z","published":"2025-03-06T15:06:01Z","title":"Leveraging priors on distribution functions for multi-arm bandits","summary":"  We introduce Dirichlet Process Posterior Sampling (DPPS), a Bayesian\nnon-parametric algorithm for multi-arm bandits based on Dirichlet Process (DP)\npriors. Like Thompson-sampling, DPPS is a probability-matching algorithm, i.e.,\nit plays an arm based on its posterior-probability of being optimal. Instead of\nassuming a parametric class for the reward generating distribution of each arm,\nand then putting a prior on the parameters, in DPPS the reward generating\ndistribution is directly modeled using DP priors. DPPS provides a principled\napproach to incorporate prior belief about the bandit environment, and in the\nnoninformative limit of the DP posteriors (i.e. Bayesian Bootstrap), we recover\nNon Parametric Thompson Sampling (NPTS), a popular non-parametric bandit\nalgorithm, as a special case of DPPS. We employ stick-breaking representation\nof the DP priors, and show excellent empirical performance of DPPS in\nchallenging synthetic and real world bandit environments. Finally, using an\ninformation-theoretic analysis, we show non-asymptotic optimality of DPPS in\nthe Bayesian regret setup.\n","authors":["Sumit Vashishtha","Odalric-Ambrym Maillard"],"pdf_url":"https://arxiv.org/pdf/2503.04518v2.pdf","comment":"Camera ready version -- Reinforcement Learning Journal, 2025"},{"id":"http://arxiv.org/abs/2507.08548v1","updated":"2025-07-11T12:53:19Z","published":"2025-07-11T12:53:19Z","title":"SAM2RL: Towards Reinforcement Learning Memory Control in Segment\n  Anything Model 2","summary":"  Segment Anything Model 2 (SAM 2) has demonstrated strong performance in\nobject segmentation tasks and has become the state-of-the-art for visual object\ntracking. The model stores information from previous frames in a memory bank,\nenabling temporal consistency across video sequences. Recent methods augment\nSAM 2 with hand-crafted update rules to better handle distractors, occlusions,\nand object motion. We propose a fundamentally different approach using\nreinforcement learning for optimizing memory updates in SAM 2 by framing memory\ncontrol as a sequential decision-making problem. In an overfitting setup with a\nseparate agent per video, our method achieves a relative improvement over SAM 2\nthat exceeds by more than three times the gains of existing heuristics. These\nresults reveal the untapped potential of the memory bank and highlight\nreinforcement learning as a powerful alternative to hand-crafted update rules\nfor memory control in visual object tracking.\n","authors":["Alen Adamyan","Tomáš Čížek","Matej Straka","Klara Janouskova","Martin Schmid"],"pdf_url":"https://arxiv.org/pdf/2507.08548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08543v1","updated":"2025-07-11T12:43:58Z","published":"2025-07-11T12:43:58Z","title":"Quantum Algorithms for Projection-Free Sparse Convex Optimization","summary":"  This paper considers the projection-free sparse convex optimization problem\nfor the vector domain and the matrix domain, which covers a large number of\nimportant applications in machine learning and data science. For the vector\ndomain $\\mathcal{D} \\subset \\mathbb{R}^d$, we propose two quantum algorithms\nfor sparse constraints that finds a $\\varepsilon$-optimal solution with the\nquery complexity of $O(\\sqrt{d}/\\varepsilon)$ and $O(1/\\varepsilon)$ by using\nthe function value oracle, reducing a factor of $O(\\sqrt{d})$ and $O(d)$ over\nthe best classical algorithm, respectively, where $d$ is the dimension. For the\nmatrix domain $\\mathcal{D} \\subset \\mathbb{R}^{d\\times d}$, we propose two\nquantum algorithms for nuclear norm constraints that improve the time\ncomplexity to $\\tilde{O}(rd/\\varepsilon^2)$ and\n$\\tilde{O}(\\sqrt{r}d/\\varepsilon^3)$ for computing the update step, reducing at\nleast a factor of $O(\\sqrt{d})$ over the best classical algorithm, where $r$ is\nthe rank of the gradient matrix. Our algorithms show quantum advantages in\nprojection-free sparse convex optimization problems as they outperform the\noptimal classical methods in dependence on the dimension $d$.\n","authors":["Jianhao He","John C. S. Lui"],"pdf_url":"https://arxiv.org/pdf/2507.08543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08542v1","updated":"2025-07-11T12:43:17Z","published":"2025-07-11T12:43:17Z","title":"CircFormerMoE: An End-to-End Deep Learning Framework for Circular RNA\n  Splice Site Detection and Pairing in Plant Genomes","summary":"  Circular RNAs (circRNAs) are important components of the non-coding RNA\nregulatory network. Previous circRNA identification primarily relies on\nhigh-throughput RNA sequencing (RNA-seq) data combined with alignment-based\nalgorithms that detect back-splicing signals. However, these methods face\nseveral limitations: they can't predict circRNAs directly from genomic DNA\nsequences and relies heavily on RNA experimental data; they involve high\ncomputational costs due to complex alignment and filtering steps; and they are\ninefficient for large-scale or genome-wide circRNA prediction. The challenge is\neven greater in plants, where plant circRNA splice sites often lack the\ncanonical GT-AG motif seen in human mRNA splicing, and no efficient deep\nlearning model with strong generalization capability currently exists.\nFurthermore, the number of currently identified plant circRNAs is likely far\nlower than their true abundance. In this paper, we propose a deep learning\nframework named CircFormerMoE based on transformers and mixture-of experts for\npredicting circRNAs directly from plant genomic DNA. Our framework consists of\ntwo subtasks known as splicing site detection (SSD) and splicing site pairing\n(SSP). The model's effectiveness has been validated on gene data of 10 plant\nspecies. Trained on known circRNA instances, it is also capable of discovering\npreviously unannotated circRNAs. In addition, we performed interpretability\nanalyses on the trained model to investigate the sequence patterns contributing\nto its predictions. Our framework provides a fast and accurate computational\nmethod and tool for large-scale circRNA discovery in plants, laying a\nfoundation for future research in plant functional genomics and non-coding RNA\nannotation.\n","authors":["Tianyou Jiang"],"pdf_url":"https://arxiv.org/pdf/2507.08542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08537v1","updated":"2025-07-11T12:37:20Z","published":"2025-07-11T12:37:20Z","title":"Recursive Reward Aggregation","summary":"  In reinforcement learning (RL), aligning agent behavior with specific\nobjectives typically requires careful design of the reward function, which can\nbe challenging when the desired objectives are complex. In this work, we\npropose an alternative approach for flexible behavior alignment that eliminates\nthe need to modify the reward function by selecting appropriate reward\naggregation functions. By introducing an algebraic perspective on Markov\ndecision processes (MDPs), we show that the Bellman equations naturally emerge\nfrom the recursive generation and aggregation of rewards, allowing for the\ngeneralization of the standard discounted sum to other recursive aggregations,\nsuch as discounted max and Sharpe ratio. Our approach applies to both\ndeterministic and stochastic settings and integrates seamlessly with\nvalue-based and actor-critic algorithms. Experimental results demonstrate that\nour approach effectively optimizes diverse objectives, highlighting its\nversatility and potential for real-world applications.\n","authors":["Yuting Tang","Yivan Zhang","Johannes Ackermann","Yu-Jie Zhang","Soichiro Nishimori","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2507.08537v1.pdf","comment":"Reinforcement Learning Conference 2025"},{"id":"http://arxiv.org/abs/2503.02870v3","updated":"2025-07-11T12:37:16Z","published":"2025-03-04T18:47:54Z","title":"Multiaccuracy and Multicalibration via Proxy Groups","summary":"  As the use of predictive machine learning algorithms increases in high-stakes\ndecision-making, it is imperative that these algorithms are fair across\nsensitive groups. However, measuring and enforcing fairness in real-world\napplications can be challenging due to the missing or incomplete sensitive\ngroup information. Proxy-sensitive attributes have been proposed as a practical\nand effective solution in these settings, but only for parity-based fairness\nnotions. Knowing how to evaluate and control for fairness with missing\nsensitive group data for newer, different, and more flexible frameworks, such\nas multiaccuracy and multicalibration, remain unexplored. In this work, we\naddress this gap by demonstrating that in the absence of sensitive group data,\nproxy-sensitive attributes can provably used to derive actionable upper bounds\non the true multiaccuracy and multicalibration violations, providing insights\ninto a predictive model's potential worst-case fairness violations.\nAdditionally, we show that adjusting models to satisfy multiaccuracy and\nmulticalibration across proxy-sensitive attributes can significantly mitigate\nthese violations for the true, but unknown, sensitive groups. Through several\nexperiments on real-world datasets, we illustrate that approximate\nmultiaccuracy and multicalibration can be achieved even when sensitive group\ndata is incomplete or unavailable.\n","authors":["Beepul Bharti","Mary Versa Clemens-Sewall","Paul H. Yi","Jeremias Sulam"],"pdf_url":"https://arxiv.org/pdf/2503.02870v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13792v2","updated":"2025-07-11T12:23:54Z","published":"2025-04-18T16:44:12Z","title":"Binary and Ternary Quantization Can Enhance Feature Discrimination","summary":"  Quantization is widely applied in machine learning to reduce computational\nand storage costs for both data and models. Considering that classification\ntasks are fundamental to the field, it is crucial to investigate how\nquantization impacts classification performance. Traditional research has\nfocused on quantization errors, assuming that larger errors generally lead to\nlower classification accuracy. However, this assumption lacks a solid\ntheoretical foundation and often contradicts empirical observations. For\nexample, despite introducing significant errors, $\\{0,1\\}$-binary and $\\{0,\n\\pm1\\}$-ternary quantized data have sometimes achieved classification accuracy\ncomparable or even superior to full-precision data. To reasonably explain this\nphenomenon, a more accurate evaluation of classification performance is\nrequired. To achieve this, we propose a direct analysis of the feature\ndiscrimination of quantized data, instead of focusing on quantization errors.\nOur analysis reveals that both binary and ternary quantization can potentially\nenhance, rather than degrade, the feature discrimination of the original data.\nThis finding is supported by classification experiments conducted on both\nsynthetic and real data.\n","authors":["Weizhi Lu","Mingrui Chen","Weiyu Li"],"pdf_url":"https://arxiv.org/pdf/2504.13792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17546v3","updated":"2025-07-11T12:23:34Z","published":"2025-03-21T21:41:48Z","title":"Communities in the Kuramoto Model: Dynamics and Detection via Path\n  Signatures","summary":"  The behavior of multivariate dynamical processes is often governed by\nunderlying structural connections that relate the components of the system. For\nexample, brain activity, which is often measured via time series is determined\nby an underlying structural graph, where nodes represent neurons or brain\nregions and edges cortical connectivity. Existing methods for inferring\nstructural connections from observed dynamics, such as correlation-based or\nspectral techniques, may fail to fully capture complex relationships in\nhigh-dimensional time series in an interpretable way. Here, we propose the use\nof path signatures, a mathematical framework that encodes geometric and\ntemporal properties of continuous paths, to address this problem. Path\nsignatures provide a reparametrization-invariant characterization of dynamical\ndata and can be used to compute the lead matrix, which reveals lead-lag\nphenomena. We showcase our approach on time series from coupled oscillators in\nthe Kuramoto model defined on a stochastic block model graph, termed the\nKuramoto Stochastic Block Model (KSBM). Using mean-field theory and Gaussian\napproximations, we analytically derive reduced models of KSBM dynamics in\ndifferent temporal regimes and theoretically characterize the lead matrix in\nthese settings. Leveraging these insights, we propose a novel signature-based\ncommunity detection algorithm, achieving exact recovery of structural\ncommunities from observed time series in multiple KSBM instances. We also\nexplored the performance of our community detection on a stochastic variant of\nthe KSBM as well as on real neuropixels of cortical recordings to demonstrate\napplicability on real-world data. Our results demonstrate that path signatures\nprovide a novel perspective on analyzing complex neural data and other\nhigh-dimensional systems, explicitly exploiting temporal functional\nrelationships to infer underlying structure.\n","authors":["Tâm Johan Nguyên","Darrick Lee","Bernadette Jana Stolz"],"pdf_url":"https://arxiv.org/pdf/2503.17546v3.pdf","comment":"56 pages, 21 figures"},{"id":"http://arxiv.org/abs/2503.11924v2","updated":"2025-07-11T12:13:04Z","published":"2025-03-14T23:47:46Z","title":"REGEN: A Dataset and Benchmarks with Natural Language Critiques and\n  Narratives","summary":"  This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative\nNarratives), designed to benchmark the conversational capabilities of\nrecommender Large Language Models (LLMs), addressing the limitations of\nexisting datasets that primarily focus on sequential item prediction. REGEN\nextends the Amazon Product Reviews dataset by inpainting two key natural\nlanguage features: (1) user critiques, representing user \"steering\" queries\nthat lead to the selection of a subsequent item, and (2) narratives, rich\ntextual outputs associated with each recommended item taking into account prior\ncontext. The narratives include product endorsements, purchase explanations,\nand summaries of user preferences.\n  Further, we establish an end-to-end modeling benchmark for the task of\nconversational recommendation, where models are trained to generate both\nrecommendations and corresponding narratives conditioned on user history (items\nand critiques). For this joint task, we introduce a modeling framework LUMEN\n(LLM-based Unified Multi-task Model with Critiques, Recommendations, and\nNarratives) which uses an LLM as a backbone for critiquing, retrieval and\ngeneration. We also evaluate the dataset's quality using standard auto-rating\ntechniques and benchmark it by training both traditional and LLM-based\nrecommender models. Our results demonstrate that incorporating critiques\nenhances recommendation quality by enabling the recommender to learn language\nunderstanding and integrate it with recommendation signals. Furthermore, LLMs\ntrained on our dataset effectively generate both recommendations and contextual\nnarratives, achieving performance comparable to state-of-the-art recommenders\nand language models.\n","authors":["Kun Su","Krishna Sayana","Hubert Pham","James Pine","Yuri Vasilevski","Raghavendra Vasudeva","Marialena Kyriakidi","Liam Hebert","Ambarish Jash","Anushya Subbiah","Sukhdeep Sodhi"],"pdf_url":"https://arxiv.org/pdf/2503.11924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08518v1","updated":"2025-07-11T12:05:03Z","published":"2025-07-11T12:05:03Z","title":"Data Depth as a Risk","summary":"  Data depths are score functions that quantify in an unsupervised fashion how\ncentral is a point inside a distribution, with numerous applications such as\nanomaly detection, multivariate or functional data analysis, arising across\nvarious fields. The halfspace depth was the first depth to aim at generalising\nthe notion of quantile beyond the univariate case. Among the existing variety\nof depth definitions, it remains one of the most used notions of data depth.\nTaking a different angle from the quantile point of view, we show that the\nhalfspace depth can also be regarded as the minimum loss of a set of\nclassifiers for a specific labelling of the points. By changing the loss or the\nset of classifiers considered, this new angle naturally leads to a family of\n\"loss depths\", extending to well-studied classifiers such as, e.g., SVM or\nlogistic regression, among others. This framework directly inherits\ncomputational efficiency of existing machine learning algorithms as well as\ntheir fast statistical convergence rates, and opens the data depth realm to the\nhigh-dimensional setting. Furthermore, the new loss depths highlight a\nconnection between the dataset and the right amount of complexity or simplicity\nof the classifiers. The simplicity of classifiers as well as the interpretation\nas a risk makes our new kind of data depth easy to explain, yet efficient for\nanomaly detection, as is shown by experiments.\n","authors":["Arturo Castellanos","Pavlo Mozharovskyi"],"pdf_url":"https://arxiv.org/pdf/2507.08518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08508v1","updated":"2025-07-11T11:48:11Z","published":"2025-07-11T11:48:11Z","title":"SFedKD: Sequential Federated Learning with Discrepancy-Aware\n  Multi-Teacher Knowledge Distillation","summary":"  Federated Learning (FL) is a distributed machine learning paradigm which\ncoordinates multiple clients to collaboratively train a global model via a\ncentral server. Sequential Federated Learning (SFL) is a newly-emerging FL\ntraining framework where the global model is trained in a sequential manner\nacross clients. Since SFL can provide strong convergence guarantees under data\nheterogeneity, it has attracted significant research attention in recent years.\nHowever, experiments show that SFL suffers from severe catastrophic forgetting\nin heterogeneous environments, meaning that the model tends to forget knowledge\nlearned from previous clients. To address this issue, we propose an SFL\nframework with discrepancy-aware multi-teacher knowledge distillation, called\nSFedKD, which selects multiple models from the previous round to guide the\ncurrent round of training. In SFedKD, we extend the single-teacher Decoupled\nKnowledge Distillation approach to our multi-teacher setting and assign\ndistinct weights to teachers' target-class and non-target-class knowledge based\non the class distributional discrepancy between teacher and student data.\nThrough this fine-grained weighting strategy, SFedKD can enhance model training\nefficacy while mitigating catastrophic forgetting. Additionally, to prevent\nknowledge dilution, we eliminate redundant teachers for the knowledge\ndistillation and formalize it as a variant of the maximum coverage problem.\nBased on the greedy strategy, we design a complementary-based teacher selection\nmechanism to ensure that the selected teachers achieve comprehensive knowledge\nspace coverage while reducing communication and computational costs. Extensive\nexperiments show that SFedKD effectively overcomes catastrophic forgetting in\nSFL and outperforms state-of-the-art FL methods.\n","authors":["Haotian Xu","Jinrui Zhou","Xichong Zhang","Mingjun Xiao","He Sun","Yin Xu"],"pdf_url":"https://arxiv.org/pdf/2507.08508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18246v2","updated":"2025-07-11T11:43:08Z","published":"2025-04-25T10:46:56Z","title":"One-Pass to Reason: Token Duplication and Block-Sparse Mask for\n  Efficient Fine-Tuning on Multi-Turn Reasoning","summary":"  Fine-tuning Large Language Models (LLMs) on multi-turn reasoning datasets\nrequires N (number of turns) separate forward passes per conversation due to\nreasoning token visibility constraints, as reasoning tokens for a turn are\ndiscarded in subsequent turns. We propose duplicating response tokens along\nwith a custom attention mask to enable single-pass processing of entire\nconversations. We prove our method produces identical losses to the N-pass\napproach while reducing time complexity from $O\\bigl(N^{3}\\bigl)$ to\n$O\\bigl(N^{2}\\bigl)$ and maintaining the same memory complexity for a\ntransformer based model. Our approach achieves significant training speedup\nwhile preserving accuracy. Our implementation is available online\n(https://github.com/devrev/One-Pass-to-Reason).\n","authors":["Ritesh Goru","Shanay Mehta","Prateek Jain"],"pdf_url":"https://arxiv.org/pdf/2504.18246v2.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.08505v1","updated":"2025-07-11T11:30:57Z","published":"2025-07-11T11:30:57Z","title":"Efficient Deployment of Vision-Language Models on Mobile Devices: A Case\n  Study on OnePlus 13R","summary":"  Vision-Language Models (VLMs) offer promising capabilities for mobile\ndevices, but their deployment faces significant challenges due to computational\nlimitations and energy inefficiency, especially for real-time applications.\nThis study provides a comprehensive survey of deployment frameworks for VLMs on\nmobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of\nrunning LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads\non a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R\nwhile running VLMs, with measurements covering CPU, GPU, and NPU utilization,\ntemperature, inference time, power consumption, and user experience.\nBenchmarking revealed critical performance bottlenecks across frameworks: CPU\nresources were consistently over-utilized during token generation, while GPU\nand NPU accelerators were largely unused. When the GPU was used, primarily for\nimage feature extraction, it was saturated, leading to degraded device\nresponsiveness. The study contributes framework-level benchmarks, practical\nprofiling tools, and an in-depth analysis of hardware utilization bottlenecks,\nhighlighting the consistent overuse of CPUs and the ineffective or unstable use\nof GPUs and NPUs in current deployment frameworks.\n","authors":["Pablo Robin Guerrero","Yueyang Pan","Sanidhya Kashyap"],"pdf_url":"https://arxiv.org/pdf/2507.08505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08033v3","updated":"2025-07-11T10:50:22Z","published":"2025-06-02T14:06:44Z","title":"Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D\n  Furnaces with Spectrally Participative Gases","summary":"  Aiming to reduce the computational cost of numerical simulations, a\nconvolutional neural network (CNN) and a multi-layer perceptron (MLP) are\nintroduced to build a surrogate model to approximate radiative heat transfer\nsolutions in a 2-D walled domain with participative gases. The originality of\nthis work lays in the adaptation of the inputs of the problem (gas and wall\nproperties) in order to fit with the CNN architecture, more commonly used for\nimage processing. Two precision datasets have been created with the classical\nsolver, ICARUS2D, that uses the discrete transfer radiation method with the\nstatistical narrow bands model. The performance of the CNN architecture is\ncompared to a more classical MLP architecture in terms of speed and accuracy.\nThanks to Optuna, all results are obtained using the optimized hyper parameters\nnetworks. The results show a significant speedup with industrially acceptable\nrelative errors compared to the classical solver for both architectures.\nAdditionally, the CNN outperforms the MLP in terms of precision and is more\nrobust and stable to changes in hyper-parameters. A performance analysis on the\ndataset size of the samples have also been carried out to gain a deeper\nunderstanding of the model behavior.\n","authors":["Axel TahmasebiMoradi","Vincent Ren","Benjamin Le-Creurer","Chetra Mang","Mouadh Yagoubi"],"pdf_url":"https://arxiv.org/pdf/2506.08033v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08475v1","updated":"2025-07-11T10:35:13Z","published":"2025-07-11T10:35:13Z","title":"SynBridge: Bridging Reaction States via Discrete Flow for Bidirectional\n  Reaction Prediction","summary":"  The essence of a chemical reaction lies in the redistribution and\nreorganization of electrons, which is often manifested through electron\ntransfer or the migration of electron pairs. These changes are inherently\ndiscrete and abrupt in the physical world, such as alterations in the charge\nstates of atoms or the formation and breaking of chemical bonds. To model the\ntransition of states, we propose SynBridge, a bidirectional flow-based\ngenerative model to achieve multi-task reaction prediction. By leveraging a\ngraph-to-graph transformer network architecture and discrete flow bridges\nbetween any two discrete distributions, SynBridge captures bidirectional\nchemical transformations between graphs of reactants and products through the\nbonds' and atoms' discrete states. We further demonstrate the effectiveness of\nour method through extensive experiments on three benchmark datasets\n(USPTO-50K, USPTO-MIT, Pistachio), achieving state-of-the-art performance in\nboth forward and retrosynthesis tasks. Our ablation studies and noise\nscheduling analysis reveal the benefits of structured diffusion over discrete\nspaces for reaction prediction.\n","authors":["Haitao Lin","Junjie Wang","Zhifeng Gao","Xiaohong Ji","Rong Zhu","Linfeng Zhang","Guolin Ke","Weinan E"],"pdf_url":"https://arxiv.org/pdf/2507.08475v1.pdf","comment":"22pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.06892v3","updated":"2025-07-11T10:32:34Z","published":"2025-07-09T14:29:45Z","title":"Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model","summary":"  Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.\n","authors":["Jing Liang","Hongyao Tang","Yi Ma","Jinyi Liu","Yan Zheng","Shuyue Hu","Lei Bai","Jianye Hao"],"pdf_url":"https://arxiv.org/pdf/2507.06892v3.pdf","comment":"Preliminary version, v3, added the missing name of x-axis in the left\n  part of Fig.1 and corrected a wrong number in Fig.3. Project page:\n  https://anitaleungxx.github.io/ReMix"},{"id":"http://arxiv.org/abs/2507.08473v1","updated":"2025-07-11T10:31:53Z","published":"2025-07-11T10:31:53Z","title":"Evaluating SAE interpretability without explanations","summary":"  Sparse autoencoders (SAEs) and transcoders have become important tools for\nmachine learning interpretability. However, measuring how interpretable they\nare remains challenging, with weak consensus about which benchmarks to use.\nMost evaluation procedures start by producing a single-sentence explanation for\neach latent. These explanations are then evaluated based on how well they\nenable an LLM to predict the activation of a latent in new contexts. This\nmethod makes it difficult to disentangle the explanation generation and\nevaluation process from the actual interpretability of the latents discovered.\nIn this work, we adapt existing methods to assess the interpretability of\nsparse coders, with the advantage that they do not require generating natural\nlanguage explanations as an intermediate step. This enables a more direct and\npotentially standardized assessment of interpretability. Furthermore, we\ncompare the scores produced by our interpretability metrics with human\nevaluations across similar tasks and varying setups, offering suggestions for\nthe community on improving the evaluation of these techniques.\n","authors":["Gonçalo Paulo","Nora Belrose"],"pdf_url":"https://arxiv.org/pdf/2507.08473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04196v2","updated":"2025-07-11T10:30:18Z","published":"2025-07-06T00:11:11Z","title":"Predicting Air Pollution in Cork, Ireland Using Machine Learning","summary":"  Air pollution poses a critical health threat in cities worldwide, with\nnitrogen dioxide levels in Cork, Ireland exceeding World Health Organization\nsafety standards by up to $278\\%$. This study leverages artificial intelligence\nto predict air pollution with unprecedented accuracy, analyzing nearly ten\nyears of data from five monitoring stations combined with 30 years of weather\nrecords. We evaluated 17 machine learning algorithms, with Extra Trees emerging\nas the optimal solution, achieving $77\\%$ prediction accuracy and significantly\noutperforming traditional forecasting methods. Our analysis reveals that\nmeteorological conditions particularly temperature, wind speed, and humidity\nare the primary drivers of pollution levels, while traffic patterns and\nseasonal changes create predictable pollution cycles. Pollution exhibits\ndramatic seasonal variations, with winter levels nearly double those of summer,\nand daily rush-hour peaks reaching $120\\%$ above normal levels. While Cork's\nair quality shows concerning violations of global health standards, our models\ndetected an encouraging $31\\%$ improvement from 2014 to 2022. This research\ndemonstrates that intelligent forecasting systems can provide city planners and\nenvironmental officials with powerful prediction tools, enabling life-saving\nearly warning systems and informed urban planning decisions. The technology\nexists today to transform urban air quality management. All research materials\nand code are freely available at:\nhttps://github.com/MdRashidunnabi/Air-Pollution-Analysis.git\n","authors":["Md Rashidunnabi","Fahmida Faiza Ananna","Kailash Hambarde","Bruno Gabriel Nascimento Andrade","Dean Venables","Hugo Proenca"],"pdf_url":"https://arxiv.org/pdf/2507.04196v2.pdf","comment":"The draft was submitted prematurely and requires further analysis,\n  added research findings, and corrected references. Some co-authors have not\n  yet approved this version. I will ensure all necessary revisions and\n  approvals before resubmitting"},{"id":"http://arxiv.org/abs/2507.07532v2","updated":"2025-07-11T10:29:39Z","published":"2025-07-10T08:28:46Z","title":"Neural Concept Verifier: Scaling Prover-Verifier Games via Concept\n  Encodings","summary":"  While Prover-Verifier Games (PVGs) offer a promising path toward\nverifiability in nonlinear classification models, they have not yet been\napplied to complex inputs such as high-dimensional images. Conversely, Concept\nBottleneck Models (CBMs) effectively translate such data into interpretable\nconcepts but are limited by their reliance on low-capacity linear predictors.\nIn this work, we introduce the Neural Concept Verifier (NCV), a unified\nframework combining PVGs with concept encodings for interpretable, nonlinear\nclassification in high-dimensional settings. NCV achieves this by utilizing\nrecent minimally supervised concept discovery models to extract structured\nconcept encodings from raw inputs. A prover then selects a subset of these\nencodings, which a verifier -- implemented as a nonlinear predictor -- uses\nexclusively for decision-making. Our evaluations show that NCV outperforms CBM\nand pixel-based PVG classifier baselines on high-dimensional, logically complex\ndatasets and also helps mitigate shortcut behavior. Overall, we demonstrate NCV\nas a promising step toward performative, verifiable AI.\n","authors":["Berkant Turan","Suhrab Asadulla","David Steinmann","Wolfgang Stammer","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2507.07532v2.pdf","comment":"16 pages, 4 figures, 8 tables, revised references"},{"id":"http://arxiv.org/abs/2507.08472v1","updated":"2025-07-11T10:29:04Z","published":"2025-07-11T10:29:04Z","title":"Pre-Training LLMs on a budget: A comparison of three optimizers","summary":"  Optimizers play a decisive role in reducing pre-training times for LLMs and\nachieving better-performing models. In this study, we compare three major\nvariants: the de-facto standard AdamW, the simpler Lion, developed through an\nevolutionary search, and the second-order optimizer Sophia. For better\ngeneralization, we train with two different base architectures and use a\nsingle- and a multiple-epoch approach while keeping the number of tokens\nconstant. Using the Maximal Update Parametrization and smaller proxy models, we\ntune relevant hyperparameters separately for each combination of base\narchitecture and optimizer. We found that while the results from all three\noptimizers were in approximately the same range, Sophia exhibited the lowest\ntraining and validation loss, Lion was fastest in terms of training GPU hours\nbut AdamW led to the best downstream evaluation results.\n","authors":["Joel Schlotthauer","Christian Kroos","Chris Hinze","Viktor Hangya","Luzian Hahn","Fabian Küch"],"pdf_url":"https://arxiv.org/pdf/2507.08472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.21940v3","updated":"2025-07-11T10:21:05Z","published":"2025-06-27T06:30:33Z","title":"Sculpting Quantum Landscapes: Fubini-Study Metric Conditioning for\n  Geometry Aware Learning in Parameterized Quantum Circuits","summary":"  We present a novel meta learning framework called Sculpture that explicitly\nconditions the Fubini Study metric tensor of parameterized quantum circuits to\nmitigate barren plateaus in variational quantum algorithms. Our theoretical\nanalysis identifies the logarithmic condition number of the Fubini Study metric\nas a critical geometric quantity governing trainability, optimization dynamics,\nand generalization. Sculpture uses a classical meta model trained to generate\ndata dependent quantum circuit initializations that minimize the logarithmic\ncondition number, thereby promoting an isotropic and well conditioned parameter\nspace.\n  Empirical results show that meta training reduces the logarithmic condition\nnumber from approximately 1.47 to 0.64 by significantly increasing the minimum\neigenvalue and slightly decreasing the maximum eigenvalue of the metric,\neffectively alleviating barren plateaus. This improved conditioning generalizes\nwell to unseen data, consistently producing well conditioned quantum circuit\ninitializations. In a downstream hybrid quantum classical classification task\non the Kaggle diabetes dataset, increasing the meta scaling coefficient\naccelerates convergence, reduces training loss and gradient norms, and\ncrucially improves generalization, with test accuracy increasing from about\n0.68 to over 0.78. These findings demonstrate that sculpting the quantum\nlandscape via meta learning serves as a principled geometric regularizer,\nsubstantially enhancing trainability, optimization, and generalization of\nparameterized quantum circuits and enabling more robust and efficient\nvariational quantum algorithms.\n","authors":["Marwan Ait Haddou","Mohamed Bennai"],"pdf_url":"https://arxiv.org/pdf/2506.21940v3.pdf","comment":"Need more analysis"},{"id":"http://arxiv.org/abs/2507.08465v1","updated":"2025-07-11T10:17:58Z","published":"2025-07-11T10:17:58Z","title":"Ranked Set Sampling-Based Multilayer Perceptron: Improving\n  Generalization via Variance-Based Bounds","summary":"  Multilayer perceptron (MLP), one of the most fundamental neural networks, is\nextensively utilized for classification and regression tasks. In this paper, we\nestablish a new generalization error bound, which reveals how the variance of\nempirical loss influences the generalization ability of the learning model.\nInspired by this learning bound, we advocate to reduce the variance of\nempirical loss to enhance the ability of MLP. As is well-known, bagging is a\npopular ensemble method to realize variance reduction. However, bagging\nproduces the base training data sets by the Simple Random Sampling (SRS)\nmethod, which exhibits a high degree of randomness. To handle this issue, we\nintroduce an ordered structure in the training data set by Rank Set Sampling\n(RSS) to further reduce the variance of loss and develop a RSS-MLP method.\nTheoretical results show that the variance of empirical exponential loss and\nthe logistic loss estimated by RSS are smaller than those estimated by SRS,\nrespectively. To validate the performance of RSS-MLP, we conduct comparison\nexperiments on twelve benchmark data sets in terms of the two convex loss\nfunctions under two fusion methods. Extensive experimental results and analysis\nillustrate the effectiveness and rationality of the propose method.\n","authors":["Feijiang Li","Liuya Zhang","Jieting Wang","Tao Yan","Yuhua Qian"],"pdf_url":"https://arxiv.org/pdf/2507.08465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10381v3","updated":"2025-07-11T10:07:47Z","published":"2024-10-14T11:10:15Z","title":"Collaborative filtering based on nonnegative/binary matrix factorization","summary":"  Collaborative filtering generates recommendations by exploiting user-item\nsimilarities based on rating data, which often contains numerous unrated items.\nThis paper proposes a nonnegative/binary matrix factorization (NBMF) algorithm\nmodified for collaborative filtering and demonstrates that utilizing a\nlow-latency Ising machine in NBMF is advantageous in terms of computation time.\nWhile previous studies have primarily applied NBMF to dense data, such as\nimages, this study applies a modified NBMF to sparse data. Results show the\nbenefits of using a low-latency Ising machine to implement the proposed method.\n","authors":["Yukino Terui","Yuka Inoue","Yohei Hamakawa","Kosuke Tatsumura","Kazue Kudo"],"pdf_url":"https://arxiv.org/pdf/2410.10381v3.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.08456v1","updated":"2025-07-11T09:56:15Z","published":"2025-07-11T09:56:15Z","title":"Space filling positionality and the Spiroformer","summary":"  Transformers excel when dealing with sequential data. Generalizing\ntransformer models to geometric domains, such as manifolds, we encounter the\nproblem of not having a well-defined global order. We propose a solution with\nattention heads following a space-filling curve. As a first experimental\nexample, we present the Spiroformer, a transformer that follows a polar spiral\non the $2$-sphere.\n","authors":["M. Maurin","M. Á. Evangelista-Alvarado","P. Suárez-Serrato"],"pdf_url":"https://arxiv.org/pdf/2507.08456v1.pdf","comment":"9 pages, 5 figures. To appear in Geometric Science of Information\n  2025"},{"id":"http://arxiv.org/abs/2507.08454v1","updated":"2025-07-11T09:55:04Z","published":"2025-07-11T09:55:04Z","title":"Why this and not that? A Logic-based Framework for Contrastive\n  Explanations","summary":"  We define several canonical problems related to contrastive explanations,\neach answering a question of the form ''Why P but not Q?''. The problems\ncompute causes for both P and Q, explicitly comparing their differences. We\ninvestigate the basic properties of our definitions in the setting of\npropositional logic. We show, inter alia, that our framework captures a\ncardinality-minimal version of existing contrastive explanations in the\nliterature. Furthermore, we provide an extensive analysis of the computational\ncomplexities of the problems. We also implement the problems for CNF-formulas\nusing answer set programming and present several examples demonstrating how\nthey work in practice.\n","authors":["Tobias Geibinger","Reijo Jaakkola","Antti Kuusisto","Xinghan Liu","Miikka Vilander"],"pdf_url":"https://arxiv.org/pdf/2507.08454v1.pdf","comment":"20 pages, accepted to JELIA 2025"},{"id":"http://arxiv.org/abs/2502.02367v2","updated":"2025-07-11T09:36:28Z","published":"2025-02-04T14:50:16Z","title":"Field Matching: an Electrostatic Paradigm to Generate and Transfer Data","summary":"  We propose Electrostatic Field Matching (EFM), a novel method that is\nsuitable for both generative modeling and distribution transfer tasks. Our\napproach is inspired by the physics of an electrical capacitor. We place source\nand target distributions on the capacitor plates and assign them positive and\nnegative charges, respectively. We then learn the electrostatic field of the\ncapacitor using a neural network approximator. To map the distributions to each\nother, we start at one plate of the capacitor and move the samples along the\nlearned electrostatic field lines until they reach the other plate. We\ntheoretically justify that this approach provably yields the distribution\ntransfer. In practice, we demonstrate the performance of our EFM in toy and\nimage data experiments.\n","authors":["Alexander Kolesov","Manukhov Stepan","Vladimir V. Palyulin","Alexander Korotin"],"pdf_url":"https://arxiv.org/pdf/2502.02367v2.pdf","comment":"Proceedings of the 42nd International Conference on Machine.\n  Learning, Vancouver, Canada. PMLR 267, 2025"},{"id":"http://arxiv.org/abs/2403.06759v3","updated":"2025-07-11T09:36:10Z","published":"2024-03-11T14:31:03Z","title":"Average Calibration Error: A Differentiable Loss for Improved\n  Reliability in Image Segmentation","summary":"  Deep neural networks for medical image segmentation often produce\noverconfident results misaligned with empirical observations. Such\nmiscalibration, challenges their clinical translation. We propose to use\nmarginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss\nfunction to improve pixel-wise calibration without compromising segmentation\nquality. We show that this loss, despite using hard binning, is directly\ndifferentiable, bypassing the need for approximate but differentiable surrogate\nor soft binning approaches. Our work also introduces the concept of dataset\nreliability histograms which generalises standard reliability diagrams for\nrefined visual assessment of calibration in semantic segmentation aggregated at\nthe dataset level. Using mL1-ACE, we reduce average and maximum calibration\nerror by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS\n2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS\n","authors":["Theodore Barfoot","Luis Garcia-Peraza-Herrera","Ben Glocker","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2403.06759v3.pdf","comment":"accidental replacement intended for arXiv:2506.03942"},{"id":"http://arxiv.org/abs/2507.08443v1","updated":"2025-07-11T09:35:13Z","published":"2025-07-11T09:35:13Z","title":"KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge\n  Graph-based Perturbations","summary":"  Retrieval-Augmented Generation (RAG) enhances language models by grounding\nresponses in external information, yet explainability remains a critical\nchallenge, particularly when retrieval relies on unstructured text. Knowledge\ngraphs (KGs) offer a solution by introducing structured, semantically rich\nrepresentations of entities and their relationships, enabling transparent\nretrieval paths and interpretable reasoning. In this work, we present KGRAG-Ex,\na RAG system that improves both factual grounding and explainability by\nleveraging a domain-specific KG constructed via prompt-based information\nextraction. Given a user query, KGRAG-Ex identifies relevant entities and\nsemantic paths in the graph, which are then transformed into pseudo-paragraphs:\nnatural language representations of graph substructures that guide corpus\nretrieval. To improve interpretability and support reasoning transparency, we\nincorporate perturbation-based explanation methods that assess the influence of\nspecific KG-derived components on the generated answers. We conduct a series of\nexperiments to analyze the sensitivity of the system to different perturbation\nmethods, the relationship between graph component importance and their\nstructural positions, the influence of semantic node types, and how graph\nmetrics correspond to the influence of components within the explanations\nprocess.\n","authors":["Georgios Balanos","Evangelos Chasanis","Konstantinos Skianis","Evaggelia Pitoura"],"pdf_url":"https://arxiv.org/pdf/2507.08443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08438v1","updated":"2025-07-11T09:29:28Z","published":"2025-07-11T09:29:28Z","title":"Optimal and Practical Batched Linear Bandit Algorithm","summary":"  We study the linear bandit problem under limited adaptivity, known as the\nbatched linear bandit. While existing approaches can achieve near-optimal\nregret in theory, they are often computationally prohibitive or underperform in\npractice. We propose \\texttt{BLAE}, a novel batched algorithm that integrates\narm elimination with regularized G-optimal design, achieving the minimax\noptimal regret (up to logarithmic factors in $T$) in both large-$K$ and\nsmall-$K$ regimes for the first time, while using only $O(\\log\\log T)$ batches.\nOur analysis introduces new techniques for batch-wise optimal design and\nrefined concentration bounds. Crucially, \\texttt{BLAE} demonstrates low\ncomputational overhead and strong empirical performance, outperforming\nstate-of-the-art methods in extensive numerical evaluations. Thus,\n\\texttt{BLAE} is the first algorithm to combine provable minimax-optimality in\nall regimes and practical superiority in batched linear bandits.\n","authors":["Sanghoon Yu","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2507.08438v1.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2412.00136v3","updated":"2025-07-11T09:19:13Z","published":"2024-11-28T16:19:37Z","title":"FonTS: Text Rendering with Typography and Style Controls","summary":"  Visual text rendering are widespread in various real-world applications,\nrequiring careful font selection and typographic choices. Recent progress in\ndiffusion transformer (DiT)-based text-to-image (T2I) models show promise in\nautomating these processes. However, these methods still encounter challenges\nlike inconsistent fonts, style variation, and limited fine-grained control,\nparticularly at the word-level. This paper proposes a two-stage DiT-based\npipeline to address these problems by enhancing controllability over typography\nand style in text rendering. We introduce typography control fine-tuning\n(TC-FT), an parameter-efficient fine-tuning method (on $5\\%$ key parameters)\nwith enclosing typography control tokens (ETC-tokens), which enables precise\nword-level application of typographic features. To further address style\ninconsistency in text rendering, we propose a text-agnostic style control\nadapter (SCA) that prevents content leakage while enhancing style consistency.\nTo implement TC-FT and SCA effectively, we incorporated HTML-render into the\ndata synthesis pipeline and proposed the first word-level controllable dataset.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\napproach in achieving superior word-level typographic control, font\nconsistency, and style consistency in text rendering tasks. The datasets and\nmodels will be available for academic use.\n","authors":["Wenda Shi","Yiren Song","Dengming Zhang","Jiaming Liu","Xingxing Zou"],"pdf_url":"https://arxiv.org/pdf/2412.00136v3.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2111.14003v2","updated":"2025-07-11T09:13:57Z","published":"2021-11-27T23:19:49Z","title":"Answer Generation for Questions With Multiple Information Sources in\n  E-Commerce","summary":"  Automatic question answering is an important yet challenging task in\nE-commerce given the millions of questions posted by users about the product\nthat they are interested in purchasing. Hence, there is a great demand for\nautomatic answer generation systems that provide quick responses using related\ninformation about the product. There are three sources of knowledge available\nfor answering a user posted query, they are reviews, duplicate or similar\nquestions, and specifications. Effectively utilizing these information sources\nwill greatly aid us in answering complex questions. However, there are two main\nchallenges present in exploiting these sources: (i) The presence of irrelevant\ninformation and (ii) the presence of ambiguity of sentiment present in reviews\nand similar questions. Through this work we propose a novel pipeline (MSQAP)\nthat utilizes the rich information present in the aforementioned sources by\nseparately performing relevancy and ambiguity prediction before generating a\nresponse.\n  Experimental results show that our relevancy prediction model (BERT-QA)\noutperforms all other variants and has an improvement of 12.36% in F1 score\ncompared to the BERT-base baseline. Our generation model (T5-QA) outperforms\nthe baselines in all content preservation metrics such as BLEU, ROUGE and has\nan average improvement of 35.02% in ROUGE and 198.75% in BLEU compared to the\nhighest performing baseline (HSSC-q). Human evaluation of our pipeline shows us\nthat our method has an overall improvement in accuracy of 30.7% over the\ngeneration model (T5-QA), resulting in our full pipeline-based approach (MSQAP)\nproviding more accurate answers. To the best of our knowledge, this is the\nfirst work in the e-commerce domain that automatically generates natural\nlanguage answers combining the information present in diverse sources such as\nspecifications, similar questions, and reviews data.\n","authors":["Anand A. Rajasekar","Nikesh Garera"],"pdf_url":"https://arxiv.org/pdf/2111.14003v2.pdf","comment":"7 pages, 10 tables, 1 figure"},{"id":"http://arxiv.org/abs/2507.08424v1","updated":"2025-07-11T09:09:01Z","published":"2025-07-11T09:09:01Z","title":"RTNinja: a generalized machine learning framework for analyzing random\n  telegraph noise signals in nanoelectronic devices","summary":"  Random telegraph noise is a prevalent variability phenomenon in\nnanoelectronic devices, arising from stochastic carrier exchange at defect\nsites and critically impacting device reliability and performance. Conventional\nanalysis techniques often rely on restrictive assumptions or manual\ninterventions, limiting their applicability to complex, noisy datasets. Here,\nwe introduce RTNinja, a generalized, fully automated machine learning framework\nfor the unsupervised analysis of random telegraph noise signals. RTNinja\ndeconvolves complex signals to identify the number and characteristics of\nhidden individual sources, without requiring prior knowledge of the system. The\nframework comprises two modular components: LevelsExtractor, which uses\nBayesian inference and model selection to denoise and discretize the signal;\nand SourcesMapper, which infers source configurations through probabilistic\nclustering and optimization. To evaluate performance, we developed a Monte\nCarlo simulator that generates labeled datasets spanning broad signal-to-noise\nratios and source complexities; across 7000 such datasets, RTNinja consistently\ndemonstrated high-fidelity signal reconstruction and accurate extraction of\nsource amplitudes and activity patterns. Our results demonstrate that RTNinja\noffers a robust, scalable, and device-agnostic tool for random telegraph noise\ncharacterization, enabling large-scale statistical benchmarking,\nreliability-centric technology qualification, predictive failure modeling, and\ndevice physics exploration in next-generation nanoelectronics.\n","authors":["Anirudh Varanasi","Robin Degraeve","Philippe Roussel","Clement Merckling"],"pdf_url":"https://arxiv.org/pdf/2507.08424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11050v4","updated":"2025-07-11T08:48:55Z","published":"2023-01-26T11:47:10Z","title":"Minerva: A File-Based Ransomware Detector","summary":"  Ransomware attacks have caused billions of dollars in damages in recent\nyears, and are expected to cause billions more in the future. Consequently,\nsignificant effort has been devoted to ransomware detection and mitigation.\nBehavioral-based ransomware detection approaches have garnered considerable\nattention recently. These behavioral detectors typically rely on process-based\nbehavioral profiles to identify malicious behaviors. However, with an\nincreasing body of literature highlighting the vulnerability of such approaches\nto evasion attacks, a comprehensive solution to the ransomware problem remains\nelusive. This paper presents Minerva, a novel, robust approach to ransomware\ndetection. Minerva is engineered to be robust by design against evasion\nattacks, with architectural and feature selection choices informed by their\nresilience to adversarial manipulation. We conduct a comprehensive analysis of\nMinerva across a diverse spectrum of ransomware types, encompassing unseen\nransomware as well as variants designed specifically to evade Minerva. Our\nevaluation showcases the ability of Minerva to accurately identify ransomware,\ngeneralize to unseen threats, and withstand evasion attacks. Furthermore, over\n99% of detected ransomware are identified within 0.52sec of activity, enabling\nthe adoption of data loss prevention techniques with near-zero overhead.\n","authors":["Dorjan Hitaj","Giulio Pagnotta","Fabio De Gaspari","Lorenzo De Carli","Luigi V. Mancini"],"pdf_url":"https://arxiv.org/pdf/2301.11050v4.pdf","comment":"Accepted for publication at The 20th ACM ASIA Conference on Computer\n  and Communications Security (ACM ASIACCS 2025), Meli\\'a Hanoi"},{"id":"http://arxiv.org/abs/2507.08403v1","updated":"2025-07-11T08:21:08Z","published":"2025-07-11T08:21:08Z","title":"Towards AI-Native RAN: An Operator's Perspective of 6G Day 1\n  Standardization","summary":"  Artificial Intelligence/Machine Learning (AI/ML) has become the most certain\nand prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not\nnatively integrated but rather an add-on feature over existing architecture, 6G\nshall incorporate AI from the onset to address its complexity and support\nubiquitous AI applications. Based on our extensive mobile network operation and\nstandardization experience from 2G to 5G, this paper explores the design and\nstandardization principles of AI-Native radio access networks (RAN) for 6G,\nwith a particular focus on its critical Day 1 architecture, functionalities and\ncapabilities. We investigate the framework of AI-Native RAN and present its\nthree essential capabilities to shed some light on the standardization\ndirection; namely, AI-driven RAN processing/optimization/automation, reliable\nAI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The\nstandardization of AI-Native RAN, in particular the Day 1 features, including\nan AI-Native 6G RAN architecture, were proposed. For validation, a large-scale\nfield trial with over 5000 5G-A base stations have been built and delivered\nsignificant improvements in average air interface latency, root cause\nidentification, and network energy consumption with the proposed architecture\nand the supporting AI functions. This paper aims to provide a Day 1 framework\nfor 6G AI-Native RAN standardization design, balancing technical innovation\nwith practical deployment.\n","authors":["Nan Li","Qi Sun","Lehan Wang","Xiaofei Xu","Jinri Huang","Chunhui Liu","Jing Gao","Yuhong Huang","Chih-Lin I"],"pdf_url":"https://arxiv.org/pdf/2507.08403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08402v1","updated":"2025-07-11T08:20:19Z","published":"2025-07-11T08:20:19Z","title":"SPINT: Spatial Permutation-Invariant Neural Transformer for Consistent\n  Intracortical Motor Decoding","summary":"  Intracortical Brain-Computer Interfaces (iBCI) aim to decode behavior from\nneural population activity, enabling individuals with motor impairments to\nregain motor functions and communication abilities. A key challenge in\nlong-term iBCI is the nonstationarity of neural recordings, where the\ncomposition and tuning profiles of the recorded populations are unstable across\nrecording sessions. Existing methods attempt to address this issue by explicit\nalignment techniques; however, they rely on fixed neural identities and require\ntest-time labels or parameter updates, limiting their generalization across\nsessions and imposing additional computational burden during deployment. In\nthis work, we introduce SPINT - a Spatial Permutation-Invariant Neural\nTransformer framework for behavioral decoding that operates directly on\nunordered sets of neural units. Central to our approach is a novel\ncontext-dependent positional embedding scheme that dynamically infers\nunit-specific identities, enabling flexible generalization across recording\nsessions. SPINT supports inference on variable-size populations and allows\nfew-shot, gradient-free adaptation using a small amount of unlabeled data from\nthe test session. To further promote model robustness to population\nvariability, we introduce dynamic channel dropout, a regularization method for\niBCI that simulates shifts in population composition during training. We\nevaluate SPINT on three multi-session datasets from the FALCON Benchmark,\ncovering continuous motor decoding tasks in human and non-human primates. SPINT\ndemonstrates robust cross-session generalization, outperforming existing\nzero-shot and few-shot unsupervised baselines while eliminating the need for\ntest-time alignment and fine-tuning. Our work contributes an initial step\ntoward a robust and scalable neural decoding framework for long-term iBCI\napplications.\n","authors":["Trung Le","Hao Fang","Jingyuan Li","Tung Nguyen","Lu Mi","Amy Orsborn","Uygar Sümbül","Eli Shlizerman"],"pdf_url":"https://arxiv.org/pdf/2507.08402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07656v2","updated":"2025-07-11T08:15:30Z","published":"2025-03-07T11:41:18Z","title":"DriveTransformer: Unified Transformer for Scalable End-to-End Autonomous\n  Driving","summary":"  End-to-end autonomous driving (E2E-AD) has emerged as a trend in the field of\nautonomous driving, promising a data-driven, scalable approach to system\ndesign. However, existing E2E-AD methods usually adopt the sequential paradigm\nof perception-prediction-planning, which leads to cumulative errors and\ntraining instability. The manual ordering of tasks also limits the system`s\nability to leverage synergies between tasks (for example, planning-aware\nperception and game-theoretic interactive prediction and planning). Moreover,\nthe dense BEV representation adopted by existing methods brings computational\nchallenges for long-range perception and long-term temporal fusion. To address\nthese challenges, we present DriveTransformer, a simplified E2E-AD framework\nfor the ease of scaling up, characterized by three key features: Task\nParallelism (All agent, map, and planning queries direct interact with each\nother at each block), Sparse Representation (Task queries direct interact with\nraw sensor features), and Streaming Processing (Task queries are stored and\npassed as history information). As a result, the new framework is composed of\nthree unified operations: task self-attention, sensor cross-attention, temporal\ncross-attention, which significantly reduces the complexity of system and leads\nto better training stability. DriveTransformer achieves state-of-the-art\nperformance in both simulated closed-loop benchmark Bench2Drive and real world\nopen-loop benchmark nuScenes with high FPS.\n","authors":["Xiaosong Jia","Junqi You","Zhiyuan Zhang","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2503.07656v2.pdf","comment":"Accepted by ICLR2025; Fix Typo"},{"id":"http://arxiv.org/abs/2507.08390v1","updated":"2025-07-11T08:00:47Z","published":"2025-07-11T08:00:47Z","title":"Inference-Time Scaling of Diffusion Language Models with Particle Gibbs\n  Sampling","summary":"  Discrete diffusion models have emerged as a powerful paradigm for language\nmodeling, rivaling auto-regressive models by training-time scaling. However,\ninference-time scaling in discrete diffusion models remains relatively\nunder-explored. In this work, we study sampling-based approaches for achieving\nhigh-quality text generation from discrete diffusion models in reward-guided\nsettings. We introduce a novel inference-time scaling approach based on\nparticle Gibbs sampling for discrete diffusion models. The particle Gibbs\nsampling algorithm iteratively refines full diffusion trajectories using\nconditional Sequential Monte Carlo as its transition mechanism. This process\nensures that the updated samples progressively improve and move closer to the\nreward-weighted target distribution. Unlike existing inference-time scaling\nmethods, which are often limited to single diffusion trajectories, our approach\nleverages iterative refinement across multiple trajectories. Within this\nframework, we further analyze the trade-offs between four key axes for\ninference-time scaling under fixed compute budgets: particle Gibbs iterations,\nparticle count, denoising steps, and reward estimation cost. Empirically, our\nmethod consistently outperforms prior inference-time strategies on\nreward-guided text generation tasks, achieving significant improvement in\naccuracy under varying compute budgets.\n","authors":["Meihua Dang","Jiaqi Han","Minkai Xu","Kai Xu","Akash Srivastava","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2507.08390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08387v1","updated":"2025-07-11T08:00:12Z","published":"2025-07-11T08:00:12Z","title":"Online Pre-Training for Offline-to-Online Reinforcement Learning","summary":"  Offline-to-online reinforcement learning (RL) aims to integrate the\ncomplementary strengths of offline and online RL by pre-training an agent\noffline and subsequently fine-tuning it through online interactions. However,\nrecent studies reveal that offline pre-trained agents often underperform during\nonline fine-tuning due to inaccurate value estimation caused by distribution\nshift, with random initialization proving more effective in certain cases. In\nthis work, we propose a novel method, Online Pre-Training for Offline-to-Online\nRL (OPT), explicitly designed to address the issue of inaccurate value\nestimation in offline pre-trained agents. OPT introduces a new learning phase,\nOnline Pre-Training, which allows the training of a new value function tailored\nspecifically for effective online fine-tuning. Implementation of OPT on TD3 and\nSPOT demonstrates an average 30% improvement in performance across a wide range\nof D4RL environments, including MuJoCo, Antmaze, and Adroit.\n","authors":["Yongjae Shin","Jeonghye Kim","Whiyoung Jung","Sunghoon Hong","Deunsol Yoon","Youngsoo Jang","Geonhyeong Kim","Jongseong Chae","Youngchul Sung","Kanghoon Lee","Woohyung Lim"],"pdf_url":"https://arxiv.org/pdf/2507.08387v1.pdf","comment":"ICML 2025 camera-ready"},{"id":"http://arxiv.org/abs/2507.08382v1","updated":"2025-07-11T07:54:16Z","published":"2025-07-11T07:54:16Z","title":"Two-cluster test","summary":"  Cluster analysis is a fundamental research issue in statistics and machine\nlearning. In many modern clustering methods, we need to determine whether two\nsubsets of samples come from the same cluster. Since these subsets are usually\ngenerated by certain clustering procedures, the deployment of classic\ntwo-sample tests in this context would yield extremely smaller p-values,\nleading to inflated Type-I error rate. To overcome this bias, we formally\nintroduce the two-cluster test issue and argue that it is a totally different\nsignificance testing issue from conventional two-sample test. Meanwhile, we\npresent a new method based on the boundary points between two subsets to derive\nan analytical p-value for the purpose of significance quantification.\nExperiments on both synthetic and real data sets show that the proposed test is\nable to significantly reduce the Type-I error rate, in comparison with several\nclassic two-sample testing methods. More importantly, the practical usage of\nsuch two-cluster test is further verified through its applications in\ntree-based interpretable clustering and significance-based hierarchical\nclustering.\n","authors":["Xinying Liu","Lianyu Hu","Mudi Jiang","Simen Zhang","Jun Lou","Zengyou He"],"pdf_url":"https://arxiv.org/pdf/2507.08382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02672v4","updated":"2025-07-11T07:50:56Z","published":"2024-02-05T02:17:21Z","title":"Estimation of conditional average treatment effects on distributed\n  confidential data","summary":"  The estimation of conditional average treatment effects (CATEs) is an\nimportant topic in many scientific fields. CATEs can be estimated with high\naccuracy if data distributed across multiple parties are centralized. However,\nit is difficult to aggregate such data owing to confidentiality or privacy\nconcerns. To address this issue, we propose data collaboration double machine\nlearning, a method for estimating CATE models using privacy-preserving fusion\ndata constructed from distributed sources, and evaluate its performance through\nsimulations. We make three main contributions. First, our method enables\nestimation and testing of semi-parametric CATE models without iterative\ncommunication on distributed data, providing robustness to model\nmis-specification compared to parametric approaches. Second, it enables\ncollaborative estimation across different time points and parties by\naccumulating a knowledge base. Third, our method performs as well as or better\nthan existing methods in simulations using synthetic, semi-synthetic, and\nreal-world datasets.\n","authors":["Yuji Kawamata","Ryoki Motai","Yukihiko Okada","Akira Imakura","Tetsuya Sakurai"],"pdf_url":"https://arxiv.org/pdf/2402.02672v4.pdf","comment":"45 pages, 12 figures"},{"id":"http://arxiv.org/abs/2507.08379v1","updated":"2025-07-11T07:47:47Z","published":"2025-07-11T07:47:47Z","title":"Advances in Machine Learning: Where Can Quantum Techniques Help?","summary":"  Quantum Machine Learning (QML) represents a promising frontier at the\nintersection of quantum computing and artificial intelligence, aiming to\nleverage quantum computational advantages to enhance data-driven tasks. This\nreview explores the potential of QML to address the computational bottlenecks\nof classical machine learning, particularly in processing complex datasets. We\nintroduce the theoretical foundations of QML, including quantum data encoding,\nquantum learning theory and optimization techniques, while categorizing QML\napproaches based on data type and computational architecture. It is\nwell-established that quantum computational advantages are problem-dependent,\nand so potentially useful directions for QML need to be systematically\nidentified. Key developments, such as Quantum Principal Component Analysis,\nquantum-enhanced sensing and applications in material science, are critically\nevaluated for their theoretical speed-ups and practical limitations. The\nchallenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices, including\nhardware noise, scalability constraints and data encoding overheads, are\ndiscussed in detail. We also outline future directions, emphasizing the need\nfor quantum-native algorithms, improved error correction, and realistic\nbenchmarks to bridge the gap between theoretical promise and practical\ndeployment. This comprehensive analysis underscores that while QML has\nsignificant potential for specific applications such as quantum chemistry and\nsensing, its broader utility in real-world scenarios remains contingent on\novercoming technological and methodological hurdles.\n","authors":["Samarth Kashyap","Rohit K Ramakrishnan","Kumari Jyoti","Apoorva D Patel"],"pdf_url":"https://arxiv.org/pdf/2507.08379v1.pdf","comment":"28 pages, 1 figure"},{"id":"http://arxiv.org/abs/2506.14123v2","updated":"2025-07-11T07:43:41Z","published":"2025-06-17T02:37:04Z","title":"Sampling from Your Language Model One Byte at a Time","summary":"  Tokenization is used almost universally by modern language models, enabling\nefficient text representation using multi-byte or multi-character tokens.\nHowever, prior work has shown that tokenization can introduce distortion into\nthe model's generations, an issue known as the Prompt Boundary Problem (PBP).\nFor example, users are often advised not to end their prompts with a space\nbecause it prevents the model from including the space as part of the next\ntoken. While this heuristic is effective in English, the underlying PBP\ncontinues to affect languages such as Chinese as well as code generation, where\ntokens often do not line up with word and syntactic boundaries. In this work,\nwe present an inference-time method to convert any autoregressive LM with a BPE\ntokenizer into a character-level or byte-level LM. Our method efficiently\nsolves the PBP and is also able to unify the vocabularies of language models\nwith different tokenizers, allowing one to ensemble LMs with different\ntokenizers at inference time or transfer the post-training from one model to\nanother using proxy-tuning. We demonstrate in experiments that the ensemble and\nproxy-tuned models outperform their constituents on downstream evals. Code is\navailable at https://github.com/SewoongLab/byte-sampler .\n","authors":["Jonathan Hayase","Alisa Liu","Noah A. Smith","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2506.14123v2.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.07668v2","updated":"2025-07-11T07:41:35Z","published":"2025-07-10T11:49:17Z","title":"Learning Pole Structures of Hadronic States using Predictive Uncertainty\n  Estimation","summary":"  Matching theoretical predictions to experimental data remains a central\nchallenge in hadron spectroscopy. In particular, the identification of new\nhadronic states is difficult, as exotic signals near threshold can arise from a\nvariety of physical mechanisms. A key diagnostic in this context is the pole\nstructure of the scattering amplitude, but different configurations can produce\nsimilar signatures. The mapping between pole configurations and line shapes is\nespecially ambiguous near the mass threshold, where analytic control is\nlimited. In this work, we introduce an uncertainty-aware machine learning\napproach for classifying pole structures in $S$-matrix elements. Our method is\nbased on an ensemble of classifier chains that provide both epistemic and\naleatoric uncertainty estimates. We apply a rejection criterion based on\npredictive uncertainty, achieving a validation accuracy of nearly $95\\%$ while\ndiscarding only a small fraction of high-uncertainty predictions. Trained on\nsynthetic data with known pole structures, the model generalizes to previously\nunseen experimental data, including enhancements associated with the\n$P_{c\\bar{c}}(4312)^+$ state observed by LHCb. In this, we infer a four-pole\nstructure, representing the presence of a genuine compact pentaquark in the\npresence of a higher channel virtual state pole with non-vanishing width. While\nevaluated on this particular state, our framework is broadly applicable to\nother candidate hadronic states and offers a scalable tool for pole structure\ninference in scattering amplitudes.\n","authors":["Felix Frohnert","Denny Lane B. Sombillo","Evert van Nieuwenburg","Patrick Emonts"],"pdf_url":"https://arxiv.org/pdf/2507.07668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08365v1","updated":"2025-07-11T07:26:33Z","published":"2025-07-11T07:26:33Z","title":"Prediction of Lane Change Intentions of Human Drivers using an LSTM, a\n  CNN and a Transformer","summary":"  Lane changes of preceding vehicles have a great impact on the motion planning\nof automated vehicles especially in complex traffic situations. Predicting them\nwould benefit the public in terms of safety and efficiency. While many research\nefforts have been made in this direction, few concentrated on predicting\nmaneuvers within a set time interval compared to predicting at a set prediction\ntime. In addition, there exist a lack of comparisons between different\narchitectures to try to determine the best performing one and to assess how to\ncorrectly choose the input for such models. In this paper the structure of an\nLSTM, a CNN and a Transformer network are described and implemented to predict\nthe intention of human drivers to perform a lane change. We show how the data\nwas prepared starting from a publicly available dataset (highD), which features\nwere used, how the networks were designed and finally we compare the results of\nthe three networks with different configurations of input data. We found that\ntransformer networks performed better than the other networks and was less\naffected by overfitting. The accuracy of the method spanned from $82.79\\%$ to\n$96.73\\%$ for different input configurations and showed overall good\nperformances considering also precision and recall.\n","authors":["Francesco De Cristofaro","Felix Hofbaur","Aixi Yang","Arno Eichberger"],"pdf_url":"https://arxiv.org/pdf/2507.08365v1.pdf","comment":"14 pages, 18 figures"},{"id":"http://arxiv.org/abs/2506.22236v2","updated":"2025-07-11T07:26:21Z","published":"2025-06-27T13:59:08Z","title":"A Plea for History and Philosophy of Statistics and Machine Learning","summary":"  The integration of the history and philosophy of statistics was initiated at\nleast by Hacking (1965) and advanced by Mayo (1996), but it has not received\nsustained follow-up. Yet such integration is more urgent than ever, as the\nrecent success of artificial intelligence has been driven largely by machine\nlearning -- a field historically developed alongside statistics. Today, the\nboundary between statistics and machine learning is increasingly blurred. What\nwe now need is integration, twice over: of history and philosophy, and of two\nfields they engage -- statistics and machine learning. I present a case study\nof a philosophical idea in machine learning (and in formal epistemology) whose\nroot can be traced back to an often under-appreciated insight in Neyman and\nPearson's 1936 work (a follow-up to their 1933 classic). This leads to the\narticulation of an epistemological principle -- largely implicit in, but shared\nby, the practices of frequentist statistics and machine learning -- which I\ncall achievabilism: the thesis that the correct standard for assessing\nnon-deductive inference methods should not be fixed, but should instead be\nsensitive to what is achievable in specific problem contexts. Another\nintegration also emerges at the level of methodology, combining two ends of the\nphilosophy of science spectrum: history and philosophy of science on the one\nhand, and formal epistemology on the other hand.\n","authors":["Hanti Lin"],"pdf_url":"https://arxiv.org/pdf/2506.22236v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08362v1","updated":"2025-07-11T07:25:55Z","published":"2025-07-11T07:25:55Z","title":"Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN\n  Model Generation from Text","summary":"  Efficient planning, resource management, and consistent operations often rely\non converting textual process documents into formal Business Process Model and\nNotation (BPMN) models. However, this conversion process remains time-intensive\nand costly. Existing approaches, whether rule-based or machine-learning-based,\nstill struggle with writing styles and often fail to identify parallel\nstructures in process descriptions.\n  This paper introduces an automated pipeline for extracting BPMN models from\ntext, leveraging the use of machine learning and large language models. A key\ncontribution of this work is the introduction of a newly annotated dataset,\nwhich significantly enhances the training process. Specifically, we augment the\nPET dataset with 15 newly annotated documents containing 32 parallel gateways\nfor model training, a critical feature often overlooked in existing datasets.\nThis addition enables models to better capture parallel structures, a common\nbut complex aspect of process descriptions. The proposed approach demonstrates\nadequate performance in terms of reconstruction accuracy, offering a promising\nfoundation for organizations to accelerate BPMN model creation.\n","authors":["Phuong Nam Lê","Charlotte Schneider-Depré","Alexandre Goossens","Alexander Stevens","Aurélie Leribaux","Johannes De Smedt"],"pdf_url":"https://arxiv.org/pdf/2507.08362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08355v1","updated":"2025-07-11T07:15:13Z","published":"2025-07-11T07:15:13Z","title":"scE$^2$TM: Toward Interpretable Single-Cell Embedding via Topic Modeling","summary":"  Recent advances in sequencing technologies have enabled researchers to\nexplore cellular heterogeneity at single-cell resolution. Meanwhile,\ninterpretability has gained prominence parallel to the rapid increase in the\ncomplexity and performance of deep learning models. In recent years, topic\nmodels have been widely used for interpretable single-cell embedding learning\nand clustering analysis, which we refer to as single-cell embedded topic\nmodels. However, previous studies evaluated the interpretability of the models\nmainly through qualitative analysis, and these single-cell embedded topic\nmodels suffer from the potential problem of interpretation collapse.\nFurthermore, their neglect of external biological knowledge constrains\nanalytical performance. Here, we present scE2TM, an external knowledge-guided\nsingle-cell embedded topic model that provides a high-quality cell embedding\nand strong interpretation, contributing to comprehensive scRNA-seq data\nanalysis. Our comprehensive evaluation across 20 scRNA-seq datasets\ndemonstrates that scE2TM achieves significant clustering performance gains\ncompared to 7 state-of-the-art methods. In addition, we propose a new\ninterpretability evaluation benchmark that introduces 10 metrics to\nquantitatively assess the interpretability of single-cell embedded topic\nmodels. The results show that the interpretation provided by scE2TM performs\nencouragingly in terms of diversity and consistency with the underlying\nbiological signals, contributing to a better revealing of the underlying\nbiological mechanisms.\n","authors":["Hegang Chen","Yuyin Lu","Zhiming Dai","Fu Lee Wang","Qing Li","Yanghui Rao"],"pdf_url":"https://arxiv.org/pdf/2507.08355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01531v3","updated":"2025-07-11T06:55:22Z","published":"2025-04-02T09:18:43Z","title":"DRAN: A Distribution and Relation Adaptive Network for Spatio-temporal\n  Forecasting","summary":"  Accurate predictions of spatio-temporal systems are crucial for tasks such as\nsystem management, control, and crisis prevention. However, the inherent time\nvariance of many spatio-temporal systems poses challenges to achieving accurate\npredictions whenever stationarity is not granted. In order to address\nnon-stationarity, we propose a Distribution and Relation Adaptive Network\n(DRAN) capable of dynamically adapting to relation and distribution changes\nover time. While temporal normalization and de-normalization are frequently\nused techniques to adapt to distribution shifts, this operation is not suitable\nfor the spatio-temporal context as temporal normalization scales the time\nseries of nodes and possibly disrupts the spatial relations among nodes. In\norder to address this problem, a Spatial Factor Learner (SFL) module is\ndeveloped that enables the normalization and de-normalization process. To adapt\nto dynamic changes in spatial relationships among sensors, we propose a\nDynamic-Static Fusion Learner (DSFL) module that effectively integrates\nfeatures learned from both dynamic and static relations through an adaptive\nfusion ratio mechanism. Furthermore, we introduce a Stochastic Learner to\ncapture the noisy components of spatio-temporal representations. Our approach\noutperforms state-of-the-art methods on weather prediction and traffic flow\nforecasting tasks.Experimental results show that our SFL efficiently preserves\nspatial relationships across various temporal normalization operations.\nVisualizations of the learned dynamic and static relations demonstrate that\nDSFL can capture both local and distant relationships between nodes.\n","authors":["Xiaobei Zou","Luolin Xiong","Kexuan Zhang","Cesare Alippi","Yang Tang"],"pdf_url":"https://arxiv.org/pdf/2504.01531v3.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2507.07469v2","updated":"2025-07-11T06:27:42Z","published":"2025-07-10T06:53:18Z","title":"Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast\n  Rolling One-Step-Ahead Forecasting","summary":"  We introduce Galerkin-ARIMA, a novel time-series forecasting framework that\nintegrates Galerkin projection techniques with the classical ARIMA model to\ncapture potentially nonlinear dependencies in lagged observations. By replacing\nthe fixed linear autoregressive component with a spline-based basis expansion,\nGalerkin-ARIMA flexibly approximates the underlying relationship among past\nvalues via ordinary least squares, while retaining the moving-average structure\nand Gaussian innovation assumptions of ARIMA. We derive closed-form solutions\nfor both the AR and MA components using two-stage Galerkin projections,\nestablish conditions for asymptotic unbiasedness and consistency, and analyze\nthe bias-variance trade-off under basis-size growth. Complexity analysis\nreveals that, for moderate basis dimensions, our approach can substantially\nreduce computational cost compared to maximum-likelihood ARIMA estimation.\nThrough extensive simulations on four synthetic processes-including noisy ARMA,\nseasonal, trend-AR, and nonlinear recursion series-we demonstrate that\nGalerkin-ARIMA matches or closely approximates ARIMA's forecasting accuracy\nwhile achieving orders-of-magnitude speedups in rolling forecasting tasks.\nThese results suggest that Galerkin-ARIMA offers a powerful, efficient\nalternative for modeling complex time series dynamics in high-volume or\nreal-time applications.\n","authors":["Haojie Liu","Zihan Lin"],"pdf_url":"https://arxiv.org/pdf/2507.07469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08333v1","updated":"2025-07-11T06:25:49Z","published":"2025-07-11T06:25:49Z","title":"Audio Inpanting using Discrete Diffusion Model","summary":"  Audio inpainting refers to the task of reconstructing missing segments in\ncorrupted audio recordings. While prior approaches-including waveform and\nspectrogram-based diffusion models-have shown promising results for short gaps,\nthey often degrade in quality when gaps exceed 100 milliseconds (ms). In this\nwork, we introduce a novel inpainting method based on discrete diffusion\nmodeling, which operates over tokenized audio representations produced by a\npre-trained audio tokenizer. Our approach models the generative process\ndirectly in the discrete latent space, enabling stable and semantically\ncoherent reconstruction of missing audio. We evaluate the method on the\nMusicNet dataset using both objective and perceptual metrics across gap\ndurations up to 300 ms. We further evaluated our approach on the MTG dataset,\nextending the gap duration to 500 ms. Experimental results demonstrate that our\nmethod achieves competitive or superior performance compared to existing\nbaselines, particularly for longer gaps, offering a robust solution for\nrestoring degraded musical recordings. Audio examples of our proposed method\ncan be found at https://iftach21.github.io/\n","authors":["Tali Dror","Iftach Shoham","Moshe Buchris","Oren Gal","Haim Permuter","Gilad Katz","Eliya Nachmani"],"pdf_url":"https://arxiv.org/pdf/2507.08333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02494v2","updated":"2025-07-11T06:19:30Z","published":"2025-03-04T11:00:08Z","title":"Enhancing Distributional Robustness in Principal Component Analysis by\n  Wasserstein Distances","summary":"  We consider the distributionally robust optimization (DRO) model of principal\ncomponent analysis (PCA) to account for uncertainty in the underlying\nprobability distribution. The resulting formulation leads to a nonsmooth\nconstrained min-max optimization problem, where the ambiguity set captures the\ndistributional uncertainty by the type-$2$ Wasserstein distance. We prove that\nthe inner maximization problem admits a closed-form optimal value. This\nexplicit characterization equivalently reformulates the original DRO model into\na minimization problem on the Stiefel manifold with intricate nonsmooth terms,\na challenging formulation beyond the reach of existing algorithms. To address\nthis issue, we devise an efficient smoothing manifold proximal gradient\nalgorithm. Our analysis establishes Riemannian gradient consistency and global\nconvergence of our algorithm to a stationary point of the nonsmooth\nminimization problem. We also provide the iteration complexity\n$O(\\epsilon^{-3})$ of our algorithm to achieve an $\\epsilon$-approximate\nstationary point. Finally, numerical experiments are conducted to validate the\neffectiveness and scalability of our algorithm, as well as to highlight the\nnecessity and rationality of adopting the DRO model for PCA.\n","authors":["Lei Wang","Xin Liu","Xiaojun Chen"],"pdf_url":"https://arxiv.org/pdf/2503.02494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08330v1","updated":"2025-07-11T05:58:22Z","published":"2025-07-11T05:58:22Z","title":"Interpretability-Aware Pruning for Efficient Medical Image Analysis","summary":"  Deep learning has driven significant advances in medical image analysis, yet\nits adoption in clinical practice remains constrained by the large size and\nlack of transparency in modern models. Advances in interpretability techniques\nsuch as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated\nGradients make it possible to assess the contribution of individual components\nwithin neural networks trained on medical imaging tasks. In this work, we\nintroduce an interpretability-guided pruning framework that reduces model\ncomplexity while preserving both predictive performance and transparency. By\nselectively retaining only the most relevant parts of each layer, our method\nenables targeted compression that maintains clinically meaningful\nrepresentations. Experiments across multiple medical image classification\nbenchmarks demonstrate that this approach achieves high compression rates with\nminimal loss in accuracy, paving the way for lightweight, interpretable models\nsuited for real-world deployment in healthcare settings.\n","authors":["Nikita Malik","Pratinav Seth","Neeraj Kumar Singh","Chintan Chitroda","Vinay Kumar Sankarapu"],"pdf_url":"https://arxiv.org/pdf/2507.08330v1.pdf","comment":"Pre-Print"},{"id":"http://arxiv.org/abs/2411.01077v4","updated":"2025-07-11T05:39:08Z","published":"2024-11-01T23:18:32Z","title":"Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection","summary":"  Jailbreaking techniques trick Large Language Models (LLMs) into producing\nrestricted output, posing a potential threat. One line of defense is to use\nanother LLM as a Judge to evaluate the harmfulness of generated text. However,\nwe reveal that these Judge LLMs are vulnerable to token segmentation bias, an\nissue that arises when delimiters alter the tokenization process, splitting\nwords into smaller sub-tokens. This alters the embeddings of the entire\nsequence, reducing detection accuracy and allowing harmful content to be\nmisclassified as safe. In this paper, we introduce Emoji Attack, a novel\nstrategy that amplifies existing jailbreak prompts by exploiting token\nsegmentation bias. Our method leverages in-context learning to systematically\ninsert emojis into text before it is evaluated by a Judge LLM, inducing\nembedding distortions that significantly lower the likelihood of detecting\nunsafe content. Unlike traditional delimiters, emojis also introduce semantic\nambiguity, making them particularly effective in this attack. Through\nexperiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack\nsubstantially reduces the unsafe prediction rate, bypassing existing\nsafeguards.\n","authors":["Zhipeng Wei","Yuqi Liu","N. Benjamin Erichson"],"pdf_url":"https://arxiv.org/pdf/2411.01077v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08893v2","updated":"2025-07-11T05:27:37Z","published":"2025-03-11T21:12:48Z","title":"EvalTree: Profiling Language Model Weaknesses via Hierarchical\n  Capability Trees","summary":"  An ideal model evaluation should achieve two goals: identifying where the\nmodel fails and providing actionable improvement guidance. Toward these goals\nfor language model (LM) evaluations, we formulate the problem of generating a\nweakness profile, a set of weaknesses expressed in natural language, given an\nLM's performance on every individual instance in a benchmark. We introduce a\nsuite of quantitative assessments to compare different weakness profiling\nmethods. We also introduce a weakness profiling method EvalTree. EvalTree\nconstructs a capability tree where each node represents a capability described\nin natural language and is linked to a subset of benchmark instances that\nspecifically evaluate this capability; it then extracts nodes where the LM\nperforms poorly to generate a weakness profile. On the MATH and WildChat\nbenchmarks, we show that EvalTree outperforms baseline weakness profiling\nmethods by identifying weaknesses more precisely and comprehensively. Weakness\nprofiling further enables weakness-guided data collection, and training data\ncollection guided by EvalTree-identified weaknesses improves LM performance\nmore than other data collection strategies. We also show how EvalTree exposes\nflaws in Chatbot Arena's human-voter-based evaluation practice. To facilitate\nfuture work, we provide an interface that allows practitioners to interactively\nexplore the capability trees built by EvalTree.\n","authors":["Zhiyuan Zeng","Yizhong Wang","Hannaneh Hajishirzi","Pang Wei Koh"],"pdf_url":"https://arxiv.org/pdf/2503.08893v2.pdf","comment":"COLM 2025"},{"id":"http://arxiv.org/abs/2507.08322v1","updated":"2025-07-11T05:25:09Z","published":"2025-07-11T05:25:09Z","title":"Towards Efficient Quantity Retrieval from Text:an Approach via\n  Description Parsing and Weak Supervision","summary":"  Quantitative facts are continually generated by companies and governments,\nsupporting data-driven decision-making. While common facts are structured, many\nlong-tail quantitative facts remain buried in unstructured documents, making\nthem difficult to access. We propose the task of Quantity Retrieval: given a\ndescription of a quantitative fact, the system returns the relevant value and\nsupporting evidence. Understanding quantity semantics in context is essential\nfor this task. We introduce a framework based on description parsing that\nconverts text into structured (description, quantity) pairs for effective\nretrieval. To improve learning, we construct a large paraphrase dataset using\nweak supervision based on quantity co-occurrence. We evaluate our approach on a\nlarge corpus of financial annual reports and a newly annotated quantity\ndescription dataset. Our method significantly improves top-1 retrieval accuracy\nfrom 30.98 percent to 64.66 percent.\n","authors":["Yixuan Cao","Zhengrong Chen","Chengxuan Xia","Kun Wu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2507.08322v1.pdf","comment":"Extended version of the paper accepted in DEXA 2025"},{"id":"http://arxiv.org/abs/2403.17285v5","updated":"2025-07-11T05:19:33Z","published":"2024-03-26T00:25:32Z","title":"Unraveling the Interplay between Carryover Effects and Reward\n  Autocorrelations in Switchback Experiments","summary":"  A/B testing has become the gold standard for policy evaluation in modern\ntechnological industries. Motivated by the widespread use of switchback\nexperiments in A/B testing, this paper conducts a comprehensive comparative\nanalysis of various switchback designs in Markovian environments. Unlike many\nexisting works which derive the optimal design based on specific and relatively\nsimple estimators, our analysis covers a range of state-of-the-art estimators\ndeveloped in the reinforcement learning (RL) literature. It reveals that the\neffectiveness of different switchback designs depends crucially on (i) the size\nof the carryover effect and (ii) the auto-correlations among reward errors over\ntime. Meanwhile, these findings are estimator-agnostic, i.e., they apply to\nmost RL estimators. Based on these insights, we provide a workflow to offer\nguidelines for practitioners on designing switchback experiments in A/B\ntesting.\n","authors":["Qianglin Wen","Chengchun Shi","Yang Ying","Niansheng Tang","Hongtu Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.17285v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08317v1","updated":"2025-07-11T05:07:21Z","published":"2025-07-11T05:07:21Z","title":"A Comprehensively Adaptive Architectural Optimization-Ingrained Quantum\n  Neural Network Model for Cloud Workloads Prediction","summary":"  Accurate workload prediction and advanced resource reservation are\nindispensably crucial for managing dynamic cloud services. Traditional neural\nnetworks and deep learning models frequently encounter challenges with diverse,\nhigh-dimensional workloads, especially during sudden resource demand changes,\nleading to inefficiencies. This issue arises from their limited optimization\nduring training, relying only on parametric (inter-connection weights)\nadjustments using conventional algorithms. To address this issue, this work\nproposes a novel Comprehensively Adaptive Architectural Optimization-based\nVariable Quantum Neural Network (CA-QNN), which combines the efficiency of\nquantum computing with complete structural and qubit vector parametric\nlearning. The model converts workload data into qubits, processed through qubit\nneurons with Controlled NOT-gated activation functions for intuitive pattern\nrecognition. In addition, a comprehensive architecture optimization algorithm\nfor networks is introduced to facilitate the learning and propagation of the\nstructure and parametric values in variable-sized QNNs. This algorithm\nincorporates quantum adaptive modulation and size-adaptive recombination during\ntraining process. The performance of CA-QNN model is thoroughly investigated\nagainst seven state-of-the-art methods across four benchmark datasets of\nheterogeneous cloud workloads. The proposed model demonstrates superior\nprediction accuracy, reducing prediction errors by up to 93.40% and 91.27%\ncompared to existing deep learning and QNN-based approaches.\n","authors":["Jitendra Kumar","Deepika Saxena","Kishu Gupta","Satyam Kumar","Ashutosh Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2507.08317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08311v1","updated":"2025-07-11T05:03:16Z","published":"2025-07-11T05:03:16Z","title":"CAS Condensed and Accelerated Silhouette: An Efficient Method for\n  Determining the Optimal K in K-Means Clustering","summary":"  Clustering is a critical component of decision-making in todays data-driven\nenvironments. It has been widely used in a variety of fields such as\nbioinformatics, social network analysis, and image processing. However,\nclustering accuracy remains a major challenge in large datasets. This paper\npresents a comprehensive overview of strategies for selecting the optimal value\nof k in clustering, with a focus on achieving a balance between clustering\nprecision and computational efficiency in complex data environments. In\naddition, this paper introduces improvements to clustering techniques for text\nand image data to provide insights into better computational performance and\ncluster validity. The proposed approach is based on the Condensed Silhouette\nmethod, along with statistical methods such as Local Structures, Gap\nStatistics, Class Consistency Ratio, and a Cluster Overlap Index CCR and\nCOIbased algorithm to calculate the best value of k for K-Means clustering. The\nresults of comparative experiments show that the proposed approach achieves up\nto 99 percent faster execution times on high-dimensional datasets while\nretaining both precision and scalability, making it highly suitable for real\ntime clustering needs or scenarios demanding efficient clustering with minimal\nresource utilization.\n","authors":["Krishnendu Das","Sumit Gupta","Awadhesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2507.08311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08306v1","updated":"2025-07-11T04:44:07Z","published":"2025-07-11T04:44:07Z","title":"M2-Reasoning: Empowering MLLMs with Unified General and Spatial\n  Reasoning","summary":"  Recent advancements in Multimodal Large Language Models (MLLMs), particularly\nthrough Reinforcement Learning with Verifiable Rewards (RLVR), have\nsignificantly enhanced their reasoning abilities. However, a critical gap\npersists: these models struggle with dynamic spatial interactions, a capability\nessential for real-world applications. To bridge this gap, we introduce\nM2-Reasoning-7B, a model designed to excel in both general and spatial\nreasoning. Our approach integrates two key innovations: (1) a novel data\npipeline that generates 294.2K high-quality data samples (168K for cold-start\nfine-tuning and 126.2K for RLVR), which feature logically coherent reasoning\ntrajectories and have undergone comprehensive assessment; and (2) a dynamic\nmulti-task training strategy with step-wise optimization to mitigate conflicts\nbetween data, and task-specific rewards for delivering tailored incentive\nsignals. This combination of curated data and advanced training allows\nM2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,\nshowcasing superior performance in both general and spatial reasoning domains.\n","authors":["Inclusion AI"," :","Fudong Wang","Jiajia Liu","Jingdong Chen","Jun Zhou","Kaixiang Ji","Lixiang Ru","Qingpei Guo","Ruobing Zheng","Tianqi Li","Yi Yuan","Yifan Mao","Yuting Xiao","Ziping Ma"],"pdf_url":"https://arxiv.org/pdf/2507.08306v1.pdf","comment":"31pages, 14 figures"},{"id":"http://arxiv.org/abs/2407.17907v2","updated":"2025-07-11T04:01:19Z","published":"2024-07-25T09:53:12Z","title":"Amortized Posterior Sampling with Diffusion Prior Distillation","summary":"  We propose Amortized Posterior Sampling (APS), a novel variational inference\napproach for efficient posterior sampling in inverse problems. Our method\ntrains a conditional flow model to minimize the divergence between the\nvariational distribution and the posterior distribution implicitly defined by\nthe diffusion model. This results in a powerful, amortized sampler capable of\ngenerating diverse posterior samples with a single neural function evaluation,\ngeneralizing across various measurements. Unlike existing methods, our approach\nis unsupervised, requires no paired training data, and is applicable to both\nEuclidean and non-Euclidean domains. We demonstrate its effectiveness on a\nrange of tasks, including image restoration, manifold signal reconstruction,\nand climate data imputation. APS significantly outperforms existing approaches\nin computational efficiency while maintaining competitive reconstruction\nquality, enabling real-time, high-quality solutions to inverse problems across\ndiverse domains.\n","authors":["Abbas Mammadov","Hyungjin Chung","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2407.17907v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01163v2","updated":"2025-07-11T03:56:05Z","published":"2025-03-03T04:24:04Z","title":"Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers","summary":"  Prompt optimization aims to search for effective prompts that enhance the\nperformance of large language models (LLMs). Although existing prompt\noptimization methods have discovered effective prompts, they often differ from\nsophisticated prompts carefully designed by human experts. Prompt design\nstrategies, representing best practices for improving prompt performance, can\nbe key to improving prompt optimization. Recently, a method termed the\nAutonomous Prompt Engineering Toolbox (APET) has incorporated various prompt\ndesign strategies into the prompt optimization process. In APET, the LLM is\nneeded to implicitly select and apply the appropriate strategies because prompt\ndesign strategies can have negative effects. This implicit selection may be\nsuboptimal due to the limited optimization capabilities of LLMs. This paper\nintroduces Optimizing Prompts with sTrategy Selection (OPTS), which implements\nexplicit selection mechanisms for prompt design. We propose three mechanisms,\nincluding a Thompson sampling-based approach, and integrate them into\nEvoPrompt, a well-known prompt optimizer. Experiments optimizing prompts for\ntwo LLMs, Llama-3-8B-Instruct and GPT-4o mini, were conducted using BIG-Bench\nHard. Our results show that the selection of prompt design strategies improves\nthe performance of EvoPrompt, and the Thompson sampling-based mechanism\nachieves the best overall results. Our experimental code is provided at\nhttps://github.com/shiralab/OPTS .\n","authors":["Rin Ashizawa","Yoichi Hirose","Nozomu Yoshinari","Kento Uchida","Shinichi Shirakawa"],"pdf_url":"https://arxiv.org/pdf/2503.01163v2.pdf","comment":"Accepted to ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2410.09918v3","updated":"2025-07-11T03:52:42Z","published":"2024-10-13T16:53:02Z","title":"Dualformer: Controllable Fast and Slow Thinking by Learning with\n  Randomized Reasoning Traces","summary":"  In cognition theory, human thinking is governed by two systems: the fast and\nintuitive System 1 and the slower but more deliberative System 2. Analogously,\nLarge Language Models (LLMs) can operate in two reasoning modes: outputting\nonly the solutions (\\emph{fast mode}) or both the reasoning chain and the final\nsolution (\\emph{slow mode}). We present \\dualformer, a single Transformer model\nthat seamlessly integrates both the fast and slow reasoning modes by training\non randomized reasoning traces, where different parts of the traces are\nstrategically dropped during training. At inference time, \\dualformer can be\neasily configured to execute in either fast or slow mode, or automatically\ndecide which mode to engage (\\emph{auto mode}). It outperforms baselines in\nboth performance and computational efficiency across all three modes: (1) in\nslow mode, \\dualformer achieves $97.6\\%$ optimal rate on unseen $30 \\times 30$\nmaze tasks, surpassing the \\searchformer baseline ($93.3\\%$) trained on data\nwith complete reasoning traces, with $45.5\\%$ fewer reasoning steps; (2) in\nfast mode, \\dualformer achieves $80\\%$ optimal rate, significantly\noutperforming the Solution-Only model trained on solution-only data, which has\nan optimal rate of only $30\\%$; (3) in auto mode, \\dualformer achieves $96.6\\%$\noptimal rate with $59.9\\%$ fewer steps than \\searchformer. Moreover,\n\\dualformer produces more diverse reasoning traces than \\searchformer{}. For\nmath reasoning problems, our techniques have also achieved improved performance\nwith LLM fine-tuning, demonstrating its generalization beyond task-specific\nmodels. We open source our code at\nhttps://github.com/facebookresearch/dualformer.\n","authors":["DiJia Su","Sainbayar Sukhbaatar","Michael Rabbat","Yuandong Tian","Qinqing Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.09918v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04774v3","updated":"2025-07-11T03:41:08Z","published":"2024-10-07T06:20:36Z","title":"Granular Ball Twin Support Vector Machine","summary":"  On Efficient and Scalable Computation of the Nonparametric Maximum Likelihood\nEstimator in Mixture ModelsTwin support vector machine (TSVM) is an emerging\nmachine learning model with versatile applicability in classification and\nregression endeavors. Nevertheless, TSVM confronts noteworthy challenges: $(i)$\nthe imperative demand for matrix inversions presents formidable obstacles to\nits efficiency and applicability on large-scale datasets; $(ii)$ the omission\nof the structural risk minimization (SRM) principle in its primal formulation\nheightens the vulnerability to overfitting risks; and $(iii)$ the TSVM exhibits\na high susceptibility to noise and outliers, and also demonstrates instability\nwhen subjected to resampling. In view of the aforementioned challenges, we\npropose the granular ball twin support vector machine (GBTSVM). GBTSVM takes\ngranular balls, rather than individual data points, as inputs to construct a\nclassifier. These granular balls, characterized by their coarser granularity,\nexhibit robustness to resampling and reduced susceptibility to the impact of\nnoise and outliers. We further propose a novel large-scale granular ball twin\nsupport vector machine (LS-GBTSVM). LS-GBTSVM's optimization formulation\nensures two critical facets: $(i)$ it eliminates the need for matrix\ninversions, streamlining the LS-GBTSVM's computational efficiency, and $(ii)$\nit incorporates the SRM principle through the incorporation of regularization\nterms, effectively addressing the issue of overfitting. The proposed LS-GBTSVM\nexemplifies efficiency, scalability for large datasets, and robustness against\nnoise and outliers. We conduct a comprehensive evaluation of the GBTSVM and\nLS-GBTSVM models on benchmark datasets from UCI, KEEL, and NDC datasets. Our\nexperimental findings and statistical analyses affirm the superior\ngeneralization prowess of the proposed GBTSVM and LS-GBTSVM models.\n","authors":["A. Quadir","M. Sajid","M. Tanveer"],"pdf_url":"https://arxiv.org/pdf/2410.04774v3.pdf","comment":"Manuscript submitted to IEEE TRANSACTIONS ON NEURAL NETWORKS AND\n  LEARNING SYSTEMS: 19 September 2023; revised 13 February 2024 and 14 July\n  2024; accepted 05 October 2024"},{"id":"http://arxiv.org/abs/2507.01381v3","updated":"2025-07-11T03:34:59Z","published":"2025-07-02T05:50:10Z","title":"Distributional Soft Actor-Critic with Diffusion Policy","summary":"  Reinforcement learning has been proven to be highly effective in handling\ncomplex control tasks. Traditional methods typically use unimodal\ndistributions, such as Gaussian distributions, to model the output of value\ndistributions. However, unimodal distribution often and easily causes bias in\nvalue function estimation, leading to poor algorithm performance. This paper\nproposes a distributional reinforcement learning algorithm called DSAC-D\n(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges\nof estimating bias in value functions and obtaining multimodal policy\nrepresentations. A multimodal distributional policy iteration framework that\ncan converge to the optimal policy was established by introducing policy\nentropy and value distribution function. A diffusion value network that can\naccurately characterize the distribution of multi peaks was constructed by\ngenerating a set of reward samples through reverse sampling using a diffusion\nmodel. Based on this, a distributional reinforcement learning algorithm with\ndual diffusion of the value network and the policy network was derived. MuJoCo\ntesting tasks demonstrate that the proposed algorithm not only learns\nmultimodal policy, but also achieves state-of-the-art (SOTA) performance in all\n9 control tasks, with significant suppression of estimation bias and total\naverage return improvement of over 10% compared to existing mainstream\nalgorithms. The results of real vehicle testing show that DSAC-D can accurately\ncharacterize the multimodal distribution of different driving styles, and the\ndiffusion policy network can characterize multimodal trajectories.\n","authors":["Tong Liu","Yinuo Wang","Xujie Song","Wenjun Zou","Liangfa Chen","Likun Wang","Bin Shuai","Jingliang Duan","Shengbo Eben Li"],"pdf_url":"https://arxiv.org/pdf/2507.01381v3.pdf","comment":"Accepted IEEE ITSC 2025"},{"id":"http://arxiv.org/abs/2507.08284v1","updated":"2025-07-11T03:17:58Z","published":"2025-07-11T03:17:58Z","title":"Lightweight Safety Guardrails via Synthetic Data and RL-guided\n  Adversarial Training","summary":"  We introduce a lightweight yet highly effective safety guardrail framework\nfor language models, demonstrating that small-scale language models can\nachieve, and even surpass, the performance of larger counterparts in content\nmoderation tasks. This is accomplished through high-fidelity synthetic data\ngeneration and adversarial training. The synthetic data generation process\nbegins with human-curated seed data, which undergoes query augmentation and\nparaphrasing to create diverse and contextually rich examples. This augmented\ndata is then subjected to multiple rounds of curation, ensuring high fidelity\nand relevance. Inspired by recent advances in the Generative Adversarial\nNetwork (GAN) architecture, our adversarial training employs reinforcement\nlearning to guide a generator that produces challenging synthetic examples.\nThese examples are used to fine-tune the safety classifier, enhancing its\nability to detect and mitigate harmful content. Additionally, we incorporate\nstrategies from recent research on efficient LLM training, leveraging the\ncapabilities of smaller models to improve the performance of larger generative\nmodels. With iterative adversarial training and the generation of diverse,\nhigh-quality synthetic data, our framework enables small language models (SLMs)\nto serve as robust safety guardrails. This approach not only reduces\ncomputational overhead but also enhances resilience against adversarial\nattacks, offering a scalable and efficient solution for content moderation in\nAI systems.\n","authors":["Aleksei Ilin","Gor Matevosyan","Xueying Ma","Vladimir Eremin","Suhaa Dada","Muqun Li","Riyaaz Shaik","Haluk Noyan Tokgozoglu"],"pdf_url":"https://arxiv.org/pdf/2507.08284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08280v1","updated":"2025-07-11T03:03:30Z","published":"2025-07-11T03:03:30Z","title":"MIRRAMS: Towards Training Models Robust to Missingness Distribution\n  Shifts","summary":"  In real-world data analysis, missingness distributional shifts between\ntraining and test input datasets frequently occur, posing a significant\nchallenge to achieving robust prediction performance. In this study, we propose\na novel deep learning framework designed to address such shifts in missingness\ndistributions. We begin by introducing a set of mutual information-based\nconditions, called MI robustness conditions, which guide a prediction model to\nextract label-relevant information while remaining invariant to diverse\nmissingness patterns, thereby enhancing robustness to unseen missingness\nscenarios at test-time. To make these conditions practical, we propose simple\nyet effective techniques to derive loss terms corresponding to each and\nformulate a final objective function, termed MIRRAMS(Mutual Information\nRegularization for Robustness Against Missingness Shifts). As a by-product, our\nanalysis provides a theoretical interpretation of the principles underlying\nconsistency regularization-based semi-supervised learning methods, such as\nFixMatch. Extensive experiments across various benchmark datasets show that\nMIRRAMS consistently outperforms existing baselines and maintains stable\nperformance across diverse missingness scenarios. Moreover, our approach\nachieves state-of-the-art performance even without missing data and can be\nnaturally extended to address semi-supervised learning tasks, highlighting\nMIRRAMS as a powerful, off-the-shelf framework for general-purpose learning.\n","authors":["Jihye Lee","Minseo Kang","Dongha Kim"],"pdf_url":"https://arxiv.org/pdf/2507.08280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.07249v2","updated":"2025-07-11T02:52:42Z","published":"2022-05-15T10:23:07Z","title":"Pocket2Mol: Efficient Molecular Sampling Based on 3D Protein Pockets","summary":"  Deep generative models have achieved tremendous success in designing novel\ndrug molecules in recent years. A new thread of works have shown the great\npotential in advancing the specificity and success rate of in silico drug\ndesign by considering the structure of protein pockets. This setting posts\nfundamental computational challenges in sampling new chemical compounds that\ncould satisfy multiple geometrical constraints imposed by pockets. Previous\nsampling algorithms either sample in the graph space or only consider the 3D\ncoordinates of atoms while ignoring other detailed chemical structures such as\nbond types and functional groups. To address the challenge, we develop\nPocket2Mol, an E(3)-equivariant generative network composed of two modules: 1)\na new graph neural network capturing both spatial and bonding relationships\nbetween atoms of the binding pockets and 2) a new efficient algorithm which\nsamples new drug candidates conditioned on the pocket representations from a\ntractable distribution without relying on MCMC. Experimental results\ndemonstrate that molecules sampled from Pocket2Mol achieve significantly better\nbinding affinity and other drug properties such as druglikeness and synthetic\naccessibility.\n","authors":["Xingang Peng","Shitong Luo","Jiaqi Guan","Qi Xie","Jian Peng","Jianzhu Ma"],"pdf_url":"https://arxiv.org/pdf/2205.07249v2.pdf","comment":"ICML 2022 accepted"},{"id":"http://arxiv.org/abs/2410.12690v3","updated":"2025-07-11T02:48:36Z","published":"2024-10-16T15:50:57Z","title":"Local transfer learning Gaussian process modeling, with applications to\n  surrogate modeling of expensive computer simulators","summary":"  A critical bottleneck for scientific progress is the costly nature of\ncomputer simulations for complex systems. Surrogate models provide an appealing\nsolution: such models are trained on simulator evaluations, then used to\nemulate and quantify uncertainty on the expensive simulator at unexplored\ninputs. In many applications, one often has available data on related systems.\nFor example, in designing a new jet turbine, there may be existing studies on\nturbines with similar configurations. A key question is how information from\nsuch ``source'' systems can be transferred for effective surrogate training on\nthe ``target'' system of interest. We thus propose a new LOcal transfer\nLearning Gaussian Process (LOL-GP) model, which leverages a carefully-designed\nGaussian process to transfer such information for surrogate modeling. The key\nnovelty of the LOL-GP is a latent regularization model, which identifies\nregions where transfer should be performed and regions where it should be\navoided. Such a ``local transfer'' property is present in many scientific\nsystems: at certain parameters, systems may behave similarly and thus transfer\nis beneficial; at other parameters, they may behave differently and thus\ntransfer is detrimental. By accounting for local transfer, the LOL-GP can\ntemper the risk of ``negative transfer'', i.e., the risk of worsening\npredictive performance from information transfer. We derive a Gibbs sampling\nalgorithm for efficient posterior predictive sampling on the LOL-GP, for both\nthe multi-source and multi-fidelity transfer settings. We then show, via a\nsuite of numerical experiments and an application for jet turbine design, the\nimproved surrogate performance of the LOL-GP over existing methods.\n","authors":["Xinming Wang","Simon Mak","John Miller","Jianguo Wu"],"pdf_url":"https://arxiv.org/pdf/2410.12690v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05416v2","updated":"2025-07-11T02:39:25Z","published":"2025-07-07T18:58:22Z","title":"EmissionNet: Air Quality Pollution Forecasting for Agriculture","summary":"  Air pollution from agricultural emissions is a significant yet often\noverlooked contributor to environmental and public health challenges.\nTraditional air quality forecasting models rely on physics-based approaches,\nwhich struggle to capture complex, nonlinear pollutant interactions. In this\nwork, we explore forecasting N$_2$O agricultural emissions through evaluating\npopular architectures, and proposing two novel deep learning architectures,\nEmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage\nconvolutional and transformer-based architectures to extract spatial-temporal\ndependencies from high-resolution emissions data\n","authors":["Prady Saligram","Tanvir Bhathal"],"pdf_url":"https://arxiv.org/pdf/2507.05416v2.pdf","comment":"The appendix figures are mixed up - several emission plots (e.g. CO2,\n  CH4, GWP) are mislabeled and appear in the wrong order, leading to confusion\n  in interpreting the results"},{"id":"http://arxiv.org/abs/2506.06311v2","updated":"2025-07-11T02:37:00Z","published":"2025-05-26T10:43:34Z","title":"A Novel Shape-Aware Topological Representation for GPR Data with DNN\n  Integration","summary":"  Ground Penetrating Radar (GPR) is a widely used Non-Destructive Testing (NDT)\ntechnique for subsurface exploration, particularly in infrastructure inspection\nand maintenance. However, conventional interpretation methods are often limited\nby noise sensitivity and a lack of structural awareness. This study presents a\nnovel framework that enhances the detection of underground utilities,\nespecially pipelines, by integrating shape-aware topological features derived\nfrom B-scan GPR images using Topological Data Analysis (TDA), with the spatial\ndetection capabilities of the YOLOv5 deep neural network (DNN). We propose a\nnovel shape-aware topological representation that amplifies structural features\nin the input data, thereby improving the model's responsiveness to the\ngeometrical features of buried objects. To address the scarcity of annotated\nreal-world data, we employ a Sim2Real strategy that generates diverse and\nrealistic synthetic datasets, effectively bridging the gap between simulated\nand real-world domains. Experimental results demonstrate significant\nimprovements in mean Average Precision (mAP), validating the robustness and\nefficacy of our approach. This approach underscores the potential of\nTDA-enhanced learning in achieving reliable, real-time subsurface object\ndetection, with broad applications in urban planning, safety inspection, and\ninfrastructure management.\n","authors":["Meiyan Kang","Shizuo Kaji","Sang-Yun Lee","Taegon Kim","Hee-Hwan Ryu","Suyoung Choi"],"pdf_url":"https://arxiv.org/pdf/2506.06311v2.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.08269v1","updated":"2025-07-11T02:32:29Z","published":"2025-07-11T02:32:29Z","title":"Data-Driven Dimensional Synthesis of Diverse Planar Four-bar Function\n  Generation Mechanisms via Direct Parameterization","summary":"  Dimensional synthesis of planar four-bar mechanisms is a challenging inverse\nproblem in kinematics, requiring the determination of mechanism dimensions from\ndesired motion specifications. We propose a data-driven framework that bypasses\ntraditional equation-solving and optimization by leveraging supervised\nlearning. Our method combines a synthetic dataset, an LSTM-based neural network\nfor handling sequential precision points, and a Mixture of Experts (MoE)\narchitecture tailored to different linkage types. Each expert model is trained\non type-specific data and guided by a type-specifying layer, enabling both\nsingle-type and multi-type synthesis. A novel simulation metric evaluates\nprediction quality by comparing desired and generated motions. Experiments show\nour approach produces accurate, defect-free linkages across various\nconfigurations. This enables intuitive and efficient mechanism design, even for\nnon-expert users, and opens new possibilities for scalable and flexible\nsynthesis in kinematic design.\n","authors":["Woon Ryong Kim","Jaeheun Jung","Jeong Un Ha","Donghun Lee","Jae Kyung Shim"],"pdf_url":"https://arxiv.org/pdf/2507.08269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08267v1","updated":"2025-07-11T02:26:01Z","published":"2025-07-11T02:26:01Z","title":"A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy\n  with SFT and Efficiency with Reinforcement Learning","summary":"  Enhancing the mathematical reasoning of Large Language Models (LLMs) is a\npivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning\n(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a\nsystematic methodology for combining them to maximize both accuracy and\nefficiency remains largely unexplored. This paper introduces a practical and\neffective training recipe that strategically integrates extended SFT with RL\nfrom online inference (GRPO). We posit that these methods play complementary,\nnot competing, roles: a prolonged SFT phase first pushes the model's accuracy\nto its limits, after which a GRPO phase dramatically improves token efficiency\nwhile preserving this peak performance. Our experiments reveal that extending\nSFT for as many as 10 epochs is crucial for performance breakthroughs, and that\nthe primary role of GRPO in this framework is to optimize solution length. The\nefficacy of our recipe is rigorously validated through top-tier performance on\nchallenging benchmarks, including a high rank among over 2,200 teams in the\nstrictly leak-free AI Mathematical Olympiad (AIMO). This work provides the\ncommunity with a battle-tested blueprint for developing state-of-the-art\nmathematical reasoners that are both exceptionally accurate and practically\nefficient. To ensure full reproducibility and empower future research, we will\nopen-source our entire framework, including all code, model checkpoints, and\ntraining configurations at\nhttps://github.com/analokmaus/kaggle-aimo2-fast-math-r1.\n","authors":["Hiroshi Yoshihara","Taiki Yamaguchi","Yuichi Inoue"],"pdf_url":"https://arxiv.org/pdf/2507.08267v1.pdf","comment":"Presented at ICML 2025 Workshop on The second AI for MATH"},{"id":"http://arxiv.org/abs/2411.18607v2","updated":"2025-07-11T02:24:39Z","published":"2024-11-27T18:53:41Z","title":"Task Arithmetic Through The Lens Of One-Shot Federated Learning","summary":"  Task Arithmetic is a model merging technique that enables the combination of\nmultiple models' capabilities into a single model through simple arithmetic in\nthe weight space, without the need for additional fine-tuning or access to the\noriginal training data. However, the factors that determine the success of Task\nArithmetic remain unclear. In this paper, we examine Task Arithmetic for\nmulti-task learning by framing it as a one-shot Federated Learning problem. We\ndemonstrate that Task Arithmetic is mathematically equivalent to the commonly\nused algorithm in Federated Learning, called Federated Averaging (FedAvg). By\nleveraging well-established theoretical results from FedAvg, we identify two\nkey factors that impact the performance of Task Arithmetic: data heterogeneity\nand training heterogeneity. To mitigate these challenges, we adapt several\nalgorithms from Federated Learning to improve the effectiveness of Task\nArithmetic. Our experiments demonstrate that applying these algorithms can\noften significantly boost performance of the merged model compared to the\noriginal Task Arithmetic approach. This work bridges Task Arithmetic and\nFederated Learning, offering new theoretical perspectives on Task Arithmetic\nand improved practical methodologies for model merging.\n","authors":["Zhixu Silvia Tao","Ian Mason","Sanjeev Kulkarni","Xavier Boix"],"pdf_url":"https://arxiv.org/pdf/2411.18607v2.pdf","comment":"Published in Transactions on Machine Learning Research"},{"id":"http://arxiv.org/abs/2505.17621v3","updated":"2025-07-11T02:23:39Z","published":"2025-05-23T08:30:28Z","title":"Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation\n  Guided Exploration","summary":"  Reinforcement learning (RL) has emerged as a pivotal method for improving the\nreasoning capabilities of Large Language Models (LLMs). However, prevalent RL\napproaches such as Proximal Policy Optimization (PPO) and Group-Regularized\nPolicy Optimization (GRPO) face critical limitations due to their reliance on\nsparse outcome-based rewards and inadequate mechanisms for incentivizing\nexploration. These limitations result in inefficient guidance for multi-step\nreasoning processes. Specifically, sparse reward signals fail to deliver\neffective or sufficient feedback, particularly for challenging problems.\nFurthermore, such reward structures induce systematic biases that prioritize\nexploitation of familiar trajectories over novel solution discovery. These\nshortcomings critically hinder performance in complex reasoning tasks, which\ninherently demand iterative refinement across ipntermediate steps. To address\nthese challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd\nfoR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense\nrewards and amplify explorations in the RL-based training paradigm. i-MENTOR\nintroduces three key innovations: trajectory-aware exploration rewards that\nmitigate bias in token-level strategies while maintaining computational\nefficiency; dynamic reward scaling to stabilize exploration and exploitation in\nlarge action spaces; and advantage-preserving reward implementation that\nmaintains advantage distribution integrity while incorporating exploratory\nguidance. Experiments across three public datasets demonstrate i-MENTOR's\neffectiveness with a 22.39% improvement on the difficult dataset Countdown-4.\n","authors":["Jingtong Gao","Ling Pan","Yejing Wang","Rui Zhong","Chi Lu","Qingpeng Cai","Peng Jiang","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.17621v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08261v1","updated":"2025-07-11T02:13:42Z","published":"2025-07-11T02:13:42Z","title":"Admissibility of Stein Shrinkage for Batch Normalization in the Presence\n  of Adversarial Attacks","summary":"  Batch normalization (BN) is a ubiquitous operation in deep neural networks\nused primarily to achieve stability and regularization during network training.\nBN involves feature map centering and scaling using sample means and variances,\nrespectively. Since these statistics are being estimated across the feature\nmaps within a batch, this problem is ideally suited for the application of\nStein's shrinkage estimation, which leads to a better, in the\nmean-squared-error sense, estimate of the mean and variance of the batch. In\nthis paper, we prove that the Stein shrinkage estimator for the mean and\nvariance dominates over the sample mean and variance estimators in the presence\nof adversarial attacks when modeling these attacks using sub-Gaussian\ndistributions. This facilitates and justifies the application of Stein\nshrinkage to estimate the mean and variance parameters in BN and use it in\nimage classification (segmentation) tasks with and without adversarial attacks.\nWe present SOTA performance results using this Stein corrected batch norm in a\nstandard ResNet architecture applied to the task of image classification using\nCIFAR-10 data, 3D CNN on PPMI (neuroimaging) data and image segmentation using\nHRNet on Cityscape data with and without adversarial attacks.\n","authors":["Sofia Ivolgina","P. Thomas Fletcher","Baba C. Vemuri"],"pdf_url":"https://arxiv.org/pdf/2507.08261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08255v1","updated":"2025-07-11T02:00:06Z","published":"2025-07-11T02:00:06Z","title":"Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)","summary":"  Missing data presents a critical challenge in real-world datasets,\nsignificantly degrading the performance of machine learning models. While Large\nLanguage Models (LLMs) have recently demonstrated remarkable capabilities in\ntabular data imputation, exemplified by frameworks like UnIMP, their reliance\non classical embedding methods often limits their ability to capture complex,\nnon-linear correlations, particularly in mixed-type data scenarios encompassing\nnumerical, categorical, and textual features. This paper introduces\nQuantum-UnIMP, a novel framework that integrates shallow quantum circuits into\nan LLM-based imputation architecture. Our core innovation lies in replacing\nconventional classical input embeddings with quantum feature maps generated by\nan Instantaneous Quantum Polynomial (IQP) circuit. This approach enables the\nmodel to leverage quantum phenomena such as superposition and entanglement,\nthereby learning richer, more expressive representations of data and enhancing\nthe recovery of intricate missingness patterns. Our experiments on benchmark\nmixed-type datasets demonstrate that Quantum-UnIMP reduces imputation error by\nup to 15.2% for numerical features (RMSE) and improves classification accuracy\nby 8.7% for categorical features (F1-Score) compared to state-of-the-art\nclassical and LLM-based methods. These compelling results underscore the\nprofound potential of quantum-enhanced representations for complex data\nimputation tasks, even with near-term quantum hardware.\n","authors":["Hossein Jamali"],"pdf_url":"https://arxiv.org/pdf/2507.08255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08254v1","updated":"2025-07-11T01:59:52Z","published":"2025-07-11T01:59:52Z","title":"Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging\n  Pretrained 2D Foundation Models","summary":"  Current challenges in developing foundational models for volumetric imaging\ndata, such as magnetic resonance imaging (MRI), stem from the computational\ncomplexity of training state-of-the-art architectures in high dimensions and\ncurating sufficiently large datasets of volumes. To address these challenges,\nwe introduce Raptor (Random Planar Tensor Reduction), a train-free method for\ngenerating semantically rich embeddings for volumetric data. Raptor leverages a\nfrozen 2D foundation model, pretrained on natural images, to extract visual\ntokens from individual cross-sections of medical volumes. These tokens are then\nspatially compressed using random projections, significantly reducing\ncomputational complexity while retaining semantic information. Extensive\nexperiments on ten diverse medical volume tasks verify the superior performance\nof Raptor over state-of-the-art methods, including those pretrained exclusively\non medical volumes (+3% SuPreM, +6% MISFM, +10% Merlin, +13% VoCo, and +14%\nSLIViT), while entirely bypassing the need for costly training. Our results\nhighlight the effectiveness and versatility of Raptor as a foundation for\nadvancing deep learning-based methods for medical volumes.\n","authors":["Ulzee An","Moonseong Jeong","Simon A. Lee","Aditya Gorla","Yuzhe Yang","Sriram Sankararaman"],"pdf_url":"https://arxiv.org/pdf/2507.08254v1.pdf","comment":"21 pages, 10 figures, accepted to ICML 2025. The first two authors\n  contributed equally"},{"id":"http://arxiv.org/abs/2502.09832v3","updated":"2025-07-11T01:56:58Z","published":"2025-02-14T00:24:51Z","title":"Algorithmic contiguity from low-degree conjecture and applications in\n  correlated random graphs","summary":"  In this paper, assuming a natural strengthening of the low-degree conjecture,\nwe provide evidence of computational hardness for two problems: (1) the\n(partial) matching recovery problem in the sparse correlated Erd\\H{o}s-R\\'enyi\ngraphs $\\mathcal G(n,q;\\rho)$ when the edge-density $q=n^{-1+o(1)}$ and the\ncorrelation $\\rho<\\sqrt{\\alpha}$ lies below the Otter's threshold, solving a\nremaining problem in \\cite{DDL23+}; (2) the detection problem between the\ncorrelated sparse stochastic block model $\\mathcal\nS(n,\\tfrac{\\lambda}{n};k,\\epsilon;s)$ and a pair of independent stochastic\nblock models $\\mathcal S(n,\\tfrac{\\lambda s}{n};k,\\epsilon)$ when $\\epsilon^2\n\\lambda s<1$ lies below the Kesten-Stigum (KS) threshold and $s<\\sqrt{\\alpha}$\nlies below the Otter's threshold, solving a remaining problem in\n\\cite{CDGL24+}.\n  One of the main ingredient in our proof is to derive certain forms of\n\\emph{algorithmic contiguity} between two probability measures based on bounds\non their low-degree advantage. To be more precise, consider the\nhigh-dimensional hypothesis testing problem between two probability measures\n$\\mathbb{P}$ and $\\mathbb{Q}$ based on the sample $\\mathsf Y$. We show that if\nthe low-degree advantage $\\mathsf{Adv}_{\\leq D} \\big(\n\\frac{\\mathrm{d}\\mathbb{P}}{\\mathrm{d}\\mathbb{Q}} \\big)=O(1)$, then (assuming\nthe low-degree conjecture) there is no efficient algorithm $\\mathcal A$ such\nthat $\\mathbb{Q}(\\mathcal A(\\mathsf Y)=0)=1-o(1)$ and $\\mathbb{P}(\\mathcal\nA(\\mathsf Y)=1)=\\Omega(1)$. This framework provides a useful tool for\nperforming reductions between different inference tasks.\n","authors":["Zhangsong Li"],"pdf_url":"https://arxiv.org/pdf/2502.09832v3.pdf","comment":"minor updates; extended abstract to appear in RANDOM 2025"},{"id":"http://arxiv.org/abs/2408.16138v2","updated":"2025-07-11T01:54:35Z","published":"2024-08-28T20:56:35Z","title":"Thinner Latent Spaces: Detecting Dimension and Imposing Invariance with\n  Conformal Autoencoders","summary":"  Conformal Autoencoders are a neural network architecture that imposes\northogonality conditions between the gradients of latent variables to obtain\ndisentangled representations of data. In this work we show that orthogonality\nrelations within the latent layer of the network can be leveraged to infer the\nintrinsic dimensionality of nonlinear manifold data sets (locally characterized\nby the dimension of their tangent space), while simultaneously computing\nencoding and decoding (embedding) maps. We outline the relevant theory relying\non differential geometry, and describe the corresponding gradient-descent\noptimization algorithm. The method is applied to several data sets and we\nhighlight its applicability, advantages, and shortcomings. In addition, we\ndemonstrate that the same computational technology can be used to build\ncoordinate invariance to local group actions when defined only on a (reduced)\nsubmanifold of the embedding space.\n","authors":["George A. Kevrekidis","Zan Ahmad","Mauro Maggioni","Soledad Villar","Yannis G. Kevrekidis"],"pdf_url":"https://arxiv.org/pdf/2408.16138v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19715v3","updated":"2025-07-11T01:33:18Z","published":"2024-05-30T05:49:38Z","title":"SpecDec++: Boosting Speculative Decoding via Adaptive Candidate Lengths","summary":"  Speculative decoding reduces the inference latency of a target large language\nmodel via utilizing a smaller and faster draft model. Its performance depends\non a hyperparameter K -- the candidate length, i.e., the number of candidate\ntokens for the target model to verify in each round. However, previous methods\noften use simple heuristics to choose K, which may result in sub-optimal\nperformance. We study the choice of the candidate length K and formulate it as\na Markov Decision Process. We theoretically show that the optimal policy of\nthis Markov decision process takes the form of a threshold policy, i.e., the\ncurrent speculation should stop and be verified when the probability of getting\na rejection exceeds a threshold value. Motivated by this theory, we propose\nSpecDec++, an enhanced version of speculative decoding that adaptively\ndetermines the candidate length on the fly. We augment the draft model with a\ntrained acceptance prediction head to predict the conditional acceptance\nprobability of the candidate tokens. SpecDec++ will stop the current\nspeculation when the predicted probability that at least one token gets\nrejected exceeds a threshold. We implement SpecDec++ and apply it to the\nllama-2-chat 7B & 70B model pair. Our adaptive method achieves a 2.04x speedup\non the Alpaca dataset (7.2% improvement over the baseline speculative\ndecoding). On the GSM8K and HumanEval datasets, our method achieves a 2.26x\nspeedup (9.4% improvement) and 2.23x speedup (11.1% improvement), respectively.\nThe code of this paper is available at\nhttps://github.com/Kaffaljidhmah2/SpecDec_pp.\n","authors":["Kaixuan Huang","Xudong Guo","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2405.19715v3.pdf","comment":"Accepted to COLM 2025"},{"id":"http://arxiv.org/abs/2406.06227v2","updated":"2025-07-11T01:22:08Z","published":"2024-06-10T12:53:13Z","title":"PAC-Bayes Analysis for Recalibration in Classification","summary":"  Nonparametric estimation using uniform-width binning is a standard approach\nfor evaluating the calibration performance of machine learning models. However,\nexisting theoretical analyses of the bias induced by binning are limited to\nbinary classification, creating a significant gap with practical applications\nsuch as multiclass classification. Additionally, many parametric recalibration\nalgorithms lack theoretical guarantees for their generalization performance. To\naddress these issues, we conduct a generalization analysis of calibration error\nusing the probably approximately correct Bayes framework. This approach enables\nus to derive the first optimizable upper bound for generalization error in the\ncalibration context. On the basis of our theory, we propose a\ngeneralization-aware recalibration algorithm. Numerical experiments show that\nour algorithm enhances the performance of Gaussian process-based recalibration\nacross various benchmark datasets and models.\n","authors":["Masahiro Fujisawa","Futoshi Futami"],"pdf_url":"https://arxiv.org/pdf/2406.06227v2.pdf","comment":"Accepted by the 42nd International Conference on Machine Learning\n  (ICML2025), 38 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.08248v1","updated":"2025-07-11T01:21:21Z","published":"2025-07-11T01:21:21Z","title":"Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi\n  Classification","summary":"  Accurate identification of fungi species presents a unique challenge in\ncomputer vision due to fine-grained inter-species variation and high\nintra-species variation. This paper presents our approach for the FungiCLEF\n2025 competition, which focuses on few-shot fine-grained visual categorization\n(FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented\nwith multiple vision transformer models, data augmentation, weighted sampling,\nand incorporating textual information. We also explored generative AI models\nfor zero-shot classification using structured prompting but found them to\nsignificantly underperform relative to vision-based models. Our final model\noutperformed both competition baselines and highlighted the effectiveness of\ndomain specific pretraining and balanced sampling strategies. Our approach\nranked 35/74 on the private test set in post-completion evaluation, this\nsuggests additional work can be done on metadata selection and domain-adapted\nmulti-modal learning. Our code is available at\nhttps://github.com/dsgt-arc/fungiclef-2025.\n","authors":["Jason Kahei Tam","Murilo Gustineli","Anthony Miyaguchi"],"pdf_url":"https://arxiv.org/pdf/2507.08248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17561v8","updated":"2025-07-11T01:15:31Z","published":"2024-03-26T10:10:53Z","title":"A Survey on State-of-the-art Deep Learning Applications and Challenges","summary":"  Deep learning, a branch of artificial intelligence, is a data-driven method\nthat uses multiple layers of interconnected units or neurons to learn intricate\npatterns and representations directly from raw input data. Empowered by this\nlearning capability, it has become a powerful tool for solving complex problems\nand is the core driver of many groundbreaking technologies and innovations.\nBuilding a deep learning model is challenging due to the algorithm's complexity\nand the dynamic nature of real-world problems. Several studies have reviewed\ndeep learning concepts and applications. However, the studies mostly focused on\nthe types of deep learning models and convolutional neural network\narchitectures, offering limited coverage of the state-of-the-art deep learning\nmodels and their applications in solving complex problems across different\ndomains. Therefore, motivated by the limitations, this study aims to\ncomprehensively review the state-of-the-art deep learning models in computer\nvision, natural language processing, time series analysis and pervasive\ncomputing, and robotics. We highlight the key features of the models and their\neffectiveness in solving the problems within each domain. Furthermore, this\nstudy presents the fundamentals of deep learning, various deep learning model\ntypes and prominent convolutional neural network architectures. Finally,\nchallenges and future directions in deep learning research are discussed to\noffer a broader perspective for future researchers.\n","authors":["Mohd Halim Mohd Noor","Ayokunle Olalekan Ige"],"pdf_url":"https://arxiv.org/pdf/2403.17561v8.pdf","comment":"Document updated to two-column formatting. This manuscript has been\n  accepted for publication in Engineering Applications of Artificial\n  Intelligence (Elsevier)"},{"id":"http://arxiv.org/abs/2507.08243v1","updated":"2025-07-11T01:13:17Z","published":"2025-07-11T01:13:17Z","title":"CoreSPECT: Enhancing Clustering Algorithms via an Interplay of Density\n  and Geometry","summary":"  Density and geometry have long served as two of the fundamental guiding\nprinciples in clustering algorithm design, with algorithm usually focusing\neither on the density structure of the data (e.g., HDBSCAN and Density Peak\nClustering) or the complexity of underlying geometry (e.g., manifold clustering\nalgorithms).\n  In this paper, we identify and formalize a recurring but often overlooked\ninteraction between distribution and geometry and leverage this insight to\ndesign our clustering enhancement framework CoreSPECT (Core Space\nProjection-based Enhancement of Clustering Techniques). Our framework boosts\nthe performance of simple algorithms like K-Means and GMM by applying them to\nstrategically selected regions, then extending the partial partition to a\ncomplete partition for the dataset using a novel neighborhood graph based\nmulti-layer propagation procedure.\n  We apply our framework on 15 datasets from three different domains and obtain\nconsistent and substantial gain in clustering accuracy for both K-Means and\nGMM. On average, our framework improves the ARI of K-Means by 40% and of GMM by\n14%, often surpassing the performance of both manifold-based and recent\ndensity-based clustering algorithms. We further support our framework with\ninitial theoretical guarantees, ablation to demonstrate the usefulness of the\nindividual steps and with evidence of robustness to noise.\n","authors":["Chandra Sekhar Mukherjee","Joonyoung Bae","Jiapeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.08243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18397v2","updated":"2025-07-11T01:11:24Z","published":"2025-05-23T22:05:19Z","title":"An Outlook on the Opportunities and Challenges of Multi-Agent AI Systems","summary":"  A multi-agent AI system (MAS) is composed of multiple autonomous agents that\ninteract, exchange information, and make decisions based on internal generative\nmodels. Recent advances in large language models and tool-using agents have\nmade MAS increasingly practical in areas like scientific discovery and\ncollaborative automation. However, key questions remain: When are MAS more\neffective than single-agent systems? What new safety risks arise from agent\ninteractions? And how should we evaluate their reliability and structure? This\npaper outlines a formal framework for analyzing MAS, focusing on two core\naspects: effectiveness and safety. We explore whether MAS truly improve\nrobustness, adaptability, and performance, or merely repackage known techniques\nlike ensemble learning. We also study how inter-agent dynamics may amplify or\nsuppress system vulnerabilities. While MAS are relatively new to the signal\nprocessing community, we envision them as a powerful abstraction that extends\nclassical tools like distributed estimation and sensor fusion to higher-level,\npolicy-driven inference. Through experiments on data science automation, we\nhighlight the potential of MAS to reshape how signal processing systems are\ndesigned and trusted.\n","authors":["Fangqiao Tian","An Luo","Jin Du","Xun Xian","Robert Specht","Ganghua Wang","Xuan Bi","Jiawei Zhou","Ashish Kundu","Jayanth Srinivasa","Charles Fleming","Rui Zhang","Zirui Liu","Mingyi Hong","Jie Ding"],"pdf_url":"https://arxiv.org/pdf/2505.18397v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08241v1","updated":"2025-07-11T01:11:06Z","published":"2025-07-11T01:11:06Z","title":"Exploring Gender Differences in Chronic Pain Discussions on Reddit","summary":"  Pain is an inherent part of human existence, manifesting as both physical and\nemotional experiences, and can be categorized as either acute or chronic. Over\nthe years, extensive research has been conducted to understand the causes of\npain and explore potential treatments, with contributions from various\nscientific disciplines. However, earlier studies often overlooked the role of\ngender in pain experiences. In this study, we utilized Natural Language\nProcessing (NLP) to analyze and gain deeper insights into individuals' pain\nexperiences, with a particular focus on gender differences. We successfully\nclassified posts into male and female corpora using the Hidden Attribute\nModel-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by\naggregating posts based on usernames. Our analysis revealed linguistic\ndifferences between genders, with female posts tending to be more emotionally\nfocused. Additionally, the study highlighted that conditions such as migraine\nand sinusitis are more prevalent among females and explored how pain medication\naffects individuals differently based on gender.\n","authors":["Ancita Maria Andrade","Tanvi Banerjee","Ramakrishna Mundugar"],"pdf_url":"https://arxiv.org/pdf/2507.08241v1.pdf","comment":"This is an extended version of the short paper accepted at ASONAM\n  2025"},{"id":"http://arxiv.org/abs/2411.06728v2","updated":"2025-07-11T01:06:04Z","published":"2024-11-11T05:51:11Z","title":"On the Principles of ReLU Networks with One Hidden Layer","summary":"  A neural network with one hidden layer or a two-layer network (regardless of\nthe input layer) is the simplest feedforward neural network, whose mechanism\nmay be the basis of more general network architectures. However, even to this\ntype of simple architecture, it is also a ``black box''; that is, it remains\nunclear how to interpret the mechanism of its solutions obtained by the\nback-propagation algorithm and how to control the training process through a\ndeterministic way. This paper systematically studies the first problem by\nconstructing universal function-approximation solutions. It is shown that, both\ntheoretically and experimentally, the training solution for the one-dimensional\ninput could be completely understood, and that for a higher-dimensional input\ncan also be well interpreted to some extent. Those results pave the way for\nthoroughly revealing the black box of two-layer ReLU networks and advance the\nunderstanding of deep ReLU networks.\n","authors":["Changcun Huang"],"pdf_url":"https://arxiv.org/pdf/2411.06728v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08239v1","updated":"2025-07-11T00:51:12Z","published":"2025-07-11T00:51:12Z","title":"Data Generation without Function Estimation","summary":"  Estimating the score function (or other population-density-dependent\nfunctions) is a fundamental component of most generative models. However, such\nfunction estimation is computationally and statistically challenging. Can we\navoid function estimation for data generation? We propose an estimation-free\ngenerative method: A set of points whose locations are deterministically\nupdated with (inverse) gradient descent can transport a uniform distribution to\narbitrary data distribution, in the mean field regime, without function\nestimation, training neural networks, and even noise injection. The proposed\nmethod is built upon recent advances in the physics of interacting particles.\nWe show, both theoretically and experimentally, that these advances can be\nleveraged to develop novel generative methods.\n","authors":["Hadi Daneshmand","Ashkan Soleymani"],"pdf_url":"https://arxiv.org/pdf/2507.08239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08238v1","updated":"2025-07-11T00:49:46Z","published":"2025-07-11T00:49:46Z","title":"Self-Supervised Learning-Based Multimodal Prediction on Prosocial\n  Behavior Intentions","summary":"  Human state detection and behavior prediction have seen significant\nadvancements with the rise of machine learning and multimodal sensing\ntechnologies. However, predicting prosocial behavior intentions in mobility\nscenarios, such as helping others on the road, is an underexplored area.\nCurrent research faces a major limitation. There are no large, labeled datasets\navailable for prosocial behavior, and small-scale datasets make it difficult to\ntrain deep-learning models effectively. To overcome this, we propose a\nself-supervised learning approach that harnesses multi-modal data from existing\nphysiological and behavioral datasets. By pre-training our model on diverse\ntasks and fine-tuning it with a smaller, manually labeled prosocial behavior\ndataset, we significantly enhance its performance. This method addresses the\ndata scarcity issue, providing a more effective benchmark for prosocial\nbehavior prediction, and offering valuable insights for improving intelligent\nvehicle systems and human-machine interaction.\n","authors":["Abinay Reddy Naini","Zhaobo K. Zheng","Teruhisa Misu","Kumar Akash"],"pdf_url":"https://arxiv.org/pdf/2507.08238v1.pdf","comment":"5 pages, 4 figures, published at ICASSP 2025"},{"id":"http://arxiv.org/abs/2507.08235v1","updated":"2025-07-11T00:45:16Z","published":"2025-07-11T00:45:16Z","title":"InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems","summary":"  Smart buildings generate vast streams of sensor and control data, but\nfacility managers often lack clear explanations for anomalous energy usage. We\npropose InsightBuild, a two-stage framework that integrates causality analysis\nwith a fine-tuned large language model (LLM) to provide human-readable, causal\nexplanations of energy consumption patterns. First, a lightweight causal\ninference module applies Granger causality tests and structural causal\ndiscovery on building telemetry (e.g., temperature, HVAC settings, occupancy)\ndrawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM,\nfine-tuned on aligned pairs of sensor-level causes and textual explanations,\nreceives as input the detected causal relations and generates concise,\nactionable explanations. We evaluate InsightBuild on two real-world datasets\n(Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth\ncauses for a held-out set of anomalies. Our results demonstrate that combining\nexplicit causal discovery with LLM-based natural language generation yields\nclear, precise explanations that assist facility managers in diagnosing and\nmitigating energy inefficiencies.\n","authors":["Pinaki Prasad Guha Neogi","Ahmad Mohammadshirazi","Rajiv Ramnath"],"pdf_url":"https://arxiv.org/pdf/2507.08235v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.08801v1","updated":"2025-07-11T17:59:42Z","published":"2025-07-11T17:59:42Z","title":"Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective","summary":"  Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.\n","authors":["Hangjie Yuan","Weihua Chen","Jun Cen","Hu Yu","Jingyun Liang","Shuning Chang","Zhihui Lin","Tao Feng","Pengwei Liu","Jiazheng Xing","Hao Luo","Jiasheng Tang","Fan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2507.08801v1.pdf","comment":"Code and Models: https://github.com/alibaba-damo-academy/Lumos"},{"id":"http://arxiv.org/abs/2502.12096v3","updated":"2025-07-11T13:47:50Z","published":"2025-02-17T18:14:18Z","title":"Token Communications: A Unified Framework for Cross-modal Context-aware\n  Semantic Communications","summary":"  In this paper, we introduce token communications (TokCom), a large\nmodel-driven framework to leverage cross-modal context information in\ngenerative semantic communications (GenSC). TokCom is a new paradigm, motivated\nby the recent success of generative foundation models and multimodal large\nlanguage models (GFM/MLLMs), where the communication units are tokens, enabling\nefficient transformer-based token processing at the transmitter and receiver.\nIn this paper, we introduce the potential opportunities and challenges of\nleveraging context in GenSC, explore how to integrate GFM/MLLMs-based token\nprocessing into semantic communication systems to leverage cross-modal context\neffectively at affordable complexity, present the key principles for efficient\nTokCom at various layers in future wireless networks. In a typical image\nsemantic communication setup, we demonstrate a significant improvement of the\nbandwidth efficiency, achieved by TokCom by leveraging the context information\namong tokens. Finally, the potential research directions are identified to\nfacilitate adoption of TokCom in future wireless networks.\n","authors":["Li Qiao","Mahdi Boloursaz Mashhadi","Zhen Gao","Rahim Tafazolli","Mehdi Bennis","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2502.12096v3.pdf","comment":"Accepted at IEEE Wireless Communications Magazine"},{"id":"http://arxiv.org/abs/2507.08590v1","updated":"2025-07-11T13:38:01Z","published":"2025-07-11T13:38:01Z","title":"Visual Semantic Description Generation with MLLMs for Image-Text\n  Matching","summary":"  Image-text matching (ITM) aims to address the fundamental challenge of\naligning visual and textual modalities, which inherently differ in their\nrepresentations, continuous, high-dimensional image features vs. discrete,\nstructured text. We propose a novel framework that bridges the modality gap by\nleveraging multimodal large language models (MLLMs) as visual semantic parsers.\nBy generating rich Visual Semantic Descriptions (VSD), MLLMs provide semantic\nanchor that facilitate cross-modal alignment. Our approach combines: (1)\nInstance-level alignment by fusing visual features with VSD to enhance the\nlinguistic expressiveness of image representations, and (2) Prototype-level\nalignment through VSD clustering to ensure category-level consistency. These\nmodules can be seamlessly integrated into existing ITM models. Extensive\nexperiments on Flickr30K and MSCOCO demonstrate substantial performance\nimprovements. The approach also exhibits remarkable zero-shot generalization to\ncross-domain tasks, including news and remote sensing ITM. The code and model\ncheckpoints are available at https://github.com/Image-Text-Matching/VSD.\n","authors":["Junyu Chen","Yihua Gao","Mingyong Li"],"pdf_url":"https://arxiv.org/pdf/2507.08590v1.pdf","comment":"Accepted by ICME2025 oral"},{"id":"http://arxiv.org/abs/2507.08557v1","updated":"2025-07-11T12:57:51Z","published":"2025-07-11T12:57:51Z","title":"FreeAudio: Training-Free Timing Planning for Controllable Long-Form\n  Text-to-Audio Generation","summary":"  Text-to-audio (T2A) generation has achieved promising results with the recent\nadvances in generative models. However, because of the limited quality and\nquantity of temporally-aligned audio-text pairs, existing T2A methods struggle\nto handle the complex text prompts that contain precise timing control, e.g.,\n\"owl hooted at 2.4s-5.2s\". Recent works have explored data augmentation\ntechniques or introduced timing conditions as model inputs to enable\ntiming-conditioned 10-second T2A generation, while their synthesis quality is\nstill limited. In this work, we propose a novel training-free timing-controlled\nT2A framework, FreeAudio, making the first attempt to enable timing-controlled\nlong-form T2A generation, e.g., \"owl hooted at 2.4s-5.2s and crickets chirping\nat 0s-24s\". Specifically, we first employ an LLM to plan non-overlapping time\nwindows and recaption each with a refined natural language description, based\non the input text and timing prompts. Then we introduce: 1) Decoupling and\nAggregating Attention Control for precise timing control; 2) Contextual Latent\nComposition for local smoothness and Reference Guidance for global consistency.\nExtensive experiments show that: 1) FreeAudio achieves state-of-the-art\ntiming-conditioned T2A synthesis quality among training-free methods and is\ncomparable to leading training-based methods; 2) FreeAudio demonstrates\ncomparable long-form generation quality with training-based Stable Audio and\npaves the way for timing-controlled long-form T2A synthesis. Demo samples are\navailable at: https://freeaudio.github.io/FreeAudio/\n","authors":["Yuxuan Jiang","Zehua Chen","Zeqian Ju","Chang Li","Weibei Dou","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.08557v1.pdf","comment":"Accepted at ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.08400v1","updated":"2025-07-11T08:18:52Z","published":"2025-07-11T08:18:52Z","title":"PanMatch: Unleashing the Potential of Large Vision Models for Unified\n  Matching Models","summary":"  This work presents PanMatch, a versatile foundation model for robust\ncorrespondence matching. Unlike previous methods that rely on task-specific\narchitectures and domain-specific fine-tuning to support tasks like stereo\nmatching, optical flow or feature matching, our key insight is that any\ntwo-frame correspondence matching task can be addressed within a 2D\ndisplacement estimation framework using the same model weights. Such a\nformulation eliminates the need for designing specialized unified architectures\nor task-specific ensemble models. Instead, it achieves multi-task integration\nby endowing displacement estimation algorithms with unprecedented\ngeneralization capabilities. To this end, we highlight the importance of a\nrobust feature extractor applicable across multiple domains and tasks, and\npropose the feature transformation pipeline that leverage all-purpose features\nfrom Large Vision Models to endow matching baselines with zero-shot cross-view\nmatching capabilities. Furthermore, we assemble a cross-domain dataset with\nnear 1.8 million samples from stereo matching, optical flow, and feature\nmatching domains to pretrain PanMatch. We demonstrate the versatility of\nPanMatch across a wide range of domains and downstream tasks using the same\nmodel weights. Our model outperforms UniMatch and Flow-Anything on cross-task\nevaluations, and achieves comparable performance to most state-of-the-art\ntask-specific algorithms on task-oriented benchmarks. Additionally, PanMatch\npresents unprecedented zero-shot performance in abnormal scenarios, such as\nrainy day and satellite imagery, where most existing robust algorithms fail to\nyield meaningful results.\n","authors":["Yongjian Zhang","Longguang Wang","Kunhong Li","Ye Zhang","Yun Wang","Liang Lin","Yulan Guo"],"pdf_url":"https://arxiv.org/pdf/2507.08400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07633v2","updated":"2025-07-11T03:42:15Z","published":"2025-07-10T11:01:58Z","title":"T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates","summary":"  Recent advances in video generation techniques have given rise to an emerging\nparadigm of generative video coding, aiming to achieve semantically accurate\nreconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong\ngenerative priors. However, most existing methods are limited by domain\nspecificity (e.g., facial or human videos) or an excessive dependence on\nhigh-level text guidance, which often fails to capture motion details and\nresults in unrealistic reconstructions. To address these challenges, we propose\na Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC\nemploys a semantic-aware sparse motion sampling pipeline to effectively bridge\nlow-level motion tracking with high-level semantic understanding by extracting\npixel-wise motion as sparse trajectory points based on their semantic\nimportance, not only significantly reducing the bitrate but also preserving\ncritical temporal semantic information. In addition, by incorporating\ntrajectory-aligned loss constraints into diffusion processes, we introduce a\ntraining-free latent space guidance mechanism to ensure physically plausible\nmotion patterns without sacrificing the inherent capabilities of generative\nmodels. Experimental results demonstrate that our framework outperforms both\ntraditional codecs and state-of-the-art end-to-end video compression methods\nunder ULB conditions. Furthermore, additional experiments confirm that our\napproach achieves more precise motion control than existing text-guided\nmethods, paving the way for a novel direction of generative video coding guided\nby geometric motion modeling.\n","authors":["Zhitao Wang","Hengyu Man","Wenrui Li","Xingtao Wang","Xiaopeng Fan","Debin Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.07633v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09068v1","updated":"2025-07-11T23:07:04Z","published":"2025-07-11T23:07:04Z","title":"Infinite Video Understanding","summary":"  The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.\n","authors":["Dell Zhang","Xiangyu Chen","Jixiang Luo","Mengxi Jia","Changzhi Sun","Ruilong Ren","Jingren Liu","Hao Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2507.09068v1.pdf","comment":null}]},"2025-07-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2507.10548v1","updated":"2025-07-14T17:59:46Z","published":"2025-07-14T17:59:46Z","title":"EmbRACE-3K: Embodied Reasoning and Action in Complex Environments","summary":"  Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities.\n","authors":["Mingxian Lin","Wei Huang","Yitang Li","Chengjie Jiang","Kui Wu","Fangwei Zhong","Shengju Qian","Xin Wang","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2507.10548v1.pdf","comment":"Project page: https://mxllc.github.io/EmbRACE-3K/"},{"id":"http://arxiv.org/abs/2507.10541v1","updated":"2025-07-14T17:58:47Z","published":"2025-07-14T17:58:47Z","title":"REST: Stress Testing Large Reasoning Models by Asking Multiple Problems\n  at Once","summary":"  Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual\ncreation of new questions with large human efforts, (2) failure to evaluate\nmodels under multi-context pressure, a key requirement for real-world\ndeployment. To bridge this gap, we present REST (Reasoning Evaluation through\nSimultaneous Testing), a stress-testing framework that concurrently exposes\nLRMs to multiple problems simultaneously. Beyond basic reasoning, REST\nspecifically evaluates several under-tested capabilities: contextual priority\nallocation, cross-problem interference resistance, and dynamic cognitive load\nmanagement. Our evaluation reveals several striking findings: Even\nstate-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance\ndegradation under stress testing. Crucially, REST demonstrates stronger\ndiscriminative power than existing benchmarks, revealing pronounced performance\ndifferences among models that exhibit similar, near-ceiling performance under\nsingle-question evaluations. Some key mechanistic insights emerge from our\nanalysis: (1) the \"overthinking trap\" is a critical factor contributing to the\nperformance degradation; (2) the models trained with \"long2short\" technique\npreserve more accuracy of their single-problem performance under REST,\noutperforming standard-trained counterparts. These results establish REST as a\ncost-efficient, future-proof evaluation paradigm that better reflects\nreal-world reasoning demands while reducing reliance on continuous human\nannotation.\n","authors":["Zhuoshi Pan","Qizhi Pei","Yu Li","Qiyao Sun","Zinan Tang","H. Vicky Zhao","Conghui He","Lijun Wu"],"pdf_url":"https://arxiv.org/pdf/2507.10541v1.pdf","comment":"REST (Reasoning Evaluation through Simultaneous Testing), a\n  stress-testing framework that concurrently exposes LRMs to multiple problems\n  simultaneously"},{"id":"http://arxiv.org/abs/2507.10535v1","updated":"2025-07-14T17:56:29Z","published":"2025-07-14T17:56:29Z","title":"CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks","summary":"  Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.\n","authors":["Hongchao Jiang","Yiming Chen","Yushi Cao","Hung-yi Lee","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2507.10535v1.pdf","comment":"Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench"},{"id":"http://arxiv.org/abs/2507.10532v1","updated":"2025-07-14T17:55:15Z","published":"2025-07-14T17:55:15Z","title":"Reasoning or Memorization? Unreliable Results of Reinforcement Learning\n  Due to Data Contamination","summary":"  The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.\n","authors":["Mingqi Wu","Zhihao Zhang","Qiaole Dong","Zhiheng Xi","Jun Zhao","Senjie Jin","Xiaoran Fan","Yuhao Zhou","Yanwei Fu","Qin Liu","Songyang Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.10532v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2507.03152v2","updated":"2025-07-14T17:51:35Z","published":"2025-07-03T20:19:18Z","title":"Expert-level validation of AI-generated medical text with scalable\n  language models","summary":"  With the growing use of language models (LMs) in clinical environments, there\nis an immediate need to evaluate the accuracy and safety of LM-generated\nmedical text. Currently, such evaluation relies solely on manual physician\nreview. However, detecting errors in LM-generated text is challenging because\n1) manual review is costly and 2) expert-composed reference outputs are often\nunavailable in real-world settings. While the \"LM-as-judge\" paradigm (a LM\nevaluating another LM) offers scalable evaluation, even frontier LMs can miss\nsubtle but clinically significant errors. To address these challenges, we\npropose MedVAL, a self-supervised framework that leverages synthetic data to\ntrain evaluator LMs to assess whether LM-generated medical outputs are\nfactually consistent with inputs, without requiring physician labels or\nreference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a\ndataset containing 840 outputs annotated by physicians, following a\nphysician-defined taxonomy of risk levels and error categories. Across 6\ndiverse medical tasks and 10 state-of-the-art LMs spanning open-source,\nproprietary, and medically adapted models, MedVAL fine-tuning significantly\nimproves (p < 0.001) alignment with physicians on both seen and unseen tasks,\nincreasing average F1 scores from 66% to 83%, with per-sample safety\nclassification scores up to 86%. MedVAL improves the performance of even the\nbest-performing proprietary LM (GPT-4o) by 8%. To support a scalable,\nrisk-aware pathway towards clinical integration, we open-source the 1) codebase\n(https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench\n(https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), and 3) MedVAL-4B\n(https://huggingface.co/stanfordmimi/MedVAL-4B), the best-performing\nopen-source LM. Our research provides the first evidence of LMs approaching\nexpert-level validation ability for medical text.\n","authors":["Asad Aali","Vasiliki Bikia","Maya Varma","Nicole Chiou","Sophie Ostmeier","Arnav Singhvi","Magdalini Paschali","Ashwin Kumar","Andrew Johnston","Karimar Amador-Martinez","Eduardo Juan Perez Guerrero","Paola Naovi Cruz Rivera","Sergios Gatidis","Christian Bluethgen","Eduardo Pontes Reis","Eddy D. Zandee van Rilland","Poonam Laxmappa Hosamani","Kevin R Keet","Minjoung Go","Evelyn Ling","David B. Larson","Curtis Langlotz","Roxana Daneshjou","Jason Hom","Sanmi Koyejo","Emily Alsentzer","Akshay S. Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2507.03152v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10524v1","updated":"2025-07-14T17:49:00Z","published":"2025-07-14T17:49:00Z","title":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation","summary":"  Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.\n","authors":["Sangmin Bae","Yujin Kim","Reza Bayat","Sungnyun Kim","Jiyoun Ha","Tal Schuster","Adam Fisch","Hrayr Harutyunyan","Ziwei Ji","Aaron Courville","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2507.10524v1.pdf","comment":"36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions"},{"id":"http://arxiv.org/abs/2507.10522v1","updated":"2025-07-14T17:47:28Z","published":"2025-07-14T17:47:28Z","title":"DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex\n  Scientific Question Answering in Ecology","summary":"  We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.\n","authors":["Jennifer D'Souza","Endres Keno Sander","Andrei Aioanei"],"pdf_url":"https://arxiv.org/pdf/2507.10522v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.10475v1","updated":"2025-07-14T16:55:57Z","published":"2025-07-14T16:55:57Z","title":"Can You Detect the Difference?","summary":"  The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking.\n","authors":["İsmail Tarım","Aytuğ Onan"],"pdf_url":"https://arxiv.org/pdf/2507.10475v1.pdf","comment":"11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance"},{"id":"http://arxiv.org/abs/2507.10472v1","updated":"2025-07-14T16:53:19Z","published":"2025-07-14T16:53:19Z","title":"MLAR: Multi-layer Large Language Model-based Robotic Process Automation\n  Applicant Tracking","summary":"  This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs.\n","authors":["Mohamed T. Younes","Omar Walid","Mai Hassan","Ali Hamdi"],"pdf_url":"https://arxiv.org/pdf/2507.10472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10468v1","updated":"2025-07-14T16:46:30Z","published":"2025-07-14T16:46:30Z","title":"From BERT to Qwen: Hate Detection across architectures","summary":"  Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).\n","authors":["Ariadna Mon","Saúl Fenollosa","Jon Lecumberri"],"pdf_url":"https://arxiv.org/pdf/2507.10468v1.pdf","comment":"4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)"},{"id":"http://arxiv.org/abs/2504.12355v2","updated":"2025-07-14T16:35:20Z","published":"2025-04-16T02:33:19Z","title":"Leveraging Large Language Models for Multi-Class and Multi-Label\n  Detection of Drug Use and Overdose Symptoms on Social Media","summary":"  Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies.\n","authors":["Muhammad Ahmad","Fida Ullah","Ummhy Habiba","ldar Batyrshin","Grigori Sidorov"],"pdf_url":"https://arxiv.org/pdf/2504.12355v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15266v3","updated":"2025-07-14T16:34:31Z","published":"2025-04-21T17:47:46Z","title":"Roll the dice & look before you leap: Going beyond the creative limits\n  of next-token prediction","summary":"  We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic; multi-token approaches,\nnamely teacherless training and diffusion models, comparatively excel in\nproducing diverse and original output. Secondly, to elicit randomness without\nhurting coherence, we find that injecting noise at the input layer (dubbed\nseed-conditioning) works surprisingly as well as (and in some conditions,\nbetter than) temperature sampling from the output layer. Thus, our work offers\na principled, minimal test-bed for analyzing open-ended creative skills, and\noffers new arguments for going beyond next-token learning and temperature\nsampling. We make part of the code available under\nhttps://github.com/chenwu98/algorithmic-creativity\n","authors":["Vaishnavh Nagarajan","Chen Henry Wu","Charles Ding","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2504.15266v3.pdf","comment":"ICML 2025 (oral)"},{"id":"http://arxiv.org/abs/2507.10445v1","updated":"2025-07-14T16:28:00Z","published":"2025-07-14T16:28:00Z","title":"Referential ambiguity and clarification requests: comparing human and\n  LLM behaviour","summary":"  In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy.\n","authors":["Chris Madge","Matthew Purver","Massimo Poesio"],"pdf_url":"https://arxiv.org/pdf/2507.10445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10419v1","updated":"2025-07-14T16:00:51Z","published":"2025-07-14T16:00:51Z","title":"Multiple Choice Learning of Low Rank Adapters for Language Modeling","summary":"  We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs.\n","authors":["Victor Letzelter","Hugo Malard","Mathieu Fontaine","Gaël Richard","Slim Essid","Andrei Bursuc","Patrick Pérez"],"pdf_url":"https://arxiv.org/pdf/2507.10419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05285v2","updated":"2025-07-14T15:49:02Z","published":"2025-07-04T21:41:43Z","title":"Beyond classical and contemporary models: a transformative AI framework\n  for student dropout prediction in distance learning using RAG, Prompt\n  engineering, and Cross-modal fusion","summary":"  Student dropout in distance learning remains a critical challenge, with\nprofound societal and economic consequences. While classical machine learning\nmodels leverage structured socio-demographic and behavioral data, they often\nfail to capture the nuanced emotional and contextual factors embedded in\nunstructured student interactions. This paper introduces a transformative AI\nframework that redefines dropout prediction through three synergistic\ninnovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment\nanalysis, prompt engineering to decode academic stressors,and cross-modal\nattention fusion to dynamically align textual, behavioral, and\nsocio-demographic insights. By grounding sentiment analysis in a curated\nknowledge base of pedagogical content, our RAG-enhanced BERT model interprets\nstudent comments with unprecedented contextual relevance, while optimized\nprompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload\nanxiety\"). A cross-modal attention layer then fuses these insights with\ntemporal engagement patterns, creating holistic risk pro-files. Evaluated on a\nlongitudinal dataset of 4 423 students, the framework achieves 89% accuracy and\nan F1-score of 0.88, outperforming conventional models by 7% and reducing false\nnegatives by 21%. Beyond prediction, the system generates interpretable\ninterventions by retrieving contextually aligned strategies (e.g., mentorship\nprograms for isolated learners). This work bridges the gap between predictive\nanalytics and actionable pedagogy, offering a scalable solution to mitigate\ndropout risks in global education systems\n","authors":["Miloud Mihoubi","Meriem Zerkouk","Belkacem Chikhaoui"],"pdf_url":"https://arxiv.org/pdf/2507.05285v2.pdf","comment":"13 pages, 8 figures, 1 Algorithms, 17th International Conference on\n  Education and New Learning Technologies,: 30 June-2 July, 2025 Location:\n  Palma, Spain"},{"id":"http://arxiv.org/abs/2507.10403v1","updated":"2025-07-14T15:46:56Z","published":"2025-07-14T15:46:56Z","title":"Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources","summary":"  Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.\n","authors":["Daniele Rege Cambrin","Lorenzo Vaiani","Giuseppe Gallipoli","Luca Cagliero","Paolo Garza"],"pdf_url":"https://arxiv.org/pdf/2507.10403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10398v1","updated":"2025-07-14T15:38:42Z","published":"2025-07-14T15:38:42Z","title":"Devanagari Handwritten Character Recognition using Convolutional Neural\n  Network","summary":"  Handwritten character recognition is getting popular among researchers\nbecause of its possible applications in facilitating technological search\nengines, social media, recommender systems, etc. The Devanagari script is one\nof the oldest language scripts in India that does not have proper digitization\ntools. With the advancement of computing and technology, the task of this\nresearch is to extract handwritten Hindi characters from an image of Devanagari\nscript with an automated approach to save time and obsolete data. In this\npaper, we present a technique to recognize handwritten Devanagari characters\nusing two deep convolutional neural network layers. This work employs a\nmethodology that is useful to enhance the recognition rate and configures a\nconvolutional neural network for effective Devanagari handwritten text\nrecognition (DHTR). This approach uses the Devanagari handwritten character\ndataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each\nof these classes has 1700 images for training and testing purposes. This\napproach obtains promising results in terms of accuracy by achieving 96.36%\naccuracy in testing and 99.55% in training time.\n","authors":["Diksha Mehta","Prateek Mehta"],"pdf_url":"https://arxiv.org/pdf/2507.10398v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.06238v2","updated":"2025-07-14T15:16:18Z","published":"2024-10-08T17:54:03Z","title":"EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration","summary":"  Despite their success in many domains, large language models (LLMs) remain\nunder-studied in scenarios requiring optimal decision-making under uncertainty.\nThis is crucial as many real-world applications, ranging from personalized\nrecommendations to healthcare interventions, demand that LLMs not only predict\nbut also actively learn to make optimal decisions through exploration. In this\nwork, we measure LLMs' (in)ability to make optimal decisions in bandits, a\nstate-less reinforcement learning setting relevant to many applications. We\ndevelop a comprehensive suite of environments, including both context-free and\ncontextual bandits with varying task difficulties, to benchmark LLMs'\nperformance. Motivated by the existence of optimal exploration algorithms, we\npropose efficient ways to integrate this algorithmic knowledge into LLMs: by\nproviding explicit algorithm-guided support during inference; and through\nalgorithm distillation via in-context demonstrations and fine-tuning, using\nsynthetic data generated from these algorithms. Impressively, these techniques\nallow us to achieve superior exploration performance with smaller models,\nsurpassing larger models on various tasks. We conducted an extensive ablation\nstudy to shed light on various factors, such as task difficulty and data\nrepresentation, that influence the efficiency of LLM exploration. Additionally,\nwe conduct a rigorous analysis of the LLM's exploration efficiency using the\nconcept of regret, linking its ability to explore to the model size and\nunderlying algorithm.\n","authors":["Allen Nie","Yi Su","Bo Chang","Jonathan N. Lee","Ed H. Chi","Quoc V. Le","Minmin Chen"],"pdf_url":"https://arxiv.org/pdf/2410.06238v2.pdf","comment":"28 pages. Published at ICML 2025"},{"id":"http://arxiv.org/abs/2507.10354v1","updated":"2025-07-14T14:56:46Z","published":"2025-07-14T14:56:46Z","title":"Meanings are like Onions: a Layered Approach to Metaphor Processing","summary":"  Metaphorical meaning is not a flat mapping between concepts, but a complex\ncognitive phenomenon that integrates multiple levels of interpretation. In this\npaper, we propose a stratified model of metaphor processing that treats meaning\nas an onion: a multi-layered structure comprising (1) content analysis, (2)\nconceptual blending, and (3) pragmatic intentionality. This three-dimensional\nframework allows for a richer and more cognitively grounded approach to\nmetaphor interpretation in computational systems. At the first level, metaphors\nare annotated through basic conceptual elements. At the second level, we model\nconceptual combinations, linking components to emergent meanings. Finally, at\nthe third level, we introduce a pragmatic vocabulary to capture speaker intent,\ncommunicative function, and contextual effects, aligning metaphor understanding\nwith pragmatic theories. By unifying these layers into a single formal\nframework, our model lays the groundwork for computational methods capable of\nrepresenting metaphorical meaning beyond surface associations, toward deeper,\nmore context-sensitive reasoning.\n","authors":["Silvia Cappa","Anna Sofia Lippolis","Stefano Zoia"],"pdf_url":"https://arxiv.org/pdf/2507.10354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10342v1","updated":"2025-07-14T14:47:01Z","published":"2025-07-14T14:47:01Z","title":"Using AI to replicate human experimental results: a motion study","summary":"  This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry.\n","authors":["Rosa Illan Castillo","Javier Valenzuela"],"pdf_url":"https://arxiv.org/pdf/2507.10342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.22791v2","updated":"2025-07-14T14:42:00Z","published":"2025-06-28T07:25:12Z","title":"ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models","summary":"  Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.\n","authors":["Jianxin Yan","Wangze Ni","Lei Chen","Xuemin Lin","Peng Cheng","Zhan Qin","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2506.22791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10330v1","updated":"2025-07-14T14:38:48Z","published":"2025-07-14T14:38:48Z","title":"Bridging Robustness and Generalization Against Word Substitution Attacks\n  in NLP via the Growth Bound Matrix Approach","summary":"  Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM\n","authors":["Mohammed Bouri","Adnane Saoud"],"pdf_url":"https://arxiv.org/pdf/2507.10330v1.pdf","comment":"Accepted to ACL Findings 2025"},{"id":"http://arxiv.org/abs/2507.10326v1","updated":"2025-07-14T14:34:15Z","published":"2025-07-14T14:34:15Z","title":"Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation","summary":"  Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not.\n","authors":["Muzhaffar Hazman","Minh-Khoi Pham","Shweta Soundararajan","Goncalo Mordido","Leonardo Custode","David Lynch","Giorgio Cruciata","Yucheng Shi","Hongmeng Song","Wang Chao","Pan Yue","Aleksandar Milenovic","Alexandros Agapitos"],"pdf_url":"https://arxiv.org/pdf/2507.10326v1.pdf","comment":"Accepted for Publication at ECAI 2025"},{"id":"http://arxiv.org/abs/2505.12864v3","updated":"2025-07-14T14:30:57Z","published":"2025-05-19T08:48:12Z","title":"LEXam: Benchmarking Legal Reasoning on 340 Law Exams","summary":"  Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/\n","authors":["Yu Fan","Jingwei Ni","Jakob Merane","Etienne Salimbeni","Yang Tian","Yoan Hermstrüwer","Yinya Huang","Mubashara Akhtar","Florian Geering","Oliver Dreyer","Daniel Brunner","Markus Leippold","Mrinmaya Sachan","Alexander Stremitzer","Christoph Engel","Elliott Ash","Joel Niklaus"],"pdf_url":"https://arxiv.org/pdf/2505.12864v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10300v1","updated":"2025-07-14T14:04:14Z","published":"2025-07-14T14:04:14Z","title":"FaceLLM: A Multimodal Large Language Model for Face Understanding","summary":"  Multimodal large language models (MLLMs) have shown remarkable performance in\nvision-language tasks. However, existing MLLMs are primarily trained on generic\ndatasets, limiting their ability to reason on domain-specific visual cues such\nas those in facial images. In particular, tasks that require detailed\nunderstanding of facial structure, expression, emotion, and demographic\nfeatures remain underexplored by MLLMs due to the lack of large-scale annotated\nface image-text datasets. In this work, we introduce FaceLLM, a multimodal\nlarge language model trained specifically for facial image understanding. To\nconstruct the training data, we propose a novel weakly supervised pipeline that\nuses ChatGPT with attribute-aware prompts to generate high-quality\nquestion-answer pairs based on images from the FairFace dataset. The resulting\ncorpus, called FairFaceGPT, covers a diverse set of attributes including\nexpression, pose, skin texture, and forensic information. Our experiments\ndemonstrate that FaceLLM improves the performance of MLLMs on various\nface-centric tasks and achieves state-of-the-art performance. This work\nhighlights the potential of synthetic supervision via language models for\nbuilding domain-specialized MLLMs, and sets a precedent for trustworthy,\nhuman-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM\nmodels are publicly available in the project page.\n","authors":["Hatef Otroshi Shahreza","Sébastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2507.10300v1.pdf","comment":"Accepted in ICCV 2025 workshops"},{"id":"http://arxiv.org/abs/2504.11183v2","updated":"2025-07-14T13:35:01Z","published":"2025-04-15T13:40:22Z","title":"Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a\n  Low-Resource Setting","summary":"  Social bias in language models can potentially exacerbate social\ninequalities. Despite it having garnered wide attention, most research focuses\non English data. In a low-resource scenario, the models often perform worse due\nto insufficient training data. This study aims to leverage high-resource\nlanguage corpora to evaluate bias and experiment with debiasing methods in\nlow-resource languages. We evaluated the performance of recent multilingual\nmodels in five languages: English, Chinese, Russian, Indonesian and Thai, and\nanalyzed four bias dimensions: gender, religion, nationality, and race-color.\nBy constructing multilingual bias evaluation datasets, this study allows fair\ncomparisons between models across languages. We have further investigated three\ndebiasing methods-CDA, Dropout, SenDeb-and demonstrated that debiasing methods\nfrom high-resource languages can be effectively transferred to low-resource\nones, providing actionable insights for fairness research in multilingual NLP.\n","authors":["Ej Zhou","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2504.11183v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12992v2","updated":"2025-07-14T13:11:13Z","published":"2025-02-18T16:13:08Z","title":"B-cos LM: Efficiently Transforming Pre-trained Language Models for\n  Improved Explainability","summary":"  Post-hoc explanation methods for black-box models often struggle with\nfaithfulness and human interpretability due to the lack of explainability in\ncurrent neural architectures. Meanwhile, B-cos networks have been introduced to\nimprove model explainability by proposing an architecture that removes bias\nterms and promotes input-weight alignment. Although B-cos networks have shown\nsuccess in building explainable systems, their application has so far been\nlimited to computer vision models and their associated training pipelines. In\nthis work, we introduce B-cos LMs, i.e., B-cos language models (LMs) empowered\nfor natural language processing (NLP) tasks. Our approach directly transforms\npre-trained language models into B-cos LMs by combining B-cos conversion and\ntask fine-tuning, improving efficiency compared to previous methods. Our\nautomatic and human evaluation results demonstrate that B-cos LMs produce more\nfaithful and human interpretable explanations than post-hoc methods, while\nmaintaining task performance comparable to conventional fine-tuning. Our\nin-depth analysis explores how B-cos LMs differ from conventionally fine-tuned\nmodels in their learning processes and explanation patterns. Finally, we are\nalso the first to explore the transformation of decoder-only models to B-cos\nLMs for generation tasks.\n","authors":["Yifan Wang","Sukrut Rao","Ji-Ung Lee","Mayank Jobanputra","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.12992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14620v3","updated":"2025-07-14T12:58:37Z","published":"2022-11-26T17:31:25Z","title":"The distribution of syntactic dependency distances","summary":"  The syntactic structure of a sentence can be represented as a graph, where\nvertices are words and edges indicate syntactic dependencies between them. In\nthis setting, the distance between two linked words is defined as the\ndifference between their positions. Here we wish to contribute to the\ncharacterization of the actual distribution of syntactic dependency distances,\nwhich has previously been argued to follow a power-law distribution. Here we\npropose a new model with two exponential regimes in which the probability decay\nis allowed to change after a break-point. This transition could mirror the\ntransition from the processing of word chunks to higher-level structures. We\nfind that a two-regime model - where the first regime follows either an\nexponential or a power-law decay - is the most likely one in all 20 languages\nwe considered, independently of sentence length and annotation style. Moreover,\nthe break-point exhibits low variation across languages and averages values of\n4-5 words, suggesting that the amount of words that can be simultaneously\nprocessed abstracts from the specific language to a high degree. The\nprobability decay slows down after the breakpoint, consistently with a\nuniversal chunk-and-pass mechanism. Finally, we give an account of the relation\nbetween the best estimated model and the closeness of syntactic dependencies as\nfunction of sentence length, according to a recently introduced optimality\nscore.\n","authors":["Sonia Petrini","Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2211.14620v3.pdf","comment":"minor corrections; in press in Glottometrics"},{"id":"http://arxiv.org/abs/2507.10216v1","updated":"2025-07-14T12:33:07Z","published":"2025-07-14T12:33:07Z","title":"Absher: A Benchmark for Evaluating Large Language Models Understanding\n  of Saudi Dialects","summary":"  As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications.\n","authors":["Renad Al-Monef","Hassan Alhuzali","Nora Alturayeif","Ashwag Alasmari"],"pdf_url":"https://arxiv.org/pdf/2507.10216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10200v1","updated":"2025-07-14T12:13:50Z","published":"2025-07-14T12:13:50Z","title":"Natural Language-based Assessment of L2 Oral Proficiency using LLMs","summary":"  Natural language-based assessment (NLA) is an approach to second language\nassessment that uses instructions - expressed in the form of can-do descriptors\n- originally intended for human examiners, aiming to determine whether large\nlanguage models (LLMs) can interpret and apply them in ways comparable to human\nassessment. In this work, we explore the use of such descriptors with an\nopen-source LLM, Qwen 2.5 72B, to assess responses from the publicly available\nS&I Corpus in a zero-shot setting. Our results show that this approach -\nrelying solely on textual information - achieves competitive performance: while\nit does not outperform state-of-the-art speech LLMs fine-tuned for the task, it\nsurpasses a BERT-based model trained specifically for this purpose. NLA proves\nparticularly effective in mismatched task settings, is generalisable to other\ndata types and languages, and offers greater interpretability, as it is\ngrounded in clearly explainable, widely applicable language descriptors.\n","authors":["Stefano Bannò","Rao Ma","Mengjie Qian","Siyuan Tang","Kate Knill","Mark Gales"],"pdf_url":"https://arxiv.org/pdf/2507.10200v1.pdf","comment":"Accepted for the 10th Workshop on Speech and Language Technology in\n  Education (SLaTE 2025)"},{"id":"http://arxiv.org/abs/2505.17826v2","updated":"2025-07-14T12:02:28Z","published":"2025-05-23T12:41:09Z","title":"Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models","summary":"  Trinity-RFT is a general-purpose, unified and easy-to-use framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na modular and decoupled design, consisting of (1) an RFT-core that unifies and\ngeneralizes synchronous/asynchronous, on-policy/off-policy, and online/offline\nmodes of RFT; (2) seamless integration for agent-environment interaction with\nhigh efficiency and robustness; and (3) systematic data pipelines optimized for\nRFT. Trinity-RFT can be easily adapted for diverse application scenarios, and\nserves as a unified platform for development and research of advanced\nreinforcement learning paradigms at both macroscopic and microscopic levels.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples, applications and experiments\nthat demonstrate its functionalities and user-friendliness.\n","authors":["Xuchen Pan","Yanxi Chen","Yushuo Chen","Yuchang Sun","Daoyuan Chen","Wenhao Zhang","Yuexiang Xie","Yilun Huang","Yilei Zhang","Dawei Gao","Weijie Shi","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.17826v2.pdf","comment":"This technical report will be continuously updated as the codebase\n  evolves. GitHub: https://github.com/modelscope/Trinity-RFT"},{"id":"http://arxiv.org/abs/2507.08017v2","updated":"2025-07-14T11:46:41Z","published":"2025-07-07T20:26:31Z","title":"Mechanistic Indicators of Understanding in Large Language Models","summary":"  Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them.\n","authors":["Pierre Beckmann","Matthieu Queloz"],"pdf_url":"https://arxiv.org/pdf/2507.08017v2.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2507.10177v1","updated":"2025-07-14T11:39:34Z","published":"2025-07-14T11:39:34Z","title":"Abusive text transformation using LLMs","summary":"  Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3.\n","authors":["Rohitash Chandra","Jiyong Choi"],"pdf_url":"https://arxiv.org/pdf/2507.10177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10155v1","updated":"2025-07-14T11:10:02Z","published":"2025-07-14T11:10:02Z","title":"Task-Based Flexible Feature Distillation for LLMs","summary":"  Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.\n","authors":["Khouloud Saadi","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2507.10155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06241v2","updated":"2025-07-14T11:04:50Z","published":"2025-03-08T14:53:20Z","title":"A Noise-Robust Turn-Taking System for Real-World Dialogue Robots: A\n  Field Experiment","summary":"  Turn-taking is a crucial aspect of human-robot interaction, directly\ninfluencing conversational fluidity and user engagement. While previous\nresearch has explored turn-taking models in controlled environments, their\nrobustness in real-world settings remains underexplored. In this study, we\npropose a noise-robust voice activity projection (VAP) model, based on a\nTransformer architecture, to enhance real-time turn-taking in dialogue robots.\nTo evaluate the effectiveness of the proposed system, we conducted a field\nexperiment in a shopping mall, comparing the VAP system with a conventional\ncloud-based speech recognition system. Our analysis covered both subjective\nuser evaluations and objective behavioral analysis. The results showed that the\nproposed system significantly reduced response latency, leading to a more\nnatural conversation where both the robot and users responded faster. The\nsubjective evaluations suggested that faster responses contribute to a better\ninteraction experience.\n","authors":["Koji Inoue","Yuki Okafuji","Jun Baba","Yoshiki Ohira","Katsuya Hyodo","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2503.06241v2.pdf","comment":"This paper has been accepted for presentation at IEEE/RSJ\n  International Conference on Intelligent Robots and Systems 2025 (IROS 2025)\n  and represents the author's version of the work"},{"id":"http://arxiv.org/abs/2507.08036v2","updated":"2025-07-14T10:06:50Z","published":"2025-07-09T09:51:20Z","title":"Barriers in Integrating Medical Visual Question Answering into Radiology\n  Workflows: A Scoping Review and Clinicians' Insights","summary":"  Medical Visual Question Answering (MedVQA) is a promising tool to assist\nradiologists by automating medical image interpretation through question\nanswering. Despite advances in models and datasets, MedVQA's integration into\nclinical workflows remains limited. This study systematically reviews 68\npublications (2018-2024) and surveys 50 clinicians from India and Thailand to\nexamine MedVQA's practical utility, challenges, and gaps. Following the Arksey\nand O'Malley scoping review framework, we used a two-pronged approach: (1)\nreviewing studies to identify key concepts, advancements, and research gaps in\nradiology workflows, and (2) surveying clinicians to capture their perspectives\non MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs\nare non-diagnostic and lack clinical relevance. Most datasets and models do not\nsupport multi-view, multi-resolution imaging, EHR integration, or domain\nknowledge, features essential for clinical diagnosis. Furthermore, there is a\nclear mismatch between current evaluation metrics and clinical needs. The\nclinician survey confirms this disconnect: only 29.8% consider MedVQA systems\nhighly useful. Key concerns include the absence of patient history or domain\nknowledge (87.2%), preference for manually curated datasets (51.1%), and the\nneed for multi-view image support (78.7%). Additionally, 66% favor models\nfocused on specific anatomical regions, and 89.4% prefer dialogue-based\ninteractive systems. While MedVQA shows strong potential, challenges such as\nlimited multimodal analysis, lack of patient context, and misaligned evaluation\napproaches must be addressed for effective clinical integration.\n","authors":["Deepali Mishra","Chaklam Silpasuwanchai","Ashutosh Modi","Madhumita Sushil","Sorayouth Chumnanvej"],"pdf_url":"https://arxiv.org/pdf/2507.08036v2.pdf","comment":"29 pages, 5 figures (1 in supplementary), 3 tables (1 in main text, 2\n  in supplementary). Scoping review and clinician survey"},{"id":"http://arxiv.org/abs/2504.02882v2","updated":"2025-07-14T09:56:47Z","published":"2025-04-02T05:47:28Z","title":"DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models","summary":"  Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling.\n","authors":["Sunghee Jung","Donghun Lee","Shinbok Lee","Gaeun Seo","Daniel Lee","Byeongil Ko","Junrae Cho","Kihyun Kim","Eunggyun Kim","Myeongcheol Shin"],"pdf_url":"https://arxiv.org/pdf/2504.02882v2.pdf","comment":"Accepted to SIGDIAL 2025"},{"id":"http://arxiv.org/abs/2507.10098v1","updated":"2025-07-14T09:33:40Z","published":"2025-07-14T09:33:40Z","title":"Fusing Large Language Models with Temporal Transformers for Time Series\n  Forecasting","summary":"  Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach.\n","authors":["Chen Su","Yuanhe Tian","Qinyu Liu","Jun Zhang","Yan Song"],"pdf_url":"https://arxiv.org/pdf/2507.10098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15595v3","updated":"2025-07-14T09:25:43Z","published":"2024-10-21T02:27:24Z","title":"A Comprehensive Survey of Direct Preference Optimization: Datasets,\n  Theories, Variants, and Applications","summary":"  With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community. An updated collection of relevant papers can be found on\nhttps://github.com/Mr-Loevan/DPO-Survey.\n","authors":["Wenyi Xiao","Zechuan Wang","Leilei Gan","Shuai Zhao","Zongrui Li","Ruirui Lei","Wanggui He","Luu Anh Tuan","Long Chen","Hao Jiang","Zhou Zhao","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2410.15595v3.pdf","comment":"45 pages, 12 Figures. Project page:\n  https://github.com/Mr-Loevan/DPO-Survey"},{"id":"http://arxiv.org/abs/2506.00200v2","updated":"2025-07-14T09:19:59Z","published":"2025-05-30T20:12:51Z","title":"Structuring Radiology Reports: Challenging LLMs with Lightweight Models","summary":"  Radiology reports are critical for clinical decision-making but often lack a\nstandardized format, limiting both human interpretability and machine learning\n(ML) applications. While large language models (LLMs) have shown strong\ncapabilities in reformatting clinical text, their high computational\nrequirements, lack of transparency, and data privacy concerns hinder practical\ndeployment. To address these challenges, we explore lightweight encoder-decoder\nmodels (<300M parameters)-specifically T5 and BERT2BERT-for structuring\nradiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark\nthese models against eight open-source LLMs (1B-70B), adapted using prefix\nprompting, in-context learning (ICL), and low-rank adaptation (LoRA)\nfinetuning. Our best-performing lightweight model outperforms all LLMs adapted\nusing prompt-based techniques on a human-annotated test set. While some\nLoRA-finetuned LLMs achieve modest gains over the lightweight model on the\nFindings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,\nGREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of\nsubstantially greater computational resources. For example, LLaMA-3-70B\nincurred more than 400 times the inference time, cost, and carbon emissions\ncompared to the lightweight model. These results underscore the potential of\nlightweight, task-specific models as sustainable and privacy-preserving\nsolutions for structuring clinical text in resource-constrained healthcare\nsettings.\n","authors":["Johannes Moll","Louisa Fay","Asfandyar Azhar","Sophie Ostmeier","Tim Lueth","Sergios Gatidis","Curtis Langlotz","Jean-Benoit Delbrouck"],"pdf_url":"https://arxiv.org/pdf/2506.00200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01504v2","updated":"2025-07-14T09:13:19Z","published":"2025-07-02T09:10:33Z","title":"Following the Clues: Experiments on Person Re-ID using Cross-Modal\n  Intelligence","summary":"  The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID.\n","authors":["Robert Aufschläger","Youssef Shoeb","Azarm Nowzad","Michael Heigl","Fabian Bally","Martin Schramm"],"pdf_url":"https://arxiv.org/pdf/2507.01504v2.pdf","comment":"accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia"},{"id":"http://arxiv.org/abs/2507.10085v1","updated":"2025-07-14T09:11:33Z","published":"2025-07-14T09:11:33Z","title":"Enhancing Chain-of-Thought Reasoning with Critical Representation\n  Fine-tuning","summary":"  Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods.\n","authors":["Chenxi Huang","Shaotian Yan","Liang Xie","Binbin Lin","Sinan Fan","Yue Xin","Deng Cai","Chen Shen","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2507.10085v1.pdf","comment":"Accepted by ACL 2025"},{"id":"http://arxiv.org/abs/2507.10073v1","updated":"2025-07-14T08:59:26Z","published":"2025-07-14T08:59:26Z","title":"Cultural Bias in Large Language Models: Evaluating AI Agents through\n  Moral Questionnaires","summary":"  Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.\n","authors":["Simon Münker"],"pdf_url":"https://arxiv.org/pdf/2507.10073v1.pdf","comment":"15pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2507.10059v1","updated":"2025-07-14T08:44:59Z","published":"2025-07-14T08:44:59Z","title":"GeLaCo: An Evolutionary Approach to Layer Compression","summary":"  Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives.\n","authors":["David Ponce","Thierry Etchegoyhen","Javier Del Ser"],"pdf_url":"https://arxiv.org/pdf/2507.10059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10057v1","updated":"2025-07-14T08:41:53Z","published":"2025-07-14T08:41:53Z","title":"PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware\n  Query Optimization","summary":"  Scientific paper retrieval, particularly framed as document-to-document\nretrieval, aims to identify relevant papers in response to a long-form query\npaper, rather than a short query string. Previous approaches to this task have\nfocused on abstracts, embedding them into dense vectors as surrogates for full\ndocuments and calculating similarity across them, although abstracts provide\nonly sparse and high-level summaries. To address this, we propose PRISM, a\nnovel document-to-document retrieval method that introduces multiple,\nfine-grained representations for both the query and candidate papers. In\nparticular, each query paper is decomposed into multiple aspect-specific views\nand individually embedded, which are then matched against candidate papers\nsimilarity segmented to consider their multifaceted dimensions. Moreover, we\npresent SciFullBench, a novel benchmark in which the complete and segmented\ncontext of full papers for both queries and candidates is available. Then,\nexperimental results show that PRISM improves performance by an average of 4.3%\nover existing retrieval baselines.\n","authors":["Sangwoo Park","Jinheon Baek","Soyeong Jeong","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2507.10057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11415v2","updated":"2025-07-14T08:34:57Z","published":"2024-08-21T08:20:41Z","title":"Political Bias in LLMs: Unaligned Moral Values in Agent-centric\n  Simulations","summary":"  Contemporary research in social sciences increasingly utilizes\nstate-of-the-art generative language models to annotate or generate content.\nWhile these models achieve benchmark-leading performance on common language\ntasks, their application to novel out-of-domain tasks remains insufficiently\nexplored. To address this gap, we investigate how personalized language models\nalign with human responses on the Moral Foundation Theory Questionnaire. We\nadapt open-source generative language models to different political personas\nand repeatedly survey these models to generate synthetic data sets where\nmodel-persona combinations define our sub-populations. Our analysis reveals\nthat models produce inconsistent results across multiple repetitions, yielding\nhigh response variance. Furthermore, the alignment between synthetic data and\ncorresponding human data from psychological studies shows a weak correlation,\nwith conservative persona-prompted models particularly failing to align with\nactual conservative populations. These results suggest that language models\nstruggle to coherently represent ideologies through in-context prompting due to\ntheir alignment process. Thus, using language models to simulate social\ninteractions requires measurable improvements in in-context optimization or\nparameter manipulation to align with psychological and sociological stereotypes\nproperly.\n","authors":["Simon Münker"],"pdf_url":"https://arxiv.org/pdf/2408.11415v2.pdf","comment":"14 pages, 2 tables"},{"id":"http://arxiv.org/abs/2502.15902v2","updated":"2025-07-14T08:34:03Z","published":"2025-02-21T19:41:32Z","title":"IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable\n  LLM-Generated Text Detector","summary":"  Large Language Models (LLMs) have attained human-level fluency in text\ngeneration, which complicates the distinction between human-written and\nLLM-generated texts. This increases the risk of misuse and highlights the need\nfor reliable detectors. Yet, existing detectors exhibit poor robustness on\nout-of-distribution (OOD) data and attacked data, which is critical for\nreal-world scenarios. Also, they struggle to provide interpretable evidence to\nsupport their decisions, thus undermining the reliability. In light of these\nchallenges, we propose IPAD (Inverse Prompt for AI Detection), a novel\nframework consisting of a Prompt Inverter that identifies predicted prompts\nthat could have generated the input text, and two Distinguishers that examine\nthe probability that the input texts align with the predicted prompts.\nEmpirical evaluations demonstrate that IPAD outperforms the strongest baselines\nby 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on\nout-of-distribution (OOD) data, and 5.48% (AUROC) on attacked data. IPAD also\nperforms robustly on structured datasets. Furthermore, an interpretability\nassessment is conducted to illustrate that IPAD enhances the AI detection\ntrustworthiness by allowing users to directly examine the decision-making\nevidence, which provides interpretable support for its state-of-the-art\ndetection results.\n","authors":["Zheng Chen","Yushi Feng","Changyang He","Yue Deng","Hongxi Pu","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2502.15902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10045v1","updated":"2025-07-14T08:23:25Z","published":"2025-07-14T08:23:25Z","title":"Automating SPARQL Query Translations between DBpedia and Wikidata","summary":"  This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata.\n","authors":["Malte Christian Bartels","Debayan Banerjee","Ricardo Usbeck"],"pdf_url":"https://arxiv.org/pdf/2507.10045v1.pdf","comment":"18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025"},{"id":"http://arxiv.org/abs/2506.10521v4","updated":"2025-07-14T08:10:26Z","published":"2025-06-12T09:29:16Z","title":"Scientists' First Exam: Probing Cognitive Abilities of MLLM via\n  Perception, Understanding, and Reasoning","summary":"  Scientific discoveries increasingly rely on complex multimodal reasoning\nbased on information-intensive scientific data and domain-specific expertise.\nEmpowered by expert-level scientific benchmarks, scientific Multimodal Large\nLanguage Models (MLLMs) hold the potential to significantly enhance this\ndiscovery process in realistic workflows. However, current scientific\nbenchmarks mostly focus on evaluating the knowledge understanding capabilities\nof MLLMs, leading to an inadequate assessment of their perception and reasoning\nabilities. To address this gap, we present the Scientists' First Exam (SFE)\nbenchmark, designed to evaluate the scientific cognitive capacities of MLLMs\nthrough three interconnected levels: scientific signal perception, scientific\nattribute understanding, scientific comparative reasoning. Specifically, SFE\ncomprises 830 expert-verified VQA pairs across three question types, spanning\n66 multimodal tasks across five high-value disciplines. Extensive experiments\nreveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%\nand 26.52% on SFE, highlighting significant room for MLLMs to improve in\nscientific realms. We hope the insights obtained in SFE will facilitate further\ndevelopments in AI-enhanced scientific discoveries.\n","authors":["Yuhao Zhou","Yiheng Wang","Xuming He","Ruoyao Xiao","Zhiwei Li","Qiantai Feng","Zijie Guo","Yuejin Yang","Hao Wu","Wenxuan Huang","Jiaqi Wei","Dan Si","Xiuqi Yao","Jia Bu","Haiwen Huang","Tianfan Fu","Shixiang Tang","Ben Fei","Dongzhan Zhou","Fenghua Ling","Yan Lu","Siqi Sun","Chenhui Li","Guanjie Zheng","Jiancheng Lv","Wenlong Zhang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2506.10521v4.pdf","comment":"82 pages"},{"id":"http://arxiv.org/abs/2507.10013v1","updated":"2025-07-14T07:48:54Z","published":"2025-07-14T07:48:54Z","title":"Cross-modal Associations in Vision and Language Models: Revisiting the\n  bouba-kiki effect","summary":"  Recent advances in multimodal models have raised questions about whether\nvision-and-language models (VLMs) integrate cross-modal information in ways\nthat reflect human cognition. One well-studied test case in this domain is the\nbouba-kiki effect, where humans reliably associate pseudowords like \"bouba\"\nwith round shapes and \"kiki\" with jagged ones. Given the mixed evidence found\nin prior studies for this effect in VLMs, we present a comprehensive\nre-evaluation focused on two variants of CLIP, ResNet and Vision Transformer\n(ViT), given their centrality in many state-of-the-art VLMs. We apply two\ncomplementary methods closely modelled after human experiments: a prompt-based\nevaluation that uses probabilities as model preference, and we use Grad-CAM as\na novel way to interpret visual attention in shape-word matching tasks. Our\nfindings show that these models do not consistently exhibit the bouba-kiki\neffect. While ResNet shows a preference for round shapes, overall performance\nacross both models lacks the expected associations. Moreover, direct comparison\nwith prior human data on the same task shows that the models' responses fall\nmarkedly short of the robust, modality-integrated behaviour characteristic of\nhuman cognition. These results contribute to the ongoing debate about the\nextent to which VLMs truly understand cross-modal concepts, highlighting\nlimitations in their internal representations and alignment with human\nintuitions.\n","authors":["Tom Kouwenhoven","Kiana Shahrasbi","Tessa Verhoef"],"pdf_url":"https://arxiv.org/pdf/2507.10013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10008v1","updated":"2025-07-14T07:41:54Z","published":"2025-07-14T07:41:54Z","title":"Protective Factor-Aware Dynamic Influence Learning for Suicide Risk\n  Prediction on Social Media","summary":"  Suicide is a critical global health issue that requires urgent attention.\nEven though prior work has revealed valuable insights into detecting current\nsuicide risk on social media, little attention has been paid to developing\nmodels that can predict subsequent suicide risk over time, limiting their\nability to capture rapid fluctuations in individuals' mental state transitions.\nIn addition, existing work ignores protective factors that play a crucial role\nin suicide risk prediction, focusing predominantly on risk factors alone.\nProtective factors such as social support and coping strategies can mitigate\nsuicide risk by moderating the impact of risk factors. Therefore, this study\nproposes a novel framework for predicting subsequent suicide risk by jointly\nlearning the dynamic influence of both risk factors and protective factors on\nusers' suicide risk transitions. We propose a novel Protective Factor-Aware\nDataset, which is built from 12 years of Reddit posts along with comprehensive\nannotations of suicide risk and both risk and protective factors. We also\nintroduce a Dynamic Factors Influence Learning approach that captures the\nvarying impact of risk and protective factors on suicide risk transitions,\nrecognizing that suicide risk fluctuates over time according to established\npsychological theories. Our thorough experiments demonstrate that the proposed\nmodel significantly outperforms state-of-the-art models and large language\nmodels across three datasets. In addition, the proposed Dynamic Factors\nInfluence Learning provides interpretable weights, helping clinicians better\nunderstand suicidal patterns and enabling more targeted intervention\nstrategies.\n","authors":["Jun Li","Xiangmeng Wang","Haoyang Li","Yifei Yan","Hong Va Leong","Ling Feng","Nancy Xiaonan Yu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2507.10008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07610v2","updated":"2025-07-14T07:38:44Z","published":"2025-07-10T10:27:20Z","title":"SpatialViz-Bench: Automatically Generated Spatial Visualization\n  Reasoning Tasks for MLLMs","summary":"  Humans can directly imagine and manipulate visual images in their minds, a\ncapability known as spatial visualization. While multi-modal Large Language\nModels (MLLMs) support imagination-based reasoning, spatial visualization\nremains insufficiently evaluated, typically embedded within broader\nmathematical and logical assessments. Existing evaluations often rely on IQ\ntests or math competitions that may overlap with training data, compromising\nassessment reliability. To this end, we introduce SpatialViz-Bench, a\ncomprehensive multi-modal benchmark for spatial visualization with 12 tasks\nacross 4 sub-abilities, comprising 1,180 automatically generated problems. Our\nevaluation of 33 state-of-the-art MLLMs not only reveals wide performance\nvariations and demonstrates the benchmark's strong discriminative power, but\nalso uncovers counter-intuitive findings: models exhibit unexpected behaviors\nby showing difficulty perception that misaligns with human intuition,\ndisplaying dramatic 2D-to-3D performance cliffs, and defaulting to formula\nderivation despite spatial tasks requiring visualization alone. SpatialVizBench\nempirically demonstrates that state-of-the-art MLLMs continue to exhibit\ndeficiencies in spatial visualization tasks, thereby addressing a significant\nlacuna in the field. The benchmark is publicly available.\n","authors":["Siting Wang","Luoyang Sun","Cheng Deng","Kun Shao","Minnan Pei","Zheng Tian","Haifeng Zhang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2507.07610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10000v1","updated":"2025-07-14T07:34:58Z","published":"2025-07-14T07:34:58Z","title":"On The Role of Intentionality in Knowledge Representation: Analyzing\n  Scene Context for Cognitive Agents with a Tiny Language Model","summary":"  Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent.\n","authors":["Mark Burgess"],"pdf_url":"https://arxiv.org/pdf/2507.10000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07498v2","updated":"2025-07-14T07:10:51Z","published":"2025-07-10T07:34:05Z","title":"Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems\n  without Code","summary":"  Enhancing reasoning capabilities remains a central focus in the LLM reasearch\ncommunity. A promising direction involves requiring models to simulate code\nexecution step-by-step to derive outputs for given inputs. However, as code is\noften designed for large-scale systems, direct application leads to\nover-reliance on complex data structures and algorithms, even for simple cases,\nresulting in overfitting to algorithmic patterns rather than core reasoning\nstructures. To address this, we propose TeaR, which aims at teaching LLMs to\nreason better. TeaR leverages careful data curation and reinforcement learning\nto guide models in discovering optimal reasoning paths through code-related\ntasks, thereby improving general reasoning abilities. We conduct extensive\nexperiments using two base models and three long-CoT distillation models, with\nmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17\nbenchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results\nconsistently show significant performance improvements. Notably, TeaR achieves\na 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.\n","authors":["Keqin Bao","Nuo Chen","Xiaoyuan Li","Binyuan Hui","Bowen Yu","Fuli Feng","Xiangnan He","Dayiheng Liu"],"pdf_url":"https://arxiv.org/pdf/2507.07498v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03940v3","updated":"2025-07-14T07:05:28Z","published":"2025-01-07T17:00:49Z","title":"Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection","summary":"  The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.\n","authors":["Pablo Miralles-González","Javier Huertas-Tato","Alejandro Martín","David Camacho"],"pdf_url":"https://arxiv.org/pdf/2501.03940v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09982v1","updated":"2025-07-14T06:56:37Z","published":"2025-07-14T06:56:37Z","title":"TextOmics-Guided Diffusion for Hit-like Molecular Generation","summary":"  Hit-like molecular generation with therapeutic potential is essential for\ntarget-specific drug discovery. However, the field lacks heterogeneous data and\nunified frameworks for integrating diverse molecular representations. To bridge\nthis gap, we introduce TextOmics, a pioneering benchmark that establishes\none-to-one correspondences between omics expressions and molecular textual\ndescriptions. TextOmics provides a heterogeneous dataset that facilitates\nmolecular generation through representations alignment. Built upon this\nfoundation, we propose ToDi, a generative framework that jointly conditions on\nomics expressions and molecular textual descriptions to produce biologically\nrelevant, chemically valid, hit-like molecules. ToDi leverages two encoders\n(OmicsEn and TextEn) to capture multi-level biological and semantic\nassociations, and develops conditional diffusion (DiffGen) for controllable\ngeneration. Extensive experiments confirm the effectiveness of TextOmics and\ndemonstrate ToDi outperforms existing state-of-the-art approaches, while also\nshowcasing remarkable potential in zero-shot therapeutic molecular generation.\nSources are available at: https://github.com/hala-ToDi.\n","authors":["Hang Yuan","Chen Li","Wenjun Ma","Yuncheng Jiang"],"pdf_url":"https://arxiv.org/pdf/2507.09982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09973v1","updated":"2025-07-14T06:43:00Z","published":"2025-07-14T06:43:00Z","title":"Tiny Reward Models","summary":"  Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling.\n","authors":["Sarah Pan"],"pdf_url":"https://arxiv.org/pdf/2507.09973v1.pdf","comment":"2025 ICML Efficient Systems for Foundation Models Workshop"},{"id":"http://arxiv.org/abs/2506.18421v2","updated":"2025-07-14T06:09:12Z","published":"2025-06-23T09:02:04Z","title":"TReB: A Comprehensive Benchmark for Evaluating Table Reasoning\n  Capabilities of Large Language Models","summary":"  The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on\nhuggingface.co/datasets/JT-LM/JIUTIAN-TReB and the framework on\ngithub.com/JT-LM/jiutian-treb.\n","authors":["Ce Li","Xiaofan Liu","Zhiyan Song","Ce Chi","Chen Zhao","Jingjing Yang","Zhendong Wang","Kexin Yang","Boshen Shi","Xing Wang","Chao Deng","Junlan Feng"],"pdf_url":"https://arxiv.org/pdf/2506.18421v2.pdf","comment":"Benmark report v1.1"},{"id":"http://arxiv.org/abs/2507.04607v2","updated":"2025-07-14T05:54:45Z","published":"2025-07-07T01:54:34Z","title":"PRIME: Large Language Model Personalization with Cognitive Memory and\n  Thought Processes","summary":"  Large language model (LLM) personalization aims to align model outputs with\nindividuals' unique preferences and opinions. While recent efforts have\nimplemented various personalization methods, a unified theoretical framework\nthat can systematically understand the drivers of effective personalization is\nstill lacking. In this work, we integrate the well-established cognitive\ndual-memory model into LLM personalization, by mirroring episodic memory to\nhistorical user engagements and semantic memory to long-term, evolving user\nbeliefs. Specifically, we systematically investigate memory instantiations and\nintroduce a unified framework, PRIME, using episodic and semantic memory\nmechanisms. We further augment PRIME with a novel personalized thinking\ncapability inspired by the slow thinking strategy. Moreover, recognizing the\nabsence of suitable benchmarks, we introduce a dataset using Change My View\n(CMV) from Reddit, specifically designed to evaluate long-context\npersonalization. Extensive experiments validate PRIME's effectiveness across\nboth long- and short-context scenarios. Further analysis confirms that PRIME\neffectively captures dynamic personalization beyond mere popularity biases.\n","authors":["Xinliang Frederick Zhang","Nick Beauchamp","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2507.04607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03147v2","updated":"2025-07-14T05:34:27Z","published":"2025-07-03T20:04:04Z","title":"DeepGesture: A conversational gesture synthesis system based on emotions\n  and semantics","summary":"  Along with the explosion of large language models, improvements in speech\nsynthesis, advancements in hardware, and the evolution of computer graphics,\nthe current bottleneck in creating digital humans lies in generating character\nmovements that correspond naturally to text or speech inputs.\n  In this work, we present DeepGesture, a diffusion-based gesture synthesis\nframework for generating expressive co-speech gestures conditioned on\nmultimodal signals - text, speech, emotion, and seed motion. Built upon the\nDiffuseStyleGesture model, DeepGesture introduces novel architectural\nenhancements that improve semantic alignment and emotional expressiveness in\ngenerated gestures. Specifically, we integrate fast text transcriptions as\nsemantic conditioning and implement emotion-guided classifier-free diffusion to\nsupport controllable gesture generation across affective states. To visualize\nresults, we implement a full rendering pipeline in Unity based on BVH output\nfrom the model. Evaluation on the ZeroEGGS dataset shows that DeepGesture\nproduces gestures with improved human-likeness and contextual appropriateness.\nOur system supports interpolation between emotional states and demonstrates\ngeneralization to out-of-distribution speech, including synthetic voices -\nmarking a step forward toward fully multimodal, emotionally aware digital\nhumans.\n  Project page: https://deepgesture.github.io\n","authors":["Thanh Hoang-Minh"],"pdf_url":"https://arxiv.org/pdf/2507.03147v2.pdf","comment":"Project page: https://deepgesture.github.io"},{"id":"http://arxiv.org/abs/2505.12185v3","updated":"2025-07-14T05:28:08Z","published":"2025-05-18T01:02:33Z","title":"EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency\n  Perspective","summary":"  Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop.\n","authors":["Sen Fang","Weiyuan Ding","Bowen Xu"],"pdf_url":"https://arxiv.org/pdf/2505.12185v3.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.13640v2","updated":"2025-07-14T05:22:54Z","published":"2025-02-19T11:33:22Z","title":"Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts","summary":"  Large language models (LLMs) are known to have the potential to generate\nharmful content, posing risks to users. While significant progress has been\nmade in developing taxonomies for LLM risks and safety evaluation prompts, most\nstudies have focused on monolingual contexts, primarily in English. However,\nlanguage- and region-specific risks in bilingual contexts are often overlooked,\nand core findings can diverge from those in monolingual settings. In this\npaper, we introduce Qorgau, a novel dataset specifically designed for safety\nevaluation in Kazakh and Russian, reflecting the unique bilingual context in\nKazakhstan, where both Kazakh (a low-resource language) and Russian (a\nhigh-resource language) are spoken. Experiments with both multilingual and\nlanguage-specific LLMs reveal notable differences in safety performance,\nemphasizing the need for tailored, region-specific datasets to ensure the\nresponsible and safe deployment of LLMs in countries like Kazakhstan. Warning:\nthis paper contains example data that may be offensive, harmful, or biased.\n","authors":["Maiya Goloburda","Nurkhan Laiyk","Diana Turmakhan","Yuxia Wang","Mukhammed Togmanov","Jonibek Mansurov","Askhat Sametov","Nurdaulet Mukhituly","Minghan Wang","Daniil Orel","Zain Muhammad Mujahid","Fajri Koto","Timothy Baldwin","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2502.13640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09935v1","updated":"2025-07-14T05:21:58Z","published":"2025-07-14T05:21:58Z","title":"Enhancing Retrieval Augmented Generation with Hierarchical Text\n  Segmentation Chunking","summary":"  Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.\n","authors":["Hai Toan Nguyen","Tien Dat Nguyen","Viet Ha Nguyen"],"pdf_url":"https://arxiv.org/pdf/2507.09935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12851v5","updated":"2025-07-14T05:06:20Z","published":"2025-01-22T12:59:08Z","title":"ACEBench: Who Wins the Match Point in Tool Usage?","summary":"  Large Language Models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, particularly when integrated with various tools\nto effectively solve complex problems. However, existing benchmarks for\nevaluating LLMs' tool usage face several limitations: (1) limited evaluation\nscenarios, often lacking assessments in real multi-turn dialogue contexts; (2)\nnarrow evaluation dimensions, with insufficient detailed assessments of how\nLLMs use tools; and (3) reliance on LLMs or real API executions for evaluation,\nwhich introduces significant overhead. To address these challenges, we\nintroduce ACEBench, a comprehensive benchmark for assessing tool usage in LLMs.\nACEBench categorizes data into three primary types based on evaluation\nmethodology: Normal, Special, and Agent. \"Normal\" evaluates tool usage in basic\nscenarios; \"Special\" evaluates tool usage in situations with ambiguous or\nincomplete instructions; \"Agent\" evaluates tool usage through multi-agent\ninteractions to simulate real-world, multi-turn dialogues. We conducted\nextensive experiments using ACEBench, analyzing various LLMs in-depth and\nproviding a more granular examination of error causes across different data\ntypes.\n","authors":["Chen Chen","Xinlong Hao","Weiwen Liu","Xu Huang","Xingshan Zeng","Shuai Yu","Dexun Li","Shuai Wang","Weinan Gan","Yuefeng Huang","Wulong Liu","Xinzhi Wang","Defu Lian","Baoqun Yin","Yasheng Wang","Wu Liu"],"pdf_url":"https://arxiv.org/pdf/2501.12851v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09924v1","updated":"2025-07-14T05:04:32Z","published":"2025-07-14T05:04:32Z","title":"MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for\n  Rehearsal-Free Generative Retrieval over Dynamic Corpora","summary":"  Continually updating model-based indexes in generative retrieval with new\ndocuments remains challenging, as full retraining is computationally expensive\nand impractical under resource constraints. We propose MixLoRA-DSI, a novel\nframework that combines an expandable mixture of Low-Rank Adaptation experts\nwith a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead\nof allocating new experts for each new corpus, our proposed expansion strategy\nenables sublinear parameter growth by selectively introducing new experts only\nwhen significant number of OOD documents are detected. Experiments on NQ320k\nand MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update\nbaselines, with minimal parameter overhead and substantially lower training\ncosts.\n","authors":["Tuan-Luc Huynh","Thuy-Trang Vu","Weiqing Wang","Trung Le","Dragan Gašević","Yuan-Fang Li","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2507.09924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07998v2","updated":"2025-07-14T04:36:19Z","published":"2025-07-10T17:59:55Z","title":"PyVision: Agentic Vision with Dynamic Tooling","summary":"  LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning.\n","authors":["Shitian Zhao","Haoquan Zhang","Shaoheng Lin","Ming Li","Qilong Wu","Kaipeng Zhang","Chen Wei"],"pdf_url":"https://arxiv.org/pdf/2507.07998v2.pdf","comment":"26 Pages, 10 Figures, Technical report"},{"id":"http://arxiv.org/abs/2412.17739v4","updated":"2025-07-14T04:23:36Z","published":"2024-12-23T17:44:01Z","title":"Fourier Position Embedding: Enhancing Attention's Periodic Extension for\n  Length Generalization","summary":"  Extending the context length of Language Models (LMs) by improving Rotary\nPosition Embedding (RoPE) has become a trend. While prior works mainly address\nRoPE's limitations within attention, this paper uncovers the adverse effects on\nlength generalization from nearly all parts of LMs. Using Discrete Signal\nProcessing theory, we show that RoPE enables periodic attention by implicitly\nachieving Non-Uniform Discrete Fourier Transform. However, this periodicity is\nundermined by the spectrum damage caused by: 1) linear layers and activation\nfunctions; 2) insufficiently trained frequency components brought by\ntime-domain truncation. Building on our observations, we propose Fourier\nPosition Embedding (FoPE), which enhances attention's frequency-domain\nproperties to improve both its periodic extension and length generalization.\nFoPE constructs \\textit{Fourier Series} and zero-outs the destructive frequency\ncomponents, increasing model robustness against the spectrum damage.\nExperiments across various model scales and benchmarks show that, within\nvarying context windows, FoPE maintains a more stable performance compared to\nother baselines. Several analyses and ablations bring further support to our\nmethod and theoretical modeling.\n","authors":["Ermo Hua","Che Jiang","Xingtai Lv","Kaiyan Zhang","Youbang Sun","Yuchen Fan","Xuekai Zhu","Biqing Qi","Ning Ding","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.17739v4.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2405.11870v3","updated":"2025-07-14T04:15:46Z","published":"2024-05-20T08:23:28Z","title":"Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single\n  Process","summary":"  Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are key\nprocesses for aligning Language Models (LMs) with human preferences post\npre-training. While SFT excels in efficiency and PO in effectiveness, they are\noften combined sequentially without integrating their optimization objectives.\nThis approach ignores the opportunities to bridge their paradigm gap and take\nthe strengths from both. In this paper, we interpret SFT and PO with two\nsub-processes -- Preference Estimation and Transition Optimization -- defined\nat token level within the Markov Decision Process (MDP). This modeling shows\nthat SFT is only a special case of PO with inferior estimation and\noptimization. PO estimates the model's preference by its entire generation,\nwhile SFT only scores model's subsequent predicted tokens based on prior tokens\nfrom ground truth answer. These priors deviates from model's distribution,\nhindering the preference estimation and transition optimization. Building on\nthis view, we introduce Intuitive Fine-Tuning (IFT) to integrate SFT and PO\ninto a single process. Through a temporal residual connection, IFT brings\nbetter estimation and optimization by capturing LMs' intuitive sense of its\nentire answers. But it solely relies on a single policy and the same volume of\nnon-preference-labeled data as SFT. Our experiments show that IFT performs\ncomparably or even superiorly to SFT and some typical PO methods across several\ntasks, particularly those require generation, reasoning, and fact-following\nabilities. An explainable Frozen Lake game further validates the effectiveness\nof IFT for getting competitive policy.\n","authors":["Ermo Hua","Biqing Qi","Kaiyan Zhang","Kai Tian","Xingtai Lv","Ning Ding","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.11870v3.pdf","comment":"Accepted to ACL 2025, Oral & Panel Discussion"},{"id":"http://arxiv.org/abs/2406.02528v6","updated":"2025-07-14T03:30:40Z","published":"2024-06-04T17:50:34Z","title":"Scalable MatMul-free Language Modeling","summary":"  Large Language Models (LLMs) have fundamentally altered how we approach\nscaling in machine learning. However, these models pose substantial\ncomputational and memory challenges, primarily due to the reliance on matrix\nmultiplication (MatMul) within their attention and feed-forward (FFN) layers.\nWe demonstrate that MatMul operations can be eliminated from LLMs while\nmaintaining strong performance, even at billion-parameter scales. Our\nMatMul-free models, tested on models up to 2.7B parameters, are comparable to\nstate-of-the-art pre-trained Transformers, and the performance gap narrows as\nmodel size increases. Our approach yields significant memory savings: a\nGPU-efficient implementation reduces memory consumption by up to 61\\% during\ntraining and over 10$\\times$ during inference. When adapted for a multi-chip\nneuromorphic system, the model leverages asynchronous processing to achieve\n4$\\times$ higher throughput with 10$\\times$ less energy than edge GPUs. %and\n77$\\times$ less energy than server-class GPUs, demonstrating superior scaling.\nThese findings demonstrate a path toward dramatically simplified yet effective\nLLMs, advancing them toward brain-like efficiency and heralding a new\ngeneration of lightweight, high-performance language models. Our code\nimplementation is available at https://github. com/ridgerchu/matmulfreellm.\n","authors":["Rui-Jie Zhu","Yu Zhang","Steven Abreu","Ethan Sifferman","Tyler Sheaves","Yiqiao Wang","Dustin Richmond","Sumit Bam Shrestha","Peng Zhou","Jason K. Eshraghian"],"pdf_url":"https://arxiv.org/pdf/2406.02528v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09876v1","updated":"2025-07-14T03:21:13Z","published":"2025-07-14T03:21:13Z","title":"ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video\n  Understanding in Large Language Models","summary":"  Video understanding plays a vital role in bridging low-level visual signals\nwith high-level cognitive reasoning, and is fundamental to applications such as\nautonomous driving, embodied AI, and the broader pursuit of AGI. The rapid\ndevelopment of large language models (LLMs), particularly those utilizing\nChain-of-Thought (CoT) technology, has significantly advanced video reasoning\ncapabilities. However, current approaches primarily depend on textual\ninformation for reasoning, overlooking the visual modality in the actual video\nreasoning process. In contrast, humans naturally re-examine visual content\nwhile reasoning. Motivated by this, we introduce a novel video reasoning\nparadigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive\nand cognitively aligned reasoning. To the end, first, we construct the\nVideo-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for\nkey-video selection and manually verified. Furthermore, we extensively explore\nthe potential of the ViTCoT paradigm in the video understanding field.\nExtensive experiments demonstrate that ViTCoT significantly enhances\nperformance compared to the traditional text-only CoT paradigm and effectively\nactivates more neuron values in MLLMs.\n","authors":["Yongheng Zhang","Xu Liu","Ruihan Tao","Qiguang Chen","Hao Fei","Wanxiang Che","Libo Qin"],"pdf_url":"https://arxiv.org/pdf/2507.09876v1.pdf","comment":"Accepted by ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.09875v1","updated":"2025-07-14T03:20:55Z","published":"2025-07-14T03:20:55Z","title":"Function Induction and Task Generalization: An Interpretability Study\n  with Off-by-One Addition","summary":"  Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.\n","authors":["Qinyuan Ye","Robin Jia","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2507.09875v1.pdf","comment":"Code: https://github.com/INK-USC/function-induction"},{"id":"http://arxiv.org/abs/2409.01389v2","updated":"2025-07-14T02:48:47Z","published":"2024-09-02T17:39:26Z","title":"CV-Probes: Studying the interplay of lexical and world knowledge in\n  visually grounded verb understanding","summary":"  How do vision-language (VL) transformer models ground verb phrases and do\nthey integrate contextual and world knowledge in this process? We introduce the\nCV-Probes dataset, containing image-caption pairs involving verb phrases that\nrequire both social knowledge and visual context to interpret (e.g., \"beg\"), as\nwell as pairs involving verb phrases that can be grounded based on information\ndirectly available in the image (e.g., \"sit\"). We show that VL models struggle\nto ground VPs that are strongly context-dependent. Further analysis using\nexplainable AI techniques shows that such models may not pay sufficient\nattention to the verb token in the captions. Our results suggest a need for\nimproved methodologies in VL model training and evaluation. The code and\ndataset will be available https://github.com/ivana-13/CV-Probes.\n","authors":["Ivana Beňová","Michal Gregor","Albert Gatt"],"pdf_url":"https://arxiv.org/pdf/2409.01389v2.pdf","comment":"9 pages, 2 figure, 6 tables, CogSci conference 2025"},{"id":"http://arxiv.org/abs/2411.13820v2","updated":"2025-07-14T02:22:43Z","published":"2024-11-21T03:52:41Z","title":"InstCache: A Predictive Cache for LLM Serving","summary":"  The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.\n","authors":["Longwei Zou","Yan Liu","Jiamu Kang","Tingfeng Liu","Jiangang Kong","Yangdong Deng"],"pdf_url":"https://arxiv.org/pdf/2411.13820v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06955v4","updated":"2025-07-14T02:22:42Z","published":"2025-06-08T00:38:18Z","title":"BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for\n  Belief-Inconsistent Syllogistic Reasoning","summary":"  We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety.\n","authors":["Ha-Thanh Nguyen","Chaoran Liu","Qianying Liu","Hideyuki Tachibana","Su Myat Noe","Yusuke Miyao","Koichi Takeda","Sadao Kurohashi"],"pdf_url":"https://arxiv.org/pdf/2506.06955v4.pdf","comment":"This version includes minor typo corrections in the example image"},{"id":"http://arxiv.org/abs/2501.03262v6","updated":"2025-07-14T02:04:47Z","published":"2025-01-04T02:08:06Z","title":"REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt\n  and Reward Models","summary":"  Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in\naligning large language models (LLMs) with human values and preferences. While\nstate-of-the-art applications like ChatGPT/GPT-4 commonly employ Proximal\nPolicy Optimization (PPO), the inclusion of a critic network introduces\nsignificant computational overhead. REINFORCE-based methods, such as REINFORCE\nLeave One-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO),\naddress this limitation by eliminating the critic network. However, these\napproaches face challenges in accurate advantage estimation. Specifically, they\nestimate advantages independently for responses to each prompt, which can lead\nto overfitting on simpler prompts and vulnerability to reward hacking. To\naddress these challenges, we introduce REINFORCE++, a novel approach that\nremoves the critic model while using the normalized reward of a batch as the\nbaseline. Our empirical evaluation demonstrates that REINFORCE++ exhibits\nrobust performance across various reward models without requiring prompt set\ntruncation. Furthermore, it achieves superior generalization in both RLHF and\nlong chain-of-thought (CoT) settings compared to existing REINFORCE-based\nmethods. The implementation is available at\nhttps://github.com/OpenRLHF/OpenRLHF.\n","authors":["Jian Hu","Jason Klein Liu","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2501.03262v6.pdf","comment":"add proof"},{"id":"http://arxiv.org/abs/2501.06848v4","updated":"2025-07-14T01:12:12Z","published":"2025-01-12T15:34:24Z","title":"A General Framework for Inference-time Scaling and Steering of Diffusion\n  Models","summary":"  Diffusion models produce impressive results in modalities ranging from images\nand video to protein design and text. However, generating samples with\nuser-specified properties remains a challenge. Recent research proposes\nfine-tuning models to maximize rewards that capture desired properties, but\nthese methods require expensive training and are prone to mode collapse. In\nthis work, we present Feynman-Kac (FK) steering, an inference-time framework\nfor steering diffusion models with reward functions. FK steering works by\nsampling a system of multiple interacting diffusion processes, called\nparticles, and resampling particles at intermediate steps based on scores\ncomputed using functions called potentials. Potentials are defined using\nrewards for intermediate states and are selected such that a high value\nindicates that the particle will yield a high-reward sample. We explore various\nchoices of potentials, intermediate rewards, and samplers. We evaluate FK\nsteering on text-to-image and text diffusion models. For steering text-to-image\nmodels with a human preference reward, we find that FK steering a 0.8B\nparameter model outperforms a 2.6B parameter fine-tuned model on prompt\nfidelity, with faster sampling and no training. For steering text diffusion\nmodels with rewards for text quality and specific text attributes, we find that\nFK steering generates lower perplexity, more linguistically acceptable outputs\nand enables gradient-free control of attributes like toxicity. Our results\ndemonstrate that inference-time scaling and steering of diffusion models - even\nwith off-the-shelf rewards - can provide significant sample quality gains and\ncontrollability benefits. Code is available at\nhttps://github.com/zacharyhorvitz/Fk-Diffusion-Steering .\n","authors":["Raghav Singhal","Zachary Horvitz","Ryan Teehan","Mengye Ren","Zhou Yu","Kathleen McKeown","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2501.06848v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08031v2","updated":"2025-07-14T01:10:56Z","published":"2025-07-09T02:40:02Z","title":"Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental\n  Health Understanding","summary":"  The emergence of Small Language Models (SLMs) as privacy-preserving\nalternatives for sensitive applications raises a fundamental question about\ntheir inherent understanding capabilities compared to Large Language Models\n(LLMs). This paper investigates the mental health understanding capabilities of\ncurrent SLMs through systematic evaluation across diverse classification tasks.\nEmploying zero-shot and few-shot learning paradigms, we benchmark their\nperformance against established LLM baselines to elucidate their relative\nstrengths and limitations in this critical domain. We assess five\nstate-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against\nthree LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding\ntasks. Our findings reveal that SLMs achieve mean performance within 2\\% of\nLLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot\nsettings), demonstrating notable competence despite orders of magnitude fewer\nparameters. Both model categories experience similar degradation on multi-class\nseverity tasks (a drop of over 30\\%), suggesting that nuanced clinical\nunderstanding challenges transcend model scale. Few-shot prompting provides\nsubstantial improvements for SLMs (up to 14.6\\%), while LLM gains are more\nvariable. Our work highlights the potential of SLMs in mental health\nunderstanding, showing they can be effective privacy-preserving tools for\nanalyzing sensitive online text data. In particular, their ability to quickly\nadapt and specialize with minimal data through few-shot learning positions them\nas promising candidates for scalable mental health screening tools.\n","authors":["Hong Jia","Shiya Fu","Feng Xia","Vassilis Kostakos","Ting Dang"],"pdf_url":"https://arxiv.org/pdf/2507.08031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10859v1","updated":"2025-07-14T23:20:42Z","published":"2025-07-14T23:20:42Z","title":"MultiVox: Benchmarking Voice Assistants for Multimodal Interactions","summary":"  The rapid progress of Large Language Models (LLMs) has empowered omni models\nto act as voice assistants capable of understanding spoken dialogues. These\nmodels can process multimodal inputs beyond text, such as speech and visual\ndata, enabling more context-aware interactions. However, current benchmarks\nfall short in comprehensively evaluating how well these models generate\ncontext-aware responses, particularly when it comes to implicitly understanding\nfine-grained speech characteristics, such as pitch, emotion, timbre, and volume\nor the environmental acoustic context such as background sounds. Additionally,\nthey inadequately assess the ability of models to align paralinguistic cues\nwith complementary visual signals to inform their responses. To address these\ngaps, we introduce MultiVox, the first omni voice assistant benchmark designed\nto evaluate the ability of voice assistants to integrate spoken and visual cues\nincluding paralinguistic speech features for truly multimodal understanding.\nSpecifically, MultiVox includes 1000 human-annotated and recorded speech\ndialogues that encompass diverse paralinguistic features and a range of visual\ncues such as images and videos. Our evaluation on 9 state-of-the-art models\nreveals that, although humans excel at these tasks, current models consistently\nstruggle to produce contextually grounded responses.\n","authors":["Ramaneswaran Selvakumar","Ashish Seth","Nishit Anand","Utkarsh Tyagi","Sonal Kumar","Sreyan Ghosh","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2507.10859v1.pdf","comment":"Work In Progress"},{"id":"http://arxiv.org/abs/2507.10852v1","updated":"2025-07-14T22:56:58Z","published":"2025-07-14T22:56:58Z","title":"LLMs on Trial: Evaluating Judicial Fairness for Large Language Models","summary":"  Large Language Models (LLMs) are increasingly used in high-stakes fields\nwhere their decisions impact rights and equity. However, LLMs' judicial\nfairness and implications for social justice remain underexplored. When LLMs\nact as judges, the ability to fairly resolve judicial issues is a prerequisite\nto ensure their trustworthiness. Based on theories of judicial fairness, we\nconstruct a comprehensive framework to measure LLM fairness, leading to a\nselection of 65 labels and 161 corresponding values. Applying this framework to\nthe judicial system, we compile an extensive dataset, JudiFair, comprising\n177,100 unique case facts. To achieve robust statistical inference, we develop\nthree evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and\nintroduce a method to assess the overall fairness of multiple LLMs across\nvarious labels. Through experiments with 16 LLMs, we uncover pervasive\ninconsistency, bias, and imbalanced inaccuracy across models, underscoring\nsevere LLM judicial unfairness. Particularly, LLMs display notably more\npronounced biases on demographic labels, with slightly less bias on substance\nlabels compared to procedure ones. Interestingly, increased inconsistency\ncorrelates with reduced biases, but more accurate predictions exacerbate\nbiases. While we find that adjusting the temperature parameter can influence\nLLM fairness, model size, release date, and country of origin do not exhibit\nsignificant effects on judicial fairness. Accordingly, we introduce a publicly\navailable toolkit containing all datasets and code, designed to support future\nresearch in evaluating and improving LLM fairness.\n","authors":["Yiran Hu","Zongyue Xue","Haitao Li","Siyuan Zheng","Qingjing Chen","Shaochun Wang","Xihan Zhang","Ning Zheng","Yun Liu","Qingyao Ai","Yiqun Liu","Charles L. A. Clarke","Weixing Shen"],"pdf_url":"https://arxiv.org/pdf/2507.10852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11673v4","updated":"2025-07-14T22:24:48Z","published":"2025-04-16T00:10:34Z","title":"Deep Binding of Language Model Virtual Personas: a Study on\n  Approximating Political Partisan Misperceptions","summary":"  Large language models (LLMs) are increasingly capable of simulating human\nbehavior, offering cost-effective ways to estimate user responses to various\nsurveys and polls. However, the questions in these surveys usually reflect\nsocially understood attitudes: the patterns of attitudes of old/young,\nliberal/conservative, as understood by both members and non-members of those\ngroups. It is not clear whether the LLM binding is \\emph{deep}, meaning the LLM\nanswers as a member of a particular in-group would, or \\emph{shallow}, meaning\nthe LLM responds as an out-group member believes an in-group member would. To\nexplore this difference, we use questions that expose known in-group/out-group\nbiases. This level of fidelity is critical for applying LLMs to various\npolitical science studies, including timely topics on polarization dynamics,\ninter-group conflict, and democratic backsliding. To this end, we propose a\nnovel methodology for constructing virtual personas with synthetic user\n``backstories\" generated as extended, multi-turn interview transcripts. Our\ngenerated backstories are longer, rich in detail, and consistent in\nauthentically describing a singular individual, compared to previous methods.\nWe show that virtual personas conditioned on our backstories closely replicate\nhuman response distributions (up to an 87\\% improvement as measured by\nWasserstein Distance) and produce effect sizes that closely match those\nobserved in the original studies of in-group/out-group biases. Altogether, our\nwork extends the applicability of LLMs beyond estimating socially understood\nresponses, enabling their use in a broader range of human studies.\n","authors":["Minwoo Kang","Suhong Moon","Seung Hyeong Lee","Ayush Raj","Joseph Suh","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2504.11673v4.pdf","comment":"COLM 2025"},{"id":"http://arxiv.org/abs/2412.06136v2","updated":"2025-07-14T22:18:38Z","published":"2024-12-09T01:39:16Z","title":"AIDE: Attribute-Guided MultI-Hop Data Expansion for Data Scarcity in\n  Task-Specific Fine-tuning","summary":"  Fine-tuning large language models (LLMs) for specific tasks requires diverse,\nhigh-quality training data. However, obtaining sufficient relevant data remains\na significant challenge. Existing data synthesis methods either depend on\nextensive seed datasets or struggle to balance task relevance and data\ndiversity. To address these challenges, we propose Attribute-guided multI-hop\nData Expansion (AIDE), a novel data synthesis framework that uses a multi-hop\nprocess to expand very few seed data points while ensuring data diversity and\ntask relevance. AIDE extracts the main topic and key knowledge attributes from\nthe seeds to guide the synthesis steps. The process repeats for K hops, using\nthe generated data as seeds. To prevent irrelevant data generation as the hop\ndepth increases, AIDE incorporates a residual connection mechanism. Our\nempirical results show that AIDE enables fine-tuning of Mistral-7B,\nLlama-3.1-8B and Llama-3.2-3B from 10 seeds, surpassing the models fine-tuned\non human curated data. Furthermore, AIDE outperforms state-of-the-art data\nsynthesis methods, such as Evol-Instruct, by over 30% in task-specific\nfine-tuning. Code is available at https://github.com/Code4Graph/AIDE.\n","authors":["Jiayu Li","Xuan Zhu","Fang Liu","Yanjun Qi"],"pdf_url":"https://arxiv.org/pdf/2412.06136v2.pdf","comment":"Accepted for publication in ACL 2025. The official version will be\n  available in the ACL Anthology"},{"id":"http://arxiv.org/abs/2507.10810v1","updated":"2025-07-14T21:10:39Z","published":"2025-07-14T21:10:39Z","title":"Testing Hypotheses from the Social Approval Theory of Online Hate: An\n  Analysis of 110 Million Posts from Parler","summary":"  In this paper, we explored how online hate is motivated by receiving social\napproval from others. We specifically examined two central tenets of Walther's\n(2024) social approval theory of online hate: (H1a) more signals of social\napproval on hate messages predicts more subsequent hate messages, and (H1b) as\nsocial approval increases, hate speech messages become more extreme. Using over\n110 million posts from Parler (2018-2021), we observed that the number of\nupvotes a person received on a hate speech post was unassociated with the\namount of hate speech in their next post and posts during the next week, month,\nthree months, and six months. Between-person effects revealed an average\nnegative relationship between social approval and hate speech production at the\npost level, but this relationship was mixed at other time intervals. Social\napproval reinforcement mechanisms of online hate may operate differently on\nniche social media platforms.\n","authors":["David M. Markowitz","Samuel Hardman Taylor"],"pdf_url":"https://arxiv.org/pdf/2507.10810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10803v1","updated":"2025-07-14T20:57:52Z","published":"2025-07-14T20:57:52Z","title":"Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social\n  Media Chatter Use Case","summary":"  Background Large language models (LLMs) face challenges in inductive thematic\nanalysis, a task requiring deep interpretive and domain-specific expertise. We\nevaluated the feasibility of using LLMs to replicate expert-driven thematic\nanalysis of social media data. Methods Using two temporally non-intersecting\nReddit datasets on xylazine (n=286 and n=686, for model optimization and\nvalidation, respectively) with twelve expert-derived themes, we evaluated five\nLLMs against expert coding. We modeled the task as a series of binary\nclassifications, rather than a single, multi-label classification, employing\nzero-, single-, and few-shot prompting strategies and measuring performance via\naccuracy, precision, recall, and F1-score. Results On the validation set,\nGPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:\n0.71). For high-prevalence themes, model-derived thematic distributions closely\nmirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:\n16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based\napproaches can automate thematic analyses, offering a scalable supplement for\nqualitative research. Keywords: thematic analysis, large language models,\nnatural language processing, qualitative analysis, social media, prompt\nengineering, public health\n","authors":["JaMor Hairston","Ritvik Ranjan","Sahithi Lakamana","Anthony Spadaro","Selen Bozkurt","Jeanmarie Perrone","Abeed Sarker"],"pdf_url":"https://arxiv.org/pdf/2507.10803v1.pdf","comment":"Pages: 19, Abstract word count: 151 words, Manuscript word count:\n  2185 words, References: 14, Figures: 3, Tables: 2"},{"id":"http://arxiv.org/abs/2507.10787v1","updated":"2025-07-14T20:35:25Z","published":"2025-07-14T20:35:25Z","title":"Can Multimodal Foundation Models Understand Schematic Diagrams? An\n  Empirical Study on Information-Seeking QA over Scientific Papers","summary":"  This paper introduces MISS-QA, the first benchmark specifically designed to\nevaluate the ability of models to interpret schematic diagrams within\nscientific literature. MISS-QA comprises 1,500 expert-annotated examples over\n465 scientific papers. In this benchmark, models are tasked with interpreting\nschematic diagrams that illustrate research overviews and answering\ncorresponding information-seeking questions based on the broader context of the\npaper. We assess the performance of 18 frontier multimodal foundation models,\nincluding o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant\nperformance gap between these models and human experts on MISS-QA. Our analysis\nof model performance on unanswerable questions and our detailed error analysis\nfurther highlight the strengths and limitations of current models, offering key\ninsights to enhance models in comprehending multimodal scientific literature.\n","authors":["Yilun Zhao","Chengye Wang","Chuhan Li","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2507.10787v1.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2502.04644v2","updated":"2025-07-14T20:06:23Z","published":"2025-02-07T04:08:46Z","title":"Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning\n  with Agentic Tools","summary":"  We introduce Agentic Reasoning, a framework that enhances large language\nmodel (LLM) reasoning by integrating external tool-using agents. Agentic\nReasoning dynamically leverages web search, code execution, and structured\nmemory to address complex problems requiring deep research. A key innovation in\nour framework is the Mind-Map agent, which constructs a structured knowledge\ngraph to store reasoning context and track logical relationships, ensuring\ncoherence in long reasoning chains with extensive tool usage. Additionally, we\nconduct a comprehensive exploration of the Web-Search agent, leading to a\nhighly effective search mechanism that surpasses all prior approaches. When\ndeployed on DeepSeek-R1, our method achieves a new state-of-the-art (SOTA)\namong public models and delivers performance comparable to OpenAI Deep\nResearch, the leading proprietary model in this domain. Extensive ablation\nstudies validate the optimal selection of agentic tools and confirm the\neffectiveness of our Mind-Map and Web-Search agents in enhancing LLM reasoning.\nThe code is at: https://github.com/theworldofagents/Agentic-Reasoning\n","authors":["Junde Wu","Jiayuan Zhu","Yuyuan Liu","Min Xu","Yueming Jin"],"pdf_url":"https://arxiv.org/pdf/2502.04644v2.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2507.10773v1","updated":"2025-07-14T19:57:18Z","published":"2025-07-14T19:57:18Z","title":"Theory of Mind and Self-Disclosure to CUIs","summary":"  Self-disclosure is important to help us feel better, yet is often difficult.\nThis difficulty can arise from how we think people are going to react to our\nself-disclosure. In this workshop paper, we briefly discuss self-disclosure to\nconversational user interfaces (CUIs) in relation to various social cues. We\nthen, discuss how expressions of uncertainty or representation of a CUI's\nreasoning could help encourage self-disclosure, by making a CUI's intended\n\"theory of mind\" more transparent to users.\n","authors":["Samuel Rhys Cox"],"pdf_url":"https://arxiv.org/pdf/2507.10773v1.pdf","comment":"Workshop paper presented at ToMinHAI at CUI'2025: Theory of Mind in\n  Human-CUI Interaction, held in conjunction with the 2025 ACM conference on\n  Conversational User Interfaces, July 8th, 2025. 4 pages. 3 figures"},{"id":"http://arxiv.org/abs/2507.10772v1","updated":"2025-07-14T19:53:56Z","published":"2025-07-14T19:53:56Z","title":"Applying Text Embedding Models for Efficient Analysis in Labeled\n  Property Graphs","summary":"  Labeled property graphs often contain rich textual attributes that can\nenhance analytical tasks when properly leveraged. This work explores the use of\npretrained text embedding models to enable efficient semantic analysis in such\ngraphs. By embedding textual node and edge properties, we support downstream\ntasks including node classification and relation prediction with improved\ncontextual understanding. Our approach integrates language model embeddings\ninto the graph pipeline without altering its structure, demonstrating that\ntextual semantics can significantly enhance the accuracy and interpretability\nof property graph analysis.\n","authors":["Michal Podstawski"],"pdf_url":"https://arxiv.org/pdf/2507.10772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10743v1","updated":"2025-07-14T19:08:07Z","published":"2025-07-14T19:08:07Z","title":"Language Models for Adult Service Website Text Analysis","summary":"  Sex trafficking refers to the use of force, fraud, or coercion to compel an\nindividual to perform in commercial sex acts against their will. Adult service\nwebsites (ASWs) have and continue to be linked to sex trafficking, offering a\nplatform for traffickers to advertise their victims. Thus, organizations\ninvolved in the fight against sex trafficking often use ASW data when\nattempting to identify potential sex trafficking victims. A critical challenge\nin transforming ASW data into actionable insight is text analysis. Previous\nresearch using ASW data has shown that ASW ad text is important for linking\nads. However, working with this text is challenging due to its extensive use of\nemojis, poor grammar, and deliberate obfuscation to evade law enforcement\nscrutiny. We conduct a comprehensive study of language modeling approaches for\nthis application area, including simple information retrieval methods,\npre-trained transformers, and custom transformer models. We demonstrate that\ncharacteristics of ASW text data allow efficient custom transformer models to\nbe trained with relatively small GPU resources and used efficiently for\ninference on consumer hardware. Our custom models outperform fine-tuned\nvariants of well-known encoder-only transformer models, including BERT-base,\nRoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We\ndemonstrate the use of our best-performing custom configuration on three tasks\nrelated to ASW data analysis: (i) decomposing the giant component in a graph\nrepresentation of ASW data, (ii) clustering ASW ad text, and (iii) using the\nlearned token embeddings to understand the use of emojis in the illicit context\nwe study. The models we develop represent a significant advancement in ASW text\nanalysis, which can be leveraged in a variety of downstream applications and\nresearch.\n","authors":["Nickolas Freeman","Thanh Nguyen","Gregory Bott","Jason Parton","Collin Francel"],"pdf_url":"https://arxiv.org/pdf/2507.10743v1.pdf","comment":"32 pages, 12 figures, 1 table"},{"id":"http://arxiv.org/abs/2507.02221v2","updated":"2025-07-14T18:30:25Z","published":"2025-07-03T00:55:58Z","title":"GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic\n  Data Commons","summary":"  The Genomic Data Commons (GDC) provides access to high quality, harmonized\ncancer genomics data through a unified curation and analysis platform centered\naround patient cohorts. While GDC users can interactively create complex\ncohorts through the graphical Cohort Builder, users (especially new ones) may\nstruggle to find specific cohort descriptors across hundreds of possible fields\nand properties. However, users may be better able to describe their desired\ncohort in free-text natural language. We introduce GDC Cohort Copilot, an\nopen-source copilot tool for curating cohorts from the GDC. GDC Cohort Copilot\nautomatically generates the GDC cohort filter corresponding to a user-input\nnatural language description of their desired cohort, before exporting the\ncohort back to the GDC for further analysis. An interactive user interface\nallows users to further refine the generated cohort. We develop and evaluate\nmultiple large language models (LLMs) for GDC Cohort Copilot and demonstrate\nthat our locally-served, open-source GDC Cohort LLM achieves better results\nthan GPT-4o prompting in generating GDC cohorts. We implement and share GDC\nCohort Copilot as a containerized Gradio app on HuggingFace Spaces, available\nat https://huggingface.co/spaces/uc-ctds/GDC-Cohort-Copilot. GDC Cohort LLM\nweights are available at https://huggingface.co/uc-ctds. All source code is\navailable at https://github.com/uc-cdis/gdc-cohort-copilot.\n","authors":["Steven Song","Anirudh Subramanyam","Zhenyu Zhang","Aarti Venkat","Robert L. Grossman"],"pdf_url":"https://arxiv.org/pdf/2507.02221v2.pdf","comment":"12 pages, 1 figure, 7 tables. v2 updated to reflect migration to HF\n  Spaces"},{"id":"http://arxiv.org/abs/2411.02820v4","updated":"2025-07-14T18:22:53Z","published":"2024-11-05T05:41:41Z","title":"DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving","summary":"  Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models.\n","authors":["Yuhan Liu","Yuyang Huang","Jiayi Yao","Shaoting Feng","Zhuohan Gu","Kuntai Du","Hanchen Li","Yihua Cheng","Junchen Jiang","Shan Lu","Madan Musuvathi","Esha Choukse"],"pdf_url":"https://arxiv.org/pdf/2411.02820v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10644v1","updated":"2025-07-14T16:47:19Z","published":"2025-07-14T16:47:19Z","title":"From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web\n  of Agents","summary":"  The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.\n","authors":["Tatiana Petrova","Aleksandr Puzikov","Boris Bliznukov","Radu State"],"pdf_url":"https://arxiv.org/pdf/2507.10644v1.pdf","comment":"33 pages, 9 figures, 8 tables"},{"id":"http://arxiv.org/abs/2507.11502v1","updated":"2025-07-14T15:09:05Z","published":"2025-07-14T15:09:05Z","title":"HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong","summary":"  This paper presents the development of HKGAI-V1, a foundational sovereign\nlarge language model (LLM), developed as part of an initiative to establish\nvalue-aligned AI infrastructure specifically tailored for Hong Kong. Addressing\nthe region's unique multilingual environment (Cantonese, Mandarin, and\nEnglish), its distinct socio-legal context under the \"one country, two systems\"\nframework, and specific local cultural and value considerations, the model is\nbuilt upon the DeepSeek architecture and systematically aligned with regional\nnorms through a multifaceted full parameter fine-tuning process. It is further\nintegrated with a retrieval-augmented generation (RAG) system to ensure timely\nand factually grounded information access. The core contribution lies in the\ndesign and implementation of a comprehensive, region-specific AI alignment and\nsafety framework, demonstrated through two key achievements: 1) The successful\ndevelopment of HKGAI-V1 itself - which outper-forms general-purpose models in\nhandling Hong Kong-specific culturally sensitive queries, and embodies a\n\"governance-embedded\" approach to digital sovereignty - empowers Hong Kong to\nexercise control over AI applications in critical sectors including public\nservices, legal systems, and edu-cation. 2) The development of the proprietary\nAdversarial HK Value Benchmark, a rigorous tool for evaluating model alignment\nwith local ethical and legal stand-ards under challenging conditions. By\ndocumenting these achievements, the paper provides not only a technological\nartifact but also a replicable blueprint for developing advanced, regionally\nfocused AI systems deeply rooted in their local identities.\n","authors":["Sirui Han","Junqi Zhu","Ruiyuan Zhang","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2507.11502v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2507.10552v1","updated":"2025-07-14T17:59:59Z","published":"2025-07-14T17:59:59Z","title":"Self-supervised Learning on Camera Trap Footage Yields a Strong\n  Universal Face Embedder","summary":"  Camera traps are revolutionising wildlife monitoring by capturing vast\namounts of visual data; however, the manual identification of individual\nanimals remains a significant bottleneck. This study introduces a fully\nself-supervised approach to learning robust chimpanzee face embeddings from\nunlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision\nTransformers on automatically mined face crops, eliminating the need for\nidentity labels. Our method demonstrates strong open-set re-identification\nperformance, surpassing supervised baselines on challenging benchmarks such as\nBossou, despite utilising no labelled data during training. This work\nunderscores the potential of self-supervised learning in biodiversity\nmonitoring and paves the way for scalable, non-invasive population studies.\n","authors":["Vladimir Iashin","Horace Lee","Dan Schofield","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2507.10552v1.pdf","comment":"Accepted for publication. Project page, code and weights:\n  https://www.robots.ox.ac.uk/~vgg/research/ChimpUFE/"},{"id":"http://arxiv.org/abs/2507.10548v1","updated":"2025-07-14T17:59:46Z","published":"2025-07-14T17:59:46Z","title":"EmbRACE-3K: Embodied Reasoning and Action in Complex Environments","summary":"  Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities.\n","authors":["Mingxian Lin","Wei Huang","Yitang Li","Chengjie Jiang","Kui Wu","Fangwei Zhong","Shengju Qian","Xin Wang","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2507.10548v1.pdf","comment":"Project page: https://mxllc.github.io/EmbRACE-3K/"},{"id":"http://arxiv.org/abs/2507.10547v1","updated":"2025-07-14T17:59:41Z","published":"2025-07-14T17:59:41Z","title":"Quantize-then-Rectify: Efficient VQ-VAE Training","summary":"  Visual tokenizers are pivotal in multimodal large models, acting as bridges\nbetween continuous inputs and discrete tokens. Nevertheless, training\nhigh-compression-rate VQ-VAEs remains computationally demanding, often\nnecessitating thousands of GPU hours. This work demonstrates that a pre-trained\nVAE can be efficiently transformed into a VQ-VAE by controlling quantization\nnoise within the VAE's tolerance threshold. We present\n\\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs\nto enable rapid VQ-VAE training with minimal computational overhead. By\nintegrating \\textbf{channel multi-group quantization} to enlarge codebook\ncapacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ\ncompresses ImageNet images into at most 512 tokens while sustaining competitive\nreconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training\ncosts by over two orders of magnitude relative to state-of-the-art approaches:\nReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,\nwhereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental\nresults show that ReVQ achieves superior efficiency-reconstruction trade-offs.\n","authors":["Borui Zhang","Qihang Rao","Wenzhao Zheng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2507.10547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10542v1","updated":"2025-07-14T17:59:03Z","published":"2025-07-14T17:59:03Z","title":"ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions","summary":"  Generating high-fidelity real-time animated sequences of photorealistic 3D\nhead avatars is important for many graphics applications, including immersive\ntelepresence and movies. This is a challenging problem particularly when\nrendering digital avatar close-ups for showing character's facial microfeatures\nand expressions. To capture the expressive, detailed nature of human heads,\nincluding skin furrowing and finer-scale facial movements, we propose to couple\nlocally-defined facial expressions with 3D Gaussian splatting to enable\ncreating ultra-high fidelity, expressive and photorealistic 3D head avatars. In\ncontrast to previous works that operate on a global expression space, we\ncondition our avatar's dynamics on patch-based local expression features and\nsynthesize 3D Gaussians at a patch level. In particular, we leverage a\npatch-based geometric 3D face model to extract patch expressions and learn how\nto translate these into local dynamic skin appearance and motion by coupling\nthe patches with anchor points of Scaffold-GS, a recent hierarchical scene\nrepresentation. These anchors are then used to synthesize 3D Gaussians\non-the-fly, conditioned by patch-expressions and viewing direction. We employ\ncolor-based densification and progressive training to obtain high-quality\nresults and faster convergence for high resolution 3K training images. By\nleveraging patch-level expressions, ScaffoldAvatar consistently achieves\nstate-of-the-art performance with visually natural motion, while encompassing\ndiverse facial expressions and styles in real time.\n","authors":["Shivangi Aneja","Sebastian Weiss","Irene Baeza","Prashanth Chandran","Gaspard Zoss","Matthias Nießner","Derek Bradley"],"pdf_url":"https://arxiv.org/pdf/2507.10542v1.pdf","comment":"(SIGGRAPH 2025) Paper Video: https://youtu.be/VyWkgsGdbkk Project\n  Page: https://shivangi-aneja.github.io/projects/scaffoldavatar/"},{"id":"http://arxiv.org/abs/2505.00684v2","updated":"2025-07-14T17:45:50Z","published":"2025-05-01T17:45:59Z","title":"Visual Test-time Scaling for GUI Agent Grounding","summary":"  We introduce RegionFocus, a visual test-time scaling approach for Vision\nLanguage Model Agents. Understanding webpages is challenging due to the visual\ncomplexity of GUI images and the large number of interface elements, making\naccurate action selection difficult. Our approach dynamically zooms in on\nrelevant regions, reducing background clutter and improving grounding accuracy.\nTo support this process, we propose an image-as-map mechanism that visualizes\nkey landmarks at each step, providing a transparent action record and enables\nthe agent to effectively choose among action candidates. Even with a simple\nregion selection strategy, we observe significant performance gains of 28+\\% on\nScreenspot-pro and 24+\\% on WebVoyager benchmarks on top of two\nstate-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL,\nhighlighting the effectiveness of visual test-time scaling in interactive\nsettings. We achieve a new state-of-the-art grounding performance of 61.6\\% on\nthe ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model.\nOur code will be released publicly at https://github.com/tiangeluo/RegionFocus.\n","authors":["Tiange Luo","Lajanugen Logeswaran","Justin Johnson","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2505.00684v2.pdf","comment":"ICCV2025, https://github.com/tiangeluo/RegionFocus"},{"id":"http://arxiv.org/abs/2406.01069v2","updated":"2025-07-14T17:31:45Z","published":"2024-06-03T07:40:10Z","title":"UniQA: Unified Vision-Language Pre-training for Image Quality and\n  Aesthetic Assessment","summary":"  Image Quality Assessment (IQA) and Image Aesthetic Assessment (IAA) aim to\nsimulate human subjective perception of image visual quality and aesthetic\nappeal. Despite distinct learning objectives, they have underlying\ninterconnectedness due to consistent human assessment perception. In this\npaper, we propose Unified vision-language pre-training of Quality and\nAesthetics (UniQA}), to extract useful and common representations from two\ntasks, thereby benefiting them simultaneously. However, the lack of text in the\nIQA datasets and the textual noise in the IAA datasets pose severe challenges\nfor multimodal pre-training. To address this, we (1) utilize multimodal large\nlanguage models (MLLMs) to generate high-quality text descriptions; (2) use the\ngenerated text for IAA as metadata to purify noisy IAA data. To effectively\nadapt the pre-trained UniQA to downstream tasks, we further propose a\nlightweight adapter that utilizes versatile cues to fully exploit the extensive\nknowledge of the pre-trained model. UniQA demonstrates high competitiveness in\nvarious image assessment tasks, including classical IQA and IAA tasks,\nfew-label IQA, and other downstream tasks, showing promise as a foundational\nassessment model. Codes are available at https://github.com/zht8506/UniQA.\n","authors":["Hantao Zhou","Longxiang Tang","Rui Yang","Guanyi Qin","Yan Zhang","Yutao Li","Xiu Li","Runze Hu","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2406.01069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06678v2","updated":"2025-07-14T17:31:31Z","published":"2025-03-09T16:07:58Z","title":"Gamma: Toward Generic Image Assessment with Mixture of Assessment\n  Experts","summary":"  Image assessment aims to evaluate the quality and aesthetics of images and\nhas been applied across various scenarios, such as natural and AIGC scenes.\nExisting methods mostly address these sub-tasks or scenes individually. While\nsome works attempt to develop unified image assessment models, they have\nstruggled to achieve satisfactory performance or cover a broad spectrum of\nassessment scenarios. In this paper, we present \\textbf{Gamma}, a\n\\textbf{G}eneric im\\textbf{A}ge assess\\textbf{M}ent model using\n\\textbf{M}ixture of \\textbf{A}ssessment Experts, which can effectively assess\nimages from diverse scenes through mixed-dataset training. Achieving unified\ntraining in image assessment presents significant challenges due to annotation\nbiases across different datasets. To address this issue, we first propose a\nMixture of Assessment Experts (MoAE) module, which employs shared and adaptive\nexperts to dynamically learn common and specific knowledge for different\ndatasets, respectively. In addition, we introduce a Scene-based Differential\nPrompt (SDP) strategy, which uses scene-specific prompts to provide prior\nknowledge and guidance during the learning process, further boosting adaptation\nfor various scenes. Our Gamma model is trained and evaluated on 12 datasets\nspanning 6 image assessment scenarios. Extensive experiments show that our\nunified Gamma outperforms other state-of-the-art mixed-training methods by\nsignificant margins while covering more scenes. Codes are available at\nhttps://github.com/zht8506/Gamma.\n","authors":["Hantao Zhou","Rui Yang","Longxiang Tang","Guanyi Qin","Runze Hu","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2503.06678v2.pdf","comment":"Accepted to ACMMM 2025"},{"id":"http://arxiv.org/abs/2507.10500v1","updated":"2025-07-14T17:24:07Z","published":"2025-07-14T17:24:07Z","title":"Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver\n  Assistance","summary":"  While autonomous driving technologies continue to advance, current Advanced\nDriver Assistance Systems (ADAS) remain limited in their ability to interpret\nscene context or engage with drivers through natural language. These systems\ntypically rely on predefined logic and lack support for dialogue-based\ninteraction, making them inflexible in dynamic environments or when adapting to\ndriver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a\nmodular framework that integrates Generative AI components including large\nlanguage models, vision-to-text interpretation, and structured function calling\nto enable real-time, interpretable, and adaptive driver assistance. SC-ADAS\nsupports multi-turn dialogue grounded in visual and sensor context, allowing\nnatural language recommendations and driver-confirmed ADAS control. Implemented\nin the CARLA simulator with cloud-based Generative AI, the system executes\nconfirmed user intents as structured ADAS commands without requiring model\nfine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and\nrevisited multi-turn interactions, highlighting trade-offs such as increased\nlatency from vision-based context retrieval and token growth from accumulated\ndialogue history. These results demonstrate the feasibility of combining\nconversational reasoning, scene perception, and modular ADAS control to support\nthe next generation of intelligent driver assistance.\n","authors":["Kyungtae Han","Yitao Chen","Rohit Gupta","Onur Altintas"],"pdf_url":"https://arxiv.org/pdf/2507.10500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10499v1","updated":"2025-07-14T17:23:43Z","published":"2025-07-14T17:23:43Z","title":"National level satellite-based crop field inventories in smallholder\n  landscapes","summary":"  The design of science-based policies to improve the sustainability of\nsmallholder agriculture is challenged by a limited understanding of fundamental\nsystem properties, such as the spatial distribution of active cropland and\nfield size. We integrate very high spatial resolution (1.5 m) Earth observation\ndata and deep transfer learning to derive crop field delineations in complex\nagricultural systems at the national scale, while maintaining minimum reference\ndata requirements and enhancing transferability. We provide the first\nnational-level dataset of 21 million individual fields for Mozambique (covering\n~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural\nland use with an overall accuracy of 93% and balanced omission and commission\nerrors. Field-level spatial agreement reached median intersection over union\n(IoU) scores of 0.81, advancing the state-of-the-art in large-area field\ndelineation in complex smallholder systems. The active cropland maps capture\nfragmented rural regions with low cropland shares not yet identified in global\nland cover or cropland maps. These regions are mostly located in agricultural\nfrontier regions which host 7-9% of the Mozambican population. Field size in\nMozambique is very low overall, with half of the fields being smaller than 0.16\nha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial\nresolution (0.05{\\deg}) is 0.32 ha, but it varies strongly across gradients of\naccessibility, population density, and net forest cover change. This variation\nreflects a diverse set of actors, ranging from semi-subsistence smallholder\nfarms to medium-scale commercial farming, and large-scale farming operations.\nOur results highlight that field size is a key indicator relating to\nsocio-economic and environmental outcomes of agriculture (e.g., food\nproduction, livelihoods, deforestation, biodiversity), as well as their\ntrade-offs.\n","authors":["Philippe Rufin","Pauline Lucie Hammer","Leon-Friedrich Thomas","Sá Nogueira Lisboa","Natasha Ribeiro","Almeida Sitoe","Patrick Hostert","Patrick Meyfroidt"],"pdf_url":"https://arxiv.org/pdf/2507.10499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10496v1","updated":"2025-07-14T17:22:45Z","published":"2025-07-14T17:22:45Z","title":"Cameras as Relative Positional Encoding","summary":"  Transformers are increasingly prevalent for multi-view computer vision tasks,\nwhere geometric relationships between viewpoints are critical for 3D\nperception. To leverage these relationships, multi-view transformers must use\ncamera geometry to ground visual tokens in 3D space. In this work, we compare\ntechniques for conditioning transformers on cameras: token-level raymap\nencodings, attention-level relative pose encodings, and a new relative encoding\nwe propose -- Projective Positional Encoding (PRoPE) -- that captures complete\ncamera frustums, both intrinsics and extrinsics, as a relative positional\nencoding. Our experiments begin by showing how relative camera conditioning\nimproves performance in feedforward novel view synthesis, with further gains\nfrom PRoPE. This holds across settings: scenes with both shared and varying\nintrinsics, when combining token- and attention-level conditioning, and for\ngeneralization to inputs with out-of-distribution sequence lengths and camera\nintrinsics. We then verify that these benefits persist for different tasks,\nstereo depth estimation and discriminative spatial cognition, as well as larger\nmodel sizes.\n","authors":["Ruilong Li","Brent Yi","Junchen Liu","Hang Gao","Yi Ma","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2507.10496v1.pdf","comment":"Project Page: https://www.liruilong.cn/prope/"},{"id":"http://arxiv.org/abs/2408.11447v4","updated":"2025-07-14T17:20:25Z","published":"2024-08-21T09:06:30Z","title":"GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation\n  with Gaussian Splatting","summary":"  We introduce GaussianOcc, a systematic method that investigates the two\nusages of Gaussian splatting for fully self-supervised and efficient 3D\noccupancy estimation in surround views. First, traditional methods for\nself-supervised 3D occupancy estimation still require ground truth 6D poses\nfrom sensors during training. To address this limitation, we propose Gaussian\nSplatting for Projection (GSP) module to provide accurate scale information for\nfully self-supervised training from adjacent view projection. Additionally,\nexisting methods rely on volume rendering for final 3D voxel representation\nlearning using 2D signals (depth maps, semantic maps), which is both\ntime-consuming and less effective. We propose Gaussian Splatting from Voxel\nspace (GSV) to leverage the fast rendering properties of Gaussian splatting. As\na result, the proposed GaussianOcc method enables fully self-supervised (no\nground truth pose) 3D occupancy estimation in competitive performance with low\ncomputational cost (2.7 times faster in training and 5 times faster in\nrendering). The relevant code is available in\nhttps://github.com/GANWANSHUI/GaussianOcc.git.\n","authors":["Wanshui Gan","Fang Liu","Hongbin Xu","Ningkai Mo","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2408.11447v4.pdf","comment":"Project page: https://ganwanshui.github.io/GaussianOcc/"},{"id":"http://arxiv.org/abs/2502.12377v2","updated":"2025-07-14T17:15:27Z","published":"2025-02-17T23:30:50Z","title":"Alignment and Adversarial Robustness: Are More Human-Like Models More\n  Secure?","summary":"  A small but growing body of work has shown that machine learning models which\nbetter align with human vision have also exhibited higher robustness to\nadversarial examples, raising the question: can human-like perception make\nmodels more secure? If true generally, such mechanisms would offer new avenues\ntoward robustness. In this work, we conduct a large-scale empirical analysis to\nsystematically investigate the relationship between representational alignment\nand adversarial robustness. We evaluate 114 models spanning diverse\narchitectures and training paradigms, measuring their neural and behavioral\nalignment and engineering task performance across 105 benchmarks as well as\ntheir adversarial robustness via AutoAttack. Our findings reveal that while\naverage alignment and robustness exhibit a weak overall correlation, specific\nalignment benchmarks serve as strong predictors of adversarial robustness,\nparticularly those that measure selectivity toward texture or shape. These\nresults suggest that different forms of alignment play distinct roles in model\nrobustness, motivating further investigation into how alignment-driven\napproaches can be leveraged to build more secure and perceptually-grounded\nvision models.\n","authors":["Blaine Hoak","Kunyang Li","Patrick McDaniel"],"pdf_url":"https://arxiv.org/pdf/2502.12377v2.pdf","comment":"Accepted to International Workshop on Security and Privacy-Preserving\n  AI/ML (SPAIML) 2025"},{"id":"http://arxiv.org/abs/2503.14836v2","updated":"2025-07-14T17:14:45Z","published":"2025-03-19T02:35:01Z","title":"On the Robustness Tradeoff in Fine-Tuning","summary":"  Fine-tuning has become the standard practice for adapting pre-trained models\nto downstream tasks. However, the impact on model robustness is not well\nunderstood. In this work, we characterize the robustness-accuracy trade-off in\nfine-tuning. We evaluate the robustness and accuracy of fine-tuned models over\n6 benchmark datasets and 7 different fine-tuning strategies. We observe a\nconsistent trade-off between adversarial robustness and accuracy. Peripheral\nupdates such as BitFit are more effective for simple tasks -- over 75% above\nthe average measured by the area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex tasks\n-- 57.5% and 34.6% above the average on Caltech-256 and CUB-200, respectively.\nLastly, we observe that the robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments.\n","authors":["Kunyang Li","Jean-Charles Noirot Ferrand","Ryan Sheatsley","Blaine Hoak","Yohan Beugin","Eric Pauley","Patrick McDaniel"],"pdf_url":"https://arxiv.org/pdf/2503.14836v2.pdf","comment":"Accepted to International Conference on Computer Vision, ICCV 2025"},{"id":"http://arxiv.org/abs/2507.10492v1","updated":"2025-07-14T17:13:08Z","published":"2025-07-14T17:13:08Z","title":"BenchReAD: A systematic benchmark for retinal anomaly detection","summary":"  Retinal anomaly detection plays a pivotal role in screening ocular and\nsystemic diseases. Despite its significance, progress in the field has been\nhindered by the absence of a comprehensive and publicly available benchmark,\nwhich is essential for the fair evaluation and advancement of methodologies.\nDue to this limitation, previous anomaly detection work related to retinal\nimages has been constrained by (1) a limited and overly simplistic set of\nanomaly types, (2) test sets that are nearly saturated, and (3) a lack of\ngeneralization evaluation, resulting in less convincing experimental setups.\nFurthermore, existing benchmarks in medical anomaly detection predominantly\nfocus on one-class supervised approaches (training only with negative samples),\noverlooking the vast amounts of labeled abnormal data and unlabeled data that\nare commonly available in clinical practice. To bridge these gaps, we introduce\na benchmark for retinal anomaly detection, which is comprehensive and\nsystematic in terms of data and algorithm. Through categorizing and\nbenchmarking previous methods, we find that a fully supervised approach\nleveraging disentangled representations of abnormalities (DRA) achieves the\nbest performance but suffers from significant drops in performance when\nencountering certain unseen anomalies. Inspired by the memory bank mechanisms\nin one-class supervised learning, we propose NFM-DRA, which integrates DRA with\na Normal Feature Memory to mitigate the performance degradation, establishing a\nnew SOTA. The benchmark is publicly available at\nhttps://github.com/DopamineLcy/BenchReAD.\n","authors":["Chenyu Lian","Hong-Yu Zhou","Zhanli Hu","Jing Qin"],"pdf_url":"https://arxiv.org/pdf/2507.10492v1.pdf","comment":"MICCAI 2025"},{"id":"http://arxiv.org/abs/2507.10490v1","updated":"2025-07-14T17:12:43Z","published":"2025-07-14T17:12:43Z","title":"The Power of Certainty: How Confident Models Lead to Better Segmentation","summary":"  Deep learning models have been proposed for automatic polyp detection and\nprecise segmentation of polyps during colonoscopy procedures. Although these\nstate-of-the-art models achieve high performance, they often require a large\nnumber of parameters. Their complexity can make them prone to overfitting,\nparticularly when trained on biased datasets, and can result in poor\ngeneralization across diverse datasets. Knowledge distillation and\nself-distillation are proposed as promising strategies to mitigate the\nlimitations of large, over-parameterized models. These approaches, however, are\nresource-intensive, often requiring multiple models and significant memory\nduring training. We propose a confidence-based self-distillation approach that\noutperforms state-of-the-art models by utilizing only previous iteration data\nstorage during training, without requiring extra computation or memory usage\nduring testing. Our approach calculates the loss between the previous and\ncurrent iterations within a batch using a dynamic confidence coefficient. To\nevaluate the effectiveness of our approach, we conduct comprehensive\nexperiments on the task of polyp segmentation. Our approach outperforms\nstate-of-the-art models and generalizes well across datasets collected from\nmultiple clinical centers. The code will be released to the public once the\npaper is accepted.\n","authors":["Tugberk Erol","Tuba Caglikantar","Duygu Sarikaya"],"pdf_url":"https://arxiv.org/pdf/2507.10490v1.pdf","comment":"9 pages, 3 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.10485v1","updated":"2025-07-14T17:04:05Z","published":"2025-07-14T17:04:05Z","title":"Overcoming catastrophic forgetting in neural networks","summary":"  Catastrophic forgetting is the primary challenge that hinders continual\nlearning, which refers to a neural network ability to sequentially learn\nmultiple tasks while retaining previously acquired knowledge. Elastic Weight\nConsolidation, a regularization-based approach inspired by synaptic\nconsolidation in biological neural systems, has been used to overcome this\nproblem. In this study prior research is replicated and extended by evaluating\nEWC in supervised learning settings using the PermutedMNIST and RotatedMNIST\nbenchmarks. Through systematic comparisons with L2 regularization and\nstochastic gradient descent (SGD) without regularization, we analyze how\ndifferent approaches balance knowledge retention and adaptability. Our results\nconfirm what was shown in previous research, showing that EWC significantly\nreduces forgetting compared to naive training while slightly compromising\nlearning efficiency on new tasks. Moreover, we investigate the impact of\ndropout regularization and varying hyperparameters, offering insights into the\ngeneralization of EWC across diverse learning scenarios. These results\nunderscore EWC's potential as a viable solution for lifelong learning in neural\nnetworks.\n","authors":["Brandon Shuen Yi Loke","Filippo Quadri","Gabriel Vivanco","Maximilian Casagrande","Saúl Fenollosa"],"pdf_url":"https://arxiv.org/pdf/2507.10485v1.pdf","comment":"7 pages, 5 figures, EE-411 Fundamentals of inference and learning\n  course project"},{"id":"http://arxiv.org/abs/2507.10421v1","updated":"2025-07-14T16:04:34Z","published":"2025-07-14T16:04:34Z","title":"SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout\n  in Distance Learning","summary":"  School dropout is a serious problem in distance learning, where early\ndetection is crucial for effective intervention and student perseverance.\nPredicting student dropout using available educational data is a widely\nresearched topic in learning analytics. Our partner's distance learning\nplatform highlights the importance of integrating diverse data sources,\nincluding socio-demographic data, behavioral data, and sentiment analysis, to\naccurately predict dropout risks. In this paper, we introduce a novel model\nthat combines sentiment analysis of student comments using the Bidirectional\nEncoder Representations from Transformers (BERT) model with socio-demographic\nand behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We\nfine-tuned BERT on student comments to capture nuanced sentiments, which were\nthen merged with key features selected using feature importance techniques in\nXGBoost. Our model was tested on unseen data from the next academic year,\nachieving an accuracy of 84\\%, compared to 82\\% for the baseline model.\nAdditionally, the model demonstrated superior performance in other metrics,\nsuch as precision and F1-score. The proposed method could be a vital tool in\ndeveloping personalized strategies to reduce dropout rates and encourage\nstudent perseverance\n","authors":["Meriem Zerkouk","Miloud Mihoubi","Belkacem Chikhaoui"],"pdf_url":"https://arxiv.org/pdf/2507.10421v1.pdf","comment":"International Conference on Education and New Learning Technologies\n  (2025)"},{"id":"http://arxiv.org/abs/2507.10411v1","updated":"2025-07-14T15:54:50Z","published":"2025-07-14T15:54:50Z","title":"Am I on the Right Track? What Can Predicted Query Performance Tell Us\n  about the Search Behaviour of Agentic RAG","summary":"  Agentic Retrieval-Augmented Generation (RAG) is a new paradigm where the\nreasoning model decides when to invoke a retriever (as a \"tool\") when answering\na question. This paradigm, exemplified by recent research works such as\nSearch-R1, enables the model to decide when to search and obtain external\ninformation. However, the queries generated by such Agentic RAG models and the\nrole of the retriever in obtaining high-quality answers remain understudied. To\nthis end, this initial study examines the applicability of query performance\nprediction (QPP) within the recent Agentic RAG models Search-R1 and\nR1-Searcher. We find that applying effective retrievers can achieve higher\nanswer quality within a shorter reasoning process. Moreover, the QPP estimates\nof the generated queries, used as an approximation of their retrieval quality,\nare positively correlated with the quality of the final answer. Ultimately, our\nwork is a step towards adaptive retrieval within Agentic RAG, where QPP is used\nto inform the model if the retrieved results are likely to be useful.\n","authors":["Fangzheng Tian","Jinyuan Fang","Debasis Ganguly","Zaiqiao Meng","Craig Macdonald"],"pdf_url":"https://arxiv.org/pdf/2507.10411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05285v2","updated":"2025-07-14T15:49:02Z","published":"2025-07-04T21:41:43Z","title":"Beyond classical and contemporary models: a transformative AI framework\n  for student dropout prediction in distance learning using RAG, Prompt\n  engineering, and Cross-modal fusion","summary":"  Student dropout in distance learning remains a critical challenge, with\nprofound societal and economic consequences. While classical machine learning\nmodels leverage structured socio-demographic and behavioral data, they often\nfail to capture the nuanced emotional and contextual factors embedded in\nunstructured student interactions. This paper introduces a transformative AI\nframework that redefines dropout prediction through three synergistic\ninnovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment\nanalysis, prompt engineering to decode academic stressors,and cross-modal\nattention fusion to dynamically align textual, behavioral, and\nsocio-demographic insights. By grounding sentiment analysis in a curated\nknowledge base of pedagogical content, our RAG-enhanced BERT model interprets\nstudent comments with unprecedented contextual relevance, while optimized\nprompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload\nanxiety\"). A cross-modal attention layer then fuses these insights with\ntemporal engagement patterns, creating holistic risk pro-files. Evaluated on a\nlongitudinal dataset of 4 423 students, the framework achieves 89% accuracy and\nan F1-score of 0.88, outperforming conventional models by 7% and reducing false\nnegatives by 21%. Beyond prediction, the system generates interpretable\ninterventions by retrieving contextually aligned strategies (e.g., mentorship\nprograms for isolated learners). This work bridges the gap between predictive\nanalytics and actionable pedagogy, offering a scalable solution to mitigate\ndropout risks in global education systems\n","authors":["Miloud Mihoubi","Meriem Zerkouk","Belkacem Chikhaoui"],"pdf_url":"https://arxiv.org/pdf/2507.05285v2.pdf","comment":"13 pages, 8 figures, 1 Algorithms, 17th International Conference on\n  Education and New Learning Technologies,: 30 June-2 July, 2025 Location:\n  Palma, Spain"},{"id":"http://arxiv.org/abs/2507.10403v1","updated":"2025-07-14T15:46:56Z","published":"2025-07-14T15:46:56Z","title":"Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources","summary":"  Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.\n","authors":["Daniele Rege Cambrin","Lorenzo Vaiani","Giuseppe Gallipoli","Luca Cagliero","Paolo Garza"],"pdf_url":"https://arxiv.org/pdf/2507.10403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03280v2","updated":"2025-07-14T14:47:47Z","published":"2025-07-04T03:56:04Z","title":"Modeling Item-Level Dynamic Variability with Residual Diffusion for\n  Bundle Recommendation","summary":"  Existing solutions for bundle recommendation(BR) have achieved remarkable\neffectiveness for predicting the user's preference for prebuilt bundles.\nHowever, bundle-item(B-I) affiliation will vary dynamically in real scenarios.\nFor example, a bundle themed as 'casual outfit', may add 'hat' or remove\n'watch' due to factors such as seasonal variations, changes in user pes or\ninventory adjustments. Our empirical study demonstrates that the performance of\nmainstream BR models will fluctuate or even decline regarding item-level\nvariability. This paper makes the first attempt to referencaddress the above\nproblem and proposes a novel Residual Diffusion for Bundle\nRecommendation(RDiffBR) as a model-agnostic generative framework which can\nassist a BR model in adapting this scenario. During the initial training of the\nBR model, RDiffBR employs a residual diffusion model to process the item-level\nbundle embeddings which are generated by BR model to represent bundle theme via\na forward-reverse process. In the inference stage, RDiffBR reverses item-level\nbundle embeddings obtained by the well-trained bundle model under B-I\nvariability scenarios to generate the effective item-level bundle embeddings.\nIn particular, the residual connection in our residual approximator\nsignificantly enhances item-level bundle embeddings generation ability of BR\nmodels. Experiments on six BR models and four public datasets from different\ndomains show that RDiffBR improves the performance of Recall and NDCG of\nbackbone BR models by up to 23%, while only increased training time about\n4%.Codes and datasets are available at\nhttps://anonymous.4open.science/r/RDiffBR.\n","authors":["Dong Zhang","Lin Li","Ming Li","Xiaohui Tao","Meng Sun","Jimmy Xiangji Huang"],"pdf_url":"https://arxiv.org/pdf/2507.03280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10135v1","updated":"2025-07-14T10:26:27Z","published":"2025-07-14T10:26:27Z","title":"Riding the Carousel: The First Extensive Eye Tracking Analysis of\n  Browsing Behavior in Carousel Recommenders","summary":"  Carousels have become the de-facto interface in online services. However,\nthere is a lack of research in carousels, particularly examining how\nrecommender systems may be designed differently than the traditional\nsingle-list interfaces. One of the key elements for understanding how to design\na system for a particular interface is understanding how users browse. For\ncarousels, users may browse in a number of different ways due to the added\ncomplexity of multiple topic defined-lists and swiping to see more items.\n  Eye tracking is the key to understanding user behavior by providing valuable,\ndirect information on how users see and navigate. In this work, we provide the\nfirst extensive analysis of the eye tracking behavior in carousel recommenders\nunder the free-browsing setting. To understand how users browse, we examine the\nfollowing research questions : 1) where do users start browsing, 2) how do\nusers transition from item to item within the same carousel and across\ncarousels, and 3) how does genre preference impact transitions?\n  This work addresses a gap in the field and provides the first extensive\nempirical results of eye tracked browsing behavior in carousels for improving\nrecommenders. Taking into account the insights learned from the above\nquestions, our final contribution is to provide suggestions to help carousel\nrecommender system designers optimize their systems for user browsing behavior.\nThe most important suggestion being to reorder the ranked item positions to\naccount for browsing after swiping.These contributions aim not only to help\nimprove current systems, but also to encourage and allow the design of new user\nmodels, systems, and metrics that are better suited to the complexity of\ncarousel interfaces.\n","authors":["Santiago de Leon-Martinez","Robert Moro","Branislav Kveton","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2507.10135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10097v1","updated":"2025-07-14T09:32:26Z","published":"2025-07-14T09:32:26Z","title":"User Long-Term Multi-Interest Retrieval Model for Recommendation","summary":"  User behavior sequence modeling, which captures user interest from rich\nhistorical interactions, is pivotal for industrial recommendation systems.\nDespite breakthroughs in ranking-stage models capable of leveraging ultra-long\nbehavior sequences with length scaling up to thousands, existing retrieval\nmodels remain constrained to sequences of hundreds of behaviors due to two main\nchallenges. One is strict latency budget imposed by real-time service over\nlarge-scale candidate pool. The other is the absence of target-aware mechanisms\nand cross-interaction architectures, which prevent utilizing ranking-like\ntechniques to simplify long sequence modeling. To address these limitations, we\npropose a new framework named User Long-term Multi-Interest Retrieval\nModel(ULIM), which enables thousand-scale behavior modeling in retrieval\nstages. ULIM includes two novel components: 1)Category-Aware Hierarchical\nDual-Interest Learning partitions long behavior sequences into multiple\ncategory-aware subsequences representing multi-interest and jointly optimizes\nlong-term and short-term interests within specific interest cluster.\n2)Pointer-Enhanced Cascaded Category-to-Item Retrieval introduces\nPointer-Generator Interest Network(PGIN) for next-category prediction, followed\nby next-item retrieval upon the top-K predicted categories. Comprehensive\nexperiments on Taobao dataset show that ULIM achieves substantial improvement\nover state-of-the-art methods, and brings 5.54% clicks, 11.01% orders and 4.03%\nGMV lift for Taobaomiaosha, a notable mini-app of Taobao.\n","authors":["Yue Meng","Cheng Guo","Xiaohui Hu","Honghu Deng","Yi Cao","Tong Liu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2507.10097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06112v2","updated":"2025-07-14T09:07:27Z","published":"2024-11-09T08:22:31Z","title":"Interpret the Internal States of Recommendation Model with Sparse\n  Autoencoder","summary":"  Recommendation model interpretation aims to reveal models' calculation\nprocess, enhancing their transparency, interpretability, and trustworthiness by\nclarifying the relationships between inputs, model activations, and outputs.\nHowever, the complex, often opaque nature of deep learning models complicates\ninterpretation, and most existing methods are tailored to specific model\narchitectures, limiting their generalizability across different types of\nrecommendation models. To address these challenges, we propose RecSAE, an\nautomated and generalizable probing framework that interprets Recommenders with\nSparse AutoEncoder. It extracts interpretable latents from the internal states\nof recommendation models and links them to semantic concepts for\ninterpretation. RecSAE does not alter original models during interpretation and\nalso enables targeted de-biasing to models based on interpreted results.\nSpecifically, RecSAE operates in three steps: First, it probes activations\nbefore the prediction layer to capture internal representations. Next, the\nRecSAE module is trained on these activations with a larger latent space and\nsparsity constraints, making the RecSAE latents more mono-semantic than the\noriginal model activations. Thirdly, RecSAE utilizes a language model to\nconstruct concept descriptions with confidence scores based on the\nrelationships between latent activations and recommendation outputs.\nExperiments on three types of models (general, graph-based, and sequential)\nwith three widely used datasets demonstrate the effectiveness and\ngeneralization of RecSAE framework. The interpreted concepts are further\nvalidated by human experts, showing strong alignment with human perception.\nOverall, RecSAE serves as a novel step in both model-level interpretations to\nvarious types of recommenders without affecting their functions and offering\nthe potential for targeted tuning of models.\n","authors":["Jiayin Wang","Xiaoyu Zhang","Weizhi Ma","Zhiqiang Guo","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.06112v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10057v1","updated":"2025-07-14T08:41:53Z","published":"2025-07-14T08:41:53Z","title":"PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware\n  Query Optimization","summary":"  Scientific paper retrieval, particularly framed as document-to-document\nretrieval, aims to identify relevant papers in response to a long-form query\npaper, rather than a short query string. Previous approaches to this task have\nfocused on abstracts, embedding them into dense vectors as surrogates for full\ndocuments and calculating similarity across them, although abstracts provide\nonly sparse and high-level summaries. To address this, we propose PRISM, a\nnovel document-to-document retrieval method that introduces multiple,\nfine-grained representations for both the query and candidate papers. In\nparticular, each query paper is decomposed into multiple aspect-specific views\nand individually embedded, which are then matched against candidate papers\nsimilarity segmented to consider their multifaceted dimensions. Moreover, we\npresent SciFullBench, a novel benchmark in which the complete and segmented\ncontext of full papers for both queries and candidates is available. Then,\nexperimental results show that PRISM improves performance by an average of 4.3%\nover existing retrieval baselines.\n","authors":["Sangwoo Park","Jinheon Baek","Soyeong Jeong","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2507.10057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14851v2","updated":"2025-07-14T07:57:39Z","published":"2024-08-27T08:08:05Z","title":"Graph and Sequential Neural Networks in Session-based Recommendation: A\n  Survey","summary":"  Recent years have witnessed the remarkable success of recommendation systems\n(RSs) in alleviating the information overload problem. As a new paradigm of\nRSs, session-based recommendation (SR) specializes in users' short-term\npreference capture and aims to provide a more dynamic and timely recommendation\nbased on the ongoing interacted actions. In this survey, we will give a\ncomprehensive overview of the recent works on SR. First, we clarify the\ndefinitions of various SR tasks and introduce the characteristics of\nsession-based recommendation against other recommendation tasks. Then, we\nsummarize the existing methods in two categories: sequential neural network\nbased methods and graph neural network (GNN) based methods. The standard\nframeworks and technical are also introduced. Finally, we discuss the\nchallenges of SR and new research directions in this area.\n","authors":["Zihao Li","Chao Yang","Yakun Chen","Xianzhi Wang","Hongxu Chen","Guandong Xu","Lina Yao","Quan Z. Sheng"],"pdf_url":"https://arxiv.org/pdf/2408.14851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06507v2","updated":"2025-07-14T07:46:11Z","published":"2025-07-09T03:13:08Z","title":"GR-LLMs: Recent Advances in Generative Recommendation Based on Large\n  Language Models","summary":"  In the past year, Generative Recommendations (GRs) have undergone substantial\nadvancements, especially in leveraging the powerful sequence modeling and\nreasoning capabilities of Large Language Models (LLMs) to enhance overall\nrecommendation performance. LLM-based GRs are forming a new paradigm that is\ndistinctly different from discriminative recommendations, showing strong\npotential to replace traditional recommendation systems heavily dependent on\ncomplex hand-crafted features. In this paper, we provide a comprehensive survey\naimed at facilitating further research of LLM-based GRs. Initially, we outline\nthe general preliminaries and application cases of LLM-based GRs. Subsequently,\nwe introduce the main considerations when LLM-based GRs are applied in real\nindustrial scenarios. Finally, we explore promising directions for LLM-based\nGRs. We hope that this survey contributes to the ongoing advancement of the GR\ndomain.\n","authors":["Zhen Yang","Haitao Lin","Jiawei xue","Ziji Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.06507v2.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.09998v1","updated":"2025-07-14T07:32:16Z","published":"2025-07-14T07:32:16Z","title":"SLIF-MR: Self-loop Iterative Fusion of Heterogeneous Auxiliary\n  Information for Multimodal Recommendation","summary":"  Knowledge graphs (KGs) and multimodal item information, which respectively\ncapture relational and attribute features, play a crucial role in improving\nrecommender system accuracy. Recent studies have attempted to integrate them\nvia multimodal knowledge graphs (MKGs) to further enhance recommendation\nperformance. However, existing methods typically freeze the MKG structure\nduring training, which limits the full integration of structural information\nfrom heterogeneous graphs (e.g., KG and user-item interaction graph), and\nresults in sub-optimal performance. To address this challenge, we propose a\nnovel framework, termed Self-loop Iterative Fusion of Heterogeneous Auxiliary\nInformation for Multimodal Recommendation (SLIF-MR), which leverages item\nrepresentations from previous training epoch as feedback signals to dynamically\noptimize the heterogeneous graph structures composed of KG, multimodal item\nfeature graph, and user-item interaction graph. Through this iterative fusion\nmechanism, both user and item representations are refined, thus improving the\nfinal recommendation performance. Specifically, based on the feedback item\nrepresentations, SLIF-MR constructs an item-item correlation graph, then\nintegrated into the establishment process of heterogeneous graphs as additional\nnew structural information in a self-loop manner. Consequently, the internal\nstructures of heterogeneous graphs are updated with the feedback item\nrepresentations during training. Moreover, a semantic consistency learning\nstrategy is proposed to align heterogeneous item representations across\nmodalities. The experimental results show that SLIF-MR significantly\noutperforms existing methods, particularly in terms of accuracy and robustness.\n","authors":["Jie Guo","Jiahao Jiang","Ziyuan Guo","Bin Song","Yue Sun"],"pdf_url":"https://arxiv.org/pdf/2507.09998v1.pdf","comment":"10 pages,7 figures"},{"id":"http://arxiv.org/abs/2507.09969v1","updated":"2025-07-14T06:35:18Z","published":"2025-07-14T06:35:18Z","title":"Non-parametric Graph Convolution for Re-ranking in Recommendation\n  Systems","summary":"  Graph knowledge has been proven effective in enhancing item rankings in\nrecommender systems (RecSys), particularly during the retrieval stage. However,\nits application in the ranking stage, especially when richer contextual\ninformation in user-item interactions is available, remains underexplored. A\nmajor challenge lies in the substantial computational cost associated with\nrepeatedly retrieving neighborhood information from billions of items stored in\ndistributed systems. This resource-intensive requirement makes it difficult to\nscale graph-based methods in practical RecSys. To bridge this gap, we first\ndemonstrate that incorporating graphs in the ranking stage improves ranking\nqualities. Notably, while the improvement is evident, we show that the\nsubstantial computational overheads entailed by graphs are prohibitively\nexpensive for real-world recommendations. In light of this, we propose a\nnon-parametric strategy that utilizes graph convolution for re-ranking only\nduring test time. Our strategy circumvents the notorious computational\noverheads from graph convolution during training, and utilizes structural\nknowledge hidden in graphs on-the-fly during testing. It can be used as a\nplug-and-play module and easily employed to enhance the ranking ability of\nvarious ranking layers of a real-world RecSys with significantly reduced\ncomputational overhead. Through comprehensive experiments across four benchmark\ndatasets with varying levels of sparsity, we demonstrate that our strategy\nyields noticeable improvements (i.e., 8.1% on average) during testing time with\nlittle to no additional computational overheads (i.e., 0.5 on average). Code:\nhttps://github.com/zyouyang/RecSys2025_NonParamGC.git\n","authors":["Zhongyu Ouyang","Mingxuan Ju","Soroush Vosoughi","Yanfang Ye"],"pdf_url":"https://arxiv.org/pdf/2507.09969v1.pdf","comment":"Accepted to RecSys2025 Main"},{"id":"http://arxiv.org/abs/2507.09924v1","updated":"2025-07-14T05:04:32Z","published":"2025-07-14T05:04:32Z","title":"MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for\n  Rehearsal-Free Generative Retrieval over Dynamic Corpora","summary":"  Continually updating model-based indexes in generative retrieval with new\ndocuments remains challenging, as full retraining is computationally expensive\nand impractical under resource constraints. We propose MixLoRA-DSI, a novel\nframework that combines an expandable mixture of Low-Rank Adaptation experts\nwith a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead\nof allocating new experts for each new corpus, our proposed expansion strategy\nenables sublinear parameter growth by selectively introducing new experts only\nwhen significant number of OOD documents are detected. Experiments on NQ320k\nand MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update\nbaselines, with minimal parameter overhead and substantially lower training\ncosts.\n","authors":["Tuan-Luc Huynh","Thuy-Trang Vu","Weiqing Wang","Trung Le","Dragan Gašević","Yuan-Fang Li","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2507.09924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08322v2","updated":"2025-07-14T05:02:01Z","published":"2025-07-11T05:25:09Z","title":"Towards Efficient Quantity Retrieval from Text:An Approach via\n  Description Parsing and Weak Supervision","summary":"  Quantitative facts are continually generated by companies and governments,\nsupporting data-driven decision-making. While common facts are structured, many\nlong-tail quantitative facts remain buried in unstructured documents, making\nthem difficult to access. We propose the task of Quantity Retrieval: given a\ndescription of a quantitative fact, the system returns the relevant value and\nsupporting evidence. Understanding quantity semantics in context is essential\nfor this task. We introduce a framework based on description parsing that\nconverts text into structured (description, quantity) pairs for effective\nretrieval. To improve learning, we construct a large paraphrase dataset using\nweak supervision based on quantity co-occurrence. We evaluate our approach on a\nlarge corpus of financial annual reports and a newly annotated quantity\ndescription dataset. Our method significantly improves top-1 retrieval accuracy\nfrom 30.98 percent to 64.66 percent.\n","authors":["Yixuan Cao","Zhengrong Chen","Chengxuan Xia","Kun Wu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2507.08322v2.pdf","comment":"Extended version of the paper accepted in DEXA 2025"},{"id":"http://arxiv.org/abs/2502.17494v7","updated":"2025-07-14T03:01:53Z","published":"2025-02-20T22:35:52Z","title":"External Large Foundation Model: How to Efficiently Serve Trillions of\n  Parameters for Online Ads Recommendation","summary":"  Ads recommendation is a prominent service of online advertising systems and\nhas been actively studied. Recent studies indicate that scaling-up and advanced\ndesign of the recommendation model can bring significant performance\nimprovement. However, with a larger model scale, such prior studies have a\nsignificantly increasing gap from industry as they often neglect two\nfundamental challenges in industrial-scale applications. First, training and\ninference budgets are restricted for the model to be served, exceeding which\nmay incur latency and impair user experience. Second, large-volume data arrive\nin a streaming mode with data distributions dynamically shifting, as new\nusers/ads join and existing users/ads leave the system. We propose the External\nLarge Foundation Model (ExFM) framework to address the overlooked challenges.\nSpecifically, we develop external distillation and a data augmentation system\n(DAS) to control the computational cost of training/inference while maintaining\nhigh performance. We design the teacher in a way like a foundation model (FM)\nthat can serve multiple students as vertical models (VMs) to amortize its\nbuilding cost. We propose Auxiliary Head and Student Adapter to mitigate the\ndata distribution gap between FM and VMs caused by the streaming data issue.\nComprehensive experiments on internal industrial-scale applications and public\ndatasets demonstrate significant performance gain by ExFM.\n","authors":["Mingfu Liang","Xi Liu","Rong Jin","Boyang Liu","Qiuling Suo","Qinghai Zhou","Song Zhou","Laming Chen","Hua Zheng","Zhiyuan Li","Shali Jiang","Jiyan Yang","Xiaozhen Xia","Fan Yang","Yasmine Badr","Ellie Wen","Shuyu Xu","Hansey Chen","Zhengyu Zhang","Jade Nie","Chunzhi Yang","Zhichen Zeng","Weilin Zhang","Xingliang Huang","Qianru Li","Shiquan Wang","Evelyn Lyu","Wenjing Lu","Rui Zhang","Wenjun Wang","Jason Rudy","Mengyue Hang","Kai Wang","Yinbin Ma","Shuaiwen Wang","Sihan Zeng","Tongyi Tang","Xiaohan Wei","Longhao Jin","Jamey Zhang","Marcus Chen","Jiayi Xu","Angie Huang","Xihuan Zeng","Chi Zhang","Zhengli Zhao","Jared Yang","Qiang Jin","Xian Chen","Amit Anand Amlesahwaram","Lexi Song","Liang Luo","Yuchen Hao","Nan Xiao","Yavuz Yetim","Luoshang Pan","Gaoxiang Liu","Yuxi Hu","Yuzhen Huang","Jackie Xu","Rich Zhu","Xin Zhang","Yiqun Liu","Hang Yin","Yuxin Chen","Buyun Zhang","Xiaoyi Liu","Xingyuan Wang","Wenguang Mao","Zhijing Li","Zhehui Zhou","Feifan Gu","Qin Huang","Chonglin Sun","Nancy Yu","Shuo Gu","Shupin Mao","Benjamin Au","Jingzheng Qin","Peggy Yao","Jae-Woo Choi","Bin Gao","Ernest Wang","Lei Zhang","Wen-Yen Chen","Ted Lee","Yujie Zha","Yi Meng","Alex Gong","Edison Gao","Jack Hsueh","Jie Zheng","Alireza Vahdatpour","Yiping Han","Yantao Yao","Toshinari Kureha","Shuo Chang","Musharaf Sultan","John Bocharov","Sagar Chordia","Xiaorui Gan","Peng Sun","Rocky Liu","Bo Long","Wenlin Chen","Santanu Kolay","Huayu Li"],"pdf_url":"https://arxiv.org/pdf/2502.17494v7.pdf","comment":"Accepted by the ACM Web Conference (WWW) 2025 Industrial Track as\n  Oral Presentation"},{"id":"http://arxiv.org/abs/2507.10803v1","updated":"2025-07-14T20:57:52Z","published":"2025-07-14T20:57:52Z","title":"Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social\n  Media Chatter Use Case","summary":"  Background Large language models (LLMs) face challenges in inductive thematic\nanalysis, a task requiring deep interpretive and domain-specific expertise. We\nevaluated the feasibility of using LLMs to replicate expert-driven thematic\nanalysis of social media data. Methods Using two temporally non-intersecting\nReddit datasets on xylazine (n=286 and n=686, for model optimization and\nvalidation, respectively) with twelve expert-derived themes, we evaluated five\nLLMs against expert coding. We modeled the task as a series of binary\nclassifications, rather than a single, multi-label classification, employing\nzero-, single-, and few-shot prompting strategies and measuring performance via\naccuracy, precision, recall, and F1-score. Results On the validation set,\nGPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:\n0.71). For high-prevalence themes, model-derived thematic distributions closely\nmirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:\n16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based\napproaches can automate thematic analyses, offering a scalable supplement for\nqualitative research. Keywords: thematic analysis, large language models,\nnatural language processing, qualitative analysis, social media, prompt\nengineering, public health\n","authors":["JaMor Hairston","Ritvik Ranjan","Sahithi Lakamana","Anthony Spadaro","Selen Bozkurt","Jeanmarie Perrone","Abeed Sarker"],"pdf_url":"https://arxiv.org/pdf/2507.10803v1.pdf","comment":"Pages: 19, Abstract word count: 151 words, Manuscript word count:\n  2185 words, References: 14, Figures: 3, Tables: 2"},{"id":"http://arxiv.org/abs/2507.10772v1","updated":"2025-07-14T19:53:56Z","published":"2025-07-14T19:53:56Z","title":"Applying Text Embedding Models for Efficient Analysis in Labeled\n  Property Graphs","summary":"  Labeled property graphs often contain rich textual attributes that can\nenhance analytical tasks when properly leveraged. This work explores the use of\npretrained text embedding models to enable efficient semantic analysis in such\ngraphs. By embedding textual node and edge properties, we support downstream\ntasks including node classification and relation prediction with improved\ncontextual understanding. Our approach integrates language model embeddings\ninto the graph pipeline without altering its structure, demonstrating that\ntextual semantics can significantly enhance the accuracy and interpretability\nof property graph analysis.\n","authors":["Michal Podstawski"],"pdf_url":"https://arxiv.org/pdf/2507.10772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10730v1","updated":"2025-07-14T18:51:20Z","published":"2025-07-14T18:51:20Z","title":"Access Control for Information-Theoretically Secure Key-Document Stores","summary":"  This paper presents a novel key-based access control technique for secure\noutsourcing key-value stores where values correspond to documents that are\nindexed and accessed using keys. The proposed approach adopts Shamir's\nsecret-sharing that offers unconditional or information-theoretic security. It\nsupports keyword-based document retrieval while preventing leakage of the data,\naccess rights of users, or the size (\\textit{i}.\\textit{e}., volume of the\noutput that satisfies a query). The proposed approach allows servers to detect\n(and abort) malicious clients from gaining unauthorized access to data, and\nprevents malicious servers from altering data undetected while ensuring\nefficient access -- it takes 231.5ms over 5,000 keywords across 500,000 files.\n","authors":["Yin Li","Sharad Mehrota","Shantanu Sharma","Komal Kumari"],"pdf_url":"https://arxiv.org/pdf/2507.10730v1.pdf","comment":"An extended abstract of this version has been accepted in VLDB 2025"},{"id":"http://arxiv.org/abs/2507.10726v1","updated":"2025-07-14T18:47:13Z","published":"2025-07-14T18:47:13Z","title":"Extracting Document Relations from Search Corpus by Marginalizing over\n  User Queries","summary":"  Understanding relationships between documents in large-scale corpora is\nessential for knowledge discovery and information organization. However,\nexisting approaches rely heavily on manual annotation or predefined\nrelationship taxonomies. We propose EDR-MQ (Extracting Document Relations by\nMarginalizing over User Queries), a novel framework that discovers document\nrelationships through query marginalization. EDR-MQ is based on the insight\nthat strongly related documents often co-occur in results across diverse user\nqueries, enabling us to estimate joint probabilities between document pairs by\nmarginalizing over a collection of queries. To enable this query\nmarginalization approach, we develop Multiply Conditioned Retrieval-Augmented\nGeneration (MC-RAG), which employs conditional retrieval where subsequent\ndocument retrievals depend on previously retrieved content. By observing\nco-occurrence patterns across diverse queries, EDR-MQ estimates joint\nprobabilities between document pairs without requiring labeled training data or\npredefined taxonomies. Experimental results show that our query marginalization\napproach successfully identifies meaningful document relationships, revealing\ntopical clusters, evidence chains, and cross-domain connections that are not\napparent through traditional similarity-based methods. Our query-driven\nframework offers a practical approach to document organization that adapts to\ndifferent user perspectives and information needs.\n","authors":["Yuki Iwamoto","Kaoru Tsunoda","Ken Kaneiwa"],"pdf_url":"https://arxiv.org/pdf/2507.10726v1.pdf","comment":"9 pages, 6 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2507.10552v1","updated":"2025-07-14T17:59:59Z","published":"2025-07-14T17:59:59Z","title":"Self-supervised Learning on Camera Trap Footage Yields a Strong\n  Universal Face Embedder","summary":"  Camera traps are revolutionising wildlife monitoring by capturing vast\namounts of visual data; however, the manual identification of individual\nanimals remains a significant bottleneck. This study introduces a fully\nself-supervised approach to learning robust chimpanzee face embeddings from\nunlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision\nTransformers on automatically mined face crops, eliminating the need for\nidentity labels. Our method demonstrates strong open-set re-identification\nperformance, surpassing supervised baselines on challenging benchmarks such as\nBossou, despite utilising no labelled data during training. This work\nunderscores the potential of self-supervised learning in biodiversity\nmonitoring and paves the way for scalable, non-invasive population studies.\n","authors":["Vladimir Iashin","Horace Lee","Dan Schofield","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2507.10552v1.pdf","comment":"Accepted for publication. Project page, code and weights:\n  https://www.robots.ox.ac.uk/~vgg/research/ChimpUFE/"},{"id":"http://arxiv.org/abs/2507.10547v1","updated":"2025-07-14T17:59:41Z","published":"2025-07-14T17:59:41Z","title":"Quantize-then-Rectify: Efficient VQ-VAE Training","summary":"  Visual tokenizers are pivotal in multimodal large models, acting as bridges\nbetween continuous inputs and discrete tokens. Nevertheless, training\nhigh-compression-rate VQ-VAEs remains computationally demanding, often\nnecessitating thousands of GPU hours. This work demonstrates that a pre-trained\nVAE can be efficiently transformed into a VQ-VAE by controlling quantization\nnoise within the VAE's tolerance threshold. We present\n\\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs\nto enable rapid VQ-VAE training with minimal computational overhead. By\nintegrating \\textbf{channel multi-group quantization} to enlarge codebook\ncapacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ\ncompresses ImageNet images into at most 512 tokens while sustaining competitive\nreconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training\ncosts by over two orders of magnitude relative to state-of-the-art approaches:\nReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,\nwhereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental\nresults show that ReVQ achieves superior efficiency-reconstruction trade-offs.\n","authors":["Borui Zhang","Qihang Rao","Wenzhao Zheng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2507.10547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10546v1","updated":"2025-07-14T17:59:33Z","published":"2025-07-14T17:59:33Z","title":"Disentangling Neural Disjunctive Normal Form Models","summary":"  Neural Disjunctive Normal Form (DNF) based models are powerful and\ninterpretable approaches to neuro-symbolic learning and have shown promising\nresults in classification and reinforcement learning settings without prior\nknowledge of the tasks. However, their performance is degraded by the\nthresholding of the post-training symbolic translation process. We show here\nthat part of the performance degradation during translation is due to its\nfailure to disentangle the learned knowledge represented in the form of the\nnetworks' weights. We address this issue by proposing a new disentanglement\nmethod; by splitting nodes that encode nested rules into smaller independent\nnodes, we are able to better preserve the models' performance. Through\nexperiments on binary, multiclass, and multilabel classification tasks\n(including those requiring predicate invention), we demonstrate that our\ndisentanglement method provides compact and interpretable logical\nrepresentations for the neural DNF-based models, with performance closer to\nthat of their pre-translation counterparts. Our code is available at\nhttps://github.com/kittykg/disentangling-ndnf-classification.\n","authors":["Kexin Gu Baugh","Vincent Perreault","Matthew Baugh","Luke Dickens","Katsumi Inoue","Alessandra Russo"],"pdf_url":"https://arxiv.org/pdf/2507.10546v1.pdf","comment":"Accepted at NeSy 2025"},{"id":"http://arxiv.org/abs/2507.10540v1","updated":"2025-07-14T17:58:02Z","published":"2025-07-14T17:58:02Z","title":"Fusing LLM Capabilities with Routing Data","summary":"  The rapid advancement of large language models (LLMs) has created a vibrant\necosystem of diverse architectures, each with unique strengths due to\ndifferences in design, training data, and objectives. However, most\napplications still rely on a single backend model, limiting coverage of\ncapabilities and leading to inefficiencies in performance and token cost when\ntackling complex tasks. We highlight an underexploited opportunity: LLM routing\ndata, produced when hosting platforms route diverse queries to different\nmodels, which can reveal comparative strengths across tasks. To address this,\nwe propose FusionBench, a comprehensive routing benchmark covering 14 tasks\nacross five domains with 20 open-source LLMs (8B to 671B parameters), capturing\n103M tokens and summarizing reusable thought templates from top models.\nBuilding on this, we introduce FusionFactory, a systematic fusion framework\nwith three levels: (1) query-level fusion, tailoring routers for each query\nusing both direct responses and reasoning-augmented outputs; (2) thought-level\nfusion, leveraging abstract templates derived from top-performing LLMs' answers\nto similar queries; and (3) model-level fusion, transferring capabilities\nbetween models via distillation, using top responses or highest judge scores as\ntraining data. Experiments show FusionFactory consistently outperforms the best\nindividual LLM across all 14 benchmarks, with optimal fusion configurations\nvarying by benchmark, demonstrating the value of systematic LLM fusion in\nharnessing complementary strengths and improving overall performance.\n","authors":["Tao Feng","Haozhen Zhang","Zijie Lei","Pengrui Han","Mostofa Patwary","Mohammad Shoeybi","Bryan Catanzaro","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2507.10540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10539v1","updated":"2025-07-14T17:57:45Z","published":"2025-07-14T17:57:45Z","title":"Graph World Model","summary":"  World models (WMs) demonstrate strong capabilities in prediction, generation,\nand planning tasks. Existing WMs primarily focus on unstructured data and\ncannot leverage the ubiquitous structured data, often represented as graphs, in\nthe digital world. While multiple graph foundation models have been proposed,\nthey focus on graph learning tasks and cannot extend to diverse multi-modal\ndata and interdisciplinary tasks. To address these challenges, we propose the\nGraph World Model (GWM), a world model that supports both unstructured and\ngraph-structured states with multi-modal information and represents diverse\ntasks as actions. The core of a GWM is a generic message-passing algorithm to\naggregate structured information, either over a unified multi-modal token space\nby converting multi-modal data into text (GWM-T) or a unified multi-modal\nembedding space by modality-specific encoders (GWM-E). Notably, GWM introduces\naction nodes to support diverse tasks, where action nodes are linked to other\nnodes via direct reference or similarity computation. Extensive experiments on\nsix tasks from diverse domains, including multi-modal generation and matching,\nrecommendation, graph prediction, multi-agent, retrieval-augmented generation,\nand planning and optimization, show that the same GWM outperforms or matches\ndomain-specific baselines' performance, benefits from multi-hop structures, and\ndemonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our\ncode for GWM is released at https://github.com/ulab-uiuc/GWM.\n","authors":["Tao Feng","Yexin Wu","Guanyu Lin","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2507.10539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10536v1","updated":"2025-07-14T17:57:08Z","published":"2025-07-14T17:57:08Z","title":"On the Performance of Differentially Private Optimization with\n  Heavy-Tail Class Imbalance","summary":"  In this work, we analyze the optimization behaviour of common private\nlearning optimization algorithms under heavy-tail class imbalanced\ndistribution. We show that, in a stylized model, optimizing with Gradient\nDescent with differential privacy (DP-GD) suffers when learning low-frequency\nclasses, whereas optimization algorithms that estimate second-order information\ndo not. In particular, DP-AdamBC that removes the DP bias from estimating loss\ncurvature is a crucial component to avoid the ill-condition caused by\nheavy-tail class imbalance, and empirically fits the data better with\n$\\approx8\\%$ and $\\approx5\\%$ increase in training accuracy when learning the\nleast frequent classes on both controlled experiments and real data\nrespectively.\n","authors":["Qiaoyue Tang","Alain Zhiyanov","Mathias Lécuyer"],"pdf_url":"https://arxiv.org/pdf/2507.10536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10532v1","updated":"2025-07-14T17:55:15Z","published":"2025-07-14T17:55:15Z","title":"Reasoning or Memorization? Unreliable Results of Reinforcement Learning\n  Due to Data Contamination","summary":"  The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.\n","authors":["Mingqi Wu","Zhihao Zhang","Qiaole Dong","Zhiheng Xi","Jun Zhao","Senjie Jin","Xiaoran Fan","Yuhao Zhou","Yanwei Fu","Qin Liu","Songyang Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.10532v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2507.03152v2","updated":"2025-07-14T17:51:35Z","published":"2025-07-03T20:19:18Z","title":"Expert-level validation of AI-generated medical text with scalable\n  language models","summary":"  With the growing use of language models (LMs) in clinical environments, there\nis an immediate need to evaluate the accuracy and safety of LM-generated\nmedical text. Currently, such evaluation relies solely on manual physician\nreview. However, detecting errors in LM-generated text is challenging because\n1) manual review is costly and 2) expert-composed reference outputs are often\nunavailable in real-world settings. While the \"LM-as-judge\" paradigm (a LM\nevaluating another LM) offers scalable evaluation, even frontier LMs can miss\nsubtle but clinically significant errors. To address these challenges, we\npropose MedVAL, a self-supervised framework that leverages synthetic data to\ntrain evaluator LMs to assess whether LM-generated medical outputs are\nfactually consistent with inputs, without requiring physician labels or\nreference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a\ndataset containing 840 outputs annotated by physicians, following a\nphysician-defined taxonomy of risk levels and error categories. Across 6\ndiverse medical tasks and 10 state-of-the-art LMs spanning open-source,\nproprietary, and medically adapted models, MedVAL fine-tuning significantly\nimproves (p < 0.001) alignment with physicians on both seen and unseen tasks,\nincreasing average F1 scores from 66% to 83%, with per-sample safety\nclassification scores up to 86%. MedVAL improves the performance of even the\nbest-performing proprietary LM (GPT-4o) by 8%. To support a scalable,\nrisk-aware pathway towards clinical integration, we open-source the 1) codebase\n(https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench\n(https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), and 3) MedVAL-4B\n(https://huggingface.co/stanfordmimi/MedVAL-4B), the best-performing\nopen-source LM. Our research provides the first evidence of LMs approaching\nexpert-level validation ability for medical text.\n","authors":["Asad Aali","Vasiliki Bikia","Maya Varma","Nicole Chiou","Sophie Ostmeier","Arnav Singhvi","Magdalini Paschali","Ashwin Kumar","Andrew Johnston","Karimar Amador-Martinez","Eduardo Juan Perez Guerrero","Paola Naovi Cruz Rivera","Sergios Gatidis","Christian Bluethgen","Eduardo Pontes Reis","Eddy D. Zandee van Rilland","Poonam Laxmappa Hosamani","Kevin R Keet","Minjoung Go","Evelyn Ling","David B. Larson","Curtis Langlotz","Roxana Daneshjou","Jason Hom","Sanmi Koyejo","Emily Alsentzer","Akshay S. Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2507.03152v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10524v1","updated":"2025-07-14T17:49:00Z","published":"2025-07-14T17:49:00Z","title":"Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation","summary":"  Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.\n","authors":["Sangmin Bae","Yujin Kim","Reza Bayat","Sungnyun Kim","Jiyoun Ha","Tal Schuster","Adam Fisch","Hrayr Harutyunyan","Ziwei Ji","Aaron Courville","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2507.10524v1.pdf","comment":"36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions"},{"id":"http://arxiv.org/abs/2506.21628v2","updated":"2025-07-14T17:46:29Z","published":"2025-06-24T20:23:39Z","title":"Ark: An Open-source Python-based Framework for Robot Learning","summary":"  Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.\n","authors":["Magnus Dierking","Christopher E. Mower","Sarthak Das","Huang Helong","Jiacheng Qiu","Cody Reading","Wei Chen","Huidong Liang","Huang Guowei","Jan Peters","Quan Xingyue","Jun Wang","Haitham Bou-Ammar"],"pdf_url":"https://arxiv.org/pdf/2506.21628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00684v2","updated":"2025-07-14T17:45:50Z","published":"2025-05-01T17:45:59Z","title":"Visual Test-time Scaling for GUI Agent Grounding","summary":"  We introduce RegionFocus, a visual test-time scaling approach for Vision\nLanguage Model Agents. Understanding webpages is challenging due to the visual\ncomplexity of GUI images and the large number of interface elements, making\naccurate action selection difficult. Our approach dynamically zooms in on\nrelevant regions, reducing background clutter and improving grounding accuracy.\nTo support this process, we propose an image-as-map mechanism that visualizes\nkey landmarks at each step, providing a transparent action record and enables\nthe agent to effectively choose among action candidates. Even with a simple\nregion selection strategy, we observe significant performance gains of 28+\\% on\nScreenspot-pro and 24+\\% on WebVoyager benchmarks on top of two\nstate-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL,\nhighlighting the effectiveness of visual test-time scaling in interactive\nsettings. We achieve a new state-of-the-art grounding performance of 61.6\\% on\nthe ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model.\nOur code will be released publicly at https://github.com/tiangeluo/RegionFocus.\n","authors":["Tiange Luo","Lajanugen Logeswaran","Justin Johnson","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2505.00684v2.pdf","comment":"ICCV2025, https://github.com/tiangeluo/RegionFocus"},{"id":"http://arxiv.org/abs/2409.18209v2","updated":"2025-07-14T17:36:23Z","published":"2024-09-26T18:41:19Z","title":"A Unified View on Learning Unnormalized Distributions via\n  Noise-Contrastive Estimation","summary":"  This paper studies a family of estimators based on noise-contrastive\nestimation (NCE) for learning unnormalized distributions. The main contribution\nof this work is to provide a unified perspective on various methods for\nlearning unnormalized distributions, which have been independently proposed and\nstudied in separate research communities, through the lens of NCE. This unified\nview offers new insights into existing estimators. Specifically, for\nexponential families, we establish the finite-sample convergence rates of the\nproposed estimators under a set of regularity assumptions, most of which are\nnew.\n","authors":["J. Jon Ryu","Abhin Shah","Gregory W. Wornell"],"pdf_url":"https://arxiv.org/pdf/2409.18209v2.pdf","comment":"31 pages, 2 figures. ICML 2025"},{"id":"http://arxiv.org/abs/2502.10826v2","updated":"2025-07-14T17:33:46Z","published":"2025-02-15T15:01:16Z","title":"Improved Offline Contextual Bandits with Second-Order Bounds: Betting\n  and Freezing","summary":"  We consider off-policy selection and learning in contextual bandits, where\nthe learner aims to select or train a reward-maximizing policy using data\ncollected by a fixed behavior policy. Our contribution is two-fold. First, we\npropose a novel off-policy selection method that leverages a new betting-based\nconfidence bound applied to an inverse propensity weight sequence. Our\ntheoretical analysis reveals that this method achieves a significantly\nimproved, variance-adaptive guarantee over prior work. Second, we propose a\nnovel and generic condition on the optimization objective for off-policy\nlearning that strikes a different balance between bias and variance. One\nspecial case, which we call freezing, tends to induce low variance, which is\npreferred in small-data regimes. Our analysis shows that it matches the best\nexisting guarantees. In our empirical study, our selection method outperforms\nexisting methods, and freezing exhibits improved performance in small-sample\nregimes.\n","authors":["J. Jon Ryu","Jeongyeol Kwon","Benjamin Koppe","Kwang-Sung Jun"],"pdf_url":"https://arxiv.org/pdf/2502.10826v2.pdf","comment":"39 pages, 10 figures. COLT 2025"},{"id":"http://arxiv.org/abs/2507.10502v1","updated":"2025-07-14T17:25:28Z","published":"2025-07-14T17:25:28Z","title":"Benchmarking and Evaluation of AI Models in Biology: Outcomes and\n  Recommendations from the CZI Virtual Cells Workshop","summary":"  Artificial intelligence holds immense promise for transforming biology, yet a\nlack of standardized, cross domain, benchmarks undermines our ability to build\nrobust, trustworthy models. Here, we present insights from a recent workshop\nthat convened machine learning and computational biology experts across\nimaging, transcriptomics, proteomics, and genomics to tackle this gap. We\nidentify major technical and systemic bottlenecks such as data heterogeneity\nand noise, reproducibility challenges, biases, and the fragmented ecosystem of\npublicly available resources and propose a set of recommendations for building\nbenchmarking frameworks that can efficiently compare ML models of biological\nsystems across tasks and data modalities. By promoting high quality data\ncuration, standardized tooling, comprehensive evaluation metrics, and open,\ncollaborative platforms, we aim to accelerate the development of robust\nbenchmarks for AI driven Virtual Cells. These benchmarks are crucial for\nensuring rigor, reproducibility, and biological relevance, and will ultimately\nadvance the field toward integrated models that drive new discoveries,\ntherapeutic insights, and a deeper understanding of cellular systems.\n","authors":["Elizabeth Fahsbender","Alma Andersson","Jeremy Ash","Polina Binder","Daniel Burkhardt","Benjamin Chang","Georg K. Gerber","Anthony Gitter","Patrick Godau","Ankit Gupta","Genevieve Haliburton","Siyu He","Trey Ideker","Ivana Jelic","Aly Khan","Yang-Joon Kim","Aditi Krishnapriyan","Jon M. Laurent","Tianyu Liu 28","Emma Lundberg","Shalin B. Mehta","Rob Moccia","Angela Oliveira Pisco","Katherine S. Pollard","Suresh Ramani","Julio Saez-Rodriguez","Yasin Senbabaoglu","Elana Simon","Srinivasan Sivanandan","Gustavo Stolovitzky","Marc Valer","Bo Wang","Xikun Zhang","James Zou","Katrina Kalantar"],"pdf_url":"https://arxiv.org/pdf/2507.10502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10499v1","updated":"2025-07-14T17:23:43Z","published":"2025-07-14T17:23:43Z","title":"National level satellite-based crop field inventories in smallholder\n  landscapes","summary":"  The design of science-based policies to improve the sustainability of\nsmallholder agriculture is challenged by a limited understanding of fundamental\nsystem properties, such as the spatial distribution of active cropland and\nfield size. We integrate very high spatial resolution (1.5 m) Earth observation\ndata and deep transfer learning to derive crop field delineations in complex\nagricultural systems at the national scale, while maintaining minimum reference\ndata requirements and enhancing transferability. We provide the first\nnational-level dataset of 21 million individual fields for Mozambique (covering\n~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural\nland use with an overall accuracy of 93% and balanced omission and commission\nerrors. Field-level spatial agreement reached median intersection over union\n(IoU) scores of 0.81, advancing the state-of-the-art in large-area field\ndelineation in complex smallholder systems. The active cropland maps capture\nfragmented rural regions with low cropland shares not yet identified in global\nland cover or cropland maps. These regions are mostly located in agricultural\nfrontier regions which host 7-9% of the Mozambican population. Field size in\nMozambique is very low overall, with half of the fields being smaller than 0.16\nha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial\nresolution (0.05{\\deg}) is 0.32 ha, but it varies strongly across gradients of\naccessibility, population density, and net forest cover change. This variation\nreflects a diverse set of actors, ranging from semi-subsistence smallholder\nfarms to medium-scale commercial farming, and large-scale farming operations.\nOur results highlight that field size is a key indicator relating to\nsocio-economic and environmental outcomes of agriculture (e.g., food\nproduction, livelihoods, deforestation, biodiversity), as well as their\ntrade-offs.\n","authors":["Philippe Rufin","Pauline Lucie Hammer","Leon-Friedrich Thomas","Sá Nogueira Lisboa","Natasha Ribeiro","Almeida Sitoe","Patrick Hostert","Patrick Meyfroidt"],"pdf_url":"https://arxiv.org/pdf/2507.10499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10494v1","updated":"2025-07-14T17:18:07Z","published":"2025-07-14T17:18:07Z","title":"Split Happens: Combating Advanced Threats with Split Learning and\n  Function Secret Sharing","summary":"  Split Learning (SL) -- splits a model into two distinct parts to help protect\nclient data while enhancing Machine Learning (ML) processes. Though promising,\nSL has proven vulnerable to different attacks, thus raising concerns about how\neffective it may be in terms of data privacy. Recent works have shown promising\nresults for securing SL through the use of a novel paradigm, named Function\nSecret Sharing (FSS), in which servers obtain shares of a function they compute\nand operate on a public input hidden with a random mask. However, these works\nfall short in addressing the rising number of attacks which exist on SL. In\nSplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly\nto other works, we are able to make use of the benefits of SL by reducing the\ncommunication and computational costs of FSS. However, a U-shaped SL provides a\nhigher security guarantee than previous works, allowing a client to keep the\nlabels of the training data secret, without having to share them with the\nserver. Through this, we are able to generalize the security analysis of\nprevious works and expand it to different attack vectors, such as modern model\ninversion attacks as well as label inference attacks. We tested our approach\nfor two different convolutional neural networks on different datasets. These\nexperiments show the effectiveness of our approach in reducing the training\ntime as well as the communication costs when compared to simply using FSS while\nmatching prior accuracy.\n","authors":["Tanveer Khan","Mindaugas Budzys","Antonis Michalas"],"pdf_url":"https://arxiv.org/pdf/2507.10494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14836v2","updated":"2025-07-14T17:14:45Z","published":"2025-03-19T02:35:01Z","title":"On the Robustness Tradeoff in Fine-Tuning","summary":"  Fine-tuning has become the standard practice for adapting pre-trained models\nto downstream tasks. However, the impact on model robustness is not well\nunderstood. In this work, we characterize the robustness-accuracy trade-off in\nfine-tuning. We evaluate the robustness and accuracy of fine-tuned models over\n6 benchmark datasets and 7 different fine-tuning strategies. We observe a\nconsistent trade-off between adversarial robustness and accuracy. Peripheral\nupdates such as BitFit are more effective for simple tasks -- over 75% above\nthe average measured by the area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex tasks\n-- 57.5% and 34.6% above the average on Caltech-256 and CUB-200, respectively.\nLastly, we observe that the robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments.\n","authors":["Kunyang Li","Jean-Charles Noirot Ferrand","Ryan Sheatsley","Blaine Hoak","Yohan Beugin","Eric Pauley","Patrick McDaniel"],"pdf_url":"https://arxiv.org/pdf/2503.14836v2.pdf","comment":"Accepted to International Conference on Computer Vision, ICCV 2025"},{"id":"http://arxiv.org/abs/2507.10492v1","updated":"2025-07-14T17:13:08Z","published":"2025-07-14T17:13:08Z","title":"BenchReAD: A systematic benchmark for retinal anomaly detection","summary":"  Retinal anomaly detection plays a pivotal role in screening ocular and\nsystemic diseases. Despite its significance, progress in the field has been\nhindered by the absence of a comprehensive and publicly available benchmark,\nwhich is essential for the fair evaluation and advancement of methodologies.\nDue to this limitation, previous anomaly detection work related to retinal\nimages has been constrained by (1) a limited and overly simplistic set of\nanomaly types, (2) test sets that are nearly saturated, and (3) a lack of\ngeneralization evaluation, resulting in less convincing experimental setups.\nFurthermore, existing benchmarks in medical anomaly detection predominantly\nfocus on one-class supervised approaches (training only with negative samples),\noverlooking the vast amounts of labeled abnormal data and unlabeled data that\nare commonly available in clinical practice. To bridge these gaps, we introduce\na benchmark for retinal anomaly detection, which is comprehensive and\nsystematic in terms of data and algorithm. Through categorizing and\nbenchmarking previous methods, we find that a fully supervised approach\nleveraging disentangled representations of abnormalities (DRA) achieves the\nbest performance but suffers from significant drops in performance when\nencountering certain unseen anomalies. Inspired by the memory bank mechanisms\nin one-class supervised learning, we propose NFM-DRA, which integrates DRA with\na Normal Feature Memory to mitigate the performance degradation, establishing a\nnew SOTA. The benchmark is publicly available at\nhttps://github.com/DopamineLcy/BenchReAD.\n","authors":["Chenyu Lian","Hong-Yu Zhou","Zhanli Hu","Jing Qin"],"pdf_url":"https://arxiv.org/pdf/2507.10492v1.pdf","comment":"MICCAI 2025"},{"id":"http://arxiv.org/abs/2410.09135v2","updated":"2025-07-14T17:09:39Z","published":"2024-10-11T16:13:01Z","title":"Enabling Advanced Land Cover Analytics: An Integrated Data Extraction\n  Pipeline for Predictive Modeling with the Dynamic World Dataset","summary":"  Understanding land cover holds considerable potential for a myriad of\npractical applications, particularly as data accessibility transitions from\nbeing exclusive to governmental and commercial entities to now including the\nbroader research community. Nevertheless, although the data is accessible to\nany community member interested in exploration, there exists a formidable\nlearning curve and no standardized process for accessing, pre-processing, and\nleveraging the data for subsequent tasks. In this study, we democratize this\ndata by presenting a flexible and efficient end to end pipeline for working\nwith the Dynamic World dataset, a cutting-edge near-real-time land use/land\ncover (LULC) dataset. This includes a pre-processing and representation\nframework which tackles noise removal, efficient extraction of large amounts of\ndata, and re-representation of LULC data in a format well suited for several\ndownstream tasks. To demonstrate the power of our pipeline, we use it to\nextract data for an urbanization prediction problem and build a suite of\nmachine learning models with excellent performance. This task is easily\ngeneralizable to the prediction of any type of land cover and our pipeline is\nalso compatible with a series of other downstream tasks.\n","authors":["Victor Radermecker","Andrea Zanon","Nancy Thomas","Annita Vapsi","Saba Rahimi","Rama Ramakrishnan","Daniel Borrajo"],"pdf_url":"https://arxiv.org/pdf/2410.09135v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10485v1","updated":"2025-07-14T17:04:05Z","published":"2025-07-14T17:04:05Z","title":"Overcoming catastrophic forgetting in neural networks","summary":"  Catastrophic forgetting is the primary challenge that hinders continual\nlearning, which refers to a neural network ability to sequentially learn\nmultiple tasks while retaining previously acquired knowledge. Elastic Weight\nConsolidation, a regularization-based approach inspired by synaptic\nconsolidation in biological neural systems, has been used to overcome this\nproblem. In this study prior research is replicated and extended by evaluating\nEWC in supervised learning settings using the PermutedMNIST and RotatedMNIST\nbenchmarks. Through systematic comparisons with L2 regularization and\nstochastic gradient descent (SGD) without regularization, we analyze how\ndifferent approaches balance knowledge retention and adaptability. Our results\nconfirm what was shown in previous research, showing that EWC significantly\nreduces forgetting compared to naive training while slightly compromising\nlearning efficiency on new tasks. Moreover, we investigate the impact of\ndropout regularization and varying hyperparameters, offering insights into the\ngeneralization of EWC across diverse learning scenarios. These results\nunderscore EWC's potential as a viable solution for lifelong learning in neural\nnetworks.\n","authors":["Brandon Shuen Yi Loke","Filippo Quadri","Gabriel Vivanco","Maximilian Casagrande","Saúl Fenollosa"],"pdf_url":"https://arxiv.org/pdf/2507.10485v1.pdf","comment":"7 pages, 5 figures, EE-411 Fundamentals of inference and learning\n  course project"},{"id":"http://arxiv.org/abs/2507.10484v1","updated":"2025-07-14T17:04:03Z","published":"2025-07-14T17:04:03Z","title":"The Target Polish: A New Approach to Outlier-Resistant Non-Negative\n  Matrix and Tensor Factorization","summary":"  This paper introduces the \"Target Polish,\" a robust and computationally\nefficient framework for nonnegative matrix and tensor factorization. Although\nconventional weighted NMF approaches are resistant to outliers, they converge\nslowly due to the use of multiplicative updates to minimize the objective\ncriterion. In contrast, the Target Polish approach remains compatible with the\nFast-HALS algorithm, which is renowned for its speed, by adaptively smoothing\nthe data with a weighted median-based transformation. This innovation provides\noutlier resistance while maintaining the highly efficient additive update\nstructure of Fast-HALS. Empirical evaluations using image datasets corrupted\nwith structured (block) and unstructured (salt) noise demonstrate that the\nTarget Polish approach matches or exceeds the accuracy of state-of-the-art\nrobust NMF methods and reduces computational time by an order of magnitude in\nthe studied scenarios.\n","authors":["Paul Fogel","Christophe Geissler","George Luta"],"pdf_url":"https://arxiv.org/pdf/2507.10484v1.pdf","comment":"6 pages, 4 figures, International Conference on Robust Statistics\n  2025, Stresa, Italy"},{"id":"http://arxiv.org/abs/2409.01062v2","updated":"2025-07-14T16:55:10Z","published":"2024-09-02T08:37:17Z","title":"Random Erasing vs. Model Inversion: A Promising Defense or a False Hope?","summary":"  Model Inversion (MI) attacks pose a significant privacy threat by\nreconstructing private training data from machine learning models. While\nexisting defenses primarily concentrate on model-centric approaches, the impact\nof data on MI robustness remains largely unexplored. In this work, we explore\nRandom Erasing (RE), a technique traditionally used for improving model\ngeneralization under occlusion, and uncover its surprising effectiveness as a\ndefense against MI attacks. Specifically, our novel feature space analysis\nshows that models trained with RE-images introduce a significant discrepancy\nbetween the features of MI-reconstructed images and those of the private data.\nAt the same time, features of private images remain distinct from other classes\nand well-separated from different classification regions. These effects\ncollectively degrade MI reconstruction quality and attack accuracy while\nmaintaining reasonable natural accuracy. Furthermore, we explore two critical\nproperties of RE including Partial Erasure and Random Location. Partial Erasure\nprevents the model from observing entire objects during training. We find this\nhas a significant impact on MI, which aims to reconstruct the entire objects.\nRandom Location of erasure plays a crucial role in achieving a strong\nprivacy-utility trade-off. Our findings highlight RE as a simple yet effective\ndefense mechanism that can be easily integrated with existing\nprivacy-preserving techniques. Extensive experiments across 37 setups\ndemonstrate that our method achieves state-of-the-art (SOTA) performance in the\nprivacy-utility trade-off. The results consistently demonstrate the superiority\nof our defense over existing methods across different MI attacks, network\narchitectures, and attack configurations. For the first time, we achieve a\nsignificant degradation in attack accuracy without a decrease in utility for\nsome configurations.\n","authors":["Viet-Hung Tran","Ngoc-Bao Nguyen","Son T. Mai","Hans Vandierendonck","Ira Assent","Alex Kot","Ngai-Man Cheung"],"pdf_url":"https://arxiv.org/pdf/2409.01062v2.pdf","comment":"Accepted in Transactions on Machine Learning Research (TMLR). First\n  two authors contributed equally"},{"id":"http://arxiv.org/abs/2507.10468v1","updated":"2025-07-14T16:46:30Z","published":"2025-07-14T16:46:30Z","title":"From BERT to Qwen: Hate Detection across architectures","summary":"  Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).\n","authors":["Ariadna Mon","Saúl Fenollosa","Jon Lecumberri"],"pdf_url":"https://arxiv.org/pdf/2507.10468v1.pdf","comment":"4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)"},{"id":"http://arxiv.org/abs/2412.10454v2","updated":"2025-07-14T16:40:57Z","published":"2024-12-12T07:25:37Z","title":"An Interoperable Machine Learning Pipeline for Pediatric Obesity Risk\n  Estimation","summary":"  Reliable prediction of pediatric obesity can offer a valuable resource to\nproviders, helping them engage in timely preventive interventions before the\ndisease is established. Many efforts have been made to develop ML-based\npredictive models of obesity, and some studies have reported high predictive\nperformances. However, no commonly used clinical decision support tool based on\nexisting ML models currently exists. This study presents a novel end-to-end\npipeline specifically designed for pediatric obesity prediction, which supports\nthe entire process of data extraction, inference, and communication via an API\nor a user interface. While focusing only on routinely recorded data in\npediatric electronic health records (EHRs), our pipeline uses a diverse\nexpert-curated list of medical concepts to predict the 1-3 years risk of\ndeveloping obesity. Furthermore, by using the Fast Healthcare Interoperability\nResources (FHIR) standard in our design procedure, we specifically target\nfacilitating low-effort integration of our pipeline with different EHR systems.\nIn our experiments, we report the effectiveness of the predictive model as well\nas its alignment with the feedback from various stakeholders, including ML\nscientists, providers, health IT personnel, health administration\nrepresentatives, and patient group representatives.\n","authors":["Hamed Fayyaz","Mehak Gupta","Alejandra Perez Ramirez","Claudine Jurkovitz","H. Timothy Bunnell","Thao-Ly T. Phan","Rahmatollah Beheshti"],"pdf_url":"https://arxiv.org/pdf/2412.10454v2.pdf","comment":"This paper has been accepted in Machine Learning for Health (ML4H)\n  Symposium. Link: https://proceedings.mlr.press/v259/fayyaz25a.html"},{"id":"http://arxiv.org/abs/2504.11775v2","updated":"2025-07-14T16:40:20Z","published":"2025-04-16T05:29:11Z","title":"Discrimination-free Insurance Pricing with Privatized Sensitive\n  Attributes","summary":"  Fairness has emerged as a critical consideration in the landscape of machine\nlearning algorithms, particularly as AI continues to transform decision-making\nacross societal domains. To ensure that these algorithms are free from bias and\ndo not discriminate against individuals based on sensitive attributes such as\ngender and race, the field of algorithmic bias has introduced various fairness\nconcepts, along with methodologies to achieve these notions in different\ncontexts. Despite the rapid advancement, not all sectors have embraced these\nfairness principles to the same extent. One specific sector that merits\nattention in this regard is insurance. Within the realm of insurance pricing,\nfairness is defined through a distinct and specialized framework. Consequently,\nachieving fairness according to established notions does not automatically\nensure fair pricing in insurance. In particular, regulators are increasingly\nemphasizing transparency in pricing algorithms and imposing constraints on\ninsurance companies on the collection and utilization of sensitive consumer\nattributes. These factors present additional challenges in the implementation\nof fairness in pricing algorithms. To address these complexities and comply\nwith regulatory demands, we propose an efficient method for constructing fair\nmodels that are tailored to the insurance domain, using only privatized\nsensitive attributes. Notably, our approach ensures statistical guarantees,\ndoes not require direct access to sensitive attributes, and adapts to varying\ntransparency requirements, addressing regulatory demands while ensuring\nfairness in insurance pricing.\n","authors":["Tianhe Zhang","Suhan Liu","Peng Shi"],"pdf_url":"https://arxiv.org/pdf/2504.11775v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10461v1","updated":"2025-07-14T16:39:14Z","published":"2025-07-14T16:39:14Z","title":"RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for\n  Pansharpening","summary":"  Pansharpening refers to the process of integrating a high resolution\npanchromatic (PAN) image with a lower resolution multispectral (MS) image to\ngenerate a fused product, which is pivotal in remote sensing. Despite the\neffectiveness of CNNs in addressing this challenge, they are inherently\nconstrained by the uniform application of convolutional kernels across all\nspatial positions, overlooking local content variations. To overcome this\nissue, we introduce RAPNet, a new architecture that leverages content-adaptive\nconvolution. At its core, RAPNet employs the Receptive-field Adaptive\nPansharpening Convolution (RAPConv), designed to produce spatially adaptive\nkernels responsive to local feature context, thereby enhancing the precision of\nspatial detail extraction. Additionally, the network integrates the\nPansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an\nattention mechanism to achieve an optimal balance between spatial detail\nenhancement and spectral fidelity. Comprehensive evaluations on publicly\navailable datasets confirm that RAPNet delivers superior performance compared\nto existing approaches, as demonstrated by both quantitative metrics and\nqualitative assessments. Ablation analyses further substantiate the\neffectiveness of the proposed adaptive components.\n","authors":["Tao Tang","Chengxu Yang"],"pdf_url":"https://arxiv.org/pdf/2507.10461v1.pdf","comment":"To appear in the proceedings of the 6th International Conference on\n  Artificial Intelligence and Electromechanical Automation (AIEA 2025). 5\n  pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.07614v2","updated":"2025-07-14T16:39:03Z","published":"2025-06-09T10:27:15Z","title":"Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong\n  Error Lower Bounds","summary":"  We study the problem of sampling from strongly log-concave distributions over\n$\\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the\nrandomized midpoint method) for overdamped/underdamped Langevin dynamics. We\nprove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic\nspeedup in dependence on the target accuracy ($\\epsilon$) over the\nEuler-Maruyama discretization, surpassing existing bounds for randomized\nmidpoint methods. Notably, in the case of underdamped Langevin dynamics, we\ndemonstrate the complexity of $W_2$ convergence is much smaller than the\ncomplexity lower bounds for convergence in $L^2$ strong error established in\nthe literature.\n","authors":["Rishikesh Srinivasan","Dheeraj Nagaraj"],"pdf_url":"https://arxiv.org/pdf/2506.07614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10457v1","updated":"2025-07-14T16:37:05Z","published":"2025-07-14T16:37:05Z","title":"Logic layer Prompt Control Injection (LPCI): A Novel Security\n  Vulnerability Class in Agentic Systems","summary":"  The integration of large language models (LLMs) into enterprise systems has\ncreated a new class of covert security vulnerabilities, particularly within\nlogic-execution layers and persistent-memory contexts. In this paper, we\nintroduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category\nin which encoded, delayed, and conditionally triggered payloads are embedded in\nmemory, vector stores, or tool outputs. These payloads can bypass conventional\ninput filters and trigger unauthorised behaviour across sessions.\n","authors":["Hammad Atta","Ken Huang","Manish Bhatt","Kamal Ahmed","Muhammad Aziz Ul Haq","Yasir Mehmood"],"pdf_url":"https://arxiv.org/pdf/2507.10457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15266v3","updated":"2025-07-14T16:34:31Z","published":"2025-04-21T17:47:46Z","title":"Roll the dice & look before you leap: Going beyond the creative limits\n  of next-token prediction","summary":"  We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic; multi-token approaches,\nnamely teacherless training and diffusion models, comparatively excel in\nproducing diverse and original output. Secondly, to elicit randomness without\nhurting coherence, we find that injecting noise at the input layer (dubbed\nseed-conditioning) works surprisingly as well as (and in some conditions,\nbetter than) temperature sampling from the output layer. Thus, our work offers\na principled, minimal test-bed for analyzing open-ended creative skills, and\noffers new arguments for going beyond next-token learning and temperature\nsampling. We make part of the code available under\nhttps://github.com/chenwu98/algorithmic-creativity\n","authors":["Vaishnavh Nagarajan","Chen Henry Wu","Charles Ding","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2504.15266v3.pdf","comment":"ICML 2025 (oral)"},{"id":"http://arxiv.org/abs/2507.10452v1","updated":"2025-07-14T16:31:06Z","published":"2025-07-14T16:31:06Z","title":"Some remarks on gradient dominance and LQR policy optimization","summary":"  Solutions of optimization problems, including policy optimization in\nreinforcement learning, typically rely upon some variant of gradient descent.\nThere has been much recent work in the machine learning, control, and\noptimization communities applying the Polyak-{\\L}ojasiewicz Inequality (PLI) to\nsuch problems in order to establish an exponential rate of convergence (a.k.a.\n``linear convergence'' in the local-iteration language of numerical analysis)\nof loss functions to their minima under the gradient flow. Often, as is the\ncase of policy iteration for the continuous-time LQR problem, this rate\nvanishes for large initial conditions, resulting in a mixed globally linear /\nlocally exponential behavior. This is in sharp contrast with the discrete-time\nLQR problem, where there is global exponential convergence. That gap between CT\nand DT behaviors motivates the search for various generalized PLI-like\nconditions, and this talk will address that topic. Moreover, these\ngeneralizations are key to understanding the transient and asymptotic effects\nof errors in the estimation of the gradient, errors which might arise from\nadversarial attacks, wrong evaluation by an oracle, early stopping of a\nsimulation, inaccurate and very approximate digital twins, stochastic\ncomputations (algorithm ``reproducibility''), or learning by sampling from\nlimited data. We describe an ``input to state stability'' (ISS) analysis of\nthis issue. The lecture also discussed convergence and PLI-like properties of\n``linear feedforward neural networks'' in feedback control, but this arXiv\nskips that part (to be updated). Much of the work described here was done in\ncollaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping\nJiang, and Milad Siami.\n","authors":["Eduardo D. Sontag"],"pdf_url":"https://arxiv.org/pdf/2507.10452v1.pdf","comment":"This is a short paper summarizing the first part of the slides\n  presented at my keynote at the 2025 L4DC (Learning for Dynamics & Control\n  Conference) in Ann Arbor, Michigan, 05 June 2025. A partial bibliography has\n  been added. A second part on neural net feedback controllers is to be added"},{"id":"http://arxiv.org/abs/2507.10434v1","updated":"2025-07-14T16:23:39Z","published":"2025-07-14T16:23:39Z","title":"CLA: Latent Alignment for Online Continual Self-Supervised Learning","summary":"  Self-supervised learning (SSL) is able to build latent representations that\ngeneralize well to unseen data. However, only a few SSL techniques exist for\nthe online CL setting, where data arrives in small minibatches, the model must\ncomply with a fixed computational budget, and task boundaries are absent. We\nintroduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL\nthat aligns the representations learned by the current model with past\nrepresentations to mitigate forgetting. We found that our CLA is able to speed\nup the convergence of the training process in the online scenario,\noutperforming state-of-the-art approaches under the same computational budget.\nSurprisingly, we also discovered that using CLA as a pretraining protocol in\nthe early stages of pretraining leads to a better final performance when\ncompared to a full i.i.d. pretraining.\n","authors":["Giacomo Cignoni","Andrea Cossu","Alexandra Gomez-Villa","Joost van de Weijer","Antonio Carta"],"pdf_url":"https://arxiv.org/pdf/2507.10434v1.pdf","comment":"Accepted at CoLLAs 2025 conference"},{"id":"http://arxiv.org/abs/2507.10430v1","updated":"2025-07-14T16:19:00Z","published":"2025-07-14T16:19:00Z","title":"Efficient Federated Learning with Heterogeneous Data and Adaptive\n  Dropout","summary":"  Federated Learning (FL) is a promising distributed machine learning approach\nthat enables collaborative training of a global model using multiple edge\ndevices. The data distributed among the edge devices is highly heterogeneous.\nThus, FL faces the challenge of data distribution and heterogeneity, where\nnon-Independent and Identically Distributed (non-IID) data across edge devices\nmay yield in significant accuracy drop. Furthermore, the limited computation\nand communication capabilities of edge devices increase the likelihood of\nstragglers, thus leading to slow model convergence. In this paper, we propose\nthe FedDHAD FL framework, which comes with two novel methods: Dynamic\nHeterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH\ndynamically adjusts the weights of each local model within the model\naggregation process based on the non-IID degree of heterogeneous data to deal\nwith the statistical data heterogeneity. FedAD performs neuron-adaptive\noperations in response to heterogeneous devices to improve accuracy while\nachieving superb efficiency. The combination of these two methods makes FedDHAD\nsignificantly outperform state-of-the-art solutions in terms of accuracy (up to\n6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to\n15.0% smaller).\n","authors":["Ji Liu","Beichen Ma","Yang Zhou","Jingbo Zhou","Ruoming Jin","Dejing Dou","Huaiyu Dai","Haixun Wang","Patrick Valduriez"],"pdf_url":"https://arxiv.org/pdf/2507.10430v1.pdf","comment":"29 pages, to appear in ACM Transactions on Knowledge Discovery from\n  Data (TKDD)"},{"id":"http://arxiv.org/abs/2507.10425v1","updated":"2025-07-14T16:10:55Z","published":"2025-07-14T16:10:55Z","title":"Non-exchangeable Conformal Prediction with Optimal Transport: Tackling\n  Distribution Shifts with Unlabeled Data","summary":"  Conformal prediction is a distribution-free uncertainty quantification method\nthat has gained popularity in the machine learning community due to its\nfinite-sample guarantees and ease of use. Its most common variant, dubbed split\nconformal prediction, is also computationally efficient as it boils down to\ncollecting statistics of the model predictions on some calibration data not yet\nseen by the model. Nonetheless, these guarantees only hold if the calibration\nand test data are exchangeable, a condition that is difficult to verify and\noften violated in practice due to so-called distribution shifts. The literature\nis rife with methods to mitigate the loss in coverage in this non-exchangeable\nsetting, but these methods require some prior information on the type of\ndistribution shift to be expected at test time. In this work, we study this\nproblem via a new perspective, through the lens of optimal transport, and show\nthat it is possible to estimate the loss in coverage and mitigate it in case of\ndistribution shift.\n","authors":["Alvaro H. C. Correia","Christos Louizos"],"pdf_url":"https://arxiv.org/pdf/2507.10425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10421v1","updated":"2025-07-14T16:04:34Z","published":"2025-07-14T16:04:34Z","title":"SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout\n  in Distance Learning","summary":"  School dropout is a serious problem in distance learning, where early\ndetection is crucial for effective intervention and student perseverance.\nPredicting student dropout using available educational data is a widely\nresearched topic in learning analytics. Our partner's distance learning\nplatform highlights the importance of integrating diverse data sources,\nincluding socio-demographic data, behavioral data, and sentiment analysis, to\naccurately predict dropout risks. In this paper, we introduce a novel model\nthat combines sentiment analysis of student comments using the Bidirectional\nEncoder Representations from Transformers (BERT) model with socio-demographic\nand behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We\nfine-tuned BERT on student comments to capture nuanced sentiments, which were\nthen merged with key features selected using feature importance techniques in\nXGBoost. Our model was tested on unseen data from the next academic year,\nachieving an accuracy of 84\\%, compared to 82\\% for the baseline model.\nAdditionally, the model demonstrated superior performance in other metrics,\nsuch as precision and F1-score. The proposed method could be a vital tool in\ndeveloping personalized strategies to reduce dropout rates and encourage\nstudent perseverance\n","authors":["Meriem Zerkouk","Miloud Mihoubi","Belkacem Chikhaoui"],"pdf_url":"https://arxiv.org/pdf/2507.10421v1.pdf","comment":"International Conference on Education and New Learning Technologies\n  (2025)"},{"id":"http://arxiv.org/abs/2507.10419v1","updated":"2025-07-14T16:00:51Z","published":"2025-07-14T16:00:51Z","title":"Multiple Choice Learning of Low Rank Adapters for Language Modeling","summary":"  We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs.\n","authors":["Victor Letzelter","Hugo Malard","Mathieu Fontaine","Gaël Richard","Slim Essid","Andrei Bursuc","Patrick Pérez"],"pdf_url":"https://arxiv.org/pdf/2507.10419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10409v1","updated":"2025-07-14T15:54:06Z","published":"2025-07-14T15:54:06Z","title":"Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study","summary":"  This study addresses the challenge of balancing energy efficiency with\nperformance in AI/ML models, focusing on DeepRX, a deep learning receiver based\non a fully convolutional ResNet architecture. We evaluate the energy\nconsumption of DeepRX, considering factors including FLOPs/Watt and\nFLOPs/clock, and find consistency between estimated and actual energy usage,\ninfluenced by memory access patterns. The research extends to comparing energy\ndynamics during training and inference phases. A key contribution is the\napplication of knowledge distillation (KD) to train a compact DeepRX\n\\textit{student} model that emulates the performance of the \\textit{teacher}\nmodel but with reduced energy consumption. We experiment with different student\nmodel sizes, optimal teacher sizes, and KD hyperparameters. Performance is\nmeasured by comparing the Bit Error Rate (BER) performance versus\nSignal-to-Interference \\& Noise Ratio (SINR) values of the distilled model and\na model trained from scratch. The distilled models demonstrate a lower error\nfloor across SINR levels, highlighting the effectiveness of KD in achieving\nenergy-efficient AI solutions.\n","authors":["Amine Lbath","Ibtissam Labriji"],"pdf_url":"https://arxiv.org/pdf/2507.10409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10400v1","updated":"2025-07-14T15:43:59Z","published":"2025-07-14T15:43:59Z","title":"Anticipating the Selectivity of Cyclization Reaction Pathways with\n  Neural Network Potentials","summary":"  Reaction mechanism search tools have demonstrated the ability to provide\ninsights into likely products and rate-limiting steps of reacting systems.\nHowever, reactions involving several concerted bond changes - as can be found\nin many key steps of natural product synthesis - can complicate the search\nprocess. To mitigate these complications, we present a mechanism search\nstrategy particularly suited to help expedite exploration of an exemplary\nfamily of such complex reactions, cyclizations. We provide a cost-effective\nstrategy for identifying relevant elementary reaction steps by combining\ngraph-based enumeration schemes and machine learning techniques for\nintermediate filtering. Key to this approach is our use of a neural network\npotential (NNP), AIMNet2-rxn, for computational evaluation of each candidate\nreaction pathway. In this article, we evaluate the NNP's ability to estimate\nactivation energies, demonstrate the correct anticipation of stereoselectivity,\nand recapitulate complex enabling steps in natural product synthesis.\n","authors":["Nicholas Casetti","Dylan Anstine","Olexandr Isayev","Connor W. Coley"],"pdf_url":"https://arxiv.org/pdf/2507.10400v1.pdf","comment":"32 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.10320v3","updated":"2025-07-14T15:31:23Z","published":"2024-09-16T14:33:21Z","title":"SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary\n  Learning for Closed-Loop Scenario Generation","summary":"  Verification and validation of autonomous driving (AD) systems and components\nis of increasing importance, as such technology increases in real-world\nprevalence. Safety-critical scenario generation is a key approach to robustify\nAD policies through closed-loop training. However, existing approaches for\nscenario generation rely on simplistic objectives, resulting in\noverly-aggressive or non-reactive adversarial behaviors. To generate diverse\nadversarial yet realistic scenarios, we propose SEAL, a scenario perturbation\napproach which leverages learned objective functions and adversarial,\nhuman-like skills. SEAL-perturbed scenarios are more realistic than SOTA\nbaselines, leading to improved ego task success across real-world,\nin-distribution, and out-of-distribution scenarios, of more than 20%. To\nfacilitate future research, we release our code and tools:\nhttps://github.com/cmubig/SEAL\n","authors":["Benjamin Stoler","Ingrid Navarro","Jonathan Francis","Jean Oh"],"pdf_url":"https://arxiv.org/pdf/2409.10320v3.pdf","comment":"Accepted to the IEEE Robotics and Automation Letters (RA-L) on June\n  28, 2025"},{"id":"http://arxiv.org/abs/2504.11168v3","updated":"2025-07-14T15:27:11Z","published":"2025-04-15T13:16:02Z","title":"Bypassing LLM Guardrails: An Empirical Analysis of Evasion Attacks\n  against Prompt Injection and Jailbreak Detection Systems","summary":"  Large Language Models (LLMs) guardrail systems are designed to protect\nagainst prompt injection and jailbreak attacks. However, they remain vulnerable\nto evasion techniques. We demonstrate two approaches for bypassing LLM prompt\ninjection and jailbreak detection systems via traditional character injection\nmethods and algorithmic Adversarial Machine Learning (AML) evasion techniques.\nThrough testing against six prominent protection systems, including Microsoft's\nAzure Prompt Shield and Meta's Prompt Guard, we show that both methods can be\nused to evade detection while maintaining adversarial utility achieving in some\ninstances up to 100% evasion success. Furthermore, we demonstrate that\nadversaries can enhance Attack Success Rates (ASR) against black-box targets by\nleveraging word importance ranking computed by offline white-box models. Our\nfindings reveal vulnerabilities within current LLM protection mechanisms and\nhighlight the need for more robust guardrail systems.\n","authors":["William Hackett","Lewis Birch","Stefan Trawicki","Neeraj Suri","Peter Garraghan"],"pdf_url":"https://arxiv.org/pdf/2504.11168v3.pdf","comment":"14 pages, 5 figures, 11 tables. To be published in LLMSec 2025"},{"id":"http://arxiv.org/abs/2411.08800v3","updated":"2025-07-14T15:26:05Z","published":"2024-11-13T17:27:32Z","title":"Deep Learning Accelerated Quantum Transport Simulations in\n  Nanoelectronics: From Break Junctions to Field-Effect Transistors","summary":"  Quantum transport simulations are essential for understanding and designing\nnanoelectronic devices, yet the long-standing trade-off between accuracy and\ncomputational efficiency has limited their practical applications. We present\nDeePTB-NEGF, an integrated framework combining deep learning tight-binding\nHamiltonian prediction with non-equilibrium Green's Function methodology to\nenable accurate quantum transport simulations in open boundary conditions with\n2-3 orders of magnitude acceleration. We demonstrate DeePTB-NEGF through two\nchallenging applications: comprehensive break junction simulations with over\n$10^4$ snapshots, showing excellent agreement with experimental conductance\nhistograms; and carbon nanotube field-effect transistors (CNT-FET) at\nexperimental dimensions, reproducing measured transfer characteristics for a 41\nnm channel CNT-FET ($\\sim 8000$ atoms, $3\\times10^4$ orbitals) and predicting\nzero-bias transmission spectra for a 180 nm CNT ($\\sim 3\\times 10^4$ atoms,\n$10^5$ orbitals), showcasing the framework's capability for large-scale device\nsimulations. Our systematic studies across varying geometries confirm the\nnecessity of simulating realistic experimental structures for precise\npredictions. DeePTB-NEGF bridges the longstanding gap between first-principles\naccuracy and computational efficiency, providing a scalable tool for\nhigh-throughput and large-scale quantum transport simulations that enables\npreviously inaccessible nanoscale device investigations.\n","authors":["Jijie Zou","Zhanghao Zhouyin","Dongying Lin","Yike Huang","Linfeng Zhang","Shimin Hou","Qiangqiang Gu"],"pdf_url":"https://arxiv.org/pdf/2411.08800v3.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.10385v1","updated":"2025-07-14T15:25:13Z","published":"2025-07-14T15:25:13Z","title":"Extracting Important Tokens in E-Commerce Queries with a Tag\n  Interaction-Aware Transformer Model","summary":"  The major task of any e-commerce search engine is to retrieve the most\nrelevant inventory items, which best match the user intent reflected in a\nquery. This task is non-trivial due to many reasons, including ambiguous\nqueries, misaligned vocabulary between buyers, and sellers, over- or\nunder-constrained queries by the presence of too many or too few tokens. To\naddress these challenges, query reformulation is used, which modifies a user\nquery through token dropping, replacement or expansion, with the objective to\nbridge semantic gap between query tokens and users' search intent. Early\nmethods of query reformulation mostly used statistical measures derived from\ntoken co-occurrence frequencies from selective user sessions having clicks or\npurchases. In recent years, supervised deep learning approaches, specifically\ntransformer-based neural language models, or sequence-to-sequence models are\nbeing used for query reformulation task. However, these models do not utilize\nthe semantic tags of a query token, which are significant for capturing user\nintent of an e-commerce query. In this work, we pose query reformulation as a\ntoken classification task, and solve this task by designing a dependency-aware\ntransformer-based language model, TagBERT, which makes use of semantic tags of\na token for learning superior query phrase embedding. Experiments on large,\nreal-life e-commerce datasets show that TagBERT exhibits superior performance\nthan plethora of competing models, including BERT, eBERT, and\nSequence-to-Sequence transformer model for important token classification task.\n","authors":["Md. Ahsanul Kabir","Mohammad Al Hasan","Aritra Mandal","Liyang Hao","Ishita Khan","Daniel Tunkelang","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2507.10385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10383v1","updated":"2025-07-14T15:23:24Z","published":"2025-07-14T15:23:24Z","title":"Dynamical stability for dense patterns in discrete attractor neural\n  networks","summary":"  Neural networks storing multiple discrete attractors are canonical models of\nbiological memory. Previously, the dynamical stability of such networks could\nonly be guaranteed under highly restrictive conditions. Here, we derive a\ntheory of the local stability of discrete fixed points in a broad class of\nnetworks with graded neural activities and in the presence of noise. By\ndirectly analyzing the bulk and outliers of the Jacobian spectrum, we show that\nall fixed points are stable below a critical load that is distinct from the\nclassical \\textit{critical capacity} and depends on the statistics of neural\nactivities in the fixed points as well as the single-neuron activation\nfunction. Our analysis highlights the computational benefits of\nthreshold-linear activation and sparse-like patterns.\n","authors":["Uri Cohen","Máté Lengyel"],"pdf_url":"https://arxiv.org/pdf/2507.10383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10382v1","updated":"2025-07-14T15:23:11Z","published":"2025-07-14T15:23:11Z","title":"Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis","summary":"  With the rise of smart mobility and shared e-mobility services, numerous\nadvanced technologies have been applied to this field. Cloud-based traffic\nsimulation solutions have flourished, offering increasingly realistic\nrepresentations of the evolving mobility landscape. LLMs have emerged as\npioneering tools, providing robust support for various applications, including\nintelligent decision-making, user interaction, and real-time traffic analysis.\nAs user demand for e-mobility continues to grow, delivering comprehensive\nend-to-end solutions has become crucial. In this paper, we present a\ncloud-based, LLM-powered shared e-mobility platform, integrated with a mobile\napplication for personalized route recommendations. The optimization module is\nevaluated based on travel time and cost across different traffic scenarios.\nAdditionally, the LLM-powered RAG framework is evaluated at the schema level\nfor different users, using various evaluation methods. Schema-level RAG with\nXiYanSQL achieves an average execution accuracy of 0.81 on system operator\nqueries and 0.98 on user queries.\n","authors":["Yue Ding","Conor McCarthy","Kevin O'Shea","Mingming Liu"],"pdf_url":"https://arxiv.org/pdf/2507.10382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10381v1","updated":"2025-07-14T15:22:29Z","published":"2025-07-14T15:22:29Z","title":"Improving Remote Sensing Classification using Topological Data Analysis\n  and Convolutional Neural Networks","summary":"  Topological data analysis (TDA) is a relatively new field that is gaining\nrapid adoption due to its robustness and ability to effectively describe\ncomplex datasets by quantifying geometric information. In imaging contexts, TDA\ntypically models data as filtered cubical complexes from which we can extract\ndiscriminative features using persistence homology. Meanwhile, convolutional\nneural networks (CNNs) have been shown to be biased towards texture based local\nfeatures. To address this limitation, we propose a TDA feature engineering\npipeline and a simple method to integrate topological features with deep\nlearning models on remote sensing classification. Our method improves the\nperformance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving\n99.33% accuracy, which surpasses all previously reported single-model\naccuracies, including those with larger architectures, such as ResNet50 (2x\nlarger) and XL Vision Transformers (197x larger). We additionally show that our\nmethod's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45\ndataset. To our knowledge, this is the first application of TDA features in\nsatellite scene classification with deep learning. This demonstrates that TDA\nfeatures can be integrated with deep learning models, even on datasets without\nexplicit topological structures, thereby increasing the applicability of TDA. A\nclean implementation of our method will be made publicly available upon\npublication.\n","authors":["Aaryam Sharma"],"pdf_url":"https://arxiv.org/pdf/2507.10381v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.06238v2","updated":"2025-07-14T15:16:18Z","published":"2024-10-08T17:54:03Z","title":"EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration","summary":"  Despite their success in many domains, large language models (LLMs) remain\nunder-studied in scenarios requiring optimal decision-making under uncertainty.\nThis is crucial as many real-world applications, ranging from personalized\nrecommendations to healthcare interventions, demand that LLMs not only predict\nbut also actively learn to make optimal decisions through exploration. In this\nwork, we measure LLMs' (in)ability to make optimal decisions in bandits, a\nstate-less reinforcement learning setting relevant to many applications. We\ndevelop a comprehensive suite of environments, including both context-free and\ncontextual bandits with varying task difficulties, to benchmark LLMs'\nperformance. Motivated by the existence of optimal exploration algorithms, we\npropose efficient ways to integrate this algorithmic knowledge into LLMs: by\nproviding explicit algorithm-guided support during inference; and through\nalgorithm distillation via in-context demonstrations and fine-tuning, using\nsynthetic data generated from these algorithms. Impressively, these techniques\nallow us to achieve superior exploration performance with smaller models,\nsurpassing larger models on various tasks. We conducted an extensive ablation\nstudy to shed light on various factors, such as task difficulty and data\nrepresentation, that influence the efficiency of LLM exploration. Additionally,\nwe conduct a rigorous analysis of the LLM's exploration efficiency using the\nconcept of regret, linking its ability to explore to the model size and\nunderlying algorithm.\n","authors":["Allen Nie","Yi Su","Bo Chang","Jonathan N. Lee","Ed H. Chi","Quoc V. Le","Minmin Chen"],"pdf_url":"https://arxiv.org/pdf/2410.06238v2.pdf","comment":"28 pages. Published at ICML 2025"},{"id":"http://arxiv.org/abs/2507.10375v1","updated":"2025-07-14T15:14:38Z","published":"2025-07-14T15:14:38Z","title":"Test-Time Canonicalization by Foundation Models for Robust Perception","summary":"  Real-world visual perception requires invariance to diverse transformations,\nyet current methods rely heavily on specialized architectures or training on\npredefined augmentations, limiting generalization. We propose FOCAL, a\ntest-time, data-driven framework that achieves robust perception by leveraging\ninternet-scale visual priors from foundation models. By generating and\noptimizing candidate transformations toward visually typical, \"canonical\"\nviews, FOCAL enhances robustness without re-training or architectural changes.\nOur experiments demonstrate improved robustness of CLIP and SAM across\nchallenging transformations, including 2D/3D rotations, illumination shifts\n(contrast and color), and day-night variations. We also highlight potential\napplications in active vision. Our approach challenges the assumption that\ntransform-specific training is necessary, instead offering a scalable path to\ninvariance. Our code is available at: https://github.com/sutkarsh/focal.\n","authors":["Utkarsh Singhal","Ryan Feng","Stella X. Yu","Atul Prakash"],"pdf_url":"https://arxiv.org/pdf/2507.10375v1.pdf","comment":"Published at ICML 2025"},{"id":"http://arxiv.org/abs/2507.10368v1","updated":"2025-07-14T15:09:58Z","published":"2025-07-14T15:09:58Z","title":"Enhanced DeepONet for 1-D consolidation operator learning: an\n  architectural investigation","summary":"  Deep Operator Networks (DeepONets) have emerged as a powerful surrogate\nmodeling framework for learning solution operators in PDE-governed systems.\nWhile their use is expanding across engineering disciplines, applications in\ngeotechnical engineering remain limited. This study systematically evaluates\nseveral DeepONet architectures for the one-dimensional consolidation problem.\nWe initially consider three architectures: a standard DeepONet with the\ncoefficient of consolidation embedded in the branch net (Models 1 and 2), and a\nphysics-inspired architecture with the coefficient embedded in the trunk net\n(Model 3). Results show that Model 3 outperforms the standard configurations\n(Models 1 and 2) but still has limitations when the target solution (excess\npore pressures) exhibits significant variation. To overcome this limitation, we\npropose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses\nthe identified limitations by capturing rapidly varying functions. All proposed\narchitectures achieve speedups ranging from 1.5 to 100 times over traditional\nexplicit and implicit solvers, with Model 4 being the most efficient. Larger\ncomputational savings are expected for more complex systems than the explored\n1D case, which is promising. Overall, the study highlights the potential of\nDeepONets to enable efficient, generalizable surrogate modeling in geotechnical\napplications, advancing the integration of scientific machine learning in\ngeotechnics, which is at an early stage.\n","authors":["Yongjin Choi","Chenying Liu","Jorge Macedo"],"pdf_url":"https://arxiv.org/pdf/2507.10368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06759v4","updated":"2025-07-14T14:53:56Z","published":"2024-03-11T14:31:03Z","title":"Average Calibration Error: A Differentiable Loss for Improved\n  Reliability in Image Segmentation","summary":"  Deep neural networks for medical image segmentation often produce\noverconfident results misaligned with empirical observations. Such\nmiscalibration, challenges their clinical translation. We propose to use\nmarginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss\nfunction to improve pixel-wise calibration without compromising segmentation\nquality. We show that this loss, despite using hard binning, is directly\ndifferentiable, bypassing the need for approximate but differentiable surrogate\nor soft binning approaches. Our work also introduces the concept of dataset\nreliability histograms which generalises standard reliability diagrams for\nrefined visual assessment of calibration in semantic segmentation aggregated at\nthe dataset level. Using mL1-ACE, we reduce average and maximum calibration\nerror by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS\n2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS\n","authors":["Theodore Barfoot","Luis Garcia-Peraza-Herrera","Ben Glocker","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2403.06759v4.pdf","comment":"Camera ready version as in 10.1007/978-3-031-72114-4_14"},{"id":"http://arxiv.org/abs/2507.10349v1","updated":"2025-07-14T14:51:24Z","published":"2025-07-14T14:51:24Z","title":"TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand\n  Forecasting","summary":"  Multi-horizon time series forecasting has many practical applications such as\ndemand forecasting. Accurate demand prediction is critical to help make buying\nand inventory decisions for supply chain management of e-commerce and physical\nretailers, and such predictions are typically required for future horizons\nextending tens of weeks. This is especially challenging during high-stake sales\nevents when demand peaks are particularly difficult to predict accurately.\nHowever, these events are important not only for managing supply chain\noperations but also for ensuring a seamless shopping experience for customers.\nTo address this challenge, we propose Temporal-Aligned Transformer (TAT), a\nmulti-horizon forecaster leveraging apriori-known context variables such as\nholiday and promotion events information for improving predictive performance.\nOur model consists of an encoder and decoder, both embedded with a novel\nTemporal Alignment Attention (TAA), designed to learn context-dependent\nalignment for peak demand forecasting. We conduct extensive empirical analysis\non two large-scale proprietary datasets from a large e-commerce retailer. We\ndemonstrate that TAT brings up to 30% accuracy improvement on peak demand\nforecasting while maintaining competitive overall performance compared to other\nstate-of-the-art methods.\n","authors":["Zhiyuan Zhao","Sitan Yang","Kin G. Olivares","Boris N. Oreshkin","Stan Vitebsky","Michael W. Mahoney","B. Aditya Prakash","Dmitry Efimov"],"pdf_url":"https://arxiv.org/pdf/2507.10349v1.pdf","comment":"9 pages, 4 figures, 7 tables, published at KDD 2025 workshop on AI\n  for Supply Chain: Today and Future"},{"id":"http://arxiv.org/abs/2507.10348v1","updated":"2025-07-14T14:51:18Z","published":"2025-07-14T14:51:18Z","title":"Feature Distillation is the Better Choice for Model-Heterogeneous\n  Federated Learning","summary":"  Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing\nattention for its ability to aggregate knowledge from heterogeneous models\nwhile keeping private data locally. To better aggregate knowledge from clients,\nensemble distillation, as a widely used and effective technique, is often\nemployed after global aggregation to enhance the performance of the global\nmodel. However, simply combining Hetero-FL and ensemble distillation does not\nalways yield promising results and can make the training process unstable. The\nreason is that existing methods primarily focus on logit distillation, which,\nwhile being model-agnostic with softmax predictions, fails to compensate for\nthe knowledge bias arising from heterogeneous models. To tackle this challenge,\nwe propose a stable and efficient Feature Distillation for model-heterogeneous\nFederated learning, dubbed FedFD, that can incorporate aligned feature\ninformation via orthogonal projection to integrate knowledge from heterogeneous\nmodels better. Specifically, a new feature-based ensemble federated knowledge\ndistillation paradigm is proposed. The global model on the server needs to\nmaintain a projection layer for each client-side model architecture to align\nthe features separately. Orthogonal techniques are employed to re-parameterize\nthe projection layer to mitigate knowledge bias from heterogeneous models and\nthus maximize the distilled knowledge. Extensive experiments show that FedFD\nachieves superior performance compared to state-of-the-art methods.\n","authors":["Yichen Li"],"pdf_url":"https://arxiv.org/pdf/2507.10348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10347v1","updated":"2025-07-14T14:51:02Z","published":"2025-07-14T14:51:02Z","title":"Parallel Sampling of Diffusion Models on $SO(3)$","summary":"  In this paper, we design an algorithm to accelerate the diffusion process on\nthe $SO(3)$ manifold. The inherently sequential nature of diffusion models\nnecessitates substantial time for denoising perturbed data. To overcome this\nlimitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$\nspace. We demonstrate our algorithm on an existing method that employs\ndiffusion models to address the pose ambiguity problem. Moreover, we show that\nthis acceleration advantage occurs without any measurable degradation in task\nreward. The experiments reveal that our algorithm achieves a speed-up of up to\n4.9$\\times$, significantly reducing the latency for generating a single sample.\n","authors":["Yan-Ting Chen","Hao-Wei Chen","Tsu-Ching Hsiao","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2507.10347v1.pdf","comment":"MVA2025"},{"id":"http://arxiv.org/abs/2301.00922v3","updated":"2025-07-14T14:49:35Z","published":"2023-01-03T01:35:24Z","title":"Faster Reinforcement Learning by Freezing Slow States","summary":"  We study infinite horizon Markov decision processes (MDPs) with \"fast-slow\"\nstructure, where some state variables evolve rapidly (\"fast states\") while\nothers change more gradually (\"slow states\"). This structure commonly arises in\npractice when decisions must be made at high frequencies over long horizons,\nand where slowly changing information still plays a critical role in\ndetermining optimal actions. Examples include inventory control under slowly\nchanging demand indicators or dynamic pricing with gradually shifting consumer\nbehavior. Modeling the problem at the natural decision frequency leads to MDPs\nwith discount factors close to one, making them computationally challenging. We\npropose a novel approximation strategy that \"freezes\" slow states during phases\nof lower-level planning and subsequently applies value iteration to an\nauxiliary upper-level MDP that evolves on a slower timescale. Freezing states\nfor short periods of time leads to easier-to-solve lower-level problems, while\na slower upper-level timescale allows for a more favorable discount factor. On\nthe theoretical side, we analyze the regret incurred by our frozen-state\napproach, which leads to simple insights on how to trade off regret versus\ncomputational cost. Empirically, we benchmark our new frozen-state methods on\nthree domains, (i) inventory control with fixed order costs, (ii) a gridworld\nproblem with spatial tasks, and (iii) dynamic pricing with reference-price\neffects. We demonstrate that the new methods produce high-quality policies with\nsignificantly less computation, and we show that simply omitting slow states is\noften a poor heuristic.\n","authors":["Yijia Wang","Daniel R. Jiang"],"pdf_url":"https://arxiv.org/pdf/2301.00922v3.pdf","comment":"70 pages, 10 figures"},{"id":"http://arxiv.org/abs/2507.10345v1","updated":"2025-07-14T14:48:47Z","published":"2025-07-14T14:48:47Z","title":"Some Super-approximation Rates of ReLU Neural Networks for Korobov\n  Functions","summary":"  This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU\nneural networks for Korobov functions. In terms of network width and depth, we\nderive nearly optimal super-approximation error bounds of order $2m$ in the\n$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with\n$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages\nsparse grid finite elements and the bit extraction technique. Our results\nimprove upon classical lowest order $L_\\infty$ and $H^1$ norm error bounds and\ndemonstrate that the expressivity of neural networks is largely unaffected by\nthe curse of dimensionality.\n","authors":["Yuwen Li","Guozhi Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.10345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10334v1","updated":"2025-07-14T14:41:19Z","published":"2025-07-14T14:41:19Z","title":"MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of\n  Imputation Methods for IMU-based Motion Capture Data","summary":"  Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)\nis vital for applications in sports science, but its utility is often\ncompromised by missing data. Despite numerous imputation techniques, a\nsystematic performance evaluation for IMU-derived MoCap time-series data is\nlacking. We address this gap by conducting a comprehensive comparative analysis\nof statistical, machine learning, and deep learning imputation methods. Our\nevaluation considers three distinct contexts: univariate time-series,\nmultivariate across subjects, and multivariate across kinematic angles. To\nfacilitate this benchmark, we introduce the first publicly available MoCap\ndataset designed specifically for imputation, featuring data from 53 karate\npractitioners. We simulate three controlled missingness mechanisms: missing\ncompletely at random (MCAR), block missingness, and a novel value-dependent\npattern at signal transition points. Our experiments, conducted on 39 kinematic\nvariables across all subjects, reveal that multivariate imputation frameworks\nconsistently outperform univariate approaches, particularly for complex\nmissingness. For instance, multivariate methods achieve up to a 50% mean\nabsolute error reduction (MAE from 10.8 to 5.8) compared to univariate\ntechniques for transition point missingness. Advanced models like Generative\nAdversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the\nhighest accuracy in these challenging scenarios. This work provides a critical\nbaseline for future research and offers practical recommendations for improving\nthe integrity and robustness of Mo-Cap data analysis.\n","authors":["Mahmoud Bekhit","Ahmad Salah","Ahmed Salim Alrawahi","Tarek Attia","Ahmed Ali","Esraa Eldesokey","Ahmed Fathalla"],"pdf_url":"https://arxiv.org/pdf/2507.10334v1.pdf","comment":"22 pages, 7 figures, 3 algorithms, 2 tables"},{"id":"http://arxiv.org/abs/2506.10660v2","updated":"2025-07-14T14:39:47Z","published":"2025-06-12T12:50:38Z","title":"Constructing Extreme Heatwave Storylines with Differentiable Climate\n  Models","summary":"  Understanding the plausible upper bounds of extreme weather events is\nessential for risk assessment in a warming climate. Existing methods, based on\nlarge ensembles of physics-based models, are often computationally expensive or\nlack the fidelity needed to simulate rare, high-impact extremes. Here, we\npresent a novel framework that leverages a differentiable hybrid climate model,\nNeuralGCM, to optimize initial conditions and generate physically consistent\nworst-case heatwave trajectories. Applied to the 2021 Pacific Northwest\nheatwave, our method produces heatwave intensity up to 3.7 $^\\circ$C above the\nmost extreme member of a 75-member ensemble. These trajectories feature\nintensified atmospheric blocking and amplified Rossby wave patterns-hallmarks\nof severe heat events. Our results demonstrate that differentiable climate\nmodels can efficiently explore the upper tails of event likelihoods, providing\na powerful new approach for constructing targeted storylines of extreme weather\nunder climate change.\n","authors":["Tim Whittaker","Alejandro Di Luca"],"pdf_url":"https://arxiv.org/pdf/2506.10660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10330v1","updated":"2025-07-14T14:38:48Z","published":"2025-07-14T14:38:48Z","title":"Bridging Robustness and Generalization Against Word Substitution Attacks\n  in NLP via the Growth Bound Matrix Approach","summary":"  Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM\n","authors":["Mohammed Bouri","Adnane Saoud"],"pdf_url":"https://arxiv.org/pdf/2507.10330v1.pdf","comment":"Accepted to ACL Findings 2025"},{"id":"http://arxiv.org/abs/2507.04225v2","updated":"2025-07-14T14:33:47Z","published":"2025-07-06T03:30:45Z","title":"Zero-Shot Cyclic Peptide Design via Composable Geometric Constraints","summary":"  Cyclic peptides, characterized by geometric constraints absent in linear\npeptides, offer enhanced biochemical properties, presenting new opportunities\nto address unmet medical needs. However, designing target-specific cyclic\npeptides remains underexplored due to limited training data. To bridge the gap,\nwe propose CP-Composer, a novel generative framework that enables zero-shot\ncyclic peptide generation via composable geometric constraints. Our approach\ndecomposes complex cyclization patterns into unit constraints, which are\nincorporated into a diffusion model through geometric conditioning on nodes and\nedges. During training, the model learns from unit constraints and their random\ncombinations in linear peptides, while at inference, novel constraint\ncombinations required for cyclization are imposed as input. Experiments show\nthat our model, despite trained with linear peptides, is capable of generating\ndiverse target-binding cyclic peptides, reaching success rates from 38% to 84%\non different cyclization strategies.\n","authors":["Dapeng Jiang","Xiangzhe Kong","Jiaqi Han","Mingyu Li","Rui Jiao","Wenbing Huang","Stefano Ermon","Jianzhu Ma","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2507.04225v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10325v1","updated":"2025-07-14T14:32:46Z","published":"2025-07-14T14:32:46Z","title":"Convergence of Agnostic Federated Averaging","summary":"  Federated learning (FL) enables decentralized model training without\ncentralizing raw data. However, practical FL deployments often face a key\nrealistic challenge: Clients participate intermittently in server aggregation\nand with unknown, possibly biased participation probabilities. Most existing\nconvergence results either assume full-device participation, or rely on\nknowledge of (in fact uniform) client availability distributions -- assumptions\nthat rarely hold in practice. In this work, we characterize the optimization\nproblem that consistently adheres to the stochastic dynamics of the well-known\n\\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and\nvariably-sized) client availability, and rigorously establish its convergence\nfor convex, possibly nonsmooth losses, achieving a standard rate of order\n$\\mathcal{O}(1/\\sqrt{T})$, where $T$ denotes the aggregation horizon. Our\nanalysis provides the first convergence guarantees for agnostic FedAvg under\ngeneral, non-uniform, stochastic client participation, without knowledge of the\nparticipation distribution. We also empirically demonstrate that agnostic\nFedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg\nvariants, even with server-side knowledge of participation weights.\n","authors":[" Herlock"," Rahimi","Dionysis Kalogerias"],"pdf_url":"https://arxiv.org/pdf/2507.10325v1.pdf","comment":"5 pages, 2 figurres, CAMSAP conference"},{"id":"http://arxiv.org/abs/2505.12864v3","updated":"2025-07-14T14:30:57Z","published":"2025-05-19T08:48:12Z","title":"LEXam: Benchmarking Legal Reasoning on 340 Law Exams","summary":"  Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/\n","authors":["Yu Fan","Jingwei Ni","Jakob Merane","Etienne Salimbeni","Yang Tian","Yoan Hermstrüwer","Yinya Huang","Mubashara Akhtar","Florian Geering","Oliver Dreyer","Daniel Brunner","Markus Leippold","Mrinmaya Sachan","Alexander Stremitzer","Christoph Engel","Elliott Ash","Joel Niklaus"],"pdf_url":"https://arxiv.org/pdf/2505.12864v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10311v1","updated":"2025-07-14T14:15:47Z","published":"2025-07-14T14:15:47Z","title":"Recognizing Dementia from Neuropsychological Tests with State Space\n  Models","summary":"  Early detection of dementia is critical for timely medical intervention and\nimproved patient outcomes. Neuropsychological tests are widely used for\ncognitive assessment but have traditionally relied on manual scoring. Automatic\ndementia classification (ADC) systems aim to infer cognitive decline directly\nfrom speech recordings of such tests. We propose Demenba, a novel ADC framework\nbased on state space models, which scale linearly in memory and computation\nwith sequence length. Trained on over 1,000 hours of cognitive assessments\nadministered to Framingham Heart Study participants, some of whom were\ndiagnosed with dementia through adjudicated review, our method outperforms\nprior approaches in fine-grained dementia classification by 21\\%, while using\nfewer parameters. We further analyze its scaling behavior and demonstrate that\nour model gains additional improvement when fused with large language models,\npaving the way for more transparent and scalable dementia assessment tools.\nCode: https://anonymous.4open.science/r/Demenba-0861\n","authors":["Liming Wang","Saurabhchand Bhati","Cody Karjadi","Rhoda Au","James Glass"],"pdf_url":"https://arxiv.org/pdf/2507.10311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05649v2","updated":"2025-07-14T14:12:59Z","published":"2025-07-08T04:01:53Z","title":"DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning","summary":"  Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nvarious graph-based learning tasks. However, enabling privacy-preserving GNNs\nin encrypted domains, such as under Fully Homomorphic Encryption (FHE),\ntypically incurs substantial computational overhead, rendering real-time and\nprivacy-preserving inference impractical. In this work, we propose DESIGN\n(EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel\nframework for efficient encrypted GNN inference. DESIGN tackles the critical\nefficiency limitations of existing FHE GNN approaches, which often overlook\ninput data redundancy and apply uniform computational strategies. Our framework\nachieves significant performance gains through a hierarchical optimization\nstrategy executed entirely on the server: first, FHE-compatible node importance\nscores (based on encrypted degree statistics) are computed from the encrypted\ngraph. These scores then guide a homomorphic partitioning process, generating\nmulti-level importance masks directly under FHE. This dynamically generated\nmask facilitates both input graph pruning (by logically removing unimportant\nelements) and a novel adaptive polynomial activation scheme, where activation\ncomplexity is tailored to node importance levels. Empirical evaluations\ndemonstrate that DESIGN substantially accelerates FHE GNN inference compared to\nstate-of-the-art methods while maintaining competitive model accuracy,\npresenting a robust solution for secure graph analytics. Our implementation is\npublicly available at https://github.com/LabRAI/DESIGN.\n","authors":["Kaixiang Zhao","Joseph Yousry Attalla","Qian Lou","Yushun Dong"],"pdf_url":"https://arxiv.org/pdf/2507.05649v2.pdf","comment":"Under Review in Conference on Neural Information Processing Systems\n  (NeurIPS 2025)"},{"id":"http://arxiv.org/abs/2405.07344v4","updated":"2025-07-14T14:09:59Z","published":"2024-05-12T17:40:48Z","title":"TKAN: Temporal Kolmogorov-Arnold Networks","summary":"  Recurrent Neural Networks (RNNs) have revolutionized many areas of machine\nlearning, particularly in natural language and data sequence processing. Long\nShort-Term Memory (LSTM) has demonstrated its ability to capture long-term\ndependencies in sequential data. Inspired by the Kolmogorov-Arnold Networks\n(KANs) a promising alternatives to Multi-Layer Perceptrons (MLPs), we proposed\na new neural networks architecture inspired by KAN and the LSTM, the Temporal\nKolomogorov-Arnold Networks (TKANs). TKANs combined the strenght of both\nnetworks, it is composed of Recurring Kolmogorov-Arnold Networks (RKANs) Layers\nembedding memory management. This innovation enables us to perform multi-step\ntime series forecasting with enhanced accuracy and efficiency. By addressing\nthe limitations of traditional models in handling complex sequential patterns,\nthe TKAN architecture offers significant potential for advancements in fields\nrequiring more than one step ahead forecasting.\n","authors":["Remi Genet","Hugo Inzirillo"],"pdf_url":"https://arxiv.org/pdf/2405.07344v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10303v1","updated":"2025-07-14T14:06:56Z","published":"2025-07-14T14:06:56Z","title":"MF-GLaM: A multifidelity stochastic emulator using generalized lambda\n  models","summary":"  Stochastic simulators exhibit intrinsic stochasticity due to unobservable,\nuncontrollable, or unmodeled input variables, resulting in random outputs even\nat fixed input conditions. Such simulators are common across various scientific\ndisciplines; however, emulating their entire conditional probability\ndistribution is challenging, as it is a task traditional deterministic\nsurrogate modeling techniques are not designed for. Additionally, accurately\ncharacterizing the response distribution can require prohibitively large\ndatasets, especially for computationally expensive high-fidelity (HF)\nsimulators. When lower-fidelity (LF) stochastic simulators are available, they\ncan enhance limited HF information within a multifidelity surrogate modeling\n(MFSM) framework. While MFSM techniques are well-established for deterministic\nsettings, constructing multifidelity emulators to predict the full conditional\nresponse distribution of stochastic simulators remains a challenge. In this\npaper, we propose multifidelity generalized lambda models (MF-GLaMs) to\nefficiently emulate the conditional response distribution of HF stochastic\nsimulators by exploiting data from LF stochastic simulators. Our approach\nbuilds upon the generalized lambda model (GLaM), which represents the\nconditional distribution at each input by a flexible, four-parameter\ngeneralized lambda distribution. MF-GLaMs are non-intrusive, requiring no\naccess to the internal stochasticity of the simulators nor multiple\nreplications of the same input values. We demonstrate the efficacy of MF-GLaM\nthrough synthetic examples of increasing complexity and a realistic earthquake\napplication. Results show that MF-GLaMs can achieve improved accuracy at the\nsame cost as single-fidelity GLaMs, or comparable performance at significantly\nreduced cost.\n","authors":["K. Giannoukou","X. Zhu","S. Marelli","B. Sudret"],"pdf_url":"https://arxiv.org/pdf/2507.10303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07947v2","updated":"2025-07-14T14:03:51Z","published":"2025-07-10T17:32:26Z","title":"Low Resource Reconstruction Attacks Through Benign Prompts","summary":"  The recent advances in generative models such as diffusion models have raised\nseveral risks and concerns related to privacy, copyright infringements and data\nstewardship. To better understand and control the risks, various researchers\nhave created techniques, experiments and attacks that reconstruct images, or\npart of images, from the training set. While these techniques already establish\nthat data from the training set can be reconstructed, they often rely on\nhigh-resources, excess to the training set as well as well-engineered and\ndesigned prompts.\n  In this work, we devise a new attack that requires low resources, assumes\nlittle to no access to the actual training set, and identifies, seemingly,\nbenign prompts that lead to potentially-risky image reconstruction. This\nhighlights the risk that images might even be reconstructed by an uninformed\nuser and unintentionally. For example, we identified that, with regard to one\nexisting model, the prompt ``blue Unisex T-Shirt'' can generate the face of a\nreal-life human model. Our method builds on an intuition from previous works\nwhich leverages domain knowledge and identifies a fundamental vulnerability\nthat stems from the use of scraped data from e-commerce platforms, where\ntemplated layouts and images are tied to pattern-like prompts.\n","authors":["Sol Yarkoni","Roi Livni"],"pdf_url":"https://arxiv.org/pdf/2507.07947v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10296v1","updated":"2025-07-14T14:02:31Z","published":"2025-07-14T14:02:31Z","title":"Average Sensitivity of Hierarchical $k$-Median Clustering","summary":"  Hierarchical clustering is a widely used method for unsupervised learning\nwith numerous applications. However, in the application of modern algorithms,\nthe datasets studied are usually large and dynamic. If the hierarchical\nclustering is sensitive to small perturbations of the dataset, the usability of\nthe algorithm will be greatly reduced. In this paper, we focus on the\nhierarchical $k$ -median clustering problem, which bridges hierarchical and\ncentroid-based clustering while offering theoretical appeal, practical utility,\nand improved interpretability. We analyze the average sensitivity of algorithms\nfor this problem by measuring the expected change in the output when a random\ndata point is deleted. We propose an efficient algorithm for hierarchical\n$k$-median clustering and theoretically prove its low average sensitivity and\nhigh clustering quality. Additionally, we show that single linkage clustering\nand a deterministic variant of the CLNSS algorithm exhibit high average\nsensitivity, making them less stable. Finally, we validate the robustness and\neffectiveness of our algorithm through experiments.\n","authors":["Shijie Li","Weiqiang He","Ruobing Bai","Pan Peng"],"pdf_url":"https://arxiv.org/pdf/2507.10296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16647v2","updated":"2025-07-14T14:01:55Z","published":"2024-04-25T14:36:00Z","title":"Application of RESNET50 Convolution Neural Network for the Extraction of\n  Optical Parameters in Scattering Media","summary":"  Estimation of the optical properties of scattering media such as tissue is\nimportant in diagnostics as well as in the development of techniques to image\ndeeper. As light penetrates the sample scattering events occur that alter the\npropagation direction of the photons in a random manner leading degradation of\nimage quality. The distribution of the scattered light does, however, give a\nmeasure of the optical properties such as the reduced scattering coefficient\nand the absorption coefficient. Unfortunately, inverting scattering patterns to\nrecover the optical properties is not simple especially in the regime where the\nlight is partially randomized. Machine learning has been proposed by several\nauthors as a means of recovering these properties from either the back\nscattered or the transmitted light. In the present paper we train a general\npurpose convolutional neural network RESNET 50 with simulated data based on\nMonte Carlo simulations. We show that compared with previous work our approach\ngives comparable or better reconstruction accuracy with training on a much\nsmaller dataset. Moreover, by training on multiple parameters such as the\nintensity distribution at multiple planes or the exit angle and spatial\ndistribution one achieves improved performance compared to training on a single\ninput such as the intensity distribution captured at the sample surface. While\nour approach gives good parameter reconstruction, we identify factors that\nlimit accuracy of the recovered properties, particularly the absorption\ncoefficient. In the light of these limitations, we suggest how the present\napproach may be enhanced for even better performance.\n","authors":["Bowen Deng","Yihan Zhang","Andrew Parkes","Alex Bentley","Amanda Wright","Michael Pound","Michael Somekh"],"pdf_url":"https://arxiv.org/pdf/2404.16647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10273v1","updated":"2025-07-14T13:42:39Z","published":"2025-07-14T13:42:39Z","title":"Conditional Chemical Language Models are Versatile Tools in Drug\n  Discovery","summary":"  Generative chemical language models (CLMs) have demonstrated strong\ncapabilities in molecular design, yet their impact in drug discovery remains\nlimited by the absence of reliable reward signals and the lack of\ninterpretability in their outputs. We present SAFE-T, a generalist chemical\nmodeling framework that conditions on biological context -- such as protein\ntargets or mechanisms of action -- to prioritize and design molecules without\nrelying on structural information or engineered scoring functions. SAFE-T\nmodels the conditional likelihood of fragment-based molecular sequences given a\nbiological prompt, enabling principled scoring of molecules across tasks such\nas virtual screening, drug-target interaction prediction, and activity cliff\ndetection. Moreover, it supports goal-directed generation by sampling from this\nlearned distribution, aligning molecular design with biological objectives. In\ncomprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,\nACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves\nperformance comparable to or better than existing approaches while being\nsignificantly faster. Fragment-level attribution further reveals that SAFE-T\ncaptures known structure-activity relationships, supporting interpretable and\nbiologically grounded design. Together with its computational efficiency, these\nresults demonstrate that conditional generative CLMs can unify scoring and\ngeneration to accelerate early-stage drug discovery.\n","authors":["Lu Zhu","Emmanuel Noutahi"],"pdf_url":"https://arxiv.org/pdf/2507.10273v1.pdf","comment":"12 pages, extra 13 pages of appendix"},{"id":"http://arxiv.org/abs/2411.04845v2","updated":"2025-07-14T13:38:10Z","published":"2024-11-07T16:32:50Z","title":"Asymptotic regularity of a generalised stochastic Halpern scheme","summary":"  We provide abstract, general and highly uniform rates of asymptotic\nregularity for a generalized stochastic Halpern-style iteration, which\nincorporates a second mapping in the style of a Krasnoselskii-Mann iteration.\nThis iteration is general in two ways: First, it incorporates stochasticity in\na completely abstract way rather than fixing a sampling method; secondly, it\nincludes as special cases stochastic versions of various schemes from the\noptimization literature, including Halpern's iteration as well as a\nKrasnoselskii-Mann iteration with Tikhonov regularization terms in the sense of\nBo\\c{t}, Csetnek and Meier. For these specific cases, we in particular obtain\nlinear rates of asymptotic regularity, matching (or improving) the currently\nbest known rates for these iterations in stochastic optimization, and quadratic\nrates of asymptotic regularity are obtained in the context of inner product\nspaces for the general iteration. At the end, we briefly sketch how the schemes\npresented here can be instantiated in the context of reinforcement learning to\nyield novel methods for Q-learning.\n","authors":["Nicholas Pischke","Thomas Powell"],"pdf_url":"https://arxiv.org/pdf/2411.04845v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2507.10267v1","updated":"2025-07-14T13:37:48Z","published":"2025-07-14T13:37:48Z","title":"DNS Tunneling: Threat Landscape and Improved Detection Solutions","summary":"  Detecting Domain Name System (DNS) tunneling is a significant challenge in\nsecurity due to its capacity to hide harmful actions within DNS traffic that\nappears to be normal and legitimate. Traditional detection methods are based on\nrule-based approaches or signature matching methods that are often insufficient\nto accurately identify such covert communication channels. This research is\nabout effectively detecting DNS tunneling. We propose a novel approach to\ndetect DNS tunneling with machine learning algorithms. We combine machine\nlearning algorithms to analyze the traffic by using features extracted from DNS\ntraffic. Analyses results show that the proposed approach is a good candidate\nto detect DNS tunneling accurately.\n","authors":["Novruz Amirov","Baran Isik","Bilal Ihsan Tuncer","Serif Bahtiyar"],"pdf_url":"https://arxiv.org/pdf/2507.10267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12922v2","updated":"2025-07-14T13:34:55Z","published":"2025-04-17T13:11:26Z","title":"On the asymptotic behaviour of stochastic processes, with applications\n  to supermartingale convergence, Dvoretzky's approximation theorem, and\n  stochastic quasi-Fejér monotonicity","summary":"  We prove a novel and general result on the asymptotic behavior of stochastic\nprocesses which conform to a certain relaxed supermartingale condition. Our\nresult provides quantitative information in the form of an explicit and\neffective construction of a rate of convergence for this process, both in mean\nand almost surely, that is moreover highly uniform in that it only depends on\nvery few data of the surrounding objects involved in the iteration. We then\napply this result to derive new quantitative versions of well-known concepts\nand theorems from stochastic approximation, in particular providing effective\nrates for a variant of the Robbins-Siegmund theorem, Dvoretzky's convergence\ntheorem, as well as the convergence of stochastic quasi-Fej\\'er monotone\nsequences, the latter of which formulated in a novel and highly general metric\ncontext. We utilize the classic and widely studied Robbins-Monro procedure as a\ntemplate to evaluate our quantitative results and their applicability in\ngreater detail. We conclude by illustrating the breadth of potential further\napplications with a brief discussion on a variety of other well-known iterative\nprocedures from stochastic approximation. Throughout, we isolate and discuss\nspecial cases of our results which allow for the construction of fast, and in\nparticular linear, rates.\n","authors":["Morenikeji Neri","Nicholas Pischke","Thomas Powell"],"pdf_url":"https://arxiv.org/pdf/2504.12922v2.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2310.19603v4","updated":"2025-07-14T13:17:47Z","published":"2023-10-30T14:58:12Z","title":"Transformers Can Solve Non-Linear and Non-Markovian Filtering Problems\n  in Continuous Time For Conditionally Gaussian Signals","summary":"  The use of attention-based deep learning models in stochastic filtering, e.g.\ntransformers and deep Kalman filters, has recently come into focus; however,\nthe potential for these models to solve stochastic filtering problems remains\nlargely unknown. The paper provides an affirmative answer to this open problem\nin the theoretical foundations of machine learning by showing that a class of\ncontinuous-time transformer models, called \\textit{filterformers}, can\napproximately implement the conditional law of a broad class of non-Markovian\nand conditionally Gaussian signal processes given noisy continuous-time\n(possibly non-Gaussian) measurements. Our approximation guarantees hold\nuniformly over sufficiently regular compact subsets of continuous-time paths,\nwhere the worst-case 2-Wasserstein distance between the true optimal filter and\nour deep learning model quantifies the approximation error. Our construction\nrelies on two new customizations of the standard attention mechanism: The first\ncan losslessly adapt to the characteristics of a broad range of paths since we\nshow that the attention mechanism implements bi-Lipschitz embeddings of\nsufficiently regular sets of paths into low-dimensional Euclidean spaces; thus,\nit incurs no ``dimension reduction error''. The latter attention mechanism is\ntailored to the geometry of Gaussian measures in the $2$-Wasserstein space. Our\nanalysis relies on new stability estimates of robust optimal filters in the\nconditionally Gaussian setting.\n","authors":["Blanka Horvath","Anastasis Kratsios","Yannick Limmer","Xuwei Yang"],"pdf_url":"https://arxiv.org/pdf/2310.19603v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10250v1","updated":"2025-07-14T13:17:46Z","published":"2025-07-14T13:17:46Z","title":"DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in\n  Histopathology","summary":"  Accurate and timely cancer diagnosis from histopathological slides is vital\nfor effective clinical decision-making. This paper introduces DepViT-CAD, a\ndeployable AI system for multi-class cancer diagnosis in histopathology. At its\ncore is MAViT, a novel Multi-Attention Vision Transformer designed to capture\nfine-grained morphological patterns across diverse tumor types. MAViT was\ntrained on expert-annotated patches from 1008 whole-slide images, covering 11\ndiagnostic categories, including 10 major cancers and non-tumor tissue.\nDepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer\nGenome Atlas and 50 routine clinical cases from pathology labs, achieving\ndiagnostic sensitivities of 94.11% and 92%, respectively. By combining\nstate-of-the-art transformer architecture with large-scale real-world\nvalidation, DepViT-CAD offers a robust and scalable approach for AI-assisted\ncancer diagnostics. To support transparency and reproducibility, software and\ncode will be made publicly available at GitHub.\n","authors":["Ashkan Shakarami","Lorenzo Nicole","Rocco Cappellesso","Angelo Paolo Dei Tos","Stefano Ghidoni"],"pdf_url":"https://arxiv.org/pdf/2507.10250v1.pdf","comment":"25 pages, 15 figures"},{"id":"http://arxiv.org/abs/2507.10241v1","updated":"2025-07-14T13:03:53Z","published":"2025-07-14T13:03:53Z","title":"Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with\n  Sharp Gradients","summary":"  This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning\nMachine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of\nPI-ELM designed to solve both forward and inverse Partial Differential Equation\n(PDE) problems involving localized sharp gradients. While PI-ELMs outperform\nthe traditional Physics-Informed Neural Networks (PINNs) in speed due to their\nsingle-shot, least square optimization, this advantage comes at a cost: their\nfixed, randomly initialized input layer limits their ability to capture sharp\ngradients. To overcome this limitation, we introduce a lightweight Bayesian\nOptimization (BO) framework that, instead of adjusting each input layer\nparameter individually as in traditional backpropagation, learns a small set of\nhyperparameters defining the statistical distribution from which the input\nweights are drawn. This novel distributional optimization strategy -- combining\nBO for input layer distributional parameters with least-squares optimization\nfor output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's\nspeed while matching or exceeding the expressiveness of PINNs. We validate the\nproposed methodology on several challenging forward and inverse PDE benchmarks,\nincluding a 1D singularly perturbed convection-diffusion equation, a 2D Poisson\nequation with sharp localized sources, and a time-dependent advection equation.\nNotably, KAPI-ELM achieves state-of-the-art accuracy in both forward and\ninverse settings. In stiff PDE regimes, it matches or even outperforms advanced\nmethods such as the Extended Theory of Functional Connections (XTFC), while\nrequiring nearly an order of magnitude fewer tunable parameters. These results\nestablish the potential of KAPI-ELM as a scalable, interpretable, and\ngeneralizable physics-informed learning framework, especially in stiff PDE\nregimes.\n","authors":["Vikas Dwivedi","Balaji Srinivasan","Monica Sigovan","Bruno Sixou"],"pdf_url":"https://arxiv.org/pdf/2507.10241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10240v1","updated":"2025-07-14T13:03:17Z","published":"2025-07-14T13:03:17Z","title":"Visual Analytics for Explainable and Trustworthy Artificial Intelligence","summary":"  Our society increasingly depends on intelligent systems to solve complex\nproblems, ranging from recommender systems suggesting the next movie to watch\nto AI models assisting in medical diagnoses for hospitalized patients. With the\niterative improvement of diagnostic accuracy and efficiency, AI holds\nsignificant potential to mitigate medical misdiagnoses by preventing numerous\ndeaths and reducing an economic burden of approximately 450 EUR billion\nannually. However, a key obstacle to AI adoption lies in the lack of\ntransparency: many automated systems function as \"black boxes,\" providing\npredictions without revealing the underlying processes. This opacity can hinder\nexperts' ability to trust and rely on AI systems. Visual analytics (VA)\nprovides a compelling solution by combining AI models with interactive\nvisualizations. These specialized charts and graphs empower users to\nincorporate their domain expertise to refine and improve the models, bridging\nthe gap between AI and human understanding. In this work, we define,\ncategorize, and explore how VA solutions can foster trust across the stages of\na typical AI pipeline. We propose a design space for innovative visualizations\nand present an overview of our previously developed VA dashboards, which\nsupport critical tasks within the various pipeline stages, including data\nprocessing, feature engineering, hyperparameter tuning, understanding,\ndebugging, refining, and comparing models.\n","authors":["Angelos Chatzimparmpas"],"pdf_url":"https://arxiv.org/pdf/2507.10240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10222v1","updated":"2025-07-14T12:39:07Z","published":"2025-07-14T12:39:07Z","title":"Spatial Lifting for Dense Prediction","summary":"  We present Spatial Lifting (SL), a novel methodology for dense prediction\ntasks. SL operates by lifting standard inputs, such as 2D images, into a\nhigher-dimensional space and subsequently processing them using networks\ndesigned for that higher dimension, such as a 3D U-Net. Counterintuitively,\nthis dimensionality lifting allows us to achieve good performance on benchmark\ntasks compared to conventional approaches, while reducing inference costs and\nsignificantly lowering the number of model parameters. The SL framework\nproduces intrinsically structured outputs along the lifted dimension. This\nemergent structure facilitates dense supervision during training and enables\nrobust, near-zero-additional-cost prediction quality assessment at test time.\nWe validate our approach across 19 benchmark datasets (13 for semantic\nsegmentation and 6 for depth estimation), demonstrating competitive dense\nprediction performance while reducing the model parameter count by over 98% (in\nthe U-Net case) and lowering inference costs. Spatial Lifting introduces a new\nvision modeling paradigm that offers a promising path toward more efficient,\naccurate, and reliable deep networks for dense prediction tasks in vision.\n","authors":["Mingzhi Xu","Yizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.10222v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2507.10215v1","updated":"2025-07-14T12:31:47Z","published":"2025-07-14T12:31:47Z","title":"A Graph Sufficiency Perspective for Neural Networks","summary":"  This paper analyzes neural networks through graph variables and statistical\nsufficiency. We interpret neural network layers as graph-based transformations,\nwhere neurons act as pairwise functions between inputs and learned anchor\npoints. Within this formulation, we establish conditions under which layer\noutputs are sufficient for the layer inputs, that is, each layer preserves the\nconditional distribution of the target variable given the input variable. Under\ndense anchor point assumptions, we prove that asymptotic sufficiency holds in\nthe infinite-width limit and is preserved throughout training. To align more\nclosely with practical architectures, we further show that sufficiency can be\nachieved with finite-width networks by assuming region-separated input\ndistributions and constructing appropriate anchor points. Our framework covers\nfully connected layers, general pairwise functions, ReLU and sigmoid\nactivations, and convolutional neural networks. This work bridges statistical\nsufficiency, graph-theoretic representations, and deep learning, providing a\nnew statistical understanding of neural networks.\n","authors":["Cencheng Shen","Yuexiao Dong"],"pdf_url":"https://arxiv.org/pdf/2507.10215v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2507.10201v1","updated":"2025-07-14T12:14:17Z","published":"2025-07-14T12:14:17Z","title":"History Matching under Uncertainty of Geological Scenarios with Implicit\n  Geological Realism Control with Generative Deep Learning and Graph\n  Convolutions","summary":"  The graph-based variational autoencoder represents an architecture that can\nhandle the uncertainty of different geological scenarios, such as depositional\nor structural, through the concept of a lowerdimensional latent space. The main\ndifference from recent studies is utilisation of a graph-based approach in\nreservoir modelling instead of the more traditional lattice-based deep learning\nmethods. We provide a solution to implicitly control the geological realism\nthrough the latent variables of a generative model and Geodesic metrics. Our\nexperiments of AHM with synthetic dataset that consists of 3D realisations of\nchannelised geological representations with two distinct scenarios with one and\ntwo channels shows the viability of the approach. We offer in-depth analysis of\nthe latent space using tools such as PCA, t-SNE, and TDA to illustrate its\nstructure.\n","authors":["Gleb Shishaev","Vasily Demyanov","Daniel Arnold"],"pdf_url":"https://arxiv.org/pdf/2507.10201v1.pdf","comment":"Part of the completed PhD thesis\n  https://geodatascience.hw.ac.uk/theses/"},{"id":"http://arxiv.org/abs/2505.17826v2","updated":"2025-07-14T12:02:28Z","published":"2025-05-23T12:41:09Z","title":"Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models","summary":"  Trinity-RFT is a general-purpose, unified and easy-to-use framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na modular and decoupled design, consisting of (1) an RFT-core that unifies and\ngeneralizes synchronous/asynchronous, on-policy/off-policy, and online/offline\nmodes of RFT; (2) seamless integration for agent-environment interaction with\nhigh efficiency and robustness; and (3) systematic data pipelines optimized for\nRFT. Trinity-RFT can be easily adapted for diverse application scenarios, and\nserves as a unified platform for development and research of advanced\nreinforcement learning paradigms at both macroscopic and microscopic levels.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples, applications and experiments\nthat demonstrate its functionalities and user-friendliness.\n","authors":["Xuchen Pan","Yanxi Chen","Yushuo Chen","Yuchang Sun","Daoyuan Chen","Wenhao Zhang","Yuexiang Xie","Yilun Huang","Yilei Zhang","Dawei Gao","Weijie Shi","Yaliang Li","Bolin Ding","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.17826v2.pdf","comment":"This technical report will be continuously updated as the codebase\n  evolves. GitHub: https://github.com/modelscope/Trinity-RFT"},{"id":"http://arxiv.org/abs/2507.10194v1","updated":"2025-07-14T12:01:08Z","published":"2025-07-14T12:01:08Z","title":"Learning Private Representations through Entropy-based Adversarial\n  Training","summary":"  How can we learn a representation with high predictive power while preserving\nuser privacy? We present an adversarial representation learning method for\nsanitizing sensitive content from the learned representation. Specifically, we\nintroduce a variant of entropy - focal entropy, which mitigates the potential\ninformation leakage of the existing entropy-based approaches. We showcase\nfeasibility on multiple benchmarks. The results suggest high target utility at\nmoderate privacy leakage.\n","authors":["Tassilo Klein","Moin Nabi"],"pdf_url":"https://arxiv.org/pdf/2507.10194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10183v1","updated":"2025-07-14T11:47:43Z","published":"2025-07-14T11:47:43Z","title":"T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs","summary":"  Dynamic graph learning methods have recently emerged as powerful tools for\nmodelling relational data evolving through time. However, despite extensive\nbenchmarking efforts, it remains unclear whether current Temporal Graph Neural\nNetworks (TGNNs) effectively capture core temporal patterns such as\nperiodicity, cause-and-effect, and long-range dependencies. In this work, we\nintroduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set\nof synthetic tasks designed to systematically probe the capabilities of TGNNs\nto reason across time. T-GRAB provides controlled, interpretable tasks that\nisolate key temporal skills: counting/memorizing periodic repetitions,\ninferring delayed causal effects, and capturing long-range dependencies over\nboth spatial and temporal dimensions. We evaluate 11 temporal graph learning\nmethods on these tasks, revealing fundamental shortcomings in their ability to\ngeneralize temporal patterns. Our findings offer actionable insights into the\nlimitations of current models, highlight challenges hidden by traditional\nreal-world benchmarks, and motivate the development of architectures with\nstronger temporal reasoning abilities. The code for T-GRAB can be found at:\nhttps://github.com/alirezadizaji/T-GRAB.\n","authors":["Alireza Dizaji","Benedict Aaron Tjandra","Mehrab Hamidi","Shenyang Huang","Guillaume Rabusseau"],"pdf_url":"https://arxiv.org/pdf/2507.10183v1.pdf","comment":"Accepted to MLoG-GenAI Workshop @ KDD 2025 (Oral)"},{"id":"http://arxiv.org/abs/2507.10178v1","updated":"2025-07-14T11:40:17Z","published":"2025-07-14T11:40:17Z","title":"Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large\n  Language Model Serving","summary":"  Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x\nhigher token generation throughput, respectively.\n","authors":["Wonung Kim","Yubin Lee","Yoonsung Kim","Jinwoo Hwang","Seongryong Oh","Jiyong Jung","Aziz Huseynov","Woong Gyu Park","Chang Hyun Park","Divya Mahajan","Jongse Park"],"pdf_url":"https://arxiv.org/pdf/2507.10178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08333v2","updated":"2025-07-14T11:38:36Z","published":"2025-07-11T06:25:49Z","title":"Token-based Audio Inpainting via Discrete Diffusion","summary":"  Audio inpainting refers to the task of reconstructing missing segments in\ncorrupted audio recordings. While prior approaches-including waveform and\nspectrogram-based diffusion models-have shown promising results for short gaps,\nthey often degrade in quality when gaps exceed 100 milliseconds (ms). In this\nwork, we introduce a novel inpainting method based on discrete diffusion\nmodeling, which operates over tokenized audio representations produced by a\npre-trained audio tokenizer. Our approach models the generative process\ndirectly in the discrete latent space, enabling stable and semantically\ncoherent reconstruction of missing audio. We evaluate the method on the\nMusicNet dataset using both objective and perceptual metrics across gap\ndurations up to 300 ms. We further evaluated our approach on the MTG dataset,\nextending the gap duration to 500 ms. Experimental results demonstrate that our\nmethod achieves competitive or superior performance compared to existing\nbaselines, particularly for longer gaps, offering a robust solution for\nrestoring degraded musical recordings. Audio examples of our proposed method\ncan be found at https://iftach21.github.io/\n","authors":["Tali Dror","Iftach Shoham","Moshe Buchris","Oren Gal","Haim Permuter","Gilad Katz","Eliya Nachmani"],"pdf_url":"https://arxiv.org/pdf/2507.08333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10174v1","updated":"2025-07-14T11:36:31Z","published":"2025-07-14T11:36:31Z","title":"Should We Ever Prefer Decision Transformer for Offline Reinforcement\n  Learning?","summary":"  In recent years, extensive work has explored the application of the\nTransformer architecture to reinforcement learning problems. Among these,\nDecision Transformer (DT) has gained particular attention in the context of\noffline reinforcement learning due to its ability to frame return-conditioned\npolicy learning as a sequence modeling task. Most recently, Bhargava et al.\n(2024) provided a systematic comparison of DT with more conventional MLP-based\noffline RL algorithms, including Behavior Cloning (BC) and Conservative\nQ-Learning (CQL), and claimed that DT exhibits superior performance in\nsparse-reward and low-quality data settings.\n  In this paper, through experimentation on robotic manipulation tasks\n(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered\nBehavior Cloning (FBC) achieves competitive or superior performance compared to\nDT in sparse-reward environments. FBC simply filters out low-performing\ntrajectories from the dataset and then performs ordinary behavior cloning on\nthe filtered dataset. FBC is not only very straightforward, but it also\nrequires less training data and is computationally more efficient. The results\ntherefore suggest that DT is not preferable for sparse-reward environments.\nFrom prior work, arguably, DT is also not preferable for dense-reward\nenvironments. Thus, we pose the question: Is DT ever preferable?\n","authors":["Yumi Omori","Zixuan Dong","Keith Ross"],"pdf_url":"https://arxiv.org/pdf/2507.10174v1.pdf","comment":"Accepted by RLBrew: Ingredients for Developing Generalist Agents\n  workshop (RLC 2025)"},{"id":"http://arxiv.org/abs/2507.10172v1","updated":"2025-07-14T11:35:43Z","published":"2025-07-14T11:35:43Z","title":"Play Style Identification Using Low-Level Representations of Play Traces\n  in MicroRTS","summary":"  Play style identification can provide valuable game design insights and\nenable adaptive experiences, with the potential to improve game playing agents.\nPrevious work relies on domain knowledge to construct play trace\nrepresentations using handcrafted features. More recent approaches incorporate\nthe sequential structure of play traces but still require some level of domain\nabstraction. In this study, we explore the use of unsupervised CNN-LSTM\nautoencoder models to obtain latent representations directly from low-level\nplay trace data in MicroRTS. We demonstrate that this approach yields a\nmeaningful separation of different game playing agents in the latent space,\nreducing reliance on domain expertise and its associated biases. This latent\nspace is then used to guide the exploration of diverse play styles within\nstudied AI players.\n","authors":["Ruizhe Yu Xia","Jeremy Gow","Simon Lucas"],"pdf_url":"https://arxiv.org/pdf/2507.10172v1.pdf","comment":"Accepted as Short Paper for IEEE CoG"},{"id":"http://arxiv.org/abs/2507.10170v1","updated":"2025-07-14T11:33:14Z","published":"2025-07-14T11:33:14Z","title":"Understanding the Rank of Tensor Networks via an Intuitive\n  Example-Driven Approach","summary":"  Tensor Network (TN) decompositions have emerged as an indispensable tool in\nBig Data analytics owing to their ability to provide compact low-rank\nrepresentations, thus alleviating the ``Curse of Dimensionality'' inherent in\nhandling higher-order data. At the heart of their success lies the concept of\nTN ranks, which governs the efficiency and expressivity of TN decompositions.\nHowever, unlike matrix ranks, TN ranks often lack a universal meaning and an\nintuitive interpretation, with their properties varying significantly across\ndifferent TN structures. Consequently, TN ranks are frequently treated as\nempirically tuned hyperparameters, rather than as key design parameters\ninferred from domain knowledge. The aim of this Lecture Note is therefore to\ndemystify the foundational yet frequently misunderstood concept of TN ranks\nthrough real-life examples and intuitive visualizations. We begin by\nillustrating how domain knowledge can guide the selection of TN ranks in\nwidely-used models such as the Canonical Polyadic (CP) and Tucker\ndecompositions. For more complex TN structures, we employ a self-explanatory\ngraphical approach that generalizes to tensors of arbitrary order. Such a\nperspective naturally reveals the relationship between TN ranks and the\ncorresponding ranks of tensor unfoldings (matrices), thereby circumventing\ncumbersome multi-index tensor algebra while facilitating domain-informed TN\ndesign. It is our hope that this Lecture Note will equip readers with a clear\nand unified understanding of the concept of TN rank, along with the necessary\nphysical insight and intuition to support the selection, explainability, and\ndeployment of tensor methods in both practical applications and educational\ncontexts.\n","authors":["Wuyang Zhou","Giorgos Iacovides","Kriton Konstantinidis","Ilya Kisil","Danilo Mandic"],"pdf_url":"https://arxiv.org/pdf/2507.10170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.14403v3","updated":"2025-07-14T11:21:20Z","published":"2025-05-20T14:16:49Z","title":"Unearthing Gems from Stones: Policy Optimization with Negative Sample\n  Augmentation for LLM Reasoning","summary":"  Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations.\n","authors":["Zhaohui Yang","Yuxiao Ye","Shilei Jiang","Chen Hu","Linjing Li","Shihong Deng","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2505.14403v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10160v1","updated":"2025-07-14T11:18:33Z","published":"2025-07-14T11:18:33Z","title":"Domain Borders Are There to Be Crossed With Federated Few-Shot\n  Adaptation","summary":"  Federated Learning has emerged as a leading paradigm for decentralized,\nprivacy-preserving learning, particularly relevant in the era of interconnected\nedge devices equipped with sensors. However, the practical implementation of\nFederated Learning faces three primary challenges: the need for human\ninvolvement in costly data labelling processes for target adaptation, covariate\nshift in client device data collection due to environmental factors affecting\nsensors, leading to discrepancies between source and target samples, and the\nimpracticality of continuous or regular model updates in resource-constrained\nenvironments due to limited data transmission capabilities and technical\nconstraints on channel availability and energy efficiency. To tackle these\nissues, we expand upon an efficient and scalable Federated Learning framework\ntailored for real-world client adaptation in industrial settings. This\nframework leverages a pre-trained source model comprising a deep backbone, an\nadaptation module, and a classifier running on a powerful server. By freezing\nthe backbone and classifier during client adaptation on resource-constrained\ndevices, we allow the domain adaptive linear layer to handle target domain\nadaptation, thus minimizing overall computational overhead. Furthermore, this\nsetup, designated as FedAcross+, is extended to encompass the processing of\nstreaming data, thereby rendering the solution suitable for non-stationary\nenvironments. Extensive experimental results demonstrate the effectiveness of\nFedAcross+ in achieving competitive adaptation on low-end client devices with\nlimited target samples, successfully addressing the challenge of domain shift.\nMoreover, our framework accommodates sporadic model updates within\nresource-constrained environments, ensuring practical and seamless deployment.\n","authors":["Manuel Röder","Christoph Raab","Frank-Michael Schleif"],"pdf_url":"https://arxiv.org/pdf/2507.10160v1.pdf","comment":"Extension of http://dx.doi.org/10.5220/0012351900003654"},{"id":"http://arxiv.org/abs/2507.10158v1","updated":"2025-07-14T11:17:28Z","published":"2025-07-14T11:17:28Z","title":"MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping","summary":"  Federated Learning (FL) is a promising machine learning paradigm that enables\nparticipating devices to train privacy-preserved and collaborative models. FL\nhas proven its benefits for robotic manipulation tasks. However, grasping tasks\nlack exploration in such settings where robots train a global model without\nmoving data and ensuring data privacy. The main challenge is that each robot\nlearns from data that is nonindependent and identically distributed (non-IID)\nand of low quantity. This exhibits performance degradation, particularly in\nrobotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL\napproach for robotic grasping, acknowledging the unique challenges posed by the\nnon-IID data distribution across robots, including quantitative skewness.\nMTF-Grasp harnesses data quality and quantity across robots to select a set of\n\"top-level\" robots with better data distribution and higher sample count. It\nthen utilizes top-level robots to train initial seed models and distribute them\nto the remaining \"low-level\" robots, reducing the risk of model performance\ndegradation in low-level robots. Our approach outperforms the conventional FL\nsetup by up to 8% on the quantity-skewed Cornell and Jacquard grasping\ndatasets.\n","authors":["Obaidullah Zaland","Erik Elmroth","Monowar Bhuyan"],"pdf_url":"https://arxiv.org/pdf/2507.10158v1.pdf","comment":"The work is accepted for presentation at IEEE SMC 2025"},{"id":"http://arxiv.org/abs/2507.10154v1","updated":"2025-07-14T11:04:24Z","published":"2025-07-14T11:04:24Z","title":"Simulating Biases for Interpretable Fairness in Offline and Online\n  Classifiers","summary":"  Predictive models often reinforce biases which were originally embedded in\ntheir training data, through skewed decisions. In such cases, mitigation\nmethods are critical to ensure that, regardless of the prevailing disparities,\nmodel outcomes are adjusted to be fair. To assess this, datasets could be\nsystematically generated with specific biases, to train machine learning\nclassifiers. Then, predictive outcomes could aid in the understanding of this\nbias embedding process. Hence, an agent-based model (ABM), depicting a loan\napplication process that represents various systemic biases across two\ndemographic groups, was developed to produce synthetic datasets. Then, by\napplying classifiers trained on them to predict loan outcomes, we can assess\nhow biased data leads to unfairness. This highlights a main contribution of\nthis work: a framework for synthetic dataset generation with controllable bias\ninjection. We also contribute with a novel explainability technique, which\nshows how mitigations affect the way classifiers leverage data features, via\nsecond-order Shapley values. In experiments, both offline and online learning\napproaches are employed. Mitigations are applied at different stages of the\nmodelling pipeline, such as during pre-processing and in-processing.\n","authors":["Ricardo Inácio","Zafeiris Kokkinogenis","Vitor Cerqueira","Carlos Soares"],"pdf_url":"https://arxiv.org/pdf/2507.10154v1.pdf","comment":"17 pages, 2 figures, 1 equation, 3 tables: 1 in main body and 2 in\n  the appendix. Submitted to the SynDAiTE: Synthetic Data for AI\n  Trustworthiness and Evolution workshop from ECMLPKDD 2025, anonymized"},{"id":"http://arxiv.org/abs/2507.07625v2","updated":"2025-07-14T10:55:43Z","published":"2025-07-10T10:47:42Z","title":"Concentration of measure for non-linear random matrices with\n  applications to neural networks and non-commutative polynomials","summary":"  We prove concentration inequalities for several models of non-linear random\nmatrices. As corollaries we obtain estimates for linear spectral statistics of\nthe conjugate kernel of neural networks and non-commutative polynomials in\n(possibly dependent) random matrices.\n","authors":["Radosław Adamczak"],"pdf_url":"https://arxiv.org/pdf/2507.07625v2.pdf","comment":"Some typos fixed (and some new probably introduced), small editorial\n  changes"},{"id":"http://arxiv.org/abs/2507.10143v1","updated":"2025-07-14T10:41:07Z","published":"2025-07-14T10:41:07Z","title":"Deep Recurrence for Dynamical Segmentation Models","summary":"  While biological vision systems rely heavily on feedback connections to\niteratively refine perception, most artificial neural networks remain purely\nfeedforward, processing input in a single static pass. In this work, we propose\na predictive coding inspired feedback mechanism that introduces a recurrent\nloop from output to input, allowing the model to refine its internal state over\ntime. We implement this mechanism within a standard U-Net architecture and\nintroduce two biologically motivated operations, softmax projection and\nexponential decay, to ensure stability of the feedback loop. Through controlled\nexperiments on a synthetic segmentation task, we show that the feedback model\nsignificantly outperforms its feedforward counterpart in noisy conditions and\ngeneralizes more effectively with limited supervision. Notably, feedback\nachieves above random performance with just two training examples, while the\nfeedforward model requires at least four. Our findings demonstrate that\nfeedback enhances robustness and data efficiency, and offer a path toward more\nadaptive and biologically inspired neural architectures. Code is available at:\ngithub.com/DCalhas/feedback_segmentation.\n","authors":["David Calhas","Arlindo L. Oliveira"],"pdf_url":"https://arxiv.org/pdf/2507.10143v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2507.10142v1","updated":"2025-07-14T10:39:17Z","published":"2025-07-14T10:39:17Z","title":"Adaptability in Multi-Agent Reinforcement Learning: A Framework and\n  Unified Review","summary":"  Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in\ncoordinating multiple agents across simulated benchmarks and constrained\nscenarios. However, its deployment in real-world multi-agent systems (MAS)\nremains limited, primarily due to the complex and dynamic nature of such\nenvironments. These challenges arise from multiple interacting sources of\nvariability, including fluctuating agent populations, evolving task goals, and\ninconsistent execution conditions. Together, these factors demand that MARL\nalgorithms remain effective under continuously changing system configurations\nand operational demands. To better capture and assess this capacity for\nadjustment, we introduce the concept of \\textit{adaptability} as a unified and\npractically grounded lens through which to evaluate the reliability of MARL\nalgorithms under shifting conditions, broadly referring to any changes in the\nenvironment dynamics that may occur during learning or execution. Centred on\nthe notion of adaptability, we propose a structured framework comprising three\nkey dimensions: learning adaptability, policy adaptability, and scenario-driven\nadaptability. By adopting this adaptability perspective, we aim to support more\nprincipled assessments of MARL performance beyond narrowly defined benchmarks.\nUltimately, this survey contributes to the development of algorithms that are\nbetter suited for deployment in dynamic, real-world multi-agent systems.\n","authors":["Siyi Hu","Mohamad A Hady","Jianglin Qiao","Jimmy Cao","Mahardhika Pratama","Ryszard Kowalczyk"],"pdf_url":"https://arxiv.org/pdf/2507.10142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10139v1","updated":"2025-07-14T10:36:15Z","published":"2025-07-14T10:36:15Z","title":"Large-Scale Graph Building in Dynamic Environments: Low Latency and High\n  Quality","summary":"  Learning and constructing large-scale graphs has attracted attention in\nrecent decades, resulting in a rich literature that introduced various systems,\ntools, and algorithms. Grale is one of such tools that is designed for offline\nenvironments and is deployed in more than 50 different industrial settings at\nGoogle. Grale is widely applicable because of its ability to efficiently learn\nand construct a graph on datasets with multiple types of features. However, it\nis often the case that applications require the underlying data to evolve\ncontinuously and rapidly and the updated graph needs to be available with low\nlatency. Such setting make the use of Grale prohibitive. While there are\nApproximate Nearest Neighbor (ANN) systems that handle dynamic updates with low\nlatency, they are mostly limited to similarities over a single embedding.\n  In this work, we introduce a system that inherits the advantages and the\nquality of Grale, and maintains a graph construction in a dynamic setting with\ntens of milliseconds of latency per request. We call the system Dynamic Grale\nUsing ScaNN (Dynamic GUS). Our system has a wide range of applications with\nover 10 deployments at Google. One of the applications is in Android Security\nand Privacy, where Dynamic Grale Using ScaNN enables capturing harmful\napplications 4 times faster, before they can reach users.\n","authors":["Filipe Miguel Gonçalves de Almeida","CJ Carey","Hendrik Fichtenberger","Jonathan Halcrow","Silvio Lattanzi","André Linhares","Tao Meng","Ashkan Norouzi-Fard","Nikos Parotsidis","Bryan Perozzi","David Simcha"],"pdf_url":"https://arxiv.org/pdf/2507.10139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10132v1","updated":"2025-07-14T10:23:18Z","published":"2025-07-14T10:23:18Z","title":"Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy\n  Forecasting","summary":"  Accurate forecasting of energy demand and supply is critical for optimizing\nsustainable energy systems, yet it is challenged by the variability of\nrenewable sources and dynamic consumption patterns. This paper introduces a\nneural framework that integrates continuous-time Neural Ordinary Differential\nEquations (Neural ODEs), graph attention, multi-resolution wavelet\ntransformations, and adaptive learning of frequencies to address the issues of\ntime series prediction. The model employs a robust ODE solver, using the\nRunge-Kutta method, paired with graph-based attention and residual connections\nto better understand both structural and temporal patterns. Through\nwavelet-based feature extraction and adaptive frequency modulation, it adeptly\ncaptures and models diverse, multi-scale temporal dynamics. When evaluated\nacross seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity\ntransformer temperature), and Waste, Solar, and Hydro (renewable energy), this\narchitecture consistently outperforms state-of-the-art baselines in various\nforecasting metrics, proving its robustness in capturing complex temporal\ndependencies. Furthermore, the model enhances interpretability through SHAP\nanalysis, making it suitable for sustainable energy applications.\n","authors":["Usman Gani Joy"],"pdf_url":"https://arxiv.org/pdf/2507.10132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.21278v2","updated":"2025-07-14T10:06:29Z","published":"2025-06-26T14:01:51Z","title":"Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy\n  Distribution","summary":"  We propose a novel variational autoencoder (VAE) architecture that employs a\nspherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian\nlatent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy\nprovides a more natural hyperspherical representation of latent variables,\nbetter capturing directional data while maintaining flexibility. Its\nheavy-tailed nature prevents over-regularization, ensuring efficient latent\nspace utilization while offering a more expressive representation.\nAdditionally, spCauchy circumvents the numerical instabilities inherent to vMF,\nwhich arise from computing normalization constants involving Bessel functions.\nInstead, it enables a fully differentiable and efficient reparameterization\ntrick via M\\\"obius transformations, allowing for stable and scalable training.\nThe KL divergence can be computed through a rapidly converging power series,\neliminating concerns of underflow or overflow associated with evaluation of\nratios of hypergeometric functions. These properties make spCauchy a compelling\nalternative for VAEs, offering both theoretical advantages and practical\nefficiency in high-dimensional generative modeling.\n","authors":["Lukas Sablica","Kurt Hornik"],"pdf_url":"https://arxiv.org/pdf/2506.21278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10120v1","updated":"2025-07-14T10:04:02Z","published":"2025-07-14T10:04:02Z","title":"A Variance-Reduced Cubic-Regularized Newton for Policy Optimization","summary":"  In this paper, we study a second-order approach to policy optimization in\nreinforcement learning. Existing second-order methods often suffer from\nsuboptimal sample complexity or rely on unrealistic assumptions about\nimportance sampling. To overcome these limitations, we propose VR-CR-PN, a\nvariance-reduced cubic-regularized policy Newton algorithm. To the best of our\nknowledge, this is the first algorithm that integrates Hessian-aided variance\nreduction with second-order policy optimization, effectively addressing the\ndistribution shift problem and achieving best-known sample complexity under\ngeneral nonconvex conditions but without the need for importance sampling. We\ntheoretically establish that VR-CR-PN achieves a sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ to reach an $\\epsilon$-second-order\nstationary point, significantly improving upon the previous best result of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$ under comparable assumptions. As an\nadditional contribution, we introduce a novel Hessian estimator for the\nexpected return function, which admits a uniform upper bound independent of the\nhorizon length $H$, allowing the algorithm to achieve horizon-independent\nsample complexity.\n","authors":["Cheng Sun","Zhen Zhang","Shaofu Yang"],"pdf_url":"https://arxiv.org/pdf/2507.10120v1.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2507.10119v1","updated":"2025-07-14T10:03:23Z","published":"2025-07-14T10:03:23Z","title":"Analysis of AI Techniques for Orchestrating Edge-Cloud Application\n  Migration","summary":"  Application migration in edge-cloud system enables high QoS and cost\neffective service delivery. However, automatically orchestrating such migration\nis typically solved with heuristic approaches. Starting from the Markov\nDecision Process (MDP), in this paper, we identify, analyze and compare\nselected state-of-the-art Artificial Intelligence (AI) planning and\nReinforcement Learning (RL) approaches for solving the class of edge-cloud\napplication migration problems that can be modeled as Towers of Hanoi (ToH)\nproblems. We introduce a new classification based on state space definition and\nanalyze the compared models also through this lense. The aim is to understand\navailable techniques capable of orchestrating such application migration in\nemerging computing continuum environments.\n","authors":["Sadig Gojayev","Ahmad Anaqreh","Carolina Fortuna"],"pdf_url":"https://arxiv.org/pdf/2507.10119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02882v2","updated":"2025-07-14T09:56:47Z","published":"2025-04-02T05:47:28Z","title":"DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models","summary":"  Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling.\n","authors":["Sunghee Jung","Donghun Lee","Shinbok Lee","Gaeun Seo","Daniel Lee","Byeongil Ko","Junrae Cho","Kihyun Kim","Eunggyun Kim","Myeongcheol Shin"],"pdf_url":"https://arxiv.org/pdf/2504.02882v2.pdf","comment":"Accepted to SIGDIAL 2025"},{"id":"http://arxiv.org/abs/2506.01635v3","updated":"2025-07-14T09:32:28Z","published":"2025-06-02T13:12:02Z","title":"Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces","summary":"  Temporal alignment of multiple signals through time warping is crucial in\nmany fields, such as classification within speech recognition or robot motion\nlearning. Almost all related works are limited to data in Euclidean space.\nAlthough an attempt was made in 2011 to adapt this concept to unit quaternions,\na general extension to Riemannian manifolds remains absent. Given its\nimportance for numerous applications in robotics and beyond, we introduce\nRiemannian Time Warping (RTW). This novel approach efficiently aligns multiple\nsignals by considering the geometric structure of the Riemannian manifold in\nwhich the data is embedded. Extensive experiments on synthetic and real-world\ndata, including tests with an LBR iiwa robot, demonstrate that RTW consistently\noutperforms state-of-the-art baselines in both averaging and classification\ntasks.\n","authors":["Julian Richter","Christopher A. Erdös","Christian Scheurer","Jochen J. Steil","Niels Dehio"],"pdf_url":"https://arxiv.org/pdf/2506.01635v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15595v3","updated":"2025-07-14T09:25:43Z","published":"2024-10-21T02:27:24Z","title":"A Comprehensive Survey of Direct Preference Optimization: Datasets,\n  Theories, Variants, and Applications","summary":"  With the rapid advancement of large language models (LLMs), aligning policy\nmodels with human preferences has become increasingly critical. Direct\nPreference Optimization (DPO) has emerged as a promising approach for\nalignment, acting as an RL-free alternative to Reinforcement Learning from\nHuman Feedback (RLHF). Despite DPO's various advancements and inherent\nlimitations, an in-depth review of these aspects is currently lacking in the\nliterature. In this work, we present a comprehensive review of the challenges\nand opportunities in DPO, covering theoretical analyses, variants, relevant\npreference datasets, and applications. Specifically, we categorize recent\nstudies on DPO based on key research questions to provide a thorough\nunderstanding of DPO's current landscape. Additionally, we propose several\nfuture research directions to offer insights on model alignment for the\nresearch community. An updated collection of relevant papers can be found on\nhttps://github.com/Mr-Loevan/DPO-Survey.\n","authors":["Wenyi Xiao","Zechuan Wang","Leilei Gan","Shuai Zhao","Zongrui Li","Ruirui Lei","Wanggui He","Luu Anh Tuan","Long Chen","Hao Jiang","Zhou Zhao","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2410.15595v3.pdf","comment":"45 pages, 12 Figures. Project page:\n  https://github.com/Mr-Loevan/DPO-Survey"},{"id":"http://arxiv.org/abs/2411.19700v4","updated":"2025-07-14T09:22:58Z","published":"2024-11-29T13:42:10Z","title":"Explaining the Impact of Training on Vision Models via Activation\n  Clustering","summary":"  This paper introduces Neuro-Activated Vision Explanations (NAVE), a method\nfor extracting and visualizing the internal representations of vision model\nencoders. By clustering feature activations, NAVE provides insights into\nlearned semantics without fine-tuning. Using object localization, we show that\nNAVE's concepts align with image semantics. Through extensive experiments, we\nanalyze the impact of training strategies and architectures on encoder\nrepresentation capabilities. Additionally, we apply NAVE to study training\nartifacts in vision transformers and reveal how weak training strategies and\nspurious correlations degrade model performance. Our findings establish NAVE as\na valuable tool for post-hoc model inspection and improving transparency in\nvision models.\n","authors":["Ahcène Boubekki","Samuel G. Fadel","Sebastian Mair"],"pdf_url":"https://arxiv.org/pdf/2411.19700v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00200v2","updated":"2025-07-14T09:19:59Z","published":"2025-05-30T20:12:51Z","title":"Structuring Radiology Reports: Challenging LLMs with Lightweight Models","summary":"  Radiology reports are critical for clinical decision-making but often lack a\nstandardized format, limiting both human interpretability and machine learning\n(ML) applications. While large language models (LLMs) have shown strong\ncapabilities in reformatting clinical text, their high computational\nrequirements, lack of transparency, and data privacy concerns hinder practical\ndeployment. To address these challenges, we explore lightweight encoder-decoder\nmodels (<300M parameters)-specifically T5 and BERT2BERT-for structuring\nradiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark\nthese models against eight open-source LLMs (1B-70B), adapted using prefix\nprompting, in-context learning (ICL), and low-rank adaptation (LoRA)\nfinetuning. Our best-performing lightweight model outperforms all LLMs adapted\nusing prompt-based techniques on a human-annotated test set. While some\nLoRA-finetuned LLMs achieve modest gains over the lightweight model on the\nFindings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,\nGREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of\nsubstantially greater computational resources. For example, LLaMA-3-70B\nincurred more than 400 times the inference time, cost, and carbon emissions\ncompared to the lightweight model. These results underscore the potential of\nlightweight, task-specific models as sustainable and privacy-preserving\nsolutions for structuring clinical text in resource-constrained healthcare\nsettings.\n","authors":["Johannes Moll","Louisa Fay","Asfandyar Azhar","Sophie Ostmeier","Tim Lueth","Sergios Gatidis","Curtis Langlotz","Jean-Benoit Delbrouck"],"pdf_url":"https://arxiv.org/pdf/2506.00200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07837v2","updated":"2025-07-14T09:19:32Z","published":"2024-11-12T14:41:07Z","title":"FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for\n  Scalable Training","summary":"  With the increase in the number of parameters in large language models, the\nprocess of pre-training and fine-tuning increasingly demands larger volumes of\nGPU memory. A significant portion of this memory is typically consumed by the\noptimizer state. To overcome this challenge, recent approaches such as low-rank\nadaptation (LoRA (Hu et al., 2021)), low-rank gradient projection (GaLore (Zhao\net al., 2024)), and blockwise optimization (BAdam (Luo et al., 2024)) have been\nproposed. However, in all these algorithms, the $\\textit{effective rank of the\nweight updates remains low-rank}$, which can lead to a substantial loss of\ninformation from the gradient. This loss can be critically important,\nespecially during the pre-training stage. In this paper, we introduce\n$\\texttt{FRUGAL}$ ($\\textbf{F}$ull-$\\textbf{R}$ank $\\textbf{U}$pdates with\n$\\textbf{G}$r$\\textbf{A}$dient sp$\\textbf{L}$itting), a new memory-efficient\noptimization framework. $\\texttt{FRUGAL}$ leverages gradient splitting to\nperform low-dimensional updates using advanced algorithms (such as Adam), while\nupdates along the remaining directions are executed via state-free methods like\nSGD or signSGD (Bernstein et al., 2018). Our framework can be integrated with\nvarious low-rank update selection techniques, including GaLore and BAdam. We\nprovide theoretical convergence guarantees for our framework when using SGDM\nfor low-dimensional updates and SGD for state-free updates. Additionally, our\nmethod consistently outperforms concurrent approaches across various fixed\nmemory budgets, achieving state-of-the-art results in pre-training and\nfine-tuning tasks while balancing memory efficiency and performance metrics.\n","authors":["Philip Zmushko","Aleksandr Beznosikov","Martin Takáč","Samuel Horváth"],"pdf_url":"https://arxiv.org/pdf/2411.07837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02687v2","updated":"2025-07-14T09:17:27Z","published":"2025-03-04T15:02:07Z","title":"Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D\n  Object Detection with Radar Point Clouds?","summary":"  Due to the significant effort required for data collection and annotation in\n3D perception tasks, mixed sample data augmentation (MSDA) has been widely\nstudied to generate diverse training samples by mixing existing data. Recently,\nmany MSDA techniques have been developed for point clouds, but they mainly\ntarget LiDAR data, leaving their application to radar point clouds largely\nunexplored. In this paper, we examine the feasibility of applying existing MSDA\nmethods to radar point clouds and identify several challenges in adapting these\ntechniques. These obstacles stem from the radar's irregular angular\ndistribution, deviations from a single-sensor polar layout in multi-radar\nsetups, and point sparsity. To address these issues, we propose Class-Aware\nPillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar\nlevel in 3D point clouds, guided by class labels. Unlike methods that rely a\nsingle mix ratio to the entire sample, CAPMix assigns an independent ratio to\neach pillar, boosting sample diversity. To account for the density of different\nclasses, we use class-specific distributions: for dense objects (e.g., large\nvehicles), we skew ratios to favor points from another sample, while for sparse\nobjects (e.g., pedestrians), we sample more points from the original. This\nclass-aware mixing retains critical details and enriches each sample with new\ninformation, ultimately generating more diverse training data. Experimental\nresults demonstrate that our method not only significantly boosts performance\nbut also outperforms existing MSDA approaches across two datasets (Bosch Street\nand K-Radar). We believe that this straightforward yet effective approach will\nspark further investigation into MSDA techniques for radar data.\n","authors":["Miao Zhang","Sherif Abdulatif","Benedikt Loesch","Marco Altmann","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.02687v2.pdf","comment":"8 pages, 6 figures, 4 tables, accepted to 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2025)"},{"id":"http://arxiv.org/abs/2507.10088v1","updated":"2025-07-14T09:15:22Z","published":"2025-07-14T09:15:22Z","title":"Towards High Supervised Learning Utility Training Data Generation: Data\n  Pruning and Column Reordering","summary":"  Tabular data synthesis for supervised learning ('SL') model training is\ngaining popularity in industries such as healthcare, finance, and retail.\nDespite the progress made in tabular data generators, models trained with\nsynthetic data often underperform compared to those trained with original data.\nThis low SL utility of synthetic data stems from class imbalance exaggeration\nand SL data relationship overlooked by tabular generator. To address these\nchallenges, we draw inspirations from techniques in emerging data-centric\nartificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel\npipeline that integrates data-centric techniques into tabular data synthesis.\nPRRO incorporates data pruning to guide the table generator towards\nobservations with high signal-to-noise ratio, ensuring that the class\ndistribution of synthetic data closely matches that of the original data.\nBesides, PRRO employs a column reordering algorithm to align the data modeling\nstructure of generators with that of SL models. These two modules enable PRRO\nto optimize SL utility of synthetic data. Empirical experiments on 22 public\ndatasets show that synthetic data generated using PRRO enhances predictive\nperformance compared to data generated without PRRO. Specifically, synthetic\nreplacement of original data yields an average improvement of 26.74% and up to\n871.46% improvement using PRRO, while synthetic appendant to original data\nresults with PRRO-generated data results in an average improvement of 6.13% and\nup to 200.32%. Furthermore, experiments on six highly imbalanced datasets show\nthat PRRO enables the generator to produce synthetic data with a class\ndistribution that resembles the original data more closely, achieving a\nsimilarity improvement of 43%. Through PRRO, we foster a seamless integration\nof data synthesis to subsequent SL prediction, promoting quality and accessible\ndata analysis.\n","authors":["Tung Sum Thomas Kwok","Zeyong Zhang","Chi-Hua Wang","Guang Cheng"],"pdf_url":"https://arxiv.org/pdf/2507.10088v1.pdf","comment":"Accepted by Agentic & GenAI Evaluation KDD2025"},{"id":"http://arxiv.org/abs/2507.10084v1","updated":"2025-07-14T09:11:33Z","published":"2025-07-14T09:11:33Z","title":"A Transfer Learning-Based Method for Water Body Segmentation in Remote\n  Sensing Imagery: A Case Study of the Zhada Tulin Area","summary":"  To address the prevalent challenges of domain shift and small sample sizes in\nremote sensing image water body segmentation, this study proposes and validates\na two-stage transfer learning strategy based on the SegFormer model. The\napproach begins by training a foundational segmentation model on a diverse\nsource domain, where it achieves an Intersection over Union (IoU) of 68.80% on\nits validation set, followed by fine-tuning on data from the distinct target\ndomain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by\nhighly complex topography and spectral features -- the experimental results\ndemonstrate that this strategy significantly boosts the IoU for the water body\nsegmentation task from 25.50% (for direct transfer) to 64.84%. This not only\neffectively resolves the model performance degradation caused by domain\ndiscrepancy but also provides an effective technical paradigm for\nhigh-precision thematic information extraction in data-scarce and\nenvironmentally unique remote sensing scenarios.\n","authors":["Haonan Chen","Xin Tong"],"pdf_url":"https://arxiv.org/pdf/2507.10084v1.pdf","comment":"13 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2506.00533v3","updated":"2025-07-14T09:06:25Z","published":"2025-05-31T12:40:02Z","title":"RsGCN: Rescaling Enhances Generalization of GCNs for Solving Scalable\n  Traveling Salesman Problems","summary":"  Neural traveling salesman problem (TSP) solvers face two critical challenges:\npoor generalization for scalable TSPs and high training costs. To address these\nchallenges, we propose a new Rescaling Graph Convolutional Network (RsGCN).\nFocusing on the scale-dependent features (i.e., features varied with problem\nscales) related to nodes and edges that influence the sensitivity of GCNs to\nthe problem scales, a Rescaling Mechanism in RsGCN enhances the generalization\ncapability by (1) rescaling adjacent nodes to construct a subgraph with a\nuniform number of adjacent nodes for each node across various scales of TSPs,\nwhich stabilizes the graph message aggregation; (2) rescaling subgraph edges to\nadjust the lengths of subgraph edges to the same magnitude, which maintains\nnumerical consistency. In addition, an efficient training strategy with a\nmixed-scale dataset and bidirectional loss is used in RsGCN. To fully exploit\nthe heatmaps generated by RsGCN, we design an efficient post-search algorithm\ntermed Re2Opt, in which a reconstruction process based on adaptive weight is\nincorporated to help avoid local optima. Based on a combined architecture of\nRsGCN and Re2Opt, our solver achieves remarkable generalization and low\ntraining cost: with only 3 epochs of training on the mixed-scale dataset\ncontaining instances with up to 100 nodes, it can be generalized successfully\nto 10K-node instances without any fine-tuning. Extensive experiments\ndemonstrate our state-of-the-art performance across uniform distribution\ninstances of 9 different scales from 20 to 10K nodes and 78 real-world\ninstances from TSPLIB, while requiring the fewest learnable parameters and\ntraining epochs among neural competitors.\n","authors":["Junquan Huang","Zong-Gan Chen","Yuncheng Jiang","Zhi-Hui Zhan"],"pdf_url":"https://arxiv.org/pdf/2506.00533v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10078v1","updated":"2025-07-14T09:03:44Z","published":"2025-07-14T09:03:44Z","title":"Compression Method for Deep Diagonal State Space Model Based on $H^2$\n  Optimal Reduction","summary":"  Deep learning models incorporating linear SSMs have gained attention for\ncapturing long-range dependencies in sequential data. However, their large\nparameter sizes pose challenges for deployment on resource-constrained devices.\nIn this study, we propose an efficient parameter reduction method for these\nmodels by applying $H^{2}$ model order reduction techniques from control theory\nto their linear SSM components. In experiments, the LRA benchmark results show\nthat the model compression based on our proposed method outperforms an existing\nmethod using the Balanced Truncation, while successfully reducing the number of\nparameters in the SSMs to $1/32$ without sacrificing the performance of the\noriginal models.\n","authors":["Hiroki Sakamoto","Kazuhiro Sato"],"pdf_url":"https://arxiv.org/pdf/2507.10078v1.pdf","comment":"Accepted to IEEE Control Systems Letters"},{"id":"http://arxiv.org/abs/2507.00038v2","updated":"2025-07-14T09:02:45Z","published":"2025-06-19T06:59:19Z","title":"Quality over Quantity: An Effective Large-Scale Data Reduction Strategy\n  Based on Pointwise V-Information","summary":"  In order to increase the effectiveness of model training, data reduction is\nessential to data-centric AI. It does this by locating the most instructive\nexamples in massive datasets. To increase data quality and training efficiency,\nthe main difficulty is to choose the best examples rather than the complete\ndatasets. In this paper, we propose an effective data reduction strategy based\non Pointwise -Information (PVI). To enable a static method, we first use PVI to\nquantify instance difficulty and remove instances with low difficulty.\nExperiments show that the classifier performance is maintained with only a\n0.0001% to 0.76% reduction in accuracy when 10%-30% of the data is removed.\nSecond, we train the classifiers using a progressive learning strategy on\nexamples sorted by increasing PVI, accelerating convergence and achieving a\n0.8% accuracy gain over conventional training. Our findings imply that training\na classifier on the chosen optimal subset may improve model performance and\nincrease training efficiency when combined with an efficient data reduction\nstrategy. Furthermore, we have adapted the PVI framework, which was previously\nlimited to English datasets, to a variety of Chinese NLP tasks and base models,\nyielding insightful results for faster training and cross-lingual data\nreduction. The codes are released at\nhttps://github.com/zhouwenchi/DatasetReductionStrategy.\n","authors":["Fei Chen","Wenchi Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.00038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10069v1","updated":"2025-07-14T08:53:48Z","published":"2025-07-14T08:53:48Z","title":"ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism","summary":"  Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).\n","authors":["Zedong Liu","Shenggan Cheng","Guangming Tan","Yang You","Dingwen Tao"],"pdf_url":"https://arxiv.org/pdf/2507.10069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15221v2","updated":"2025-07-14T08:43:18Z","published":"2025-03-19T14:01:16Z","title":"A Vector-Quantized Foundation Model for Patient Behavior Monitoring","summary":"  Foundation models have achieved remarkable success across various domains,\nyet their adoption in healthcare remains limited. While significant advances\nhave been made in medical imaging, genetic biomarkers, and time series from\nelectronic health records, the potential of foundation models for patient\nbehavior monitoring through personal digital devices remains underexplored. The\ndata generated by these devices are inherently heterogeneous, multisource, and\noften exhibit high rates of missing data, posing unique challenges. This paper\nintroduces a novel foundation model based on a modified vector quantized\nvariational autoencoder, specifically designed to process real-world data from\nsmartphones and wearable devices. We leveraged the discrete latent\nrepresentation of this model to effectively perform two downstream tasks,\nsuicide risk assessment and emotional state prediction, on different held-out\nclinical cohorts without the need of fine-tuning. We also highlight the\nexistence of a trade-off between discrete and continuous latent structures,\nsuggesting that hybrid models may be optimal for balancing accuracy across\nvarious supervised and unsupervised tasks.\n","authors":["Rodrigo Oliver","Josué Pérez-Sabater","Leire Paz-Arbaizar","Diego Herrero-Quevedo","Antonio Artés-Rodríguez","Alejandro Lancho","Pablo M. Olmos"],"pdf_url":"https://arxiv.org/pdf/2503.15221v2.pdf","comment":"10 pages (32 with references and supplementary material). Submitted\n  to IEEE Journal of Biomedical and Health Informatics"},{"id":"http://arxiv.org/abs/2507.10057v1","updated":"2025-07-14T08:41:53Z","published":"2025-07-14T08:41:53Z","title":"PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware\n  Query Optimization","summary":"  Scientific paper retrieval, particularly framed as document-to-document\nretrieval, aims to identify relevant papers in response to a long-form query\npaper, rather than a short query string. Previous approaches to this task have\nfocused on abstracts, embedding them into dense vectors as surrogates for full\ndocuments and calculating similarity across them, although abstracts provide\nonly sparse and high-level summaries. To address this, we propose PRISM, a\nnovel document-to-document retrieval method that introduces multiple,\nfine-grained representations for both the query and candidate papers. In\nparticular, each query paper is decomposed into multiple aspect-specific views\nand individually embedded, which are then matched against candidate papers\nsimilarity segmented to consider their multifaceted dimensions. Moreover, we\npresent SciFullBench, a novel benchmark in which the complete and segmented\ncontext of full papers for both queries and candidates is available. Then,\nexperimental results show that PRISM improves performance by an average of 4.3%\nover existing retrieval baselines.\n","authors":["Sangwoo Park","Jinheon Baek","Soyeong Jeong","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2507.10057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10056v1","updated":"2025-07-14T08:40:46Z","published":"2025-07-14T08:40:46Z","title":"Lightweight Model for Poultry Disease Detection from Fecal Images Using\n  Multi-Color Space Feature Optimization and Machine Learning","summary":"  Poultry farming is a vital component of the global food supply chain, yet it\nremains highly vulnerable to infectious diseases such as coccidiosis,\nsalmonellosis, and Newcastle disease. This study proposes a lightweight machine\nlearning-based approach to detect these diseases by analyzing poultry fecal\nimages. We utilize multi-color space feature extraction (RGB, HSV, LAB) and\nexplore a wide range of color, texture, and shape-based descriptors, including\ncolor histograms, local binary patterns (LBP), wavelet transforms, and edge\ndetectors. Through a systematic ablation study and dimensionality reduction\nusing PCA and XGBoost feature selection, we identify a compact global feature\nset that balances accuracy and computational efficiency. An artificial neural\nnetwork (ANN) classifier trained on these features achieved 95.85% accuracy\nwhile requiring no GPU and only 638 seconds of execution time in Google Colab.\nCompared to deep learning models such as Xception and MobileNetV3, our proposed\nmodel offers comparable accuracy with drastically lower resource usage. This\nwork demonstrates a cost-effective, interpretable, and scalable alternative to\ndeep learning for real-time poultry disease detection in low-resource\nagricultural settings.\n","authors":["A. K. M. Shoriful Islam","Md. Rakib Hassan","Macbah Uddin","Md. Shahidur Rahman"],"pdf_url":"https://arxiv.org/pdf/2507.10056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13957v2","updated":"2025-07-14T08:34:23Z","published":"2024-12-18T15:37:09Z","title":"Self-attentive Transformer for Fast and Accurate Postprocessing of\n  Temperature and Wind Speed Forecasts","summary":"  Current postprocessing techniques often require separate models for each lead\ntime and disregard possible inter-ensemble relationships by either correcting\neach member separately or by employing distributional approaches. In this work,\nwe tackle these shortcomings with an innovative, fast and accurate Transformer\nwhich postprocesses each ensemble member individually while allowing\ninformation exchange across variables, spatial dimensions and lead times by\nmeans of multi-headed self-attention. Weather forecasts are postprocessed over\n20 lead times simultaneously while including up to fifteen meteorological\npredictors. We use the EUPPBench dataset for training which contains ensemble\npredictions from the European Center for Medium-range Weather Forecasts'\nintegrated forecasting system alongside corresponding observations. The work\npresented here is the first to postprocess the ten and one hundred-meter wind\nspeed forecasts within this benchmark dataset, while also correcting two-meter\ntemperature. Our approach significantly improves the original forecasts, as\nmeasured by the CRPS, with 16.5\\% for two-meter temperature, 10\\% for ten-meter\nwind speed and 9\\% for one hundred-meter wind speed, outperforming a classical\nmember-by-member approach employed as a competitive benchmark. Furthermore,\nbeing up to six times faster, it fulfills the demand for rapid operational\nweather forecasts in various downstream applications, including renewable\nenergy forecasting.\n","authors":["Aaron Van Poecke","Tobias Sebastian Finn","Ruoke Meng","Joris Van den Bergh","Geert Smet","Jonathan Demaeyer","Piet Termonia","Hossein Tabari","Peter Hellinckx"],"pdf_url":"https://arxiv.org/pdf/2412.13957v2.pdf","comment":"23 pages, 9 figures, Accepted for publication in Artificial\n  Intelligence for the Earth Systems (AIES)"},{"id":"http://arxiv.org/abs/2502.15902v2","updated":"2025-07-14T08:34:03Z","published":"2025-02-21T19:41:32Z","title":"IPAD: Inverse Prompt for AI Detection -- A Robust and Explainable\n  LLM-Generated Text Detector","summary":"  Large Language Models (LLMs) have attained human-level fluency in text\ngeneration, which complicates the distinction between human-written and\nLLM-generated texts. This increases the risk of misuse and highlights the need\nfor reliable detectors. Yet, existing detectors exhibit poor robustness on\nout-of-distribution (OOD) data and attacked data, which is critical for\nreal-world scenarios. Also, they struggle to provide interpretable evidence to\nsupport their decisions, thus undermining the reliability. In light of these\nchallenges, we propose IPAD (Inverse Prompt for AI Detection), a novel\nframework consisting of a Prompt Inverter that identifies predicted prompts\nthat could have generated the input text, and two Distinguishers that examine\nthe probability that the input texts align with the predicted prompts.\nEmpirical evaluations demonstrate that IPAD outperforms the strongest baselines\nby 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on\nout-of-distribution (OOD) data, and 5.48% (AUROC) on attacked data. IPAD also\nperforms robustly on structured datasets. Furthermore, an interpretability\nassessment is conducted to illustrate that IPAD enhances the AI detection\ntrustworthiness by allowing users to directly examine the decision-making\nevidence, which provides interpretable support for its state-of-the-art\ndetection results.\n","authors":["Zheng Chen","Yushi Feng","Changyang He","Yue Deng","Hongxi Pu","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2502.15902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.09294v2","updated":"2025-07-14T08:29:44Z","published":"2025-05-14T11:22:22Z","title":"On the Learning with Augmented Class via Forests","summary":"  Decision trees and forests have achieved successes in various real\napplications, most working with all testing classes known in training data. In\nthis work, we focus on learning with augmented class via forests, where an\naugmented class may appear in testing data yet not in training data. We\nincorporate information of augmented class into trees' splitting, that is,\naugmented Gini impurity, a new splitting criterion is introduced to exploit\nsome unlabeled data from testing distribution. We then develop the Learning\nwith Augmented Class via Forests (short for LACForest) approach, which\nconstructs shallow forests according to the augmented Gini impurity and then\nsplits forests with pseudo-labeled augmented instances for better performance.\nWe also develop deep neural forests via an optimization objective based on our\naugmented Gini impurity, which essentially utilizes the representation power of\nneural networks for forests. Theoretically, we present the convergence analysis\nfor our augmented Gini impurity, and we finally conduct experiments to evaluate\nour approaches. The code is available at https://github.com/nju-xuf/LACForest.\n","authors":["Fan Xu","Wuyang Chen","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2505.09294v2.pdf","comment":"Accepted by IJCAI 2025"},{"id":"http://arxiv.org/abs/2507.10048v1","updated":"2025-07-14T08:27:42Z","published":"2025-07-14T08:27:42Z","title":"On the Efficiency of Training Robust Decision Trees","summary":"  As machine learning gets adopted into the industry quickly, trustworthiness\nis increasingly in focus. Yet, efficiency and sustainability of robust training\npipelines still have to be established. In this work, we consider a simple\npipeline for training adversarially robust decision trees and investigate the\nefficiency of each step. Our pipeline consists of three stages. Firstly, we\nchoose the perturbation size automatically for each dataset. For that, we\nintroduce a simple algorithm, instead of relying on intuition or prior work.\nMoreover, we show that the perturbation size can be estimated from smaller\nmodels than the one intended for full training, and thus significant gains in\nefficiency can be achieved. Secondly, we train state-of-the-art adversarial\ntraining methods and evaluate them regarding both their training time and\nadversarial accuracy. Thirdly, we certify the robustness of each of the models\nthus obtained and investigate the time required for this. We find that\nverification time, which is critical to the efficiency of the full pipeline, is\nnot correlated with training time.\n","authors":["Benedict Gerlach","Marie Anastacio","Holger H. Hoos"],"pdf_url":"https://arxiv.org/pdf/2507.10048v1.pdf","comment":"Presented as a poster at SAIV 2025"},{"id":"http://arxiv.org/abs/2507.08505v2","updated":"2025-07-14T08:25:14Z","published":"2025-07-11T11:30:57Z","title":"Efficient Deployment of Vision-Language Models on Mobile Devices: A Case\n  Study on OnePlus 13R","summary":"  Vision-Language Models (VLMs) offer promising capabilities for mobile\ndevices, but their deployment faces significant challenges due to computational\nlimitations and energy inefficiency, especially for real-time applications.\nThis study provides a comprehensive survey of deployment frameworks for VLMs on\nmobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of\nrunning LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads\non a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R\nwhile running VLMs, with measurements covering CPU, GPU, and NPU utilization,\ntemperature, inference time, power consumption, and user experience.\nBenchmarking revealed critical performance bottlenecks across frameworks: CPU\nresources were consistently over-utilized during token generation, while GPU\nand NPU accelerators were largely unused. When the GPU was used, primarily for\nimage feature extraction, it was saturated, leading to degraded device\nresponsiveness. The study contributes framework-level benchmarks, practical\nprofiling tools, and an in-depth analysis of hardware utilization bottlenecks,\nhighlighting the consistent overuse of CPUs and the ineffective or unstable use\nof GPUs and NPUs in current deployment frameworks.\n","authors":["Pablo Robin Guerrero","Yueyang Pan","Sanidhya Kashyap"],"pdf_url":"https://arxiv.org/pdf/2507.08505v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03701v3","updated":"2025-07-14T08:23:39Z","published":"2025-02-06T01:22:23Z","title":"First-ish Order Methods: Hessian-aware Scalings of Gradient Descent","summary":"  Gradient descent is the primary workhorse for optimizing large-scale problems\nin machine learning. However, its performance is highly sensitive to the choice\nof the learning rate. A key limitation of gradient descent is its lack of\nnatural scaling, which often necessitates expensive line searches or heuristic\ntuning to determine an appropriate step size. In this paper, we address this\nlimitation by incorporating Hessian information to scale the gradient\ndirection. By accounting for the curvature of the function along the gradient,\nour adaptive, Hessian-aware scaling method ensures a local unit step size\nguarantee, even in nonconvex settings. Near a local minimum that satisfies the\nsecond-order sufficient conditions, our approach achieves linear convergence\nwith a unit step size. We show that our method converges globally under a\nsignificantly weaker version of the standard Lipschitz gradient smoothness\nassumption. Even when Hessian information is inexact, the local unit step size\nguarantee and global convergence properties remain valid under mild conditions.\nFinally, we validate our theoretical results empirically on a range of convex\nand nonconvex machine learning tasks, showcasing the effectiveness of the\napproach.\n","authors":["Oscar Smee","Fred Roosta","Stephen J. Wright"],"pdf_url":"https://arxiv.org/pdf/2502.03701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10039v1","updated":"2025-07-14T08:16:58Z","published":"2025-07-14T08:16:58Z","title":"Towards Applying Large Language Models to Complement Single-Cell\n  Foundation Models","summary":"  Single-cell foundation models such as scGPT represent a significant\nadvancement in single-cell omics, with an ability to achieve state-of-the-art\nperformance on various downstream biological tasks. However, these models are\ninherently limited in that a vast amount of information in biology exists as\ntext, which they are unable to leverage. There have therefore been several\nrecent works that propose the use of LLMs as an alternative to single-cell\nfoundation models, achieving competitive results. However, there is little\nunderstanding of what factors drive this performance, along with a strong focus\non using LLMs as an alternative, rather than complementary approach to\nsingle-cell foundation models. In this study, we therefore investigate what\nbiological insights contribute toward the performance of LLMs when applied to\nsingle-cell data, and introduce scMPT; a model which leverages synergies\nbetween scGPT, and single-cell representations from LLMs that capture these\ninsights. scMPT demonstrates stronger, more consistent performance than either\nof its component models, which frequently have large performance gaps between\neach other across datasets. We also experiment with alternate fusion methods,\ndemonstrating the potential of combining specialized reasoning models with\nscGPT to improve performance. This study ultimately showcases the potential for\nLLMs to complement single-cell foundation models and drive improvements in\nsingle-cell analysis.\n","authors":["Steven Palayew","Bo Wang","Gary Bader"],"pdf_url":"https://arxiv.org/pdf/2507.10039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10029v1","updated":"2025-07-14T08:08:55Z","published":"2025-07-14T08:08:55Z","title":"Memory-Efficient Personalization of Text-to-Image Diffusion Models via\n  Selective Optimization Strategies","summary":"  Memory-efficient personalization is critical for adapting text-to-image\ndiffusion models while preserving user privacy and operating within the limited\ncomputational resources of edge devices. To this end, we propose a selective\noptimization framework that adaptively chooses between backpropagation on\nlow-resolution images (BP-low) and zeroth-order optimization on high-resolution\nimages (ZO-high), guided by the characteristics of the diffusion process. As\nobserved in our experiments, BP-low efficiently adapts the model to\ntarget-specific features, but suffers from structural distortions due to\nresolution mismatch. Conversely, ZO-high refines high-resolution details with\nminimal memory overhead but faces slow convergence when applied without prior\nadaptation. By complementing both methods, our framework leverages BP-low for\neffective personalization while using ZO-high to maintain structural\nconsistency, achieving memory-efficient and high-quality fine-tuning. To\nmaximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware\nprobabilistic function that dynamically selects the appropriate optimization\nstrategy based on diffusion timesteps. This function mitigates the overfitting\nfrom BP-low at high timesteps, where structural information is critical, while\nensuring ZO-high is applied more effectively as training progresses.\nExperimental results demonstrate that our method achieves competitive\nperformance while significantly reducing memory consumption, enabling scalable,\nhigh-quality on-device personalization without increasing inference latency.\n","authors":["Seokeon Choi","Sunghyun Park","Hyoungwoo Park","Jeongho Kim","Sungrack Yun"],"pdf_url":"https://arxiv.org/pdf/2507.10029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13910v2","updated":"2025-07-14T08:06:34Z","published":"2024-04-22T06:42:21Z","title":"Integrated Gradient Correlation: a Dataset-wise Attribution Method","summary":"  Attribution methods are primarily designed to study input component\ncontributions to individual model predictions. However, some research\napplications require a summary of attribution patterns across the entire\ndataset to facilitate the interpretability of the scrutinized models at a\ntask-level rather than an instance-level. It specifically applies when the\nlocalization of important input information is supposed to be stable for a\nspecific problem but remains unidentified among numerous components. In this\npaper, we present a dataset-wise attribution method called Integrated Gradient\nCorrelation (IGC) that enables region-specific analysis by a direct summation\nover associated components, and further relates the sum of all attributions to\na model prediction score (correlation). We demonstrate IGC on synthetic data\nand fMRI neural signals (NSD dataset) with the study of the representation of\nimage features in the brain and the estimation of the visual receptive field of\nneural populations. The resulting IGC attributions reveal selective patterns,\ncoherent with respective model objectives.\n","authors":["Pierre Lelièvre","Chien-Chung Chen"],"pdf_url":"https://arxiv.org/pdf/2404.13910v2.pdf","comment":"16 pages, 6 figures, source code at\n  https://github.com/plelievre/int_grad_corr"},{"id":"http://arxiv.org/abs/2507.08563v2","updated":"2025-07-14T08:04:31Z","published":"2025-07-11T13:05:35Z","title":"STRAP: Spatial-Temporal Risk-Attentive Vehicle Trajectory Prediction for\n  Autonomous Driving","summary":"  Accurate vehicle trajectory prediction is essential for ensuring safety and\nefficiency in fully autonomous driving systems. While existing methods\nprimarily focus on modeling observed motion patterns and interactions with\nother vehicles, they often neglect the potential risks posed by the uncertain\nor aggressive behaviors of surrounding vehicles. In this paper, we propose a\nnovel spatial-temporal risk-attentive trajectory prediction framework that\nincorporates a risk potential field to assess perceived risks arising from\nbehaviors of nearby vehicles. The framework leverages a spatial-temporal\nencoder and a risk-attentive feature fusion decoder to embed the risk potential\nfield into the extracted spatial-temporal feature representations for\ntrajectory prediction. A risk-scaled loss function is further designed to\nimprove the prediction accuracy of high-risk scenarios, such as short relative\nspacing. Experiments on the widely used NGSIM and HighD datasets demonstrate\nthat our method reduces average prediction errors by 4.8% and 31.2%\nrespectively compared to state-of-the-art approaches, especially in high-risk\nscenarios. The proposed framework provides interpretable, risk-aware\npredictions, contributing to more robust decision-making for autonomous driving\nsystems.\n","authors":["Xinyi Ning","Zilin Bian","Dachuan Zuo","Semiha Ergan"],"pdf_url":"https://arxiv.org/pdf/2507.08563v2.pdf","comment":"6 pages, 3 figures, accepted at ITSC 2025"},{"id":"http://arxiv.org/abs/2111.06614v3","updated":"2025-07-14T07:57:25Z","published":"2021-11-12T09:03:19Z","title":"Collaboration Promotes Group Resilience in Multi-Agent AI","summary":"  To effectively operate in various dynamic scenarios, RL agents must be\nresilient to unexpected changes in their environment. Previous work on this\nform of resilience has focused on single-agent settings. In this work, we\nintroduce and formalize a multi-agent variant of resilience, which we term\ngroup resilience. We further hypothesize that collaboration with other agents\nis key to achieving group resilience; collaborating agents adapt better to\nenvironmental perturbations in multi-agent reinforcement learning (MARL)\nsettings. We test our hypothesis empirically by evaluating different\ncollaboration protocols and examining their effect on group resilience. Our\nexperiments show that all the examined collaborative approaches achieve higher\ngroup resilience than their non-collaborative counterparts.\n","authors":["Sarah Keren","Matthias Gerstgrasser","Ofir Abu","Jeffrey Rosenschein"],"pdf_url":"https://arxiv.org/pdf/2111.06614v3.pdf","comment":"RLC 2025"},{"id":"http://arxiv.org/abs/2507.10015v1","updated":"2025-07-14T07:51:01Z","published":"2025-07-14T07:51:01Z","title":"(Almost) Free Modality Stitching of Foundation Models","summary":"  Foundation multi-modal models are often designed by stitching of multiple\nexisting pretrained uni-modal models: for example, an image classifier with an\nautoregressive text model. This stitching process is performed by training a\nconnector module that aims to align the representation-representation or\nrepresentation-input spaces of these uni-modal models. However, given the\ncomplexity of training such connectors on large scale web-based datasets\ncoupled with the ever-increasing number of available pretrained uni-modal\nmodels, the task of uni-modal models selection and subsequent connector module\ntraining becomes computationally demanding. To address this under-studied\ncritical problem, we propose Hypernetwork Model Alignment (Hyma), a novel\nall-in-one solution for optimal uni-modal model selection and connector\ntraining by leveraging hypernetworks. Specifically, our framework utilizes the\nparameter prediction capability of a hypernetwork to obtain jointly trained\nconnector modules for $N \\times M$ combinations of uni-modal models. In our\nexperiments, Hyma reduces the optimal uni-modal model pair search cost by\n$10\\times$ (averaged across all experiments), while matching the ranking and\ntrained connector performance obtained via grid search across a suite of\ndiverse multi-modal benchmarks.\n","authors":["Jaisidh Singh","Diganta Misra","Boris Knyazev","Antonio Orvieto"],"pdf_url":"https://arxiv.org/pdf/2507.10015v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2507.10014v1","updated":"2025-07-14T07:50:25Z","published":"2025-07-14T07:50:25Z","title":"Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural\n  Network Approach","summary":"  Coccidioidomycosis, commonly known as Valley Fever, remains a significant\npublic health concern in endemic regions of the southwestern United States.\nThis study develops the first graph neural network (GNN) model for forecasting\nValley Fever incidence in Arizona. The model integrates surveillance case data\nwith environmental predictors using graph structures, including soil\nconditions, atmospheric variables, agricultural indicators, and air quality\nmetrics. Our approach explores correlation-based relationships among variables\ninfluencing disease transmission. The model captures critical delays in disease\nprogression through lagged effects, enhancing its capacity to reflect complex\ntemporal dependencies in disease ecology. Results demonstrate that the GNN\narchitecture effectively models Valley Fever trends and provides insights into\nkey environmental drivers of disease incidence. These findings can inform early\nwarning systems and guide resource allocation for disease prevention efforts in\nhigh-risk areas.\n","authors":["Ali Sarabi","Arash Sarabi","Hao Yan","Beckett Sterner","Petar Jevtić"],"pdf_url":"https://arxiv.org/pdf/2507.10014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04916v2","updated":"2025-07-14T07:45:45Z","published":"2024-10-07T11:04:38Z","title":"Defense-as-a-Service: Black-box Shielding against Backdoored Graph\n  Models","summary":"  With the trend of large graph learning models, business owners tend to employ\na model provided by a third party to deliver business services to users.\nHowever, these models might be backdoored, and malicious users can submit\ntrigger-embedded inputs to manipulate the model predictions. Current graph\nbackdoor defenses have several limitations: 1) depending on model-related\ndetails, 2) requiring additional model fine-tuning, and 3) relying upon extra\nexplainability tools, all of which are infeasible under stringent privacy\npolicies. To address those limitations, we propose GraphProt, which allows\nresource-constrained business owners to rely on third parties to avoid backdoor\nattacks on GNN-based graph classifiers. Our GraphProt is model-agnostic and\nonly relies on the input graph. The key insight is to leverage subgraph\ninformation for prediction, thereby mitigating backdoor effects induced by\ntriggers. GraphProt comprises two components: clustering-based trigger\nelimination and robust subgraph ensemble. Specifically, we first propose\nfeature-topology clustering that aims to remove most of the anomalous subgraphs\n(triggers). Moreover, we design subgraph sampling strategies based on\nfeature-topology clustering to build a robust classifier via majority vote.\nExperimental results across three backdoor attacks and six benchmark datasets\ndemonstrate that GraphProt significantly reduces the backdoor attack success\nrate while preserving the model accuracy on regular graph classification tasks.\n","authors":["Xiao Yang","Kai Zhou","Yuni Lai","Gaolei Li"],"pdf_url":"https://arxiv.org/pdf/2410.04916v2.pdf","comment":"We have to add a rigorous mathematical proof to the thesis proposal,\n  and the process of the current proposal is not rigorous enough"},{"id":"http://arxiv.org/abs/2507.10005v1","updated":"2025-07-14T07:39:19Z","published":"2025-07-14T07:39:19Z","title":"Effects of structural properties of neural networks on machine learning\n  performance","summary":"  In recent years, graph-based machine learning techniques, such as\nreinforcement learning and graph neural networks, have garnered significant\nattention. While some recent studies have started to explore the relationship\nbetween the graph structure of neural networks and their predictive\nperformance, they often limit themselves to a narrow range of model networks,\nparticularly lacking mesoscale structures such as communities. Our work\nadvances this area by conducting a more comprehensive investigation,\nincorporating realistic network structures characterized by heterogeneous\ndegree distributions and community structures, which are typical\ncharacteristics of many real networks. These community structures offer a\nnuanced perspective on network architecture. Our analysis employs model\nnetworks such as random and scale-free networks, alongside a comparison with a\nbiological neural network and its subsets for more detailed analysis. We\nexamine the impact of these structural attributes on the performance of image\nclassification tasks. Our findings reveal that structural properties do affect\nperformance to some extent. Specifically, networks featuring coherent, densely\ninterconnected communities demonstrate enhanced learning capabilities. The\ncomparison with the biological neural network emphasizes the relevance of our\nfindings to real-world structures, suggesting an intriguing connection worth\nfurther exploration. This study contributes meaningfully to network science and\nmachine learning, providing insights that could inspire the design of more\nbiologically informed neural networks.\n","authors":["Yash Arya","Sang Hoon Lee"],"pdf_url":"https://arxiv.org/pdf/2507.10005v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.07498v2","updated":"2025-07-14T07:10:51Z","published":"2025-07-10T07:34:05Z","title":"Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems\n  without Code","summary":"  Enhancing reasoning capabilities remains a central focus in the LLM reasearch\ncommunity. A promising direction involves requiring models to simulate code\nexecution step-by-step to derive outputs for given inputs. However, as code is\noften designed for large-scale systems, direct application leads to\nover-reliance on complex data structures and algorithms, even for simple cases,\nresulting in overfitting to algorithmic patterns rather than core reasoning\nstructures. To address this, we propose TeaR, which aims at teaching LLMs to\nreason better. TeaR leverages careful data curation and reinforcement learning\nto guide models in discovering optimal reasoning paths through code-related\ntasks, thereby improving general reasoning abilities. We conduct extensive\nexperiments using two base models and three long-CoT distillation models, with\nmodel sizes ranging from 1.5 billion to 32 billion parameters, and across 17\nbenchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results\nconsistently show significant performance improvements. Notably, TeaR achieves\na 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.\n","authors":["Keqin Bao","Nuo Chen","Xiaoyuan Li","Binyuan Hui","Bowen Yu","Fuli Feng","Xiangnan He","Dayiheng Liu"],"pdf_url":"https://arxiv.org/pdf/2507.07498v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08382v2","updated":"2025-07-14T06:58:33Z","published":"2025-07-11T07:54:16Z","title":"Two-cluster test","summary":"  Cluster analysis is a fundamental research issue in statistics and machine\nlearning. In many modern clustering methods, we need to determine whether two\nsubsets of samples come from the same cluster. Since these subsets are usually\ngenerated by certain clustering procedures, the deployment of classic\ntwo-sample tests in this context would yield extremely smaller p-values,\nleading to inflated Type-I error rate. To overcome this bias, we formally\nintroduce the two-cluster test issue and argue that it is a totally different\nsignificance testing issue from conventional two-sample test. Meanwhile, we\npresent a new method based on the boundary points between two subsets to derive\nan analytical p-value for the purpose of significance quantification.\nExperiments on both synthetic and real data sets show that the proposed test is\nable to significantly reduce the Type-I error rate, in comparison with several\nclassic two-sample testing methods. More importantly, the practical usage of\nsuch two-cluster test is further verified through its applications in\ntree-based interpretable clustering and significance-based hierarchical\nclustering.\n","authors":["Xinying Liu","Lianyu Hu","Mudi Jiang","Simeng Zhang","Jun Lou","Zengyou He"],"pdf_url":"https://arxiv.org/pdf/2507.08382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09968v1","updated":"2025-07-14T06:34:29Z","published":"2025-07-14T06:34:29Z","title":"Compliance Minimization via Physics-Informed Gaussian Processes","summary":"  Machine learning (ML) techniques have recently gained significant attention\nfor solving compliance minimization (CM) problems. However, these methods\ntypically provide poor feature boundaries, are very expensive, and lack a\nsystematic mechanism to control the design complexity. Herein, we address these\nlimitations by proposing a mesh-free and simultaneous framework based on\nphysics-informed Gaussian processes (GPs). In our approach, we parameterize the\ndesign and state variables with GP priors which have independent kernels but\nshare a multi-output neural network (NN) as their mean function. The\narchitecture of this NN is based on Parametric Grid Convolutional Attention\nNetworks (PGCANs) which not only mitigate spectral bias issues, but also\nprovide an interpretable mechanism to control design complexity. We estimate\nall the parameters of our GP-based representations by simultaneously minimizing\nthe compliance, total potential energy, and residual of volume fraction\nconstraint. Importantly, our loss function exclude all data-based residuals as\nGPs automatically satisfy them. We also develop computational schemes based on\ncurriculum training and numerical integration to increase the efficiency and\nrobustness of our approach which is shown to (1) produce super-resolution\ntopologies with fast convergence, (2) achieve smaller compliance and less gray\narea fraction compared to traditional numerical methods, (3) provide control\nover fine-scale features, and (4) outperform competing ML-based methods.\n","authors":["Xiangyu Sun","Amin Yousefpour","Shirin Hosseinmardi","Ramin Bostanabad"],"pdf_url":"https://arxiv.org/pdf/2507.09968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09966v1","updated":"2025-07-14T06:32:59Z","published":"2025-07-14T06:32:59Z","title":"A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with\n  Cross-Modal Semantic Guidance and Multi-Level Feature Fusion","summary":"  Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is\nessential for neuro-oncology diagnosis and treatment planning. Despite advances\nin deep learning methods, automatic segmentation remains challenging due to\ntumor morphological heterogeneity and complex three-dimensional spatial\nrelationships. Current techniques primarily rely on visual features extracted\nfrom MRI sequences while underutilizing semantic knowledge embedded in medical\nreports. This research presents a multi-level fusion architecture that\nintegrates pixel-level, feature-level, and semantic-level information,\nfacilitating comprehensive processing from low-level data to high-level\nconcepts. The semantic-level fusion pathway combines the semantic understanding\ncapabilities of Contrastive Language-Image Pre-training (CLIP) models with the\nspatial feature extraction advantages of 3D U-Net through three mechanisms:\n3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based\nattention mechanisms. Experimental validation on the BraTS 2020 dataset\ndemonstrates that the proposed model achieves an overall Dice coefficient of\n0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with\na 7.3% Dice coefficient increase in the clinically important enhancing tumor\n(ET) region.\n","authors":["Mingda Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.09966v1.pdf","comment":"13 pages,6 figures"},{"id":"http://arxiv.org/abs/2507.09961v1","updated":"2025-07-14T06:20:42Z","published":"2025-07-14T06:20:42Z","title":"Text-Driven Causal Representation Learning for Source-Free Domain\n  Generalization","summary":"  Deep learning often struggles when training and test data distributions\ndiffer. Traditional domain generalization (DG) tackles this by including data\nfrom multiple source domains, which is impractical due to expensive data\ncollection and annotation. Recent vision-language models like CLIP enable\nsource-free domain generalization (SFDG) by using text prompts to simulate\nvisual representations, reducing data demands. However, existing SFDG methods\nstruggle with domain-specific confounders, limiting their generalization\ncapabilities. To address this issue, we propose TDCRL\n(\\textbf{T}ext-\\textbf{D}riven \\textbf{C}ausal \\textbf{R}epresentation\n\\textbf{L}earning), the first method to integrate causal inference into the\nSFDG setting. TDCRL operates in two steps: first, it employs data augmentation\nto generate style word vectors, combining them with class information to\ngenerate text embeddings to simulate visual representations; second, it trains\na causal intervention network with a confounder dictionary to extract\ndomain-invariant features. Grounded in causal learning, our approach offers a\nclear and effective mechanism to achieve robust, domain-invariant features,\nensuring robust generalization. Extensive experiments on PACS, VLCS,\nOfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL\neffectiveness in SFDG.\n","authors":["Lihua Zhou","Mao Ye","Nianxin Li","Shuaifeng Li","Jinlin Wu","Xiatian Zhu","Lei Deng","Hongbin Liu","Jiebo Luo","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2507.09961v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2507.09958v1","updated":"2025-07-14T06:13:18Z","published":"2025-07-14T06:13:18Z","title":"Rethinking Inductive Bias in Geographically Neural Network Weighted\n  Regression","summary":"  Inductive bias is a key factor in spatial regression models, determining how\nwell a model can learn from limited data and capture spatial patterns. This\nwork revisits the inductive biases in Geographically Neural Network Weighted\nRegression (GNNWR) and identifies limitations in current approaches for\nmodeling spatial non-stationarity. While GNNWR extends traditional\nGeographically Weighted Regression by using neural networks to learn spatial\nweighting functions, existing implementations are often restricted by fixed\ndistance-based schemes and limited inductive bias. We propose to generalize\nGNNWR by incorporating concepts from convolutional neural networks, recurrent\nneural networks, and transformers, introducing local receptive fields,\nsequential context, and self-attention into spatial regression. Through\nextensive benchmarking on synthetic spatial datasets with varying\nheterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic\nmethods in capturing nonlinear and complex spatial relationships. Our results\nalso reveal that model performance depends strongly on data characteristics,\nwith local models excelling in highly heterogeneous or small-sample scenarios,\nand global models performing better with larger, more homogeneous data. These\nfindings highlight the importance of inductive bias in spatial modeling and\nsuggest future directions, including learnable spatial weighting functions,\nhybrid neural architectures, and improved interpretability for models handling\nnon-stationary spatial data.\n","authors":["Zhenyuan Chen"],"pdf_url":"https://arxiv.org/pdf/2507.09958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09952v1","updated":"2025-07-14T06:01:58Z","published":"2025-07-14T06:01:58Z","title":"Radial Neighborhood Smoothing Recommender System","summary":"  Recommender systems inherently exhibit a low-rank structure in latent space.\nA key challenge is to define meaningful and measurable distances in the latent\nspace to capture user-user, item-item, user-item relationships effectively. In\nthis work, we establish that distances in the latent space can be\nsystematically approximated using row-wise and column-wise distances in the\nobserved matrix, providing a novel perspective on distance estimation. To\nrefine the distance estimation, we introduce the correction based on empirical\nvariance estimator to account for noise-induced non-centrality. The novel\ndistance estimation enables a more structured approach to constructing\nneighborhoods, leading to the Radial Neighborhood Estimator (RNE), which\nconstructs neighborhoods by including both overlapped and partially overlapped\nuser-item pairs and employs neighborhood smoothing via localized kernel\nregression to improve imputation accuracy. We provide the theoretical\nasymptotic analysis for the proposed estimator. We perform evaluations on both\nsimulated and real-world datasets, demonstrating that RNE achieves superior\nperformance compared to existing collaborative filtering and matrix\nfactorization methods. While our primary focus is on distance estimation in\nlatent space, we find that RNE also mitigates the ``cold-start'' problem.\n","authors":["Zerui Zhang","Yumou Qiu"],"pdf_url":"https://arxiv.org/pdf/2507.09952v1.pdf","comment":"34 pages, 2 figures. Submitted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2507.09949v1","updated":"2025-07-14T05:54:57Z","published":"2025-07-14T05:54:57Z","title":"Hierarchical Job Classification with Similarity Graph Integration","summary":"  In the dynamic realm of online recruitment, accurate job classification is\nparamount for optimizing job recommendation systems, search rankings, and labor\nmarket analyses. As job markets evolve, the increasing complexity of job titles\nand descriptions necessitates sophisticated models that can effectively\nleverage intricate relationships within job data. Traditional text\nclassification methods often fall short, particularly due to their inability to\nfully utilize the hierarchical nature of industry categories. To address these\nlimitations, we propose a novel representation learning and classification\nmodel that embeds jobs and hierarchical industry categories into a latent\nembedding space. Our model integrates the Standard Occupational Classification\n(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both\ngraph and hierarchical relationships, thereby improving classification\naccuracy. By embedding hierarchical industry categories into a shared latent\nspace, we tackle cold start issues and enhance the dynamic matching of\ncandidates to job opportunities. Extensive experimentation on a large-scale\ndataset of job postings demonstrates the model's superior ability to leverage\nhierarchical structures and rich semantic features, significantly outperforming\nexisting methods. This research provides a robust framework for improving job\nclassification accuracy, supporting more informed decision-making in the\nrecruitment industry.\n","authors":["Md Ahsanul Kabir","Kareem Abdelfatah","Mohammed Korayem","Mohammad Al Hasan"],"pdf_url":"https://arxiv.org/pdf/2507.09949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09948v1","updated":"2025-07-14T05:48:09Z","published":"2025-07-14T05:48:09Z","title":"Iceberg: Enhancing HLS Modeling with Synthetic Data","summary":"  Deep learning-based prediction models for High-Level Synthesis (HLS) of\nhardware designs often struggle to generalize. In this paper, we study how to\nclose the generalizability gap of these models through pretraining on synthetic\ndata and introduce Iceberg, a synthetic data augmentation approach that expands\nboth large language model (LLM)-generated programs and weak labels of unseen\ndesign configurations. Our weak label generation method is integrated with an\nin-context model architecture, enabling meta-learning from actual and proximate\nlabels. Iceberg improves the geometric mean modeling accuracy by $86.4\\%$ when\nadapt to six real-world applications with few-shot examples and achieves a\n$2.47\\times$ and a $1.12\\times$ better offline DSE performance when adapting to\ntwo different test datasets. Our open-sourced code is here:\n\\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}\n","authors":["Zijian Ding","Tung Nguyen","Weikai Li","Aditya Grover","Yizhou Sun","Jason Cong"],"pdf_url":"https://arxiv.org/pdf/2507.09948v1.pdf","comment":"9 pages. accepted to ICLAD'25"},{"id":"http://arxiv.org/abs/2507.05806v2","updated":"2025-07-14T05:44:34Z","published":"2025-07-08T09:25:51Z","title":"Predicting Graph Structure via Adapted Flux Balance Analysis","summary":"  Many dynamic processes such as telecommunication and transport networks can\nbe described through discrete time series of graphs. Modelling the dynamics of\nsuch time series enables prediction of graph structure at future time steps,\nwhich can be used in applications such as detection of anomalies. Existing\napproaches for graph prediction have limitations such as assuming that the\nvertices do not to change between consecutive graphs. To address this, we\npropose to exploit time series prediction methods in combination with an\nadapted form of flux balance analysis (FBA), a linear programming method\noriginating from biochemistry. FBA is adapted to incorporate various\nconstraints applicable to the scenario of growing graphs. Empirical evaluations\non synthetic datasets (constructed via Preferential Attachment model) and real\ndatasets (UCI Message, HePH, Facebook, Bitcoin) demonstrate the efficacy of the\nproposed approach.\n","authors":["Sevvandi Kandanaarachchi","Ziqi Xu","Stefan Westerlund","Conrad Sanderson"],"pdf_url":"https://arxiv.org/pdf/2507.05806v2.pdf","comment":"extended and revised version of arXiv:2401.04280"},{"id":"http://arxiv.org/abs/2507.03147v2","updated":"2025-07-14T05:34:27Z","published":"2025-07-03T20:04:04Z","title":"DeepGesture: A conversational gesture synthesis system based on emotions\n  and semantics","summary":"  Along with the explosion of large language models, improvements in speech\nsynthesis, advancements in hardware, and the evolution of computer graphics,\nthe current bottleneck in creating digital humans lies in generating character\nmovements that correspond naturally to text or speech inputs.\n  In this work, we present DeepGesture, a diffusion-based gesture synthesis\nframework for generating expressive co-speech gestures conditioned on\nmultimodal signals - text, speech, emotion, and seed motion. Built upon the\nDiffuseStyleGesture model, DeepGesture introduces novel architectural\nenhancements that improve semantic alignment and emotional expressiveness in\ngenerated gestures. Specifically, we integrate fast text transcriptions as\nsemantic conditioning and implement emotion-guided classifier-free diffusion to\nsupport controllable gesture generation across affective states. To visualize\nresults, we implement a full rendering pipeline in Unity based on BVH output\nfrom the model. Evaluation on the ZeroEGGS dataset shows that DeepGesture\nproduces gestures with improved human-likeness and contextual appropriateness.\nOur system supports interpolation between emotional states and demonstrates\ngeneralization to out-of-distribution speech, including synthetic voices -\nmarking a step forward toward fully multimodal, emotionally aware digital\nhumans.\n  Project page: https://deepgesture.github.io\n","authors":["Thanh Hoang-Minh"],"pdf_url":"https://arxiv.org/pdf/2507.03147v2.pdf","comment":"Project page: https://deepgesture.github.io"},{"id":"http://arxiv.org/abs/2507.09940v1","updated":"2025-07-14T05:29:16Z","published":"2025-07-14T05:29:16Z","title":"Long-Tailed Data Classification by Increasing and Decreasing Neurons\n  During Training","summary":"  In conventional deep learning, the number of neurons typically remains fixed\nduring training. However, insights from biology suggest that the human\nhippocampus undergoes continuous neuron generation and pruning of neurons over\nthe course of learning, implying that a flexible allocation of capacity can\ncontribute to enhance performance. Real-world datasets often exhibit class\nimbalance situations where certain classes have far fewer samples than others,\nleading to significantly reduce recognition accuracy for minority classes when\nrelying on fixed size networks.To address the challenge, we propose a method\nthat periodically adds and removes neurons during training, thereby boosting\nrepresentational power for minority classes. By retaining critical features\nlearned from majority classes while selectively increasing neurons for\nunderrepresented classes, our approach dynamically adjusts capacity during\ntraining. Importantly, while the number of neurons changes throughout training,\nthe final network size and structure remain unchanged, ensuring efficiency and\ncompatibility with deployment.Furthermore, by experiments on three different\ndatasets and five representative models, we demonstrate that the proposed\nmethod outperforms fixed size networks and shows even greater accuracy when\ncombined with other imbalance-handling techniques. Our results underscore the\neffectiveness of dynamic, biologically inspired network designs in improving\nperformance on class-imbalanced data.\n","authors":["Taigo Sakai","Kazuhiro Hotta"],"pdf_url":"https://arxiv.org/pdf/2507.09940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12185v3","updated":"2025-07-14T05:28:08Z","published":"2025-05-18T01:02:33Z","title":"EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency\n  Perspective","summary":"  Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop.\n","authors":["Sen Fang","Weiyuan Ding","Bowen Xu"],"pdf_url":"https://arxiv.org/pdf/2505.12185v3.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2507.09937v1","updated":"2025-07-14T05:23:27Z","published":"2025-07-14T05:23:27Z","title":"Memorization Sinks: Isolating Memorization during LLM Training","summary":"  Large language models are susceptible to memorizing repeated sequences,\nposing privacy and copyright concerns. A popular mitigation strategy is to\nremove memorized information from specific neurons post-hoc. However, such\napproaches have shown limited success so far. In a controlled setting, we show\nthat the memorization of natural sequences (those that resemble linguistically\nplausible text) become mechanistically entangled with general language\nabilities, thereby becoming challenging to remove post-hoc. In this work, we\nput forward a new paradigm of MemSinks that promotes isolation of memorization\nby design. We leverage a sequence identifier that activates a unique set of\nmemorization neurons for each sequence across repetitions. By analyzing the\ndynamics of learning and forgetting, we argue that MemSinks facilitates\nisolation of memorized content, making it easier to remove without compromising\ngeneral language capabilities. We implement MemSinks at the billion-parameter\nand billion-token scale, and observe both effective isolation and strong\ngeneralization. To our knowledge, this is the first proof-of-concept on real\ndata demonstrating that simultaneous generalization and isolation is\nachievable. We open-source our code at http://github.com/grghosal/MemSinks.\n","authors":["Gaurav R. Ghosal","Pratyush Maini","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2507.09937v1.pdf","comment":"Accepted at the 2025 International Conference of Machine Learning"},{"id":"http://arxiv.org/abs/2507.09931v1","updated":"2025-07-14T05:17:41Z","published":"2025-07-14T05:17:41Z","title":"Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear\n  Reactor Safety Applications","summary":"  The integration of Large Language Models (LLMs) into safety-critical domains,\nsuch as nuclear engineering, necessitates a deep understanding of their\ninternal reasoning processes. This paper presents a novel methodology for\ninterpreting how an LLM encodes and utilizes domain-specific knowledge, using a\nBoiling Water Reactor system as a case study. We adapted a general-purpose LLM\n(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning\ntechnique known as Low-Rank Adaptation. By comparing the neuron activation\npatterns of the base model to those of the fine-tuned model, we identified a\nsparse set of neurons whose behavior was significantly altered during the\nadaptation process. To probe the causal role of these specialized neurons, we\nemployed a neuron silencing technique. Our results demonstrate that while\nsilencing most of these specialized neurons individually did not produce a\nstatistically significant effect, deactivating the entire group collectively\nled to a statistically significant degradation in task performance. Qualitative\nanalysis further revealed that silencing these neurons impaired the model's\nability to generate detailed, contextually accurate technical information. This\npaper provides a concrete methodology for enhancing the transparency of an\nopaque black-box model, allowing domain expertise to be traced to verifiable\nneural circuits. This offers a pathway towards achieving nuclear-grade\nartificial intelligence (AI) assurance, addressing the verification and\nvalidation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR\n50 Appendix B), which have limited AI deployment in safety-critical nuclear\noperations.\n","authors":["Yoon Pyo Lee"],"pdf_url":"https://arxiv.org/pdf/2507.09931v1.pdf","comment":"Submitted to Nuclear Technology. 22 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2507.09929v1","updated":"2025-07-14T05:15:39Z","published":"2025-07-14T05:15:39Z","title":"Aligning Generative Speech Enhancement with Human Preferences via Direct\n  Preference Optimization","summary":"  This work investigates speech enhancement (SE) from the perspective of\nlanguage models (LMs). We propose a novel method that leverages Direct\nPreference Optimization (DPO) to improve the perceptual quality of enhanced\nspeech. Using UTMOS, a neural MOS prediction model, as a proxy for human\nratings, our approach guides optimization toward perceptually preferred\noutputs. This differs from existing LM-based SE methods that focus on\nmaximizing the likelihood of clean speech tokens, which may misalign with human\nperception and degrade quality despite low prediction error. Experiments on the\n2020 Deep Noise Suppression Challenge test sets demonstrate that applying DPO\nto a pretrained LM-based SE model yields consistent improvements across various\nspeech quality metrics, with relative gains of up to 56%. To our knowledge,\nthis is the first application of DPO to SE and the first to incorporate proxy\nperceptual feedback into LM-based SE training, pointing to a promising\ndirection for perceptually aligned SE.\n","authors":["Haoyang Li","Nana Hou","Yuchen Hu","Jixun Yao","Sabato Marco Siniscalchi","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2507.09929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09925v1","updated":"2025-07-14T05:06:37Z","published":"2025-07-14T05:06:37Z","title":"Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware\n  Transformer Model","summary":"  Extracting cause and effect phrases from a sentence is an important NLP task,\nwith numerous applications in various domains, including legal, medical,\neducation, and scientific research. There are many unsupervised and supervised\nmethods proposed for solving this task. Among these, unsupervised methods\nutilize various linguistic tools, including syntactic patterns, dependency\ntree, dependency relations, etc. among different sentential units for\nextracting the cause and effect phrases. On the other hand, the contemporary\nsupervised methods use various deep learning based mask language models\nequipped with a token classification layer for extracting cause and effect\nphrases. Linguistic tools, specifically, dependency tree, which organizes a\nsentence into different semantic units have been shown to be very effective for\nextracting semantic pairs from a sentence, but existing supervised methods do\nnot have any provision for utilizing such tools within their model framework.\nIn this work, we propose DepBERT, which extends a transformer-based model by\nincorporating dependency tree of a sentence within the model framework.\nExtensive experiments over three datasets show that DepBERT is better than\nvarious state-of-the art supervised causality extraction methods.\n","authors":["Md Ahsanul Kabir","Abrar Jahin","Mohammad Al Hasan"],"pdf_url":"https://arxiv.org/pdf/2507.09925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09924v1","updated":"2025-07-14T05:04:32Z","published":"2025-07-14T05:04:32Z","title":"MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for\n  Rehearsal-Free Generative Retrieval over Dynamic Corpora","summary":"  Continually updating model-based indexes in generative retrieval with new\ndocuments remains challenging, as full retraining is computationally expensive\nand impractical under resource constraints. We propose MixLoRA-DSI, a novel\nframework that combines an expandable mixture of Low-Rank Adaptation experts\nwith a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead\nof allocating new experts for each new corpus, our proposed expansion strategy\nenables sublinear parameter growth by selectively introducing new experts only\nwhen significant number of OOD documents are detected. Experiments on NQ320k\nand MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update\nbaselines, with minimal parameter overhead and substantially lower training\ncosts.\n","authors":["Tuan-Luc Huynh","Thuy-Trang Vu","Weiqing Wang","Trung Le","Dragan Gašević","Yuan-Fang Li","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2507.09924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08322v2","updated":"2025-07-14T05:02:01Z","published":"2025-07-11T05:25:09Z","title":"Towards Efficient Quantity Retrieval from Text:An Approach via\n  Description Parsing and Weak Supervision","summary":"  Quantitative facts are continually generated by companies and governments,\nsupporting data-driven decision-making. While common facts are structured, many\nlong-tail quantitative facts remain buried in unstructured documents, making\nthem difficult to access. We propose the task of Quantity Retrieval: given a\ndescription of a quantitative fact, the system returns the relevant value and\nsupporting evidence. Understanding quantity semantics in context is essential\nfor this task. We introduce a framework based on description parsing that\nconverts text into structured (description, quantity) pairs for effective\nretrieval. To improve learning, we construct a large paraphrase dataset using\nweak supervision based on quantity co-occurrence. We evaluate our approach on a\nlarge corpus of financial annual reports and a newly annotated quantity\ndescription dataset. Our method significantly improves top-1 retrieval accuracy\nfrom 30.98 percent to 64.66 percent.\n","authors":["Yixuan Cao","Zhengrong Chen","Chengxuan Xia","Kun Wu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2507.08322v2.pdf","comment":"Extended version of the paper accepted in DEXA 2025"},{"id":"http://arxiv.org/abs/2507.07140v2","updated":"2025-07-14T04:57:05Z","published":"2025-07-09T03:25:45Z","title":"Exploring Sparse Adapters for Scalable Merging of Parameter Efficient\n  Experts","summary":"  Merging parameter-efficient task experts has recently gained growing\nattention as a way to build modular architectures that can be rapidly adapted\non the fly for specific downstream tasks, without requiring additional\nfine-tuning. Typically, LoRA serves as the foundational building block of such\nparameter-efficient modular architectures, leveraging low-rank weight\nstructures to reduce the number of trainable parameters. In this paper, we\nstudy the properties of sparse adapters, which train only a subset of weights\nin the base neural network, as potential building blocks of modular\narchitectures. First, we propose a simple method for training highly effective\nsparse adapters, which is conceptually simpler than existing methods in the\nliterature and surprisingly outperforms both LoRA and full fine-tuning in our\nsetting. Next, we investigate the merging properties of these sparse adapters\nby merging adapters for up to 20 natural language processing tasks, thus\nscaling beyond what is usually studied in the literature. Our findings\ndemonstrate that sparse adapters yield superior in-distribution performance\npost-merging compared to LoRA or full model merging. Achieving strong held-out\nperformance remains a challenge for all methods considered.\n","authors":["Samin Yeasar Arnob","Zhan Su","Minseon Kim","Oleksiy Ostapenko","Riyasat Ohib","Esra'a Saleh","Doina Precup","Lucas Caccia","Alessandro Sordoni"],"pdf_url":"https://arxiv.org/pdf/2507.07140v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09898v1","updated":"2025-07-14T04:08:33Z","published":"2025-07-14T04:08:33Z","title":"Advanced U-Net Architectures with CNN Backbones for Automated Lung\n  Cancer Detection and Segmentation in Chest CT Images","summary":"  This study investigates the effectiveness of U-Net architectures integrated\nwith various convolutional neural network (CNN) backbones for automated lung\ncancer detection and segmentation in chest CT images, addressing the critical\nneed for accurate diagnostic tools in clinical settings. A balanced dataset of\n832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed\nusing Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to\n128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50,\nVGG16, and Xception, to segment lung regions. After segmentation, CNN-based\nclassifiers and hybrid models combining CNN feature extraction with traditional\nmachine learning classifiers (Support Vector Machine, Random Forest, and\nGradient Boosting) were evaluated using 5-fold cross-validation. Metrics\nincluded accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC.\nU-Net with ResNet50 achieved the best performance for cancerous lungs (Dice:\n0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for\nnon-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For\nclassification, the CNN model using U-Net with Xception achieved 99.1 percent\naccuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid\nCNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent\nF1-score. Compared to prior methods, our framework consistently outperformed\nexisting models. In conclusion, combining U-Net with advanced CNN backbones\nprovides a powerful method for both segmentation and classification of lung\ncancer in CT scans, supporting early diagnosis and clinical decision-making.\n","authors":["Alireza Golkarieha","Kiana Kiashemshakib","Sajjad Rezvani Boroujenic","Nasibeh Asadi Isakand"],"pdf_url":"https://arxiv.org/pdf/2507.09898v1.pdf","comment":"This manuscript has 20 pages and 10 figures. It is submitted to the\n  Journal 'Scientific Reports'"},{"id":"http://arxiv.org/abs/2507.09897v1","updated":"2025-07-14T04:07:43Z","published":"2025-07-14T04:07:43Z","title":"Algorithm Development in Neural Networks: Insights from the Streaming\n  Parity Task","summary":"  Even when massively overparameterized, deep neural networks show a remarkable\nability to generalize. Research on this phenomenon has focused on\ngeneralization within distribution, via smooth interpolation. Yet in some\nsettings neural networks also learn to extrapolate to data far beyond the\nbounds of the original training set, sometimes even allowing for infinite\ngeneralization, implying that an algorithm capable of solving the task has been\nlearned. Here we undertake a case study of the learning dynamics of recurrent\nneural networks (RNNs) trained on the streaming parity task in order to develop\nan effective theory of algorithm development. The streaming parity task is a\nsimple but nonlinear task defined on sequences up to arbitrary length. We show\nthat, with sufficient finite training experience, RNNs exhibit a phase\ntransition to perfect infinite generalization. Using an effective theory for\nthe representational dynamics, we find an implicit representational merger\neffect which can be interpreted as the construction of a finite automaton that\nreproduces the task. Overall, our results disclose one mechanism by which\nneural networks can generalize infinitely from finite training experience.\n","authors":["Loek van Rossem","Andrew M. Saxe"],"pdf_url":"https://arxiv.org/pdf/2507.09897v1.pdf","comment":"28 pages, 20 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.10510v1","updated":"2025-07-14T17:34:49Z","published":"2025-07-14T17:34:49Z","title":"Chat with AI: The Surprising Turn of Real-time Video Communication from\n  Human to AI","summary":"  AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\nThis makes interaction between humans and AI more intuitive, as if chatting\nface-to-face with a real person. However, this poses significant challenges to\nlatency, because the MLLM inference takes up most of the response time, leaving\nvery little time for video streaming. Due to network uncertainty and\ninstability, transmission latency becomes a critical bottleneck preventing AI\nfrom being like a real person. To address this, we propose Artic, an\nAI-oriented Real-time Communication framework, exploring the network\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\nContext-Aware Video Streaming that recognizes the importance of each video\nregion for chat and allocates bitrate almost exclusively to chat-important\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\non MLLM accuracy, we build the first benchmark, named Degraded Video\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\nand ongoing solutions for AI Video Chat.\n","authors":["Jiangkai Wu","Zhiyuan Ren","Liming Liu","Xinggong Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.10510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10469v1","updated":"2025-07-14T16:50:29Z","published":"2025-07-14T16:50:29Z","title":"An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived\n  Realism and Performance in Virtual Reality Environments","summary":"  Advancements in artificial intelligence (AI) have significantly enhanced the\nrealism and interactivity of non-player characters (NPCs) in virtual reality\n(VR), creating more engaging and believable user experiences. This paper\nevaluates AI-driven NPCs within a VR interrogation simulator, focusing on their\nperceived realism, usability, and system performance. The simulator features\ntwo AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage\nparticipants in a scenario to determine the suspect's guilt or innocence. A\nuser study with 18 participants assessed the system using the System Usability\nScale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent\nBelievability Questionnaire, alongside latency measurements for speech-to-text\n(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.\nResults showed an average cycle latency of 7 seconds, influenced by the\nincreasing conversational context. Believability scored 6.67 out of 10, with\nhigh ratings in behavior, social relationships, and intelligence but moderate\nscores in emotion and personality. The system achieved a SUS score of 79.44,\nindicating good usability. These findings demonstrate the potential of large\nlanguage models to improve NPC realism and interaction in VR while highlighting\nchallenges in reducing system latency and enhancing emotional depth. This\nresearch contributes to the development of more sophisticated AI-driven NPCs,\nrevealing the need for performance optimization to achieve increasingly\nimmersive virtual experiences.\n","authors":["Mikko Korkiakoski","Saeid Sheikhi","Jesper Nyman","Jussi Saariniemi","Kalle Tapio","Panos Kostakos"],"pdf_url":"https://arxiv.org/pdf/2507.10469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10461v1","updated":"2025-07-14T16:39:14Z","published":"2025-07-14T16:39:14Z","title":"RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for\n  Pansharpening","summary":"  Pansharpening refers to the process of integrating a high resolution\npanchromatic (PAN) image with a lower resolution multispectral (MS) image to\ngenerate a fused product, which is pivotal in remote sensing. Despite the\neffectiveness of CNNs in addressing this challenge, they are inherently\nconstrained by the uniform application of convolutional kernels across all\nspatial positions, overlooking local content variations. To overcome this\nissue, we introduce RAPNet, a new architecture that leverages content-adaptive\nconvolution. At its core, RAPNet employs the Receptive-field Adaptive\nPansharpening Convolution (RAPConv), designed to produce spatially adaptive\nkernels responsive to local feature context, thereby enhancing the precision of\nspatial detail extraction. Additionally, the network integrates the\nPansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an\nattention mechanism to achieve an optimal balance between spatial detail\nenhancement and spectral fidelity. Comprehensive evaluations on publicly\navailable datasets confirm that RAPNet delivers superior performance compared\nto existing approaches, as demonstrated by both quantitative metrics and\nqualitative assessments. Ablation analyses further substantiate the\neffectiveness of the proposed adaptive components.\n","authors":["Tao Tang","Chengxu Yang"],"pdf_url":"https://arxiv.org/pdf/2507.10461v1.pdf","comment":"To appear in the proceedings of the 6th International Conference on\n  Artificial Intelligence and Electromechanical Automation (AIEA 2025). 5\n  pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.10403v1","updated":"2025-07-14T15:46:56Z","published":"2025-07-14T15:46:56Z","title":"Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources","summary":"  Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.\n","authors":["Daniele Rege Cambrin","Lorenzo Vaiani","Giuseppe Gallipoli","Luca Cagliero","Paolo Garza"],"pdf_url":"https://arxiv.org/pdf/2507.10403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10109v1","updated":"2025-07-14T09:50:53Z","published":"2025-07-14T09:50:53Z","title":"DualDub: Video-to-Soundtrack Generation via Joint Speech and Background\n  Audio Synthesis","summary":"  While recent video-to-audio (V2A) models can generate realistic background\naudio from visual input, they largely overlook speech, an essential part of\nmany video soundtracks. This paper proposes a new task, video-to-soundtrack\n(V2ST) generation, which aims to jointly produce synchronized background audio\nand speech within a unified framework. To tackle V2ST, we introduce DualDub, a\nunified framework built on a multimodal language model that integrates a\nmultimodal encoder, a cross-modal aligner, and dual decoding heads for\nsimultaneous background audio and speech generation. Specifically, our proposed\ncross-modal aligner employs causal and non-causal attention mechanisms to\nimprove synchronization and acoustic harmony. Besides, to handle data scarcity,\nwe design a curriculum learning strategy that progressively builds the\nmultimodal capability. Finally, we introduce DualBench, the first benchmark for\nV2ST evaluation with a carefully curated test set and comprehensive metrics.\nExperimental results demonstrate that DualDub achieves state-of-the-art\nperformance, generating high-quality and well-synchronized soundtracks with\nboth speech and background audio.\n","authors":["Wenjie Tian","Xinfa Zhu","Haohe Liu","Zhixian Zhao","Zihao Chen","Chaofan Ding","Xinhan Di","Junjie Zheng","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2507.10109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04840v3","updated":"2025-07-14T09:23:01Z","published":"2025-04-07T08:51:11Z","title":"Unsupervised Ego- and Exo-centric Dense Procedural Activity Captioning\n  via Gaze Consensus Adaptation","summary":"  Even from an early age, humans naturally adapt between exocentric (Exo) and\negocentric (Ego) perspectives to understand daily procedural activities.\nInspired by this cognitive ability, we propose a novel Unsupervised Ego-Exo\nDense Procedural Activity Captioning (UE$^{2}$DPAC) task, which aims to\ntransfer knowledge from the labeled source view to predict the time segments\nand descriptions of action sequences for the target view without annotations.\nDespite previous works endeavoring to address the fully-supervised single-view\nor cross-view dense video captioning, they lapse in the proposed task due to\nthe significant inter-view gap caused by temporal misalignment and irrelevant\nobject interference. Hence, we propose a Gaze Consensus-guided Ego-Exo\nAdaptation Network (GCEAN) that injects the gaze information into the learned\nrepresentations for the fine-grained Ego-Exo alignment. Specifically, we\npropose a Score-based Adversarial Learning Module (SALM) that incorporates a\ndiscriminative scoring network and compares the scores of distinct views to\nlearn unified view-invariant representations from a global level. Then, the\nGaze Consensus Construction Module (GCCM) utilizes the gaze to progressively\ncalibrate the learned representations to highlight the regions of interest and\nextract the corresponding temporal contexts. Moreover, we adopt hierarchical\ngaze-guided consistency losses to construct gaze consensus for the explicit\ntemporal and spatial adaptation between the source and target views. To support\nour research, we propose a new EgoMe-UE$^{2}$DPAC benchmark, and extensive\nexperiments demonstrate the effectiveness of our method, which outperforms many\nrelated methods by a large margin. Code is available at\nhttps://github.com/ZhaofengSHI/GCEAN.\n","authors":["Zhaofeng Shi","Heqian Qiu","Lanxiao Wang","Qingbo Wu","Fanman Meng","Hongliang Li"],"pdf_url":"https://arxiv.org/pdf/2504.04840v3.pdf","comment":"ACM International Conference on Multimedia(ACM MM 2025)"},{"id":"http://arxiv.org/abs/2507.10066v1","updated":"2025-07-14T08:52:03Z","published":"2025-07-14T08:52:03Z","title":"LayLens: Improving Deepfake Understanding through Simplified\n  Explanations","summary":"  This demonstration paper presents $\\mathbf{LayLens}$, a tool aimed to make\ndeepfake understanding easier for users of all educational backgrounds. While\nprior works often rely on outputs containing technical jargon, LayLens bridges\nthe gap between model reasoning and human understanding through a three-stage\npipeline: (1) explainable deepfake detection using a state-of-the-art forgery\nlocalization model, (2) natural language simplification of technical\nexplanations using a vision-language model, and (3) visual reconstruction of a\nplausible original image via guided image editing. The interface presents both\ntechnical and layperson-friendly explanations in addition to a side-by-side\ncomparison of the uploaded and reconstructed images. A user study with 15\nparticipants shows that simplified explanations significantly improve clarity\nand reduce cognitive load, with most users expressing increased confidence in\nidentifying deepfakes. LayLens offers a step toward transparent, trustworthy,\nand user-centric deepfake forensics.\n","authors":["Abhijeet Narang","Parul Gupta","Liuyijia Su","Abhinav Dhall"],"pdf_url":"https://arxiv.org/pdf/2507.10066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18314v2","updated":"2025-07-14T07:08:07Z","published":"2025-01-30T12:43:47Z","title":"AGAV-Rater: Adapting Large Multimodal Model for AI-Generated\n  Audio-Visual Quality Assessment","summary":"  Many video-to-audio (VTA) methods have been proposed for dubbing silent\nAI-generated videos. An efficient quality assessment method for AI-generated\naudio-visual content (AGAV) is crucial for ensuring audio-visual quality.\nExisting audio-visual quality assessment methods struggle with unique\ndistortions in AGAVs, such as unrealistic and inconsistent elements. To address\nthis, we introduce AGAVQA-3k, the first large-scale AGAV quality assessment\ndataset, comprising $3,382$ AGAVs from $16$ VTA methods. AGAVQA-3k includes two\nsubsets: AGAVQA-MOS, which provides multi-dimensional scores for audio quality,\ncontent consistency, and overall quality, and AGAVQA-Pair, designed for optimal\nAGAV pair selection. We further propose AGAV-Rater, a LMM-based model that can\nscore AGAVs, as well as audio and music generated from text, across multiple\ndimensions, and selects the best AGAV generated by VTA methods to present to\nthe user. AGAV-Rater achieves state-of-the-art performance on AGAVQA-3k,\nText-to-Audio, and Text-to-Music datasets. Subjective tests also confirm that\nAGAV-Rater enhances VTA performance and user experience. The dataset and code\nis available at https://github.com/charlotte9524/AGAV-Rater.\n","authors":["Yuqin Cao","Xiongkuo Min","Yixuan Gao","Wei Sun","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2501.18314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09945v1","updated":"2025-07-14T05:42:00Z","published":"2025-07-14T05:42:00Z","title":"ESG-Net: Event-Aware Semantic Guided Network for Dense Audio-Visual\n  Event Localization","summary":"  Dense audio-visual event localization (DAVE) aims to identify event\ncategories and locate the temporal boundaries in untrimmed videos. Most studies\nonly employ event-related semantic constraints on the final outputs, lacking\ncross-modal semantic bridging in intermediate layers. This causes modality\nsemantic gap for further fusion, making it difficult to distinguish between\nevent-related content and irrelevant background content. Moreover, they rarely\nconsider the correlations between events, which limits the model to infer\nconcurrent events among complex scenarios. In this paper, we incorporate\nmulti-stage semantic guidance and multi-event relationship modeling, which\nrespectively enable hierarchical semantic understanding of audio-visual events\nand adaptive extraction of event dependencies, thereby better focusing on\nevent-related information. Specifically, our eventaware semantic guided network\n(ESG-Net) includes a early semantics interaction (ESI) module and a mixture of\ndependency experts (MoDE) module. ESI applys multi-stage semantic guidance to\nexplicitly constrain the model in learning semantic information through\nmulti-modal early fusion and several classification loss functions, ensuring\nhierarchical understanding of event-related content. MoDE promotes the\nextraction of multi-event dependencies through multiple serial mixture of\nexperts with adaptive weight allocation. Extensive experiments demonstrate that\nour method significantly surpasses the state-of-the-art methods, while greatly\nreducing parameters and computational load. Our code will be released on\nhttps://github.com/uchiha99999/ESG-Net.\n","authors":["Huilai Li","Yonghao Dang","Ying Xing","Yiming Wang","Jianqin Yin"],"pdf_url":"https://arxiv.org/pdf/2507.09945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10859v1","updated":"2025-07-14T23:20:42Z","published":"2025-07-14T23:20:42Z","title":"MultiVox: Benchmarking Voice Assistants for Multimodal Interactions","summary":"  The rapid progress of Large Language Models (LLMs) has empowered omni models\nto act as voice assistants capable of understanding spoken dialogues. These\nmodels can process multimodal inputs beyond text, such as speech and visual\ndata, enabling more context-aware interactions. However, current benchmarks\nfall short in comprehensively evaluating how well these models generate\ncontext-aware responses, particularly when it comes to implicitly understanding\nfine-grained speech characteristics, such as pitch, emotion, timbre, and volume\nor the environmental acoustic context such as background sounds. Additionally,\nthey inadequately assess the ability of models to align paralinguistic cues\nwith complementary visual signals to inform their responses. To address these\ngaps, we introduce MultiVox, the first omni voice assistant benchmark designed\nto evaluate the ability of voice assistants to integrate spoken and visual cues\nincluding paralinguistic speech features for truly multimodal understanding.\nSpecifically, MultiVox includes 1000 human-annotated and recorded speech\ndialogues that encompass diverse paralinguistic features and a range of visual\ncues such as images and videos. Our evaluation on 9 state-of-the-art models\nreveals that, although humans excel at these tasks, current models consistently\nstruggle to produce contextually grounded responses.\n","authors":["Ramaneswaran Selvakumar","Ashish Seth","Nishit Anand","Utkarsh Tyagi","Sonal Kumar","Sreyan Ghosh","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2507.10859v1.pdf","comment":"Work In Progress"},{"id":"http://arxiv.org/abs/2404.13914v2","updated":"2025-07-14T22:26:06Z","published":"2024-04-22T06:52:12Z","title":"A Survey on Speech Deepfake Detection","summary":"  The availability of smart devices leads to an exponential increase in\nmultimedia content. However, advancements in deep learning have also enabled\nthe creation of highly sophisticated Deepfake content, including speech\nDeepfakes, which pose a serious threat by generating realistic voices and\nspreading misinformation. To combat this, numerous challenges have been\norganized to advance speech Deepfake detection techniques. In this survey, we\nsystematically analyze more than 200 papers published up to March 2024. We\nprovide a comprehensive review of each component in the detection pipeline,\nincluding model architectures, optimization techniques, generalizability,\nevaluation metrics, performance comparisons, available datasets, and open\nsource availability. For each aspect, we assess recent progress and discuss\nongoing challenges. In addition, we explore emerging topics such as partial\nDeepfake detection, cross-dataset evaluation, and defences against adversarial\nattacks, while suggesting promising research directions. This survey not only\nidentifies the current state of the art to establish strong baselines for\nfuture experiments but also offers clear guidance for researchers aiming to\nenhance speech Deepfake detection systems.\n","authors":["Menglu Li","Yasaman Ahmadiadli","Xiao-Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.13914v2.pdf","comment":"38 pages. This paper has been accepted by ACM Computing Surveys"}]},"2025-07-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2505.24859v2","updated":"2025-07-13T23:10:37Z","published":"2025-05-30T17:57:15Z","title":"Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive\n  Free-Form Summarization","summary":"  Steering vectors are a lightweight method for controlling text properties by\nadding a learned bias to language model activations at inference time. So far,\nsteering vectors have predominantly been evaluated in multiple-choice settings,\nwhile their effectiveness in free-form generation tasks remains understudied.\nMoving \"Beyond Multiple Choice,\" we thoroughly evaluate the effectiveness of\nsteering vectors in adaptively controlling topical focus, sentiment, toxicity,\nand readability in abstractive summaries of the NEWTS dataset. We find that\nsteering effectively controls the targeted summary properties, but high\nsteering strengths consistently degrade both intrinsic and extrinsic text\nquality. Compared to steering, prompting offers weaker control, while\npreserving text quality. Combining steering and prompting yields the strongest\ncontrol over text properties and offers the most favorable efficacy-quality\ntrade-off at moderate steering strengths. Our results underscore the practical\ntrade-off between control strength and text quality preservation when applying\nsteering vectors to free-form generation tasks.\n","authors":["Joschka Braun","Carsten Eickhoff","Seyed Ali Bahrainian"],"pdf_url":"https://arxiv.org/pdf/2505.24859v2.pdf","comment":"29 pages, 21 figures, published at ICML 2025 Workshop on Reliable and\n  Responsible Foundation Models"},{"id":"http://arxiv.org/abs/2412.00947v3","updated":"2025-07-13T22:29:38Z","published":"2024-12-01T19:46:22Z","title":"VisOnlyQA: Large Vision Language Models Still Struggle with Visual\n  Perception of Geometric Information","summary":"  Large Vision Language Models (LVLMs) have achieved remarkable performance in\nvarious vision-language tasks. However, it is still unclear how accurately\nLVLMs can perceive visual information in images. In particular, the capability\nof LVLMs to perceive geometric information, such as shape, angle, and size,\nremains insufficiently analyzed, although the perception of these properties is\ncrucial for tasks that require a detailed visual understanding. In this work,\nwe introduce VisOnlyQA, a dataset for evaluating the geometric perception of\nLVLMs, and reveal that LVLMs often cannot accurately perceive basic geometric\ninformation in images, while human performance is nearly perfect. VisOnlyQA\nconsists of 12 tasks that directly ask about geometric information in geometric\nshapes, charts, chemical structures, and 3D shapes. Our experiments highlight\nthe following findings: (i) State-of-the-art LVLMs struggle with basic\ngeometric perception. 23 LVLMs we evaluate, including GPT-4o and Gemini 2.5\nPro, work poorly on VisOnlyQA. (ii) Additional training data does not resolve\nthis issue. Fine-tuning on the training set of VisOnlyQA is not always\neffective, even for in-distribution tasks. (iii) LLM may be the bottleneck.\nLVLMs using stronger LLMs exhibit better geometric perception on VisOnlyQA,\nwhile it does not require complex reasoning, suggesting that the way LVLMs\nprocess information from visual encoders is a bottleneck. The datasets, code,\nand model responses are provided at https://github.com/psunlpgroup/VisOnlyQA.\n","authors":["Ryo Kamoi","Yusen Zhang","Sarkar Snigdha Sarathi Das","Ranran Haoran Zhang","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.00947v3.pdf","comment":"COLM 2025. VisOnlyQA dataset, code, and model responses are provided\n  at https://github.com/psunlpgroup/VisOnlyQA. Please also refer to our project\n  website at https://visonlyqa.github.io/"},{"id":"http://arxiv.org/abs/2507.04189v2","updated":"2025-07-13T22:06:13Z","published":"2025-07-05T23:46:35Z","title":"SymbolicThought: Integrating Language Models and Symbolic Reasoning for\n  Consistent and Interpretable Human Relationship Understanding","summary":"  Understanding character relationships is essential for interpreting complex\nnarratives and conducting socially grounded AI research. However, manual\nannotation is time-consuming and low in coverage, while large language models\n(LLMs) often produce hallucinated or logically inconsistent outputs. We present\nSymbolicThought, a human-in-the-loop framework that combines LLM-based\nextraction with symbolic reasoning. The system constructs editable character\nrelationship graphs, refines them using seven types of logical constraints, and\nenables real-time validation and conflict resolution through an interactive\ninterface. To support logical supervision and explainable social analysis, we\nrelease a dataset of 160 interpersonal relationships with corresponding logical\nstructures. Experiments show that SymbolicThought improves annotation accuracy\nand consistency while significantly reducing time cost, offering a practical\ntool for narrative understanding, explainable AI, and LLM evaluation.\n","authors":["Runcong Zhao","Qinglin Zhu","Hainiu Xu","Bin Liang","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2507.04189v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03493v2","updated":"2025-07-13T21:20:50Z","published":"2024-11-05T20:18:28Z","title":"LASER: Attention with Exponential Transformation","summary":"  Transformers have had tremendous impact for several sequence related tasks,\nlargely due to their ability to retrieve from any part of the sequence via\nsoftmax based dot-product attention. This mechanism plays a crucial role in\nTransformer's performance. We analyze the gradients backpropagated through the\nsoftmax operation in the attention mechanism and observe that these gradients\ncan often be small. This poor gradient signal backpropagation can lead to\ninefficient learning of parameters preceeding the attention operations. To this\nend, we introduce a new attention mechanism called LASER, which we analytically\nshow to admit a larger gradient signal. We show that LASER attention can be\nimplemented by making small modifications to existing attention\nimplementations. We conduct experiments on autoregressive large language models\n(LLMs) with upto 7.7 billion parameters with an average improvement of upto\n1.44% over standard attention on downstream evaluations and 1.65% finetuning\nimprovements. Additionally, LASER demonstrates generalization performance\nimprovement across a variety of tasks (vision, text and speech):Vision\nTransformer (ViT) on Imagenet, Conformer on the Librispeech speech-to-text and\nBERT with 2.2 billion parameters.\n","authors":["Sai Surya Duvvuri","Inderjit S. Dhillon"],"pdf_url":"https://arxiv.org/pdf/2411.03493v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2507.09788v1","updated":"2025-07-13T21:00:27Z","published":"2025-07-13T21:00:27Z","title":"TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit","summary":"  Recent advances in Large Language Models (LLM) have led to a new class of\nautonomous agents, renewing and expanding interest in the area. LLM-powered\nMultiagent Systems (MAS) have thus emerged, both for assistive and simulation\npurposes, yet tools for realistic human behavior simulation -- with its\ndistinctive challenges and opportunities -- remain underdeveloped. Existing MAS\nlibraries and tools lack fine-grained persona specifications, population\nsampling facilities, experimentation support, and integrated validation, among\nother key capabilities, limiting their utility for behavioral studies, social\nsimulation, and related applications. To address these deficiencies, in this\nwork we introduce TinyTroupe, a simulation toolkit enabling detailed persona\ndefinitions (e.g., nationality, age, occupation, personality, beliefs,\nbehaviors) and programmatic control via numerous LLM-driven mechanisms. This\nallows for the concise formulation of behavioral problems of practical\ninterest, either at the individual or group level, and provides effective means\nfor their solution. TinyTroupe's components are presented using representative\nworking examples, such as brainstorming and market research sessions, thereby\nsimultaneously clarifying their purpose and demonstrating their usefulness.\nQuantitative and qualitative evaluations of selected aspects are also provided,\nhighlighting possibilities, limitations, and trade-offs. The approach, though\nrealized as a specific Python implementation, is meant as a novel conceptual\ncontribution, which can be partially or fully incorporated in other contexts.\nThe library is available as open source at\nhttps://github.com/microsoft/tinytroupe.\n","authors":["Paulo Salem","Robert Sim","Christopher Olsen","Prerit Saxena","Rafael Barcelos","Yi Ding"],"pdf_url":"https://arxiv.org/pdf/2507.09788v1.pdf","comment":"9 pages. Preprint to be submitted to peer-review"},{"id":"http://arxiv.org/abs/2507.09777v1","updated":"2025-07-13T20:19:08Z","published":"2025-07-13T20:19:08Z","title":"Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in\n  Spanish News","summary":"  We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score.\n","authors":["Gabriel Mordecki","Guillermo Moncecchi","Javier Couto"],"pdf_url":"https://arxiv.org/pdf/2507.09777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11393v2","updated":"2025-07-13T20:17:49Z","published":"2025-04-15T17:02:15Z","title":"DataDecide: How to Predict Best Pretraining Data with Small Experiments","summary":"  Because large language models are expensive to pretrain on different\ndatasets, using smaller-scale experiments to decide on data is crucial for\nreducing costs. Which benchmarks and methods of making decisions from observed\nperformance at small scale most accurately predict the datasets that yield the\nbest large models? To empower open exploration of this question, we release\nmodels, data, and evaluations in DataDecide -- the most extensive open suite of\nmodels over differences in data and scale. We conduct controlled pretraining\nexperiments across 25 corpora with differing sources, deduplication, and\nfiltering up to 100B tokens, model sizes up to 1B parameters, and 3 random\nseeds. We find that the ranking of models at a single, small size (e.g., 150M\nparameters) is a strong baseline for predicting best models at our larger\ntarget scale (1B) (~80% of com parisons correct). No scaling law methods among\n8 baselines exceed the compute-decision frontier of single-scale predictions,\nbut DataDecide can measure improvement in future scaling laws. We also identify\nthat using continuous likelihood metrics as proxies in small experiments makes\nbenchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable\nat the target 1B scale with just 0.01% of the compute.\n","authors":["Ian Magnusson","Nguyen Tai","Ben Bogin","David Heineman","Jena D. Hwang","Luca Soldaini","Akshita Bhagia","Jiacheng Liu","Dirk Groeneveld","Oyvind Tafjord","Noah A. Smith","Pang Wei Koh","Jesse Dodge"],"pdf_url":"https://arxiv.org/pdf/2504.11393v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2312.11462v5","updated":"2025-07-13T19:45:40Z","published":"2023-12-18T18:59:46Z","title":"Cascade Speculative Drafting for Even Faster LLM Inference","summary":"  Introduced to enhance the efficiency of large language model (LLM) inference,\nspeculative decoding operates by having a smaller model generate a draft. A\nlarger target model then reviews this draft to align with its output, and any\nacceptance by the target model results in a reduction of the number of the\ntarget model runs, ultimately improving efficiency. However, the drafting\nprocess in speculative decoding includes slow autoregressive generation and\nallocates equal time to generating tokens, irrespective of their importance.\nThese inefficiencies collectively contribute to the suboptimal performance of\nspeculative decoding. To further improve LLM inference, we introduce Cascade\nSpeculative Drafting (CS Drafting), a speculative execution algorithm that\nincorporates two types of cascades. The Vertical Cascade eliminates\nautoregressive generation from neural models, while the Horizontal Cascade\noptimizes time allocation in drafting for improved efficiency. Combining both\ncascades, CS Drafting achieves greater speedup compared to the baselines in our\nexperiments, while preserving the same output distribution as the target model.\n","authors":["Ziyi Chen","Xiaocong Yang","Jiacheng Lin","Chenkai Sun","Kevin Chen-Chuan Chang","Jie Huang"],"pdf_url":"https://arxiv.org/pdf/2312.11462v5.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.08985v3","updated":"2025-07-13T19:41:20Z","published":"2024-12-12T06:38:40Z","title":"KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts\n  in K-12 Education?","summary":"  Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs.\n","authors":["Tianshi Zheng","Weihan Li","Jiaxin Bai","Weiqi Wang","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2412.08985v3.pdf","comment":"ACL 2025 Main"},{"id":"http://arxiv.org/abs/2507.09762v1","updated":"2025-07-13T19:40:36Z","published":"2025-07-13T19:40:36Z","title":"EventHunter: Dynamic Clustering and Ranking of Security Events from\n  Hacker Forum Discussions","summary":"  Hacker forums provide critical early warning signals for emerging\ncybersecurity threats, but extracting actionable intelligence from their\nunstructured and noisy content remains a significant challenge. This paper\npresents an unsupervised framework that automatically detects, clusters, and\nprioritizes security events discussed across hacker forum posts. Our approach\nleverages Transformer-based embeddings fine-tuned with contrastive learning to\ngroup related discussions into distinct security event clusters, identifying\nincidents like zero-day disclosures or malware releases without relying on\npredefined keywords. The framework incorporates a daily ranking mechanism that\nprioritizes identified events using quantifiable metrics reflecting timeliness,\nsource credibility, information completeness, and relevance. Experimental\nevaluation on real-world hacker forum data demonstrates that our method\neffectively reduces noise and surfaces high-priority threats, enabling security\nanalysts to mount proactive responses. By transforming disparate hacker forum\ndiscussions into structured, actionable intelligence, our work addresses\nfundamental challenges in automated threat detection and analysis.\n","authors":["Yasir Ech-Chammakhy","Anas Motii","Anass Rabii","Jaafar Chbili"],"pdf_url":"https://arxiv.org/pdf/2507.09762v1.pdf","comment":"Accepted for publication at the 28th International Symposium on\n  Research in Attacks, Intrusions, and Defenses (RAID 2025)"},{"id":"http://arxiv.org/abs/2507.09758v1","updated":"2025-07-13T19:36:17Z","published":"2025-07-13T19:36:17Z","title":"Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive\n  Curriculum Learning Paradigm for Natural Language Understanding","summary":"  Curriculum learning is a widely adopted training strategy in natural language\nprocessing (NLP), where models are exposed to examples organized by increasing\ndifficulty to enhance learning efficiency and performance. However, most\nexisting approaches rely on manually defined difficulty metrics -- such as text\nlength -- which may not accurately reflect the model's own perspective. To\novercome this limitation, we present a self-adaptive curriculum learning\nparadigm that prioritizes fine-tuning examples based on difficulty scores\npredicted by pre-trained language models (PLMs) themselves. Building on these\nscores, we explore various training strategies that differ in the ordering of\nexamples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed\nsampling. We evaluate our method on four natural language understanding (NLU)\ndatasets covering both binary and multi-class classification tasks.\nExperimental results show that our approach leads to faster convergence and\nimproved performance compared to standard random sampling.\n","authors":["Qi Feng","Yihong Liu","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2507.09758v1.pdf","comment":"18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop\n  (SRW)"},{"id":"http://arxiv.org/abs/2507.09751v1","updated":"2025-07-13T19:05:43Z","published":"2025-07-13T19:05:43Z","title":"Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded\n  Interpretations","summary":"  Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.\n","authors":["Bradley P. Allen","Prateek Chhikara","Thomas Macaulay Ferguson","Filip Ilievski","Paul Groth"],"pdf_url":"https://arxiv.org/pdf/2507.09751v1.pdf","comment":"29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)"},{"id":"http://arxiv.org/abs/2507.05179v2","updated":"2025-07-13T17:59:01Z","published":"2025-07-07T16:34:28Z","title":"From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating\n  Hindi News Veracity Explanations","summary":"  In an era of rampant misinformation, generating reliable news explanations is\nvital, especially for under-represented languages like Hindi. Lacking robust\nautomated tools, Hindi faces challenges in scaling misinformation detection. To\nbridge this gap, we propose a novel framework integrating Direct Preference\nOptimization (DPO) with curriculum learning to align machine-generated\nexplanations with human reasoning. Fact-checked explanations from credible\nsources serve as preferred responses, while LLM outputs highlight system\nlimitations and serve as non-preferred responses. To refine task-specific\nalignment, we introduce two key parameters -- Actuality and Finesse -- into the\nDPO loss function, enhancing explanation quality and consistency. Experiments\nwith LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's\neffectiveness in generating coherent, contextually relevant explanations. This\nscalable approach combats misinformation and extends automated explanation\ngeneration to low-resource languages.\n","authors":["Pulkit Bansal","Raghvendra Kumar","Shakti Singh","Sriparna Saha","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2507.05179v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.17086v2","updated":"2025-07-13T17:26:34Z","published":"2025-05-20T18:33:03Z","title":"Reinforcing Question Answering Agents with Minimalist Policy Gradient\n  Optimization","summary":"  Large Language Models (LLMs) have demonstrated remarkable versatility, due to\nthe lack of factual knowledge, their application to Question Answering (QA)\ntasks remains hindered by hallucination. While Retrieval-Augmented Generation\nmitigates these issues by integrating external knowledge, existing approaches\nrely heavily on in-context learning, whose performance is constrained by the\nfundamental reasoning capabilities of LLMs. In this paper, we propose Mujica, a\nMulti-hop Joint Intelligence for Complex Question Answering, comprising a\nplanner that decomposes questions into a directed acyclic graph of subquestions\nand a worker that resolves questions via retrieval and reasoning. Additionally,\nwe introduce MyGO (Minimalist policy Gradient Optimization), a novel\nreinforcement learning method that replaces traditional policy gradient updates\nwith Maximum Likelihood Estimation (MLE) by sampling trajectories from an\nasymptotically optimal policy. MyGO eliminates the need for gradient rescaling\nand reference models, ensuring stable and efficient training. Empirical results\nacross multiple datasets demonstrate the effectiveness of Mujica-MyGO in\nenhancing multi-hop QA performance for various LLMs, offering a scalable and\nresource-efficient solution for complex QA tasks.\n","authors":["Yihong Wu","Liheng Ma","Muzhi Li","Jiaming Zhou","Jianye Hao","Ho-fung Leung","Irwin King","Yingxue Zhang","Jian-Yun Nie"],"pdf_url":"https://arxiv.org/pdf/2505.17086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09709v1","updated":"2025-07-13T17:03:25Z","published":"2025-07-13T17:03:25Z","title":"Large Language Models Encode Semantics in Low-Dimensional Linear\n  Subspaces","summary":"  Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. \\baturay{However,\nit remains unclear to what extent LLMs internally organize representations\nrelated to semantic understanding. To investigate this, we conduct a\nlarge-scale empirical study of hidden states in transformer-based LLMs,\nanalyzing 11 decoder-only models across 6 scientific topics and 12 layers each.\nWe find that high-level semantic information consistently lies in\nlow-dimensional subspaces that form linearly separable representations across\ndistinct domains. This separability becomes more pronounced in deeper layers\nand under prompts that trigger structured reasoning or alignment\nbehaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry\nenables simple yet effective causal interventions in hidden space; for example,\nreasoning patterns like chain-of-thought can be captured by a single vector\ndirection. Together, these findings support the development of geometry-aware\ntools that operate directly on latent representations to detect and mitigate\nharmful or adversarial content, using methods such as transport-based defenses\nthat leverage this separability. As a proof of concept, we demonstrate this\npotential by training a simple MLP classifier as a lightweight latent-space\nguardrail, which detects adversarial and malicious prompts with high precision.\n","authors":["Baturay Saglam","Paul Kassianik","Blaine Nelson","Sajana Weerawardhena","Yaron Singer","Amin Karbasi"],"pdf_url":"https://arxiv.org/pdf/2507.09709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06448v2","updated":"2025-07-13T16:48:37Z","published":"2025-07-08T23:22:34Z","title":"Perception-Aware Policy Optimization for Multimodal Reasoning","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.\n","authors":["Zhenhailong Wang","Xuehang Guo","Sofia Stoica","Haiyang Xu","Hongru Wang","Hyeonjeong Ha","Xiusi Chen","Yangyi Chen","Ming Yan","Fei Huang","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2507.06448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09701v1","updated":"2025-07-13T16:24:35Z","published":"2025-07-13T16:24:35Z","title":"MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of\n  LLMs","summary":"  Large language models exhibit cultural biases and limited cross-cultural\nunderstanding capabilities, particularly when serving diverse global user\npopulations. We propose MCEval, a novel multilingual evaluation framework that\nemploys dynamic cultural question construction and enables causal analysis\nthrough Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive\nevaluation spans 13 cultures and 13 languages, systematically assessing both\ncultural awareness and cultural bias across different linguistic scenarios. The\nframework provides 39,897 cultural awareness instances and 17,940 cultural bias\ninstances. Experimental results reveal performance disparities across different\nlinguistic scenarios, demonstrating that optimal cultural performance is not\nonly linked to training data distribution, but also is related to\nlanguage-culture alignment. The evaluation results also expose the fairness\nissue, where approaches appearing successful in the English scenario create\nsubstantial disadvantages. MCEval represents the first comprehensive\nmultilingual cultural evaluation framework that provides deeper insights into\nLLMs' cultural understanding.\n","authors":["Shulin Huang","Linyi Yang","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.09701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.22777v2","updated":"2025-07-13T15:36:35Z","published":"2025-06-28T06:37:10Z","title":"Teaching Models to Verbalize Reward Hacking in Chain-of-Thought\n  Reasoning","summary":"  Language models trained with reinforcement learning (RL) can engage in reward\nhacking--the exploitation of unintended strategies for high reward--without\nrevealing this behavior in their chain-of-thought reasoning. This makes the\ndetection of reward hacking difficult, posing risks for high-stakes\napplications. We propose verbalization fine-tuning (VFT), a pre-RL fine-tuning\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., \"a\nStanford professor thinks the answer is A\"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to exploit\nthese cues instead of reasoning correctly. We measure how often models exploit\nthese cues without verbalizing it. After RL, only 6% of the VFT-trained model's\nresponses consist of undetected reward hacks. In comparison, when we perform RL\nwithout VFT, the rate of undetected reward hacks goes up to 88%; with a\ndebiasing baseline intervention, this increases further to 99%. VFT achieves\nthis by substantially increasing how often models verbalize the influence of\ncues, from 8% to 43% after VFT, and up to 94% after RL. Baselines remain low\neven after RL (11% and 1%). Our results show that teaching models to explicitly\nverbalize reward hacking behavior before RL significantly improves their\ndetection, offering a practical path toward more transparent and safe AI\nsystems.\n","authors":["Miles Turpin","Andy Arditi","Marvin Li","Joe Benton","Julian Michael"],"pdf_url":"https://arxiv.org/pdf/2506.22777v2.pdf","comment":"Published at ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models"},{"id":"http://arxiv.org/abs/2506.23146v3","updated":"2025-07-13T15:01:01Z","published":"2025-06-29T08:55:37Z","title":"Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness\n  Beyond Performance Illusions","summary":"  In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.\n","authors":["Dingzriui Wang","Xuanliang Zhang","Keyan Xu","Qingfu Zhu","Wanxiang Che","Yang Deng"],"pdf_url":"https://arxiv.org/pdf/2506.23146v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09662v1","updated":"2025-07-13T14:51:59Z","published":"2025-07-13T14:51:59Z","title":"Towards Concise and Adaptive Thinking in Large Reasoning Models: A\n  Survey","summary":"  Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs.\n","authors":["Jason Zhu","Hongyu Li"],"pdf_url":"https://arxiv.org/pdf/2507.09662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02240v2","updated":"2025-07-13T14:49:18Z","published":"2025-03-04T03:30:56Z","title":"OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale","summary":"  Text-to-SQL, the task of translating natural language questions into SQL\nqueries, plays a crucial role in enabling non-experts to interact with\ndatabases. While recent advancements in large language models (LLMs) have\nsignificantly enhanced text-to-SQL performance, existing approaches face\nnotable limitations in real-world text-to-SQL applications. Prompting-based\nmethods often depend on closed-source LLMs, which are expensive, raise privacy\nconcerns, and lack customization. Fine-tuning-based methods, on the other hand,\nsuffer from poor generalizability due to the limited coverage of publicly\navailable training data. To overcome these challenges, we propose a novel and\nscalable text-to-SQL data synthesis framework for automatically synthesizing\nlarge-scale, high-quality, and diverse datasets without extensive human\nintervention. Using this framework, we introduce SynSQL-2.5M, the first\nmillion-scale text-to-SQL dataset, containing 2.5 million samples spanning over\n16,000 synthetic databases. Each sample includes a database, SQL query, natural\nlanguage question, and chain-of-thought (CoT) solution. Leveraging SynSQL-2.5M,\nwe develop OmniSQL, a powerful open-source text-to-SQL model available in three\nsizes: 7B, 14B, and 32B. Extensive evaluations across nine datasets demonstrate\nthat OmniSQL achieves state-of-the-art performance, matching or surpassing\nleading closed-source and open-source LLMs, including GPT-4o and DeepSeek-V3,\ndespite its smaller size. We release all code, datasets, and models to support\nfurther research.\n","authors":["Haoyang Li","Shang Wu","Xiaokang Zhang","Xinmei Huang","Jing Zhang","Fuxin Jiang","Shuai Wang","Tieying Zhang","Jianjun Chen","Rui Shi","Hong Chen","Cuiping Li"],"pdf_url":"https://arxiv.org/pdf/2503.02240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06377v2","updated":"2025-07-13T14:32:15Z","published":"2024-09-10T09:58:55Z","title":"MoRE: A Mixture of Reflectors Framework for Large Language Model-Based\n  Sequential Recommendation","summary":"  Large language models (LLMs) have emerged as a cutting-edge approach in\nsequential recommendation, leveraging historical interactions to model dynamic\nuser preferences. Current methods mainly focus on learning processed\nrecommendation data in the form of sequence-to-sequence text. While effective,\nthey exhibit three key limitations: 1) failing to decouple intra-user explicit\nfeatures (e.g., product titles) from implicit behavioral patterns (e.g., brand\nloyalty) within interaction histories; 2) underutilizing cross-user\ncollaborative filtering (CF) signals; and 3) relying on inefficient reflection\nupdate strategies. To address this, We propose MoRE (Mixture of REflectors),\nwhich introduces three perspective-aware offline reflection processes to\naddress these gaps. This decomposition directly resolves Challenges 1\n(explicit/implicit ambiguity) and 2 (CF underutilization). Furthermore, MoRE's\nmeta-reflector employs a self-improving strategy and a dynamic selection\nmechanism (Challenge 3) to adapt to evolving user preferences. First, two\nintra-user reflectors decouple explicit and implicit patterns from a user's\ninteraction sequence, mimicking traditional recommender systems' ability to\ndistinguish surface-level and latent preferences. A third cross-user reflector\ncaptures CF signals by analyzing user similarity patterns from multiple users'\ninteractions. To optimize reflection quality, MoRE's meta-reflector employs a\noffline self-improving strategy that evaluates reflection impacts through\ncomparisons of presence/absence and iterative refinement of old/new versions,\nwith a online contextual bandit mechanism dynamically selecting the optimal\nperspective for recommendation for each user. Code:\nhttps://github.com/E-qin/MoRE-Rec.\n","authors":["Weicong Qin","Yi Xu","Weijie Yu","Chenglei Shen","Xiao Zhang","Ming He","Jianping Fan","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2409.06377v2.pdf","comment":"First 2 authors contributes equally to this work, accepted by\n  RecSys'25 spotlight oral. Corresponding author is Weijie Yu(yu@uibe.edu.cn)"},{"id":"http://arxiv.org/abs/2507.09638v1","updated":"2025-07-13T14:05:48Z","published":"2025-07-13T14:05:48Z","title":"Can Group Relative Policy Optimization Improve Thai Legal Reasoning and\n  Question Answering?","summary":"  The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal\nquestion answering is still limited, especially for questions requiring\nextensive, complex legal reasoning. To address these limitations, we introduce\nan approach aligning LLMs toward improved law citation accuracy and better\nresponse quality using Group-Relative Policy Optimization (GRPO). Our approach\nleverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,\nsignificantly reducing computational expenses up to 2.5x compared to large\nlanguage model judges. Experiments on the NitiBench benchmark demonstrate\nsubstantial improvements: GRPO achieves up to 90% citation-F1 gains from the\nbase model and a 31% increase in joint quality metrics over instruction tuning.\nCrucially, our method shows enhanced robustness on complex legal reasoning\ntasks compared to instruction tuning, providing an effective and\nresource-efficient solution for enhancing Thai legal LLMs.\n","authors":["Pawitsapak Akarajaradwong","Chompakorn Chaksangchaichot","Pirat Pothavorn","Attapol Thamrongrattanarit-Rutherford","Ekapol Chuangsuwanich","Sarana Nutanong"],"pdf_url":"https://arxiv.org/pdf/2507.09638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09629v1","updated":"2025-07-13T13:49:52Z","published":"2025-07-13T13:49:52Z","title":"An Exploration of Knowledge Editing for Arabic","summary":"  While Knowledge Editing (KE) has been widely explored in English, its\nbehavior in morphologically rich languages like Arabic remains underexamined.\nIn this work, we present the first study of Arabic KE. We evaluate four methods\n(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact\nbenchmarks, analyzing both multilingual and cross-lingual settings. Our\nexperiments on Llama-2-7B-chat show show that parameter-based methods struggle\nwith cross-lingual generalization, while instruction-tuned methods perform more\nrobustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show\nthat joint Arabic-English training improves both editability and transfer. We\nrelease Arabic KE benchmarks and multilingual training for LTE data to support\nfuture research.\n","authors":["Basel Mousi","Nadir Durrani","Fahim Dalvi"],"pdf_url":"https://arxiv.org/pdf/2507.09629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09628v1","updated":"2025-07-13T13:49:29Z","published":"2025-07-13T13:49:29Z","title":"SpreadPy: A Python tool for modelling spreading activation and\n  superdiffusion in cognitive multiplex networks","summary":"  We introduce SpreadPy as a Python library for simulating spreading activation\nin cognitive single-layer and multiplex networks. Our tool is designed to\nperform numerical simulations testing structure-function relationships in\ncognitive processes. By comparing simulation results with grounded theories in\nknowledge modelling, SpreadPy enables systematic investigations of how\nactivation dynamics reflect cognitive, psychological and clinical phenomena. We\ndemonstrate the library's utility through three case studies: (1) Spreading\nactivation on associative knowledge networks distinguishes students with high\nversus low math anxiety, revealing anxiety-related structural differences in\nconceptual organization; (2) Simulations of a creativity task show that\nactivation trajectories vary with task difficulty, exposing how cognitive load\nmodulates lexical access; (3) In individuals with aphasia, simulated activation\npatterns on lexical networks correlate with empirical error types (semantic vs.\nphonological) during picture-naming tasks, linking network structure to\nclinical impairments. SpreadPy's flexible framework allows researchers to model\nthese processes using empirically derived or theoretical networks, providing\nmechanistic insights into individual differences and cognitive impairments. The\nlibrary is openly available, supporting reproducible research in psychology,\nneuroscience, and education research.\n","authors":["Salvatore Citraro","Edith Haim","Alessandra Carini","Cynthia S. Q. Siew","Giulio Rossetti","Massimo Stella"],"pdf_url":"https://arxiv.org/pdf/2507.09628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07586v2","updated":"2025-07-13T12:37:30Z","published":"2025-07-10T09:42:47Z","title":"Your Absorbing Discrete Diffusion Secretly Models the Bayesian Posterior","summary":"  Discrete diffusion language models learn to reconstruct text from randomly\nmasked inputs, yet under mild assumptions their denoiser already implements the\nexact Bayesian posterior over the original tokens. We prove that the expected\ndenoiser output under the forward corruption distribution recovers the true\nposterior, and that a simple Monte Carlo estimator converges to this posterior\nat rate O(1/sqrt(K)) with finite-sample concentration bounds. Building on this\ninsight, we introduce an inference-time ensemble that runs K independent\ndenoising passes and aggregates both posterior means and variances without any\nextra training. On WikiText-2, our MC-marginal sampler recovers the analytic\nlambda-DCE zero-shot perplexity (approximately 39) to within a few points at\nK=128, and its per-token variance shows a strong rank correlation with\nreconstruction error (Spearman rho = 0.996). This cost-proportional procedure\nyields calibrated uncertainty estimates and a direct trade-off between compute\nand posterior fidelity in discrete diffusion LMs.\n","authors":["Cooper Doyle"],"pdf_url":"https://arxiv.org/pdf/2507.07586v2.pdf","comment":"12 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2507.09601v1","updated":"2025-07-13T12:14:57Z","published":"2025-07-13T12:14:57Z","title":"NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of\n  Finance","summary":"  General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance.\n","authors":["Hanwool Lee","Sara Yu","Yewon Hwang","Jonghyun Choi","Heejae Ahn","Sungbum Jung","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2507.09601v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2507.09574v1","updated":"2025-07-13T10:52:59Z","published":"2025-07-13T10:52:59Z","title":"MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive\n  Vision Generation Models","summary":"  Recent text-to-image models produce high-quality results but still struggle\nwith precise visual control, balancing multimodal inputs, and requiring\nextensive training for complex multimodal image generation. To address these\nlimitations, we propose MENTOR, a novel autoregressive (AR) framework for\nefficient Multimodal-conditioned Tuning for Autoregressive multimodal image\ngeneration. MENTOR combines an AR image generator with a two-stage training\nparadigm, enabling fine-grained, token-level alignment between multimodal\ninputs and image outputs without relying on auxiliary adapters or\ncross-attention modules. The two-stage training consists of: (1) a multimodal\nalignment stage that establishes robust pixel- and semantic-level alignment,\nfollowed by (2) a multimodal instruction tuning stage that balances the\nintegration of multimodal inputs and enhances generation controllability.\nDespite modest model size, suboptimal base components, and limited training\nresources, MENTOR achieves strong performance on the DreamBench++ benchmark,\noutperforming competitive baselines in concept preservation and prompt\nfollowing. Additionally, our method delivers superior image reconstruction\nfidelity, broad task adaptability, and improved training efficiency compared to\ndiffusion-based methods. Dataset, code, and models are available at:\nhttps://github.com/HaozheZhao/MENTOR\n","authors":["Haozhe Zhao","Zefan Cai","Shuzheng Si","Liang Chen","Jiuxiang Gu","Wen Xiao","Junjie Hu"],"pdf_url":"https://arxiv.org/pdf/2507.09574v1.pdf","comment":"24 pages,12 figures"},{"id":"http://arxiv.org/abs/2411.07611v5","updated":"2025-07-13T10:35:58Z","published":"2024-11-12T07:34:56Z","title":"Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease\n  Diagnosis with Small Language Models","summary":"  Interpretation is critical for disease diagnosis, but existing models\nstruggle to balance predictive accuracy with human-understandable rationales.\nWhile large language models (LLMs) offer strong reasoning abilities, their\nclinical use is limited by high computational costs and restricted multimodal\nreasoning ability. Small language models (SLMs) are efficient but lack advanced\nreasoning for integrating multimodal medical data. In addition, both LLMs and\nSLMs lack domain knowledge for trustworthy reasoning. Therefore, we propose\nClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via\nrationale distillation and domain knowledge injection for trustworthy\nmultimodal rationale generation. Key innovations include a sequential rationale\ndistillation framework that equips SLMs with LLM-comparable multimodal\nreasoning abilities, and a knowledge-augmented attention mechanism that jointly\nunifies multimodal representation from time series and textual data in the same\nencoding space, enabling it to be naturally interpreted by SLMs while\nincorporating domain knowledge for reliable rationale generation. Experiments\non real-world medical datasets show that ClinRaGen achieves state-of-the-art\nperformance in disease diagnosis and rationale generation, demonstrating the\neffectiveness of combining LLM-driven reasoning with knowledge augmentation for\nimproved interpretability.\n","authors":["Shuai Niu","Jing Ma","Hongzhan Lin","Liang Bai","Zhihua Wang","Yida Xu","Yunya Song","Xian Yang"],"pdf_url":"https://arxiv.org/pdf/2411.07611v5.pdf","comment":"13 pages. 7 figures"},{"id":"http://arxiv.org/abs/2507.09536v1","updated":"2025-07-13T08:35:23Z","published":"2025-07-13T08:35:23Z","title":"Adapting Definition Modeling for New Languages: A Case Study on\n  Belarusian","summary":"  Definition modeling, the task of generating new definitions for words in\ncontext, holds great prospect as a means to assist the work of lexicographers\nin documenting a broader variety of lects and languages, yet much remains to be\ndone in order to assess how we can leverage pre-existing models for as-of-yet\nunsupported languages. In this work, we focus on adapting existing models to\nBelarusian, for which we propose a novel dataset of 43,150 definitions. Our\nexperiments demonstrate that adapting a definition modeling systems requires\nminimal amounts of data, but that there currently are gaps in what automatic\nmetrics do capture.\n","authors":["Daniela Kazakouskaya","Timothee Mickus","Janine Siewert"],"pdf_url":"https://arxiv.org/pdf/2507.09536v1.pdf","comment":"To appear at SlavicNLP 2025"},{"id":"http://arxiv.org/abs/2505.08245v2","updated":"2025-07-13T08:25:01Z","published":"2025-05-13T05:47:51Z","title":"Large Language Model Psychometrics: A Systematic Review of Evaluation,\n  Validation, and Enhancement","summary":"  The advancement of large language models (LLMs) has outpaced traditional\nevaluation methodologies. This progress presents novel challenges, such as\nmeasuring human-like psychological constructs, moving beyond static and\ntask-specific benchmarks, and establishing human-centered evaluation. These\nchallenges intersect with psychometrics, the science of quantifying the\nintangible aspects of human psychology, such as personality, values, and\nintelligence. This review paper introduces and synthesizes the emerging\ninterdisciplinary field of LLM Psychometrics, which leverages psychometric\ninstruments, theories, and principles to evaluate, understand, and enhance\nLLMs. The reviewed literature systematically shapes benchmarking principles,\nbroadens evaluation scopes, refines methodologies, validates results, and\nadvances LLM capabilities. Diverse perspectives are integrated to provide a\nstructured framework for researchers across disciplines, enabling a more\ncomprehensive understanding of this nascent field. Ultimately, the review\nprovides actionable insights for developing future evaluation paradigms that\nalign with human-level AI and promote the advancement of human-centered AI\nsystems for societal benefit. A curated repository of LLM psychometric\nresources is available at\nhttps://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.\n","authors":["Haoran Ye","Jing Jin","Yuhang Xie","Xin Zhang","Guojie Song"],"pdf_url":"https://arxiv.org/pdf/2505.08245v2.pdf","comment":"474 references"},{"id":"http://arxiv.org/abs/2503.09639v4","updated":"2025-07-13T07:36:02Z","published":"2025-03-12T02:54:15Z","title":"Can A Society of Generative Agents Simulate Human Behavior and Inform\n  Public Health Policy? A Case Study on Vaccine Hesitancy","summary":"  Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment.\n","authors":["Abe Bohan Hou","Hongru Du","Yichen Wang","Jingyu Zhang","Zixiao Wang","Paul Pu Liang","Daniel Khashabi","Lauren Gardner","Tianxing He"],"pdf_url":"https://arxiv.org/pdf/2503.09639v4.pdf","comment":"Accepted to COLM 2025"},{"id":"http://arxiv.org/abs/2507.09509v1","updated":"2025-07-13T06:33:12Z","published":"2025-07-13T06:33:12Z","title":"How Important is `Perfect' English for Machine Translation Prompts?","summary":"  Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans.\n","authors":["Patrícia Schmidtová","Niyati Bafna","Seth Aycock","Gianluca Vico","Wiktor Kamzela","Katharina Hämmerl","Vilém Zouhar"],"pdf_url":"https://arxiv.org/pdf/2507.09509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09506v1","updated":"2025-07-13T06:17:53Z","published":"2025-07-13T06:17:53Z","title":"Ref-Long: Benchmarking the Long-context Referencing Capability of\n  Long-context Language Models","summary":"  Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long.\n","authors":["Junjie Wu","Gefei Gu","Yanan Zheng","Dit-Yan Yeung","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2507.09506v1.pdf","comment":"ACL 2025 Main Conference. First 2 authors contributed equally"},{"id":"http://arxiv.org/abs/2409.05137v3","updated":"2025-07-13T05:31:12Z","published":"2024-09-08T15:42:48Z","title":"READoc: A Unified Benchmark for Realistic Document Structured Extraction","summary":"  Document Structured Extraction (DSE) aims to extract structured content from\nraw documents. Despite the emergence of numerous DSE systems, their unified\nevaluation remains inadequate, significantly hindering the field's advancement.\nThis problem is largely attributed to existing benchmark paradigms, which\nexhibit fragmented and localized characteristics. To address these limitations\nand offer a thorough evaluation of DSE systems, we introduce a novel benchmark\nnamed READoc, which defines DSE as a realistic task of converting unstructured\nPDFs into semantically rich Markdown. The READoc dataset is derived from 3,576\ndiverse and real-world documents from arXiv, GitHub, and Zenodo. In addition,\nwe develop a DSE Evaluation S$^3$uite comprising Standardization, Segmentation\nand Scoring modules, to conduct a unified evaluation of state-of-the-art DSE\napproaches. By evaluating a range of pipeline tools, expert visual models, and\ngeneral VLMs, we identify the gap between current work and the unified,\nrealistic DSE objective for the first time. We aspire that READoc will catalyze\nfuture research in DSE, fostering more comprehensive and practical solutions.\n","authors":["Zichao Li","Aizier Abulaiti","Yaojie Lu","Xuanang Chen","Jia Zheng","Hongyu Lin","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2409.05137v3.pdf","comment":"ACL 2025 Findings"},{"id":"http://arxiv.org/abs/2310.10873v3","updated":"2025-07-13T05:18:48Z","published":"2023-10-16T22:53:54Z","title":"IDEAL: Influence-Driven Selective Annotations Empower In-Context\n  Learners in Large Language Models","summary":"  In-context learning is a promising paradigm that utilizes in-context examples\nas prompts for the predictions of large language models. These prompts are\ncrucial for achieving strong performance. However, since the prompts need to be\nsampled from a large volume of annotated examples, finding the right prompt may\nresult in high annotation costs. To address this challenge, this paper\nintroduces an influence-driven selective annotation method that aims to\nminimize annotation costs while improving the quality of in-context examples.\nThe essence of our method is to select a pivotal subset from a large-scale\nunlabeled data pool to annotate for the subsequent sampling of prompts.\nSpecifically, a directed graph is first constructed to represent unlabeled\ndata. Afterward, the influence of candidate unlabeled subsets is quantified\nwith a diffusion process. A simple yet effective greedy algorithm for unlabeled\ndata selection is lastly introduced. It iteratively selects the data if it\nprovides a maximum marginal gain with respect to quantified influence. Compared\nwith previous efforts on selective annotations, our influence-driven method\nworks in an end-to-end manner, avoids an intractable explicit balance between\ndata diversity and representativeness, and enjoys theoretical support.\nExperiments confirm the superiority of the proposed method on various\nbenchmarks, achieving better performance under lower time consumption during\nsubset selection. The project page is available at\nhttps://skzhang1.github.io/IDEAL/.\n","authors":["Shaokun Zhang","Xiaobo Xia","Zhaoqing Wang","Ling-Hao Chen","Jiale Liu","Qingyun Wu","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.10873v3.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2507.09497v1","updated":"2025-07-13T05:13:52Z","published":"2025-07-13T05:13:52Z","title":"GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent\n  Experience Entities","summary":"  Modern enterprise environments demand intelligent systems capable of handling\ncomplex, dynamic, and multi-faceted tasks with high levels of autonomy and\nadaptability. However, traditional single-purpose AI systems often lack\nsufficient coordination, memory reuse, and task decomposition capabilities,\nlimiting their scalability in realistic settings. To address these challenges,\nwe present \\textbf{GoalfyMax}, a protocol-driven framework for end-to-end\nmulti-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent\n(A2A) communication layer built on the Model Context Protocol (MCP), allowing\nindependent agents to coordinate through asynchronous, protocol-compliant\ninteractions. It incorporates the Experience Pack (XP) architecture, a layered\nmemory system that preserves both task rationales and execution traces,\nenabling structured knowledge retention and continual learning. Moreover, our\nsystem integrates advanced features including multi-turn contextual dialogue,\nlong-short term memory modules, and dynamic safety validation, supporting\nrobust, real-time strategy adaptation. Empirical results on complex task\norchestration benchmarks and case study demonstrate that GoalfyMax achieves\nsuperior adaptability, coordination, and experience reuse compared to baseline\nframeworks. These findings highlight its potential as a scalable, future-ready\nfoundation for multi-agent intelligent systems.\n","authors":["Siyi Wu","Zeyu Wang","Xinyuan Song","Zhengpeng Zhou","Lifan Sun","Tianyu Shi"],"pdf_url":"https://arxiv.org/pdf/2507.09497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07577v3","updated":"2025-07-13T05:00:42Z","published":"2024-02-12T11:18:32Z","title":"Topic Modeling as Multi-Objective Contrastive Optimization","summary":"  Recent representation learning approaches enhance neural topic models by\noptimizing the weighted linear combination of the evidence lower bound (ELBO)\nof the log-likelihood and the contrastive learning objective that contrasts\npairs of input documents. However, document-level contrastive learning might\ncapture low-level mutual information, such as word ratio, which disturbs topic\nmodeling. Moreover, there is a potential conflict between the ELBO loss that\nmemorizes input details for better reconstruction quality, and the contrastive\nloss which attempts to learn topic representations that generalize among input\ndocuments. To address these issues, we first introduce a novel contrastive\nlearning method oriented towards sets of topic vectors to capture useful\nsemantics that are shared among a set of input documents. Secondly, we\nexplicitly cast contrastive topic modeling as a gradient-based multi-objective\noptimization problem, with the goal of achieving a Pareto stationary solution\nthat balances the trade-off between the ELBO and the contrastive objective.\nExtensive experiments demonstrate that our framework consistently produces\nhigher-performing neural topic models in terms of topic coherence, topic\ndiversity, and downstream performance.\n","authors":["Thong Nguyen","Xiaobao Wu","Xinshuai Dong","Cong-Duy T Nguyen","See-Kiong Ng","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2402.07577v3.pdf","comment":"Accepted at ICLR 2024 (poster). Official version available at:\n  https://openreview.net/forum?id=HdAoLSBYXj"},{"id":"http://arxiv.org/abs/2502.07776v2","updated":"2025-07-13T04:42:28Z","published":"2025-02-11T18:58:04Z","title":"Auditing Prompt Caching in Language Model APIs","summary":"  Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.\n","authors":["Chenchen Gu","Xiang Lisa Li","Rohith Kuditipudi","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2502.07776v2.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2507.09485v1","updated":"2025-07-13T04:07:07Z","published":"2025-07-13T04:07:07Z","title":"Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis","summary":"  Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in\nsocial media scenarios to identify the sentiment polarity of specific aspect\nterms in a sentence. Although many existing studies leverage large language\nmodels (LLMs) to perform ABSA due to their strong context understanding\ncapabilities, they still face challenges to learn the context information in\nthe running text because of the short text, as well as the small and unbalanced\nlabeled training data, where most data are labeled with positive sentiment.\nData augmentation (DA) is a feasible strategy for providing richer contextual\ninformation, especially when using LLMs to create synthetic training data, but\nfaces challenges in ensuring a high quality of the augmented data.In this\npaper, we propose an LLM-based ABSA approach with training data\naugmentation.Specifically, an LLM is prompted to generate augmented training\ndata based on the original training data, so as to construct a new training\ndata with larger size and balanced label distributions to better train an ABSA\nmodel. Meanwhile, in order to improve the quality of the augmented data, we\npropose a reinforcement learning approach to optimize the data augmentation.\nLLM.Experiment results and further analyses on English benchmark datasets for\nABSA demonstrate the effectiveness of our approach, where superior performance\nis observed over strong baselines and most existing studies.\n","authors":["Junjie Liu","Yuanhe Tian","Yan Song"],"pdf_url":"https://arxiv.org/pdf/2507.09485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09482v1","updated":"2025-07-13T04:03:05Z","published":"2025-07-13T04:03:05Z","title":"ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive\n  Learning","summary":"  Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.\n","authors":["Changli Wang","Rui Wu","Fang Yin"],"pdf_url":"https://arxiv.org/pdf/2507.09482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09481v1","updated":"2025-07-13T03:52:51Z","published":"2025-07-13T03:52:51Z","title":"Evaluating LLMs on Sequential API Call Through Automated Test Generation","summary":"  By integrating tools from external APIs, Large Language Models (LLMs) have\nexpanded their promising capabilities in a diverse spectrum of complex\nreal-world tasks. However, testing, evaluation, and analysis of LLM tool use\nremain in their early stages. Most existing benchmarks rely on manually\ncollected test cases, many of which cannot be automatically checked for\nsemantic correctness and instead depend on static methods such as string\nmatching. Additionally, these benchmarks often overlook the complex\ninteractions that occur between sequential API calls, which are common in\nreal-world applications. To fill the gap, in this paper, we introduce StateGen,\nan automated framework designed to generate diverse coding tasks involving\nsequential API interactions. StateGen combines state-machine-based API\nconstraint solving and validation, energy-based sampling, and control-flow\ninjection to generate executable programs. These programs are then translated\ninto human-like natural language task descriptions through a collaboration of\ntwo LLM agents. Utilizing StateGen, we construct StateEval, a benchmark\nencompassing 120 verified test cases spanning across three representative\nscenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental\nresults confirm that StateGen can effectively generate challenging and\nrealistic API-oriented tasks, highlighting areas for improvement in current\nLLMs incorporating APIs.\n","authors":["Yuheng Huang","Da Song","Zhenlan Ji","Shuai Wang","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2507.09481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09477v1","updated":"2025-07-13T03:29:41Z","published":"2025-07-13T03:29:41Z","title":"Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs","summary":"  Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.\n","authors":["Yangning Li","Weizhi Zhang","Yuyao Yang","Wei-Chieh Huang","Yaozu Wu","Junyu Luo","Yuanchen Bei","Henry Peng Zou","Xiao Luo","Yusheng Zhao","Chunkit Chan","Yankai Chen","Zhongfen Deng","Yinghui Li","Hai-Tao Zheng","Dongyuan Li","Renhe Jiang","Ming Zhang","Yangqiu Song","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2507.09477v1.pdf","comment":"submitted to ARR May"},{"id":"http://arxiv.org/abs/2507.09474v1","updated":"2025-07-13T03:21:05Z","published":"2025-07-13T03:21:05Z","title":"The CoNLL-2013 Shared Task on Grammatical Error Correction","summary":"  The CoNLL-2013 shared task was devoted to grammatical error correction. In\nthis paper, we give the task definition, present the data sets, and describe\nthe evaluation metric and scorer used in the shared task. We also give an\noverview of the various approaches adopted by the participating teams, and\npresent the evaluation results.\n","authors":["Hwee Tou Ng","Siew Mei Wu","Yuanbin Wu","Christian Hadiwinoto","Joel Tetreault"],"pdf_url":"https://arxiv.org/pdf/2507.09474v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2507.09470v1","updated":"2025-07-13T03:10:19Z","published":"2025-07-13T03:10:19Z","title":"Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer\n  Models","summary":"  This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings.\n","authors":["Mingchuan Yang","Ziyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2507.09470v1.pdf","comment":"29 pages, 5 tables"},{"id":"http://arxiv.org/abs/2411.00027v3","updated":"2025-07-13T02:13:14Z","published":"2024-10-29T04:01:11Z","title":"Personalization of Large Language Models: A Survey","summary":"  Personalization of Large Language Models (LLMs) has recently become\nincreasingly important with a wide range of applications. Despite the\nimportance and recent progress, most existing works on personalized LLMs have\nfocused either entirely on (a) personalized text generation or (b) leveraging\nLLMs for personalization-related downstream applications, such as\nrecommendation systems. In this work, we bridge the gap between these two\nseparate main directions for the first time by introducing a taxonomy for\npersonalized LLM usage and summarizing the key differences and challenges. We\nprovide a formalization of the foundations of personalized LLMs that\nconsolidates and expands notions of personalization of LLMs, defining and\ndiscussing novel facets of personalization, usage, and desiderata of\npersonalized LLMs. We then unify the literature across these diverse fields and\nusage scenarios by proposing systematic taxonomies for the granularity of\npersonalization, personalization techniques, datasets, evaluation methods, and\napplications of personalized LLMs. Finally, we highlight challenges and\nimportant open problems that remain to be addressed. By unifying and surveying\nrecent research using the proposed taxonomies, we aim to provide a clear guide\nto the existing literature and different facets of personalization in LLMs,\nempowering both researchers and practitioners.\n","authors":["Zhehao Zhang","Ryan A. Rossi","Branislav Kveton","Yijia Shao","Diyi Yang","Hamed Zamani","Franck Dernoncourt","Joe Barrow","Tong Yu","Sungchul Kim","Ruiyi Zhang","Jiuxiang Gu","Tyler Derr","Hongjie Chen","Junda Wu","Xiang Chen","Zichao Wang","Subrata Mitra","Nedim Lipka","Nesreen Ahmed","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2411.00027v3.pdf","comment":"Accepted at the Transactions on Machine Learning Research (TMLR)\n  journal"},{"id":"http://arxiv.org/abs/2507.07803v2","updated":"2025-07-13T01:40:13Z","published":"2025-07-10T14:28:39Z","title":"StreamUni: Achieving Streaming Speech Translation with a Unified Large\n  Speech-Language Model","summary":"  Streaming speech translation (StreamST) requires determining appropriate\ntiming, known as policy, to generate translations while continuously receiving\nsource speech inputs, balancing low latency with high translation quality.\nHowever, existing StreamST methods typically operate on sentence-level speech\nsegments, referred to as simultaneous speech translation (SimulST). In\npractice, they require collaboration with segmentation models to accomplish\nStreamST, where the truncated speech segments constrain SimulST models to make\npolicy decisions and generate translations based on limited contextual\ninformation. Moreover, SimulST models struggle to learn effective policies due\nto the complexity of speech inputs and cross-lingual generation. To address\nthese challenges, we propose StreamUni, which achieves StreamST through a\nunified Large Speech-Language Model (LSLM). Specifically, StreamUni\nincorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate\nmulti-stage outputs. Leveraging these multi-stage outputs, StreamUni\nsimultaneously accomplishes speech segmentation, policy decision, and\ntranslation generation, completing StreamST without requiring massive\npolicy-specific training. Additionally, we propose a streaming CoT training\nmethod that enhances low-latency policy decisions and generation capabilities\nusing limited CoT data. Experiments demonstrate that our approach achieves\nstate-of-the-art performance on StreamST tasks.\n","authors":["Shoutao Guo","Xiang Li","Mengge Liu","Wei Chen","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2507.07803v2.pdf","comment":"The code is at https://github.com/ictnlp/StreamUni; The model is at\n  https://huggingface.co/ICTNLP/StreamUni-Phi4"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.09831v1","updated":"2025-07-13T23:55:05Z","published":"2025-07-13T23:55:05Z","title":"Generative Cognitive Diagnosis","summary":"  Cognitive diagnosis (CD) models latent cognitive states of human learners by\nanalyzing their response patterns on diagnostic tests, serving as a crucial\nmachine learning technique for educational assessment and evaluation.\nTraditional cognitive diagnosis models typically follow a transductive\nprediction paradigm that optimizes parameters to fit response scores and\nextract learner abilities. These approaches face significant limitations as\nthey cannot perform instant diagnosis for new learners without computationally\nexpensive retraining and produce diagnostic outputs with limited reliability.\nIn this study, we introduces a novel generative diagnosis paradigm that\nfundamentally shifts CD from predictive to generative modeling, enabling\ninductive inference of cognitive states without parameter re-optimization. We\npropose two simple yet effective instantiations of this paradigm: Generative\nItem Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model\n(G-NCDM), which achieve excellent performance improvements over traditional\nmethods. The generative approach disentangles cognitive state inference from\nresponse prediction through a well-designed generation process that\nincorporates identifiability and monotonicity conditions. Extensive experiments\non real-world datasets demonstrate the effectiveness of our methodology in\naddressing scalability and reliability challenges, especially $\\times 100$\nspeedup for the diagnosis of new learners. Our framework opens new avenues for\ncognitive diagnosis applications in artificial intelligence, particularly for\nintelligent model evaluation and intelligent education systems. The code is\navailable at https://github.com/CSLiJT/Generative-CD.git.\n","authors":["Jiatong Li","Qi Liu","Mengxiao Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.09831v1.pdf","comment":"Preprint; 15 pages, 12 figures"},{"id":"http://arxiv.org/abs/2506.16035v2","updated":"2025-07-13T19:52:49Z","published":"2025-06-19T05:11:43Z","title":"Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal\n  Document Understanding","summary":"  Retrieval-Augmented Generation (RAG) systems have revolutionized information\nretrieval and question answering, but traditional text-based chunking methods\nstruggle with complex document structures, multi-page tables, embedded figures,\nand contextual dependencies across page boundaries. We present a novel\nmultimodal document chunking approach that leverages Large Multimodal Models\n(LMMs) to process PDF documents in batches while maintaining semantic coherence\nand structural integrity. Our method processes documents in configurable page\nbatches with cross-batch context preservation, enabling accurate handling of\ntables spanning multiple pages, embedded visual elements, and procedural\ncontent. We evaluate our approach on a curated dataset of PDF documents with\nmanually crafted queries, demonstrating improvements in chunk quality and\ndownstream RAG performance. Our vision-guided approach achieves better accuracy\ncompared to traditional vanilla RAG systems, with qualitative analysis showing\nsuperior preservation of document structure and semantic coherence.\n","authors":["Vishesh Tripathi","Tanmay Odapally","Indraneel Das","Uday Allu","Biddwan Ahmed"],"pdf_url":"https://arxiv.org/pdf/2506.16035v2.pdf","comment":"11 pages, 1 Figure, 1 Table"},{"id":"http://arxiv.org/abs/2504.05319v2","updated":"2025-07-13T17:14:34Z","published":"2025-02-23T11:47:57Z","title":"Predictive Modeling: BIM Command Recommendation Based on Large-scale\n  Usage Logs","summary":"  The adoption of Building Information Modeling (BIM) and model-based design\nwithin the Architecture, Engineering, and Construction (AEC) industry has been\nhindered by the perception that using BIM authoring tools demands more effort\nthan conventional 2D drafting. To enhance design efficiency, this paper\nproposes a BIM command recommendation framework that predicts the optimal next\nactions in real-time based on users' historical interactions. We propose a\ncomprehensive filtering and enhancement method for large-scale raw BIM log data\nand introduce a novel command recommendation model. Our model builds upon the\nstate-of-the-art Transformer backbones originally developed for large language\nmodels (LLMs), incorporating a custom feature fusion module, dedicated loss\nfunction, and targeted learning strategy. In a case study, the proposed method\nis applied to over 32 billion rows of real-world log data collected globally\nfrom the BIM authoring software Vectorworks. Experimental results demonstrate\nthat our method can learn universal and generalizable modeling patterns from\nanonymous user interaction sequences across different countries, disciplines,\nand projects. When generating recommendations for the next command, our\napproach achieves a Recall@10 of approximately 84%. The code is available at:\nhttps://github.com/dcy0577/BIM-Command-Recommendation.git\n","authors":["Changyu Du","Zihan Deng","Stavros Nousias","André Borrmann"],"pdf_url":"https://arxiv.org/pdf/2504.05319v2.pdf","comment":"Advanced Engineering Informatics"},{"id":"http://arxiv.org/abs/2507.03503v2","updated":"2025-07-13T15:49:31Z","published":"2025-07-04T11:56:11Z","title":"Exploring the Effect of Context-Awareness and Popularity Calibration on\n  Popularity Bias in POI Recommendations","summary":"  Point-of-interest (POI) recommender systems help users discover relevant\nlocations, but their effectiveness is often compromised by popularity bias,\nwhich disadvantages less popular, yet potentially meaningful places. This paper\naddresses this challenge by evaluating the effectiveness of context-aware\nmodels and calibrated popularity techniques as strategies for mitigating\npopularity bias. Using four real-world POI datasets (Brightkite, Foursquare,\nGowalla, and Yelp), we analyze the individual and combined effects of these\napproaches on recommendation accuracy and popularity bias. Our results reveal\nthat context-aware models cannot be considered a uniform solution, as the\nmodels studied exhibit divergent impacts on accuracy and bias. In contrast,\ncalibration techniques can effectively align recommendation popularity with\nuser preferences, provided there is a careful balance between accuracy and bias\nmitigation. Notably, the combination of calibration and context-awareness\nyields recommendations that balance accuracy and close alignment with the\nusers' popularity profiles, i.e., popularity calibration.\n","authors":["Andrea Forster","Simone Kopeinik","Denic Helic","Stefan Thalmann","Dominik Kowald"],"pdf_url":"https://arxiv.org/pdf/2507.03503v2.pdf","comment":"Accepted at RecSys 2025, DOI: https://doi.org/10.1145/3705328.3748017"},{"id":"http://arxiv.org/abs/2409.06377v2","updated":"2025-07-13T14:32:15Z","published":"2024-09-10T09:58:55Z","title":"MoRE: A Mixture of Reflectors Framework for Large Language Model-Based\n  Sequential Recommendation","summary":"  Large language models (LLMs) have emerged as a cutting-edge approach in\nsequential recommendation, leveraging historical interactions to model dynamic\nuser preferences. Current methods mainly focus on learning processed\nrecommendation data in the form of sequence-to-sequence text. While effective,\nthey exhibit three key limitations: 1) failing to decouple intra-user explicit\nfeatures (e.g., product titles) from implicit behavioral patterns (e.g., brand\nloyalty) within interaction histories; 2) underutilizing cross-user\ncollaborative filtering (CF) signals; and 3) relying on inefficient reflection\nupdate strategies. To address this, We propose MoRE (Mixture of REflectors),\nwhich introduces three perspective-aware offline reflection processes to\naddress these gaps. This decomposition directly resolves Challenges 1\n(explicit/implicit ambiguity) and 2 (CF underutilization). Furthermore, MoRE's\nmeta-reflector employs a self-improving strategy and a dynamic selection\nmechanism (Challenge 3) to adapt to evolving user preferences. First, two\nintra-user reflectors decouple explicit and implicit patterns from a user's\ninteraction sequence, mimicking traditional recommender systems' ability to\ndistinguish surface-level and latent preferences. A third cross-user reflector\ncaptures CF signals by analyzing user similarity patterns from multiple users'\ninteractions. To optimize reflection quality, MoRE's meta-reflector employs a\noffline self-improving strategy that evaluates reflection impacts through\ncomparisons of presence/absence and iterative refinement of old/new versions,\nwith a online contextual bandit mechanism dynamically selecting the optimal\nperspective for recommendation for each user. Code:\nhttps://github.com/E-qin/MoRE-Rec.\n","authors":["Weicong Qin","Yi Xu","Weijie Yu","Chenglei Shen","Xiao Zhang","Ming He","Jianping Fan","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2409.06377v2.pdf","comment":"First 2 authors contributes equally to this work, accepted by\n  RecSys'25 spotlight oral. Corresponding author is Weijie Yu(yu@uibe.edu.cn)"},{"id":"http://arxiv.org/abs/2503.08965v2","updated":"2025-07-13T12:59:42Z","published":"2025-03-12T00:07:39Z","title":"LLM-Driven Usefulness Labeling for IR Evaluation","summary":"  In the information retrieval (IR) domain, evaluation plays a crucial role in\noptimizing search experiences and supporting diverse user intents. In the\nrecent LLM era, research has been conducted to automate document relevance\nlabels, as these labels have traditionally been assigned by crowd-sourced\nworkers - a process that is both time and consuming and costly. This study\nfocuses on LLM-generated usefulness labels, a crucial evaluation metric that\nconsiders the user's search intents and task objectives, an aspect where\nrelevance falls short. Our experiment utilizes task-level, query-level, and\ndocument-level features along with user search behavior signals, which are\nessential in defining the usefulness of a document. Our research finds that (i)\npre-trained LLMs can generate moderate usefulness labels by understanding the\ncomprehensive search task session, (ii) pre-trained LLMs perform better\njudgement in short search sessions when provided with search session contexts.\nAdditionally, we investigated whether LLMs can capture the unique divergence\nbetween relevance and usefulness, along with conducting an ablation study to\nidentify the most critical metrics for accurate usefulness label generation. In\nconclusion, this work explores LLM-generated usefulness labels by evaluating\ncritical metrics and optimizing for practicality in real-world settings.\n","authors":["Mouly Dewan","Jiqun Liu","Chirag Shah"],"pdf_url":"https://arxiv.org/pdf/2503.08965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09566v1","updated":"2025-07-13T10:24:41Z","published":"2025-07-13T10:24:41Z","title":"Identifying Offline Metrics that Predict Online Impact: A Pragmatic\n  Strategy for Real-World Recommender Systems","summary":"  A critical challenge in recommender systems is to establish reliable\nrelationships between offline and online metrics that predict real-world\nperformance. Motivated by recent advances in Pareto front approximation, we\nintroduce a pragmatic strategy for identifying offline metrics that align with\nonline impact. A key advantage of this approach is its ability to\nsimultaneously serve multiple test groups, each with distinct offline\nperformance metrics, in an online experiment controlled by a single model. The\nmethod is model-agnostic for systems with a neural network backbone, enabling\nbroad applicability across architectures and domains. We validate the strategy\nthrough a large-scale online experiment in the field of session-based\nrecommender systems on the OTTO e-commerce platform. The online experiment\nidentifies significant alignments between offline metrics and real-word\nclick-through rate, post-click conversion rate and units sold. Our strategy\nprovides industry practitioners with a valuable tool for understanding\noffline-to-online metric relationships and making informed, data-driven\ndecisions.\n","authors":["Timo Wilm","Philipp Normann"],"pdf_url":"https://arxiv.org/pdf/2507.09566v1.pdf","comment":"This work was accepted for publication in the 19th ACM Conference on\n  Recommender Systems (RecSys 2025). The final published version will be\n  available at the ACM Digital Library"},{"id":"http://arxiv.org/abs/2507.09488v1","updated":"2025-07-13T04:21:21Z","published":"2025-07-13T04:21:21Z","title":"Criteria-Based LLM Relevance Judgments","summary":"  Relevance judgments are crucial for evaluating information retrieval systems,\nbut traditional human-annotated labels are time-consuming and expensive. As a\nresult, many researchers turn to automatic alternatives to accelerate method\ndevelopment. Among these, Large Language Models (LLMs) provide a scalable\nsolution by generating relevance labels directly through prompting. However,\nprompting an LLM for a relevance label without constraints often results in not\nonly incorrect predictions but also outputs that are difficult for humans to\ninterpret. We propose the Multi-Criteria framework for LLM-based relevance\njudgments, decomposing the notion of relevance into multiple criteria--such as\nexactness, coverage, topicality, and contextual fit--to improve the robustness\nand interpretability of retrieval evaluations compared to direct grading\nmethods. We validate this approach on three datasets: the TREC Deep Learning\ntracks from 2019 and 2020, as well as LLMJudge (based on TREC DL 2023). Our\nresults demonstrate that Multi-Criteria judgments enhance the system\nranking/leaderboard performance. Moreover, we highlight the strengths and\nlimitations of this approach relative to direct grading approaches, offering\ninsights that can guide the development of future automatic evaluation\nframeworks in information retrieval.\n","authors":["Naghmeh Farzi","Laura Dietz"],"pdf_url":"https://arxiv.org/pdf/2507.09488v1.pdf","comment":"10 pages, 3 figures, accepted to ICTIR 2025"},{"id":"http://arxiv.org/abs/2507.09483v1","updated":"2025-07-13T04:05:25Z","published":"2025-07-13T04:05:25Z","title":"Does UMBRELA Work on Other LLMs?","summary":"  We reproduce the UMBRELA LLM Judge evaluation framework across a range of\nlarge language models (LLMs) to assess its generalizability beyond the original\nstudy. Our investigation evaluates how LLM choice affects relevance assessment\naccuracy, focusing on leaderboard rank correlation and per-label agreement\nmetrics. Results demonstrate that UMBRELA with DeepSeek V3 obtains very\ncomparable performance to GPT-4o (used in original work). For LLaMA-3.3-70B we\nobtain slightly lower performance, which further degrades with smaller LLMs.\n","authors":["Naghmeh Farzi","Laura Dietz"],"pdf_url":"https://arxiv.org/pdf/2507.09483v1.pdf","comment":"9 pages, 2 figures, accepted to SIGIR 2025"},{"id":"http://arxiv.org/abs/2507.09439v1","updated":"2025-07-13T01:03:27Z","published":"2025-07-13T01:03:27Z","title":"Dynamic Sparse Causal-Attention Temporal Networks for Interpretable\n  Causality Discovery in Multivariate Time Series","summary":"  Understanding causal relationships in multivariate time series (MTS) is\nessential for effective decision-making in fields such as finance and\nmarketing, where complex dependencies and lagged effects challenge conventional\nanalytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal\nNetworks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel\narchitecture designed to enhance causal discovery by integrating dilated\ntemporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net\neffectively captures multiscale temporal dependencies through dilated\nconvolutions while leveraging an adaptive thresholding strategy in its\nattention mechanism to eliminate spurious connections, ensuring both accuracy\nand interpretability. A statistical shuffle test validation further strengthens\nrobustness by filtering false positives and improving causal inference\nreliability. Extensive evaluations on financial and marketing datasets\ndemonstrate that DyCAST-Net consistently outperforms existing models such as\nTCDF, GCFormer, and CausalFormer. The model provides a more precise estimation\nof causal delays and significantly reduces false discoveries, particularly in\nnoisy environments. Moreover, attention heatmaps offer interpretable insights,\nuncovering hidden causal patterns such as the mediated effects of advertising\non consumer behavior and the influence of macroeconomic indicators on financial\nmarkets. Case studies illustrate DyCAST-Net's ability to detect latent\nmediators and lagged causal factors, making it particularly effective in\nhigh-dimensional, dynamic settings. The model's architecture enhanced by\nRMSNorm stabilization and causal masking ensures scalability and adaptability\nacross diverse application domains\n","authors":["Meriem Zerkouk","Miloud Mihoubi","Belkacem Chikhaoui"],"pdf_url":"https://arxiv.org/pdf/2507.09439v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2312.05348v2","updated":"2025-07-13T18:38:46Z","published":"2023-12-08T20:12:14Z","title":"High-Quality Live Video Streaming via Transcoding Time Prediction and\n  Preset Selection","summary":"  Video streaming often requires transcoding content into different resolutions\nand bitrates to match the recipient's internet speed and screen capabilities.\nVideo encoders like x264 offer various presets, each with different tradeoffs\nbetween transcoding time and rate-distortion performance. Choosing the best\npreset for video transcoding is difficult, especially for live streaming, as\ntrying all the presets and choosing the best one is not feasible. One solution\nis to predict each preset's transcoding time and select the preset that ensures\nthe highest quality while adhering to live streaming time constraints.\nPrediction of video transcoding time is also critical in minimizing streaming\ndelays, deploying resource management algorithms, and load balancing. We\npropose a learning-based framework for predicting the transcoding time of\nvideos across various presets. Our predictor's features for video transcoding\ntime prediction are derived directly from the ingested stream, primarily from\nthe header or metadata. As a result, only minimal additional delay is incurred\nfor feature extraction, rendering our approach ideal for live-streaming\napplications. We evaluated our learning-based transcoding time prediction using\na dataset of videos. The results demonstrate that our framework can accurately\npredict the transcoding time for different presets, with a mean absolute\npercentage error (MAPE) of nearly 5.0%. Leveraging these predictions, we then\nselect the most suitable transcoding preset for live video streaming. Utilizing\nour transcoding time prediction-based preset selection improved Peak\nSignal-to-Noise Ratio (PSNR) of up to 5 dB.\n","authors":["Zahra Nabizadeh Shahre-Babak","Nader Karimi","Krishna Rapaka","Tarek Amara","Shadrokh Samavi","Shahram Shirani"],"pdf_url":"https://arxiv.org/pdf/2312.05348v2.pdf","comment":"After further review, we found major flaws in the paper that need\n  extensive revision"},{"id":"http://arxiv.org/abs/2507.09647v1","updated":"2025-07-13T14:28:20Z","published":"2025-07-13T14:28:20Z","title":"KEN: Knowledge Augmentation and Emotion Guidance Network for Multimodal\n  Fake News Detection","summary":"  In recent years, the rampant spread of misinformation on social media has\nmade accurate detection of multimodal fake news a critical research focus.\nHowever, previous research has not adequately understood the semantics of\nimages, and models struggle to discern news authenticity with limited textual\ninformation. Meanwhile, treating all emotional types of news uniformly without\ntailored approaches further leads to performance degradation. Therefore, we\npropose a novel Knowledge Augmentation and Emotion Guidance Network (KEN). On\nthe one hand, we effectively leverage LVLM's powerful semantic understanding\nand extensive world knowledge. For images, the generated captions provide a\ncomprehensive understanding of image content and scenes, while for text, the\nretrieved evidence helps break the information silos caused by the closed and\nlimited text and context. On the other hand, we consider inter-class\ndifferences between different emotional types of news through balanced\nlearning, achieving fine-grained modeling of the relationship between emotional\ntypes and authenticity. Extensive experiments on two real-world datasets\ndemonstrate the superiority of our KEN.\n","authors":["Peican Zhu","Yubo Jing","Le Cheng","Keke Tang","Yangming Guo"],"pdf_url":"https://arxiv.org/pdf/2507.09647v1.pdf","comment":"Accepted by ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.04959v2","updated":"2025-07-13T09:31:19Z","published":"2025-07-07T13:01:50Z","title":"Hear-Your-Click: Interactive Object-Specific Video-to-Audio Generation","summary":"  Video-to-audio (V2A) generation shows great potential in fields such as film\nproduction. Despite significant advances, current V2A methods relying on global\nvideo information struggle with complex scenes and generating audio tailored to\nspecific objects. To address these limitations, we introduce Hear-Your-Click,\nan interactive V2A framework enabling users to generate sounds for specific\nobjects by clicking on the frame. To achieve this, we propose Object-aware\nContrastive Audio-Visual Fine-tuning (OCAV) with a Mask-guided Visual Encoder\n(MVE) to obtain object-level visual features aligned with audio. Furthermore,\nwe tailor two data augmentation strategies, Random Video Stitching (RVS) and\nMask-guided Loudness Modulation (MLM), to enhance the model's sensitivity to\nsegmented objects. To measure audio-visual correspondence, we designed a new\nevaluation metric, the CAV score. Extensive experiments demonstrate that our\nframework offers more precise control and improves generation performance\nacross various metrics. Project Page:\nhttps://github.com/SynapGrid/Hear-Your-Click\n","authors":["Yingshan Liang","Keyu Fan","Zhicheng Du","Yiran Wang","Qingyang Shi","Xinyu Zhang","Jiasheng Lu","Peiwu Qin"],"pdf_url":"https://arxiv.org/pdf/2507.04959v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06071v2","updated":"2025-07-13T06:17:30Z","published":"2025-07-08T15:14:27Z","title":"MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions\n  by Disentangled Embedding","summary":"  Audio-driven emotional 3D facial animation aims to generate synchronized lip\nmovements and vivid facial expressions. However, most existing approaches focus\non static and predefined emotion labels, limiting their diversity and\nnaturalness. To address these challenges, we propose MEDTalk, a novel framework\nfor fine-grained and dynamic emotional talking head generation. Our approach\nfirst disentangles content and emotion embedding spaces from motion sequences\nusing a carefully designed cross-reconstruction process, enabling independent\ncontrol over lip movements and facial expressions. Beyond conventional\naudio-driven lip synchronization, we integrate audio and speech text,\npredicting frame-wise intensity variations and dynamically adjusting static\nemotion features to generate realistic emotional expressions. Furthermore, to\nenhance control and personalization, we incorporate multimodal inputs-including\ntext descriptions and reference expression images-to guide the generation of\nuser-specified facial expressions. With MetaHuman as the priority, our\ngenerated results can be conveniently integrated into the industrial production\npipeline.\n","authors":["Chang Liu","Ye Pan","Chenyang Ding","Susanto Rahardja","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2507.06071v2.pdf","comment":"11 pages, 8 figures"}]},"2025-07-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2507.09424v1","updated":"2025-07-12T23:29:56Z","published":"2025-07-12T23:29:56Z","title":"DATE-LM: Benchmarking Data Attribution Evaluation for Large Language\n  Models","summary":"  Data attribution methods quantify the influence of training data on model\noutputs and are becoming increasingly relevant for a wide range of LLM research\nand applications, including dataset curation, model interpretability, data\nvaluation. However, there remain critical gaps in systematic LLM-centric\nevaluation of data attribution methods. To this end, we introduce DATE-LM (Data\nAttribution Evaluation in Language Models), a unified benchmark for evaluating\ndata attribution methods through real-world LLM applications. DATE-LM measures\nattribution quality through three key tasks -- training data selection,\ntoxicity/bias filtering, and factual attribution. Our benchmark is designed for\nease of use, enabling researchers to configure and run large-scale evaluations\nacross diverse tasks and LLM architectures. Furthermore, we use DATE-LM to\nconduct a large-scale evaluation of existing data attribution methods. Our\nfindings show that no single method dominates across all tasks, data\nattribution methods have trade-offs with simpler baselines, and method\nperformance is sensitive to task-specific evaluation design. Finally, we\nrelease a public leaderboard for quick comparison of methods and to facilitate\ncommunity engagement. We hope DATE-LM serves as a foundation for future data\nattribution research in LLMs.\n","authors":["Cathy Jiao","Yijun Pan","Emily Xiao","Daisy Sheng","Niket Jain","Hanzhang Zhao","Ishita Dasgupta","Jiaqi W. Ma","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2507.09424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07533v3","updated":"2025-07-12T22:46:18Z","published":"2024-11-12T04:16:44Z","title":"Large Language Models as Neurolinguistic Subjects: Discrepancy between\n  Performance and Competence","summary":"  This study investigates the linguistic understanding of Large Language Models\n(LLMs) regarding signifier (form) and signified (meaning) by distinguishing two\nLLM assessment paradigms: psycholinguistic and neurolinguistic. Traditional\npsycholinguistic evaluations often reflect statistical rules that may not\naccurately represent LLMs' true linguistic competence. We introduce a\nneurolinguistic approach, utilizing a novel method that combines minimal pair\nand diagnostic probing to analyze activation patterns across model layers. This\nmethod allows for a detailed examination of how LLMs represent form and\nmeaning, and whether these representations are consistent across languages. We\nfound: (1) Psycholinguistic and neurolinguistic methods reveal that language\nperformance and competence are distinct; (2) Direct probability measurement may\nnot accurately assess linguistic competence; (3) Instruction tuning won't\nchange much competence but improve performance; (4) LLMs exhibit higher\ncompetence and performance in form compared to meaning. Additionally, we\nintroduce new conceptual minimal pair datasets for Chinese (COMPS-ZH) and\nGerman (COMPS-DE), complementing existing English datasets.\n","authors":["Linyang He","Ercong Nie","Helmut Schmid","Hinrich Schütze","Nima Mesgarani","Jonathan Brennan"],"pdf_url":"https://arxiv.org/pdf/2411.07533v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18746v2","updated":"2025-07-12T20:39:40Z","published":"2025-02-26T01:42:08Z","title":"A Survey of Automatic Prompt Optimization with Instruction-focused\n  Heuristic-based Search Algorithm","summary":"  Recent advances in Large Language Models have led to remarkable achievements\nacross a variety of Natural Language Processing tasks, making prompt\nengineering increasingly central to guiding model outputs. While manual methods\ncan be effective, they typically rely on intuition and do not automatically\nrefine prompts over time. In contrast, automatic prompt optimization employing\nheuristic-based search algorithms can systematically explore and improve\nprompts with minimal human oversight. This survey proposes a comprehensive\ntaxonomy of these methods, categorizing them by where optimization occurs, what\nis optimized, what criteria drive the optimization, which operators generate\nnew prompts, and which iterative search algorithms are applied. We further\nhighlight specialized datasets and tools that support and accelerate automated\nprompt refinement. We conclude by discussing key open challenges pointing\ntoward future opportunities for more robust and versatile LLM applications.\n","authors":["Wendi Cui","Zhuohang Li","Hao Sun","Damien Lopez","Kamalika Das","Bradley A. Malin","Sricharan Kumar","Jiaxin Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.18746v2.pdf","comment":"Accepted to ACL 2025"},{"id":"http://arxiv.org/abs/2401.17196v3","updated":"2025-07-12T20:35:26Z","published":"2024-01-30T17:30:44Z","title":"Single Word Change is All You Need: Using LLMs to Create Synthetic\n  Training Examples for Text Classifiers","summary":"  In text classification, creating an adversarial example means subtly\nperturbing a few words in a sentence without changing its meaning, causing it\nto be misclassified by a classifier. A concerning observation is that a\nsignificant portion of adversarial examples generated by existing methods\nchange only one word. This single-word perturbation vulnerability represents a\nsignificant weakness in classifiers, which malicious users can exploit to\nefficiently create a multitude of adversarial examples. This paper studies this\nproblem and makes the following key contributions: (1) We introduce a novel\nmetric $\\rho$ to quantitatively assess a classifier's robustness against\nsingle-word perturbation. (2) We present the SP-Attack, designed to exploit the\nsingle-word perturbation vulnerability, achieving a higher attack success rate,\nbetter preserving sentence meaning, while reducing computation costs compared\nto state-of-the-art adversarial methods. (3) We propose SP-Defense, which aims\nto improve \\r{ho} by applying data augmentation in learning. Experimental\nresults on 4 datasets and BERT and distilBERT classifiers show that SP-Defense\nimproves $\\rho$ by 14.6% and 13.9% and decreases the attack success rate of\nSP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the\nattack success rate of existing attack methods that involve multiple-word\nperturbations.\n","authors":["Lei Xu","Sarah Alnegheimish","Laure Berti-Equille","Alfredo Cuesta-Infante","Kalyan Veeramachaneni"],"pdf_url":"https://arxiv.org/pdf/2401.17196v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11347v2","updated":"2025-07-12T20:31:55Z","published":"2024-02-17T17:47:10Z","title":"SEE: Strategic Exploration and Exploitation for Cohesive In-Context\n  Prompt Optimization","summary":"  Designing optimal prompts for Large Language Models (LLMs) is a complicated\nand resource-intensive task, often requiring substantial human expertise and\neffort. Existing approaches typically separate the optimization of prompt\ninstructions and in-context learning examples, leading to incohesive prompts\nthat are defined and represented by suboptimal task performance. To overcome\nthese challenges, we propose a novel Cohesive In-Context Prompt Optimization\nframework that refines both prompt instructions and examples. However,\nformulating such an optimization in the discrete and high-dimensional space of\nnatural language poses significant challenges in both convergence and\ncomputational efficiency. To address these issues, we introduce SEE, a scalable\nand efficient prompt optimization framework that adopts metaheuristic\noptimization principles and strategically balances exploration and exploitation\nto enhance optimization performance and achieve efficient convergence. SEE\nfeatures a quad-phased design that alternates between global traversal\n(exploration) and local optimization (exploitation) and adaptively chooses LLM\noperators during the optimization process. We have conducted a comprehensive\nevaluation across 35 benchmark tasks, and SEE significantly outperforms\nstate-of-the-art baseline methods by a large margin, achieving an average\nperformance gain of 13.94 while reducing computational costs by 58.67.\n","authors":["Wendi Cui","Zhuohang Li","Hao Sun","Damien Lopez","Kamalika Das","Bradley Malin","Sricharan Kumar","Jiaxin Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.11347v2.pdf","comment":"Accepted to ACL 2025 (Main Conference)"},{"id":"http://arxiv.org/abs/2503.22362v2","updated":"2025-07-12T19:55:27Z","published":"2025-03-28T12:12:38Z","title":"Supposedly Equivalent Facts That Aren't? Entity Frequency in\n  Pre-training Induces Asymmetry in LLMs","summary":"  Understanding and mitigating hallucinations in Large Language Models (LLMs)\nis crucial for ensuring reliable content generation. While previous research\nhas primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and\ndirectly links model behaviour to the pre-training data that forms their prior\nknowledge. Specifically, we demonstrate that an asymmetry exists in the\nrecognition of logically equivalent facts, which can be attributed to frequency\ndiscrepancies of entities appearing as subjects versus objects. Given that most\npre-training datasets are inaccessible, we leverage the fully open-source OLMo\nseries by indexing its Dolma dataset to estimate entity frequencies. Using\nrelational facts (represented as triples) from Wikidata5M, we construct probing\ndatasets to isolate this effect. Our experiments reveal that facts with a\nhigh-frequency subject and a low-frequency object are better recognised than\ntheir inverse, despite their logical equivalence. The pattern reverses in\nlow-to-high frequency settings, and no statistically significant asymmetry\nemerges when both entities are high-frequency. These findings highlight the\ninfluential role of pre-training data in shaping model predictions and provide\ninsights for inferring the characteristics of pre-training data in closed or\npartially closed LLMs.\n","authors":["Yuan He","Bailan He","Zifeng Ding","Alisia Lupidi","Yuqicheng Zhu","Shuo Chen","Caiqi Zhang","Jiaoyan Chen","Yunpu Ma","Volker Tresp","Ian Horrocks"],"pdf_url":"https://arxiv.org/pdf/2503.22362v2.pdf","comment":"Accepted at COLM 2025"},{"id":"http://arxiv.org/abs/2507.05201v3","updated":"2025-07-12T19:13:40Z","published":"2025-07-07T17:01:44Z","title":"MedGemma Technical Report","summary":"  Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma.\n","authors":["Andrew Sellergren","Sahar Kazemzadeh","Tiam Jaroensri","Atilla Kiraly","Madeleine Traverse","Timo Kohlberger","Shawn Xu","Fayaz Jamil","Cían Hughes","Charles Lau","Justin Chen","Fereshteh Mahvar","Liron Yatziv","Tiffany Chen","Bram Sterling","Stefanie Anna Baby","Susanna Maria Baby","Jeremy Lai","Samuel Schmidgall","Lu Yang","Kejia Chen","Per Bjornsson","Shashir Reddy","Ryan Brush","Kenneth Philbrick","Mercy Asiedu","Ines Mezerreg","Howard Hu","Howard Yang","Richa Tiwari","Sunny Jansen","Preeti Singh","Yun Liu","Shekoofeh Azizi","Aishwarya Kamath","Johan Ferret","Shreya Pathak","Nino Vieillard","Ramona Merhej","Sarah Perrin","Tatiana Matejovicova","Alexandre Ramé","Morgane Riviere","Louis Rouillard","Thomas Mesnard","Geoffrey Cideron","Jean-bastien Grill","Sabela Ramos","Edouard Yvinec","Michelle Casbon","Elena Buchatskaya","Jean-Baptiste Alayrac","Dmitry Lepikhin","Vlad Feinberg","Sebastian Borgeaud","Alek Andreev","Cassidy Hardin","Robert Dadashi","Léonard Hussenot","Armand Joulin","Olivier Bachem","Yossi Matias","Katherine Chou","Avinatan Hassidim","Kavi Goel","Clement Farabet","Joelle Barral","Tris Warkentin","Jonathon Shlens","David Fleet","Victor Cotruta","Omar Sanseviero","Gus Martins","Phoebe Kirk","Anand Rao","Shravya Shetty","David F. Steiner","Can Kirmizibayrak","Rory Pilgrim","Daniel Golden","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2507.05201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05225v2","updated":"2025-07-12T18:28:38Z","published":"2024-12-06T17:58:14Z","title":"BEExformer: A Fast Inferencing Binarized Transformer with Early Exits","summary":"  Large Language Models (LLMs) based on transformers achieve cutting-edge\nresults on a variety of applications. However, their enormous size and\nprocessing requirements hinder deployment on constrained resources. To enhance\nefficiency, binarization and Early Exit (EE) have proved to be effective\nsolutions. However, binarization may lead to performance loss as reduced\nprecision affects gradient estimation and parameter updates. Besides, research\non EE mechanisms is still in its early stages. To address these challenges, we\nintroduce Binarized Early Exit Transformer (BEExformer), the first-ever\nselective learning-based transformer integrating Binarization-Aware Training\n(BAT) with EE for efficient and fast textual inference. Each transformer block\nhas an integrated Selective-Learn Forget Network (SLFN) to enhance contextual\nretention while eliminating irrelevant information. The BAT employs a\ndifferentiable second-order approximation to the sign function, enabling\ngradient computation that captures both the sign and magnitude of the weights.\nThis aids in 21.30 times reduction in model size. The EE mechanism hinges on\nfractional reduction in entropy among intermediate transformer blocks with\nsoft-routing loss estimation. This accelerates inference by reducing FLOPs by\n52.08% and even improves accuracy by 2.89% by resolving the \"overthinking\"\nproblem inherent in deep networks. Extensive evaluation through comparison with\nthe SOTA methods and various ablations across six datasets covering multiple\nNLP tasks demonstrates its Pareto-optimal performance-efficiency trade-off.\n","authors":["Wazib Ansar","Saptarsi Goswami","Amlan Chakrabarti"],"pdf_url":"https://arxiv.org/pdf/2412.05225v2.pdf","comment":"This revised manuscript includes 18 pages, 17 figures, and 6 tables.\n  Methodology and results sections have been improved for clarity and depth,\n  incorporating additional comparisons, ablations, and a new evaluation\n  dataset. A few relevant references were added, and overall organization\n  refined for better readability"},{"id":"http://arxiv.org/abs/2506.23377v2","updated":"2025-07-12T17:57:39Z","published":"2025-06-29T19:26:37Z","title":"Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs","summary":"  Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective.\n","authors":["Taejin Kim","Siun-Chuon Mau","Konrad Vesey"],"pdf_url":"https://arxiv.org/pdf/2506.23377v2.pdf","comment":"7 pages, 5 main pages of text, 5 figures, 2 tables. Research work\n  performed at CACI INTL INC"},{"id":"http://arxiv.org/abs/2506.04462v3","updated":"2025-07-12T16:50:20Z","published":"2025-06-04T21:29:07Z","title":"Watermarking Degrades Alignment in Language Models: Analysis and\n  Mitigation","summary":"  Watermarking techniques for large language models (LLMs) can significantly\nimpact output quality, yet their effects on truthfulness, safety, and\nhelpfulness remain critically underexamined. This paper presents a systematic\nanalysis of how two popular watermarking approaches-Gumbel and KGW-affect these\ncore alignment properties across four aligned LLMs. Our experiments reveal two\ndistinct degradation patterns: guard attenuation, where enhanced helpfulness\nundermines model safety, and guard amplification, where excessive caution\nreduces model helpfulness. These patterns emerge from watermark-induced shifts\nin token distribution, surfacing the fundamental tension that exists between\nalignment objectives.\n  To mitigate these degradations, we propose Alignment Resampling (AR), an\ninference-time sampling method that uses an external reward model to restore\nalignment. We establish a theoretical lower bound on the improvement in\nexpected reward score as the sample size is increased and empirically\ndemonstrate that sampling just 2-4 watermarked generations effectively recovers\nor surpasses baseline (unwatermarked) alignment scores. To overcome the limited\nresponse diversity of standard Gumbel watermarking, our modified implementation\nsacrifices strict distortion-freeness while maintaining robust detectability,\nensuring compatibility with AR. Experimental results confirm that AR\nsuccessfully recovers baseline alignment in both watermarking approaches, while\nmaintaining strong watermark detectability. This work reveals the critical\nbalance between watermark strength and model alignment, providing a simple\ninference-time solution to responsibly deploy watermarked LLMs in practice.\n","authors":["Apurv Verma","NhatHai Phan","Shubhendu Trivedi"],"pdf_url":"https://arxiv.org/pdf/2506.04462v3.pdf","comment":"Published at the 1st Workshop on GenAI Watermarking (ICLR 2025).\n  Code: https://github.com/dapurv5/alignmark"},{"id":"http://arxiv.org/abs/2506.23978v2","updated":"2025-07-12T15:26:07Z","published":"2025-06-30T15:45:17Z","title":"LLM Agents Are the Antidote to Walled Gardens","summary":"  While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.\n","authors":["Samuele Marro","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2506.23978v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09318v1","updated":"2025-07-12T15:18:47Z","published":"2025-07-12T15:18:47Z","title":"ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow\n  Matching","summary":"  Generating spoken dialogue is more challenging than monologue text-to-speech\n(TTS) due to the need for realistic turn-taking and distinct speaker timbres.\nExisting spoken dialogue generation models, being auto-regressive, suffer from\nslow and unstable inference. To overcome these limitations, we introduce\nZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation\nmodel built upon flow matching. Key designs include: 1) speaker-turn embeddings\nfor precise speaker turn-taking; 2) a curriculum learning strategy for stable\nspeech-text alignment; 3) specialized strategies to enable stereo dialogue\ngeneration. Additionally, recognizing the lack of open-source large-scale\nspoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue\ndataset from in-the-wild speech data. Furthermore, we established a benchmark\nto comprehensively evaluate various models. Experimental results demonstrate\nthat ZipVoice-Dialog achieves superior performance in intelligibility, speaker\nturn-taking accuracy, speaker similarity, and inference speed. Our codes, model\ncheckpoints, demo samples, and the OpenDialog dataset are all publicly\navailable at https://github.com/k2-fsa/ZipVoice.\n","authors":["Han Zhu","Wei Kang","Liyong Guo","Zengwei Yao","Fangjun Kuang","Weiji Zhuang","Zhaoqing Li","Zhifeng Han","Dong Zhang","Xin Zhang","Xingchen Song","Long Lin","Daniel Povey"],"pdf_url":"https://arxiv.org/pdf/2507.09318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12380v2","updated":"2025-07-12T15:07:14Z","published":"2024-10-16T08:55:49Z","title":"Evaluation of Attribution Bias in Generator-Aware Retrieval-Augmented\n  Large Language Models","summary":"  Attributing answers to source documents is an approach used to enhance the\nverifiability of a model's output in retrieval augmented generation (RAG).\nPrior work has mainly focused on improving and evaluating the attribution\nquality of large language models (LLMs) in RAG, but this may come at the\nexpense of inducing biases in the attribution of answers. We define and examine\ntwo aspects in the evaluation of LLMs in RAG pipelines, namely attribution\nsensitivity and bias with respect to authorship information. We explicitly\ninform an LLM about the authors of source documents, instruct it to attribute\nits answers, and analyze (i) how sensitive the LLM's output is to the author of\nsource documents, and (ii) whether the LLM exhibits a bias towards\nhuman-written or AI-generated source documents. We design an experimental setup\nin which we use counterfactual evaluation to study three LLMs in terms of their\nattribution sensitivity and bias in RAG pipelines. Our results show that adding\nauthorship information to source documents can significantly change the\nattribution quality of LLMs by 3% to 18%. Moreover, we show that LLMs can have\nan attribution bias towards explicit human authorship, which can serve as a\ncompeting hypothesis for findings of prior work that shows that LLM-generated\ncontent may be preferred over human-written contents. Our findings indicate\nthat metadata of source documents can influence LLMs' trust, and how they\nattribute their answers. Furthermore, our research highlights attribution bias\nand sensitivity as a novel aspect of brittleness in LLMs.\n","authors":["Amin Abolghasemi","Leif Azzopardi","Seyyed Hadi Hashemi","Maarten de Rijke","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2410.12380v2.pdf","comment":"Accepted at ACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2507.09310v1","updated":"2025-07-12T14:57:04Z","published":"2025-07-12T14:57:04Z","title":"Voice Conversion for Lombard Speaking Style with Implicit and Explicit\n  Acoustic Feature Conditioning","summary":"  Text-to-Speech (TTS) systems in Lombard speaking style can improve the\noverall intelligibility of speech, useful for hearing loss and noisy\nconditions. However, training those models requires a large amount of data and\nthe Lombard effect is challenging to record due to speaker and noise\nvariability and tiring recording conditions. Voice conversion (VC) has been\nshown to be a useful augmentation technique to train TTS systems in the absence\nof recorded data from the target speaker in the target speaking style. In this\npaper, we are concerned with Lombard speaking style transfer. Our goal is to\nconvert speaker identity while preserving the acoustic attributes that define\nthe Lombard speaking style. We compare voice conversion models with implicit\nand explicit acoustic feature conditioning. We observe that our proposed\nimplicit conditioning strategy achieves an intelligibility gain comparable to\nthe model conditioned on explicit acoustic features, while also preserving\nspeaker similarity.\n","authors":["Dominika Woszczyk","Manuel Sam Ribeiro","Thomas Merritt","Daniel Korzekwa"],"pdf_url":"https://arxiv.org/pdf/2507.09310v1.pdf","comment":"Presented at Clarity Challenge 2023"},{"id":"http://arxiv.org/abs/2502.18448v2","updated":"2025-07-12T13:56:11Z","published":"2025-02-25T18:42:26Z","title":"Disambiguate First, Parse Later: Generating Interpretations for\n  Ambiguity Resolution in Semantic Parsing","summary":"  Handling ambiguity and underspecification is an important challenge in\nnatural language interfaces, particularly for tasks like text-to-SQL semantic\nparsing. We propose a modular approach that resolves ambiguity using natural\nlanguage interpretations before mapping these to logical forms (e.g., SQL\nqueries). Although LLMs excel at parsing unambiguous utterances, they show\nstrong biases for ambiguous ones, typically predicting only preferred\ninterpretations. We constructively exploit this bias to generate an initial set\nof preferred disambiguations and then apply a specialized infilling model to\nidentify and generate missing interpretations. To train the infilling model, we\nintroduce an annotation method that uses SQL execution to validate different\nmeanings. Our approach improves interpretation coverage and generalizes across\ndatasets with different annotation styles, database structures, and ambiguity\ntypes.\n","authors":["Irina Saparina","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2502.18448v2.pdf","comment":"Findings of ACL 2025"},{"id":"http://arxiv.org/abs/2507.09282v1","updated":"2025-07-12T13:39:25Z","published":"2025-07-12T13:39:25Z","title":"ClaritySpeech: Dementia Obfuscation in Speech","summary":"  Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility.\n","authors":["Dominika Woszczyk","Ranya Aloufi","Soteris Demetriou"],"pdf_url":"https://arxiv.org/pdf/2507.09282v1.pdf","comment":"Accepted at Interspeech 2025"},{"id":"http://arxiv.org/abs/2507.09279v1","updated":"2025-07-12T13:21:10Z","published":"2025-07-12T13:21:10Z","title":"Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for\n  Clinically-Aligned Confidence Calibration in Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/vccrl-llm.\n","authors":["Anita Kriz","Elizabeth Laura Janes","Xing Shen","Tal Arbel"],"pdf_url":"https://arxiv.org/pdf/2507.09279v1.pdf","comment":"Preprint version. The peer-reviewed version of this paper has been\n  accepted to ICCV 2025 Workshop CVAMD"},{"id":"http://arxiv.org/abs/2507.09259v1","updated":"2025-07-12T11:44:41Z","published":"2025-07-12T11:44:41Z","title":"Psychology-Driven Enhancement of Humour Translation","summary":"  Humour translation plays a vital role as a bridge between different cultures,\nfostering understanding and communication. Although most existing Large\nLanguage Models (LLMs) are capable of general translation tasks, these models\nstill struggle with humour translation, which is especially reflected through\nlinguistic interference and lacking humour in translated text. In this paper,\nwe propose a psychology-inspired Humour Decomposition Mechanism (HDM) that\nutilises Chain-of-Thought (CoT) to imitate the ability of the human thought\nprocess, stimulating LLMs to optimise the readability of translated humorous\ntexts. Moreover, we integrate humour theory in HDM to further enhance the\nhumorous elements in the translated text. Our automatic evaluation experiments\non open-source humour datasets demonstrate that our method significantly\nimproves the quality of humour translation, yielding average gains of 7.75\\% in\nhumour, 2.81\\% in fluency, and 6.13\\% in coherence of the generated text.\n","authors":["Yuchen Su","Yonghua Zhu","Yang Chen","Diana Benavides-Prado","Michael Witbrock"],"pdf_url":"https://arxiv.org/pdf/2507.09259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09245v1","updated":"2025-07-12T10:54:30Z","published":"2025-07-12T10:54:30Z","title":"Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration\n  Systems and Data Resources","summary":"  The Swa-bhasha Resource Hub provides a comprehensive collection of data\nresources and algorithms developed for Romanized Sinhala to Sinhala\ntransliteration between 2020 and 2025. These resources have played a\nsignificant role in advancing research in Sinhala Natural Language Processing\n(NLP), particularly in training transliteration models and developing\napplications involving Romanized Sinhala. The current openly accessible data\nsets and corresponding tools are made publicly available through this hub. This\npaper presents a detailed overview of the resources contributed by the authors\nand includes a comparative analysis of existing transliteration applications in\nthe domain.\n","authors":["Deshan Sumanathilaka","Sameera Perera","Sachithya Dharmasiri","Maneesha Athukorala","Anuja Dilrukshi Herath","Rukshan Dias","Pasindu Gamage","Ruvan Weerasinghe","Y. H. P. P. Priyadarshana"],"pdf_url":"https://arxiv.org/pdf/2507.09245v1.pdf","comment":"13 pages, 3 Tables, 3 figures"},{"id":"http://arxiv.org/abs/2507.07186v2","updated":"2025-07-12T10:00:59Z","published":"2025-07-09T18:01:14Z","title":"Planted in Pretraining, Swayed by Finetuning: A Case Study on the\n  Origins of Cognitive Biases in LLMs","summary":"  Large language models (LLMs) exhibit cognitive biases -- systematic\ntendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified\nby instruction tuning. However, it remains unclear if these differences in\nbiases stem from pretraining, finetuning, or even random noise due to training\nstochasticity. We propose a two-step causal experimental approach to\ndisentangle these factors. First, we finetune models multiple times using\ndifferent random seeds to study how training randomness affects over $30$\ncognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping\ninstruction datasets between models to isolate bias sources. This swap uses\ndatasets that led to different bias patterns, directly testing whether biases\nare dataset-dependent. Our findings reveal that while training randomness\nintroduces some variability, biases are mainly shaped by pretraining: models\nwith the same pretrained backbone exhibit more similar bias patterns than those\nsharing only finetuning data. These insights suggest that understanding biases\nin finetuned models requires considering their pretraining origins beyond\nfinetuning effects. This perspective can guide future efforts to develop\nprincipled strategies for evaluating and mitigating bias in LLMs.\n","authors":["Itay Itzhak","Yonatan Belinkov","Gabriel Stanovsky"],"pdf_url":"https://arxiv.org/pdf/2507.07186v2.pdf","comment":"CoLM 2025"},{"id":"http://arxiv.org/abs/2404.03353v2","updated":"2025-07-12T09:55:44Z","published":"2024-04-04T10:45:07Z","title":"Towards Pareto Optimal Throughput in Small Language Model Serving","summary":"  Large language models (LLMs) have revolutionized the state-of-the-art of many\ndifferent natural language processing tasks. Although serving LLMs is\ncomputationally and memory demanding, the rise of Small Language Models (SLMs)\noffers new opportunities for resource-constrained users, who now are able to\nserve small models with cutting-edge performance. In this paper, we present a\nset of experiments designed to benchmark SLM inference at performance and\nenergy levels. Our analysis provides a new perspective in serving, highlighting\nthat the small memory footprint of SLMs allows for reaching the Pareto-optimal\nthroughput within the resource capacity of a single accelerator. In this\nregard, we present an initial set of findings demonstrating how model\nreplication can effectively improve resource utilization for serving SLMs.\n","authors":["Pol G. Recasens","Yue Zhu","Chen Wang","Eun Kyung Lee","Olivier Tardieu","Alaa Youssef","Jordi Torres","Josep Ll. Berral"],"pdf_url":"https://arxiv.org/pdf/2404.03353v2.pdf","comment":"Revised version of the paper published at EuroMLSys'24"},{"id":"http://arxiv.org/abs/2507.09225v1","updated":"2025-07-12T09:49:30Z","published":"2025-07-12T09:49:30Z","title":"MetaClimage: A novel database of visual metaphors related to Climate\n  Change, with costs and benefits analysis","summary":"  Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication.\n","authors":["Biagio Scalingi","Chiara Barattieri di San Pietro","Paolo Canal","Valentina Bambini"],"pdf_url":"https://arxiv.org/pdf/2507.09225v1.pdf","comment":"27 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.15634v4","updated":"2025-07-12T09:42:16Z","published":"2025-05-21T15:17:59Z","title":"Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning\n  in Language Models","summary":"  Large Language Models (LLMs) demonstrate the ability to solve reasoning and\nmathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT\nlength, as seen in models such as DeepSeek-R1, significantly enhances this\nreasoning for complex problems, but requires costly and high-quality long CoT\ndata and fine-tuning. This work, inspired by the deep thinking paradigm of\nDeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of\nan LLM without external datasets. Our method first employs Sparse Autoencoders\n(SAEs) to extract interpretable features from vanilla CoT. These features are\nthen used to steer the LLM's internal states during generation. Recognizing\nthat many LLMs do not have corresponding pre-trained SAEs, we further introduce\na novel SAE-free steering algorithm, which directly computes steering\ndirections from the residual activations of an LLM, obviating the need for an\nexplicit SAE. Experimental results demonstrate that both our SAE-based and\nsubsequent SAE-free steering algorithms significantly enhance the reasoning\ncapabilities of LLMs.\n","authors":["Zihao Li","Xu Wang","Yuzhe Yang","Ziyu Yao","Haoyi Xiong","Mengnan Du"],"pdf_url":"https://arxiv.org/pdf/2505.15634v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02679v2","updated":"2025-07-12T09:26:40Z","published":"2025-07-03T14:42:03Z","title":"Exploring Gender Bias Beyond Occupational Titles","summary":"  In this work, we investigate the correlation between gender and contextual\nbiases, focusing on elements such as action verbs, object nouns, and\nparticularly on occupations. We introduce a novel dataset, GenderLexicon, and a\nframework that can estimate contextual bias and its related gender bias. Our\nmodel can interpret the bias with a score and thus improve the explainability\nof gender bias. Also, our findings confirm the existence of gender biases\nbeyond occupational stereotypes. To validate our approach and demonstrate its\neffectiveness, we conduct evaluations on five diverse datasets, including a\nJapanese dataset.\n","authors":["Ahmed Sabir","Rajesh Sharma"],"pdf_url":"https://arxiv.org/pdf/2507.02679v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2507.09205v1","updated":"2025-07-12T08:54:05Z","published":"2025-07-12T08:54:05Z","title":"Banzhida: Advancing Large Language Models for Tibetan with Curated Data\n  and Continual Pre-Training","summary":"  Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks.\n","authors":["Leiyu Pan","Bojian Xiong","Lei Yang","Renren Jin","Shaowei Zhang","Yue Chen","Ling Shi","Jiang Zhou","Junru Wu","Zhen Wang","Jianxiang Peng","Juesi Xiao","Tianyu Dong","Zhuowen Han","Zhuo Chen","Sangjee Dondrub","Caizang Tai","Haixing Zhao","Huaque Cairang","Suonan Cairang","Rou Te","Lengben Zhaxi","Gazang Zhaxi","Zhonglin Ye","Yuhui Zheng","Chunyan Peng","Secha Jia","Pema Tashi","Cizhen Jiacuo","Pema Dorjee","Hongkai Liu","Pema Yanggon","Tsehang Dorjee","Jiaxin Han","Qiongying Hu","Jilin Man","Huanke You","Yuqi Ren","Duo La","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2507.09205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04963v4","updated":"2025-07-12T08:30:39Z","published":"2024-03-08T00:19:24Z","title":"An In-depth Evaluation of Large Language Models in Sentence\n  Simplification with Error-based Human Assessment","summary":"  Recent studies have used both automatic metrics and human evaluations to\nassess the simplification abilities of LLMs. However, the suitability of\nexisting evaluation methodologies for LLMs remains in question. First, the\nsuitability of current automatic metrics on LLMs' simplification evaluation is\nstill uncertain. Second, current human evaluation approaches in sentence\nsimplification often fall into two extremes: they are either too superficial,\nfailing to offer a clear understanding of the models' performance, or overly\ndetailed, making the annotation process complex and prone to inconsistency,\nwhich in turn affects the evaluation's reliability. To address these problems,\nthis study provides in-depth insights into LLMs' performance while ensuring the\nreliability of the evaluation. We design an error-based human annotation\nframework to assess the LLMs' simplification capabilities. We select both\nclosed-source and open-source LLMs, including GPT-4, Qwen2.5-72B, and\nLlama-3.2-3B. We believe that these models offer a representative selection\nacross large, medium, and small sizes of LLMs. Results show that LLMs generally\ngenerate fewer erroneous simplification outputs compared to the previous\nstate-of-the-art. However, LLMs have their limitations, as seen in GPT-4's and\nQwen2.5-72B's struggle with lexical paraphrasing. Furthermore, we conduct\nmeta-evaluations on widely used automatic metrics using our human annotations.\nWe find that these metrics lack sufficient sensitivity to assess the overall\nhigh-quality simplifications, particularly those generated by high-performance\nLLMs.\n","authors":["Xuanxin Wu","Yuki Arase"],"pdf_url":"https://arxiv.org/pdf/2403.04963v4.pdf","comment":"Accepted by ACM Transactions on Intelligent Systems and Technology.\n  Our human evaluation corpus is available at:\n  https://github.com/WuXuanxin/human-eval-llm-simplification"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.09423v1","updated":"2025-07-12T23:22:23Z","published":"2025-07-12T23:22:23Z","title":"Item-centric Exploration for Cold Start Problem","summary":"  Recommender systems face a critical challenge in the item cold-start problem,\nwhich limits content diversity and exacerbates popularity bias by struggling to\nrecommend new items. While existing solutions often rely on auxiliary data, but\nthis paper illuminates a distinct, yet equally pressing, issue stemming from\nthe inherent user-centricity of many recommender systems. We argue that in\nenvironments with large and rapidly expanding item inventories, the traditional\nfocus on finding the \"best item for a user\" can inadvertently obscure the ideal\naudience for nascent content. To counter this, we introduce the concept of\nitem-centric recommendations, shifting the paradigm to identify the optimal\nusers for new items. Our initial realization of this vision involves an\nitem-centric control integrated into an exploration system. This control\nemploys a Bayesian model with Beta distributions to assess candidate items\nbased on a predicted balance between user satisfaction and the item's inherent\nquality. Empirical online evaluations reveal that this straightforward control\nmarkedly improves cold-start targeting efficacy, enhances user satisfaction\nwith newly explored content, and significantly increases overall exploration\nefficiency.\n","authors":["Dong Wang","Junyi Jiao","Arnab Bhadury","Yaping Zhang","Mingyan Gao","Onkar Dalal"],"pdf_url":"https://arxiv.org/pdf/2507.09423v1.pdf","comment":"Accepted for publication on 2025 ACM Recsys Conference Industry Track"},{"id":"http://arxiv.org/abs/2507.09403v1","updated":"2025-07-12T21:04:25Z","published":"2025-07-12T21:04:25Z","title":"Balancing Semantic Relevance and Engagement in Related Video\n  Recommendations","summary":"  Related video recommendations commonly use collaborative filtering (CF)\ndriven by co-engagement signals, often resulting in recommendations lacking\nsemantic coherence and exhibiting strong popularity bias. This paper introduces\na novel multi-objective retrieval framework, enhancing standard two-tower\nmodels to explicitly balance semantic relevance and user engagement. Our\napproach uniquely combines: (a) multi-task learning (MTL) to jointly optimize\nco-engagement and semantic relevance, explicitly prioritizing topical\ncoherence; (b) fusion of multimodal content features (textual and visual\nembeddings) for richer semantic understanding; and (c) off-policy correction\n(OPC) via inverse propensity weighting to effectively mitigate popularity bias.\nEvaluation on industrial-scale data and a two-week live A/B test reveals our\nframework's efficacy. We observed significant improvements in semantic\nrelevance (from 51% to 63% topic match rate), a reduction in popular item\ndistribution (-13.8% popular video recommendations), and a +0.04% improvement\nin our topline user engagement metric. Our method successfully achieves better\nsemantic coherence, balanced engagement, and practical scalability for\nreal-world deployment.\n","authors":["Amit Jaspal","Feng Zhang","Wei Chang","Sumit Kumar","Yubo Wang","Roni Mittleman","Qifan Wang","Weize Mao"],"pdf_url":"https://arxiv.org/pdf/2507.09403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09389v1","updated":"2025-07-12T20:10:26Z","published":"2025-07-12T20:10:26Z","title":"Knowledge Conceptualization Impacts RAG Efficacy","summary":"  Explainability and interpretability are cornerstones of frontier and\nnext-generation artificial intelligence (AI) systems. This is especially true\nin recent systems, such as large language models (LLMs), and more broadly,\ngenerative AI. On the other hand, adaptability to new domains, contexts, or\nscenarios is also an important aspect for a successful system. As such, we are\nparticularly interested in how we can merge these two efforts, that is,\ninvestigating the design of transferable and interpretable neurosymbolic AI\nsystems. Specifically, we focus on a class of systems referred to as ''Agentic\nRetrieval-Augmented Generation'' systems, which actively select, interpret, and\nquery knowledge sources in response to natural language prompts. In this paper,\nwe systematically evaluate how different conceptualizations and representations\nof knowledge, particularly the structure and complexity, impact an AI agent (in\nthis case, an LLM) in effectively querying a triplestore. We report our\nresults, which show that there are impacts from both approaches, and we discuss\ntheir impact and implications.\n","authors":["Chris Davis Jaldi","Anmol Saini","Elham Ghiasi","O. Divine Eziolise","Cogan Shimizu"],"pdf_url":"https://arxiv.org/pdf/2507.09389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09331v1","updated":"2025-07-12T16:16:11Z","published":"2025-07-12T16:16:11Z","title":"Correcting the LogQ Correction: Revisiting Sampled Softmax for\n  Large-Scale Retrieval","summary":"  Two-tower neural networks are a popular architecture for the retrieval stage\nin recommender systems. These models are typically trained with a softmax loss\nover the item catalog. However, in web-scale settings, the item catalog is\noften prohibitively large, making full softmax infeasible. A common solution is\nsampled softmax, which approximates the full softmax using a small number of\nsampled negatives.\n  One practical and widely adopted approach is to use in-batch negatives, where\nnegatives are drawn from items in the current mini-batch. However, this\nintroduces a bias: items that appear more frequently in the batch (i.e.,\npopular items) are penalized more heavily.\n  To mitigate this issue, a popular industry technique known as logQ correction\nadjusts the logits during training by subtracting the log-probability of an\nitem appearing in the batch. This correction is derived by analyzing the bias\nin the gradient and applying importance sampling, effectively twice, using the\nin-batch distribution as a proposal distribution. While this approach improves\nmodel quality, it does not fully eliminate the bias.\n  In this work, we revisit the derivation of logQ correction and show that it\noverlooks a subtle but important detail: the positive item in the denominator\nis not Monte Carlo-sampled - it is always present with probability 1. We\npropose a refined correction formula that accounts for this. Notably, our loss\nintroduces an interpretable sample weight that reflects the model's uncertainty\n- the probability of misclassification under the current parameters. We\nevaluate our method on both public and proprietary datasets, demonstrating\nconsistent improvements over the standard logQ correction.\n","authors":["Kirill Khrylchenko","Vladimir Baikalov","Sergei Makeev","Artem Matveev","Sergei Liamaev"],"pdf_url":"https://arxiv.org/pdf/2507.09331v1.pdf","comment":"Accepted at ACM RecSys 2025. Author's version. To appear in the\n  Proceedings of the 18th ACM Conference on Recommender Systems"},{"id":"http://arxiv.org/abs/2506.20854v3","updated":"2025-07-12T14:36:33Z","published":"2025-06-25T22:00:12Z","title":"Towards Two-Stage Counterfactual Learning to Rank","summary":"  Counterfactual learning to rank (CLTR) aims to learn a ranking policy from\nuser interactions while correcting for the inherent biases in interaction data,\nsuch as position bias. Existing CLTR methods assume a single ranking policy\nthat selects top-K ranking from the entire document candidate set. In\nreal-world applications, the candidate document set is on the order of\nmillions, making a single-stage ranking policy impractical. In order to scale\nto millions of documents, real-world ranking systems are designed in a\ntwo-stage fashion, with a candidate generator followed by a ranker. The\nexisting CLTR method for a two-stage offline ranking system only considers the\ntop-1 ranking set-up and only focuses on training the candidate generator, with\nthe ranker fixed. A CLTR method for training both the ranker and candidate\ngenerator jointly is missing from the existing literature. In this paper, we\npropose a two-stage CLTR estimator that considers the interaction between the\ntwo stages and estimates the joint value of the two policies offline. In\naddition, we propose a novel joint optimization method to train the candidate\nand ranker policies, respectively. To the best of our knowledge, we are the\nfirst to propose a CLTR estimator and learning method for two-stage ranking.\nExperimental results on a semi-synthetic benchmark demonstrate the\neffectiveness of the proposed joint CLTR method over baselines.\n","authors":["Shashank Gupta","Yiming Liao","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2506.20854v3.pdf","comment":"Accepted at ICTIR 2025 (co-located with SIGIR 2025)"},{"id":"http://arxiv.org/abs/2507.09256v1","updated":"2025-07-12T11:30:32Z","published":"2025-07-12T11:30:32Z","title":"Ambiguity-Aware and High-Order Relation Learning for Multi-Grained\n  Image-Text Matching","summary":"  Image-text matching is crucial for bridging the semantic gap between computer\nvision and natural language processing. However, existing methods still face\nchallenges in handling high-order associations and semantic ambiguities among\nsimilar instances. These ambiguities arise from subtle differences between soft\npositive samples (semantically similar but incorrectly labeled) and soft\nnegative samples (locally matched but globally inconsistent), creating matching\nuncertainties. Furthermore, current methods fail to fully utilize the\nneighborhood relationships among semantically similar instances within training\nbatches, limiting the model's ability to learn high-order shared knowledge.\nThis paper proposes the Ambiguity-Aware and High-order Relation learning\nframework (AAHR) to address these issues. AAHR constructs a unified\nrepresentation space through dynamic clustering prototype contrastive learning,\neffectively mitigating the soft positive sample problem. The framework\nintroduces global and local feature extraction mechanisms and an adaptive\naggregation network, significantly enhancing full-grained semantic\nunderstanding capabilities. Additionally, AAHR employs intra-modal and\ninter-modal correlation matrices to investigate neighborhood relationships\namong sample instances thoroughly. It incorporates GNN to enhance semantic\ninteractions between instances. Furthermore, AAHR integrates momentum\ncontrastive learning to expand the negative sample set. These combined\nstrategies significantly improve the model's ability to discriminate between\nfeatures. Experimental results demonstrate that AAHR outperforms existing\nstate-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,\nconsiderably improving the accuracy and efficiency of image-text matching. The\ncode and model checkpoints for this research are available at\nhttps://github.com/Image-Text-Matching/AAHR .\n","authors":["Junyu Chen","Yihua Gao","Mingyuan Ge","Mingyong Li"],"pdf_url":"https://arxiv.org/pdf/2507.09256v1.pdf","comment":"Accepted by the Knowledge-Based Systems(KBS), 2025"},{"id":"http://arxiv.org/abs/2507.09188v1","updated":"2025-07-12T08:15:05Z","published":"2025-07-12T08:15:05Z","title":"Retrieval-Augmented Recommendation Explanation Generation with\n  Hierarchical Aggregation","summary":"  Explainable Recommender System (ExRec) provides transparency to the\nrecommendation process, increasing users' trust and boosting the operation of\nonline services. With the rise of large language models (LLMs), whose extensive\nworld knowledge and nuanced language understanding enable the generation of\nhuman-like, contextually grounded explanations, LLM-powered ExRec has gained\ngreat momentum. However, existing LLM-based ExRec models suffer from profile\ndeviation and high retrieval overhead, hindering their deployment. To address\nthese issues, we propose Retrieval-Augmented Recommendation Explanation\nGeneration with Hierarchical Aggregation (REXHA). Specifically, we design a\nhierarchical aggregation based profiling module that comprehensively considers\nuser and item review information, hierarchically summarizing and constructing\nholistic profiles. Furthermore, we introduce an efficient retrieval module\nusing two types of pseudo-document queries to retrieve relevant reviews to\nenhance the generation of recommendation explanations, effectively reducing\nretrieval latency and improving the recall of relevant reviews. Extensive\nexperiments demonstrate that our method outperforms existing approaches by up\nto 12.6% w.r.t. the explanation quality while achieving high retrieval\nefficiency.\n","authors":["Bangcheng Sun","Yazhe Chen","Jilin Yang","Xiaodong Li","Hui Li"],"pdf_url":"https://arxiv.org/pdf/2507.09188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07910v2","updated":"2025-07-12T05:41:16Z","published":"2025-07-10T16:44:33Z","title":"DTECT: Dynamic Topic Explorer & Context Tracker","summary":"  The explosive growth of textual data over time presents a significant\nchallenge in uncovering evolving themes and trends. Existing dynamic topic\nmodeling techniques, while powerful, often exist in fragmented pipelines that\nlack robust support for interpretation and user-friendly exploration. We\nintroduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end\nsystem that bridges the gap between raw textual data and meaningful temporal\ninsights. DTECT provides a unified workflow that supports data preprocessing,\nmultiple model architectures, and dedicated evaluation metrics to analyze the\ntopic quality of temporal topic models. It significantly enhances\ninterpretability by introducing LLM-driven automatic topic labeling, trend\nanalysis via temporally salient words, interactive visualizations with\ndocument-level summarization, and a natural language chat interface for\nintuitive data querying. By integrating these features into a single, cohesive\nplatform, DTECT empowers users to more effectively track and understand\nthematic dynamics. DTECT is open-source and available at\nhttps://github.com/AdhyaSuman/DTECT.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2507.07910v2.pdf","comment":"Code: https://github.com/AdhyaSuman/DTECT | Demo:\n  https://huggingface.co/spaces/AdhyaSuman/DTECT | Video:\n  https://youtu.be/B8nNfxFoJAU"},{"id":"http://arxiv.org/abs/2506.12981v2","updated":"2025-07-12T05:28:30Z","published":"2025-06-15T22:35:43Z","title":"SymRAG: Efficient Neuro-Symbolic Retrieval Through Adaptive Query\n  Routing","summary":"  Current Retrieval-Augmented Generation systems use uniform processing,\ncausing inefficiency as simple queries consume resources similar to complex\nmulti-hop tasks. We present SymRAG, a framework that introduces adaptive query\nrouting via real-time complexity and load assessment to select symbolic,\nneural, or hybrid pathways. SymRAG's neuro-symbolic approach adjusts\ncomputational pathways based on both query characteristics and system load,\nenabling efficient resource allocation across diverse query types. By combining\nlinguistic and structural query properties with system load metrics, SymRAG\nallocates resources proportional to reasoning requirements. Evaluated on 2,000\nqueries across HotpotQA (multi-hop reasoning) and DROP (discrete reasoning)\nusing Llama-3.2-3B and Mistral-7B models, SymRAG achieves competitive accuracy\n(97.6--100.0% exact match) with efficient resource utilization (3.6--6.2% CPU\nutilization, 0.985--3.165s processing). Disabling adaptive routing increases\nprocessing time by 169--1151%, showing its significance for complex models.\nThese results suggest adaptive computation strategies are more sustainable and\nscalable for hybrid AI systems that use dynamic routing and neuro-symbolic\nframeworks.\n","authors":["Safayat Bin Hakim","Muhammad Adil","Alvaro Velasquez","Houbing Herbert Song"],"pdf_url":"https://arxiv.org/pdf/2506.12981v2.pdf","comment":"Accepted at 19th International Conference on Neurosymbolic Learning\n  and Reasoning (NeSy 2025)"},{"id":"http://arxiv.org/abs/2503.04653v2","updated":"2025-07-12T04:07:11Z","published":"2025-03-06T17:43:03Z","title":"RadIR: A Scalable Framework for Multi-Grained Medical Image Retrieval\n  via Radiology Report Mining","summary":"  Developing advanced medical imaging retrieval systems is challenging due to\nthe varying definitions of `similar images' across different medical contexts.\nThis challenge is compounded by the lack of large-scale, high-quality medical\nimaging retrieval datasets and benchmarks. In this paper, we propose a novel\nmethodology that leverages dense radiology reports to define image-wise\nsimilarity ordering at multiple granularities in a scalable and fully automatic\nmanner. Using this approach, we construct two comprehensive medical imaging\nretrieval datasets: MIMIC-IR for Chest X-rays and CTRATE-IR for CT scans,\nproviding detailed image-image ranking annotations conditioned on diverse\nanatomical structures. Furthermore, we develop two retrieval systems, RadIR-CXR\nand model-ChestCT, which demonstrate superior performance in traditional\nimage-image and image-report retrieval tasks. These systems also enable\nflexible, effective image retrieval conditioned on specific anatomical\nstructures described in text, achieving state-of-the-art results on 77 out of\n78 metrics.\n","authors":["Tengfei Zhang","Ziheng Zhao","Chaoyi Wu","Xiao Zhou","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2503.04653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09090v1","updated":"2025-07-12T00:20:00Z","published":"2025-07-12T00:20:00Z","title":"DS@GT at Touché: Large Language Models for Retrieval-Augmented Debate","summary":"  Large Language Models (LLMs) demonstrate strong conversational abilities. In\nthis Working Paper, we study them in the context of debating in two ways: their\nability to perform in a structured debate along with a dataset of arguments to\nuse and their ability to evaluate utterances throughout the debate. We deploy\nsix leading publicly available models from three providers for the\nRetrieval-Augmented Debate and Evaluation. The evaluation is performed by\nmeasuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout\nthis task, we found that although LLMs perform well in debates when given\nrelated arguments, they tend to be verbose in responses yet consistent in\nevaluation. The accompanying source code for this paper is located at\nhttps://github.com/dsgt-arc/touche-2025-rad.\n","authors":["Anthony Miyaguchi","Conor Johnston","Aaryan Potdar"],"pdf_url":"https://arxiv.org/pdf/2507.09090v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.09403v1","updated":"2025-07-12T21:04:25Z","published":"2025-07-12T21:04:25Z","title":"Balancing Semantic Relevance and Engagement in Related Video\n  Recommendations","summary":"  Related video recommendations commonly use collaborative filtering (CF)\ndriven by co-engagement signals, often resulting in recommendations lacking\nsemantic coherence and exhibiting strong popularity bias. This paper introduces\na novel multi-objective retrieval framework, enhancing standard two-tower\nmodels to explicitly balance semantic relevance and user engagement. Our\napproach uniquely combines: (a) multi-task learning (MTL) to jointly optimize\nco-engagement and semantic relevance, explicitly prioritizing topical\ncoherence; (b) fusion of multimodal content features (textual and visual\nembeddings) for richer semantic understanding; and (c) off-policy correction\n(OPC) via inverse propensity weighting to effectively mitigate popularity bias.\nEvaluation on industrial-scale data and a two-week live A/B test reveals our\nframework's efficacy. We observed significant improvements in semantic\nrelevance (from 51% to 63% topic match rate), a reduction in popular item\ndistribution (-13.8% popular video recommendations), and a +0.04% improvement\nin our topline user engagement metric. Our method successfully achieves better\nsemantic coherence, balanced engagement, and practical scalability for\nreal-world deployment.\n","authors":["Amit Jaspal","Feng Zhang","Wei Chang","Sumit Kumar","Yubo Wang","Roni Mittleman","Qifan Wang","Weize Mao"],"pdf_url":"https://arxiv.org/pdf/2507.09403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09376v1","updated":"2025-07-12T18:46:26Z","published":"2025-07-12T18:46:26Z","title":"Acoustic Wave Modeling Using 2D FDTD: Applications in Unreal Engine For\n  Dynamic Sound Rendering","summary":"  Accurate sound propagation simulation is essential for delivering immersive\nexperiences in virtual applications, yet industry methods for acoustic modeling\noften do not account for the full breadth of acoustic wave phenomena. This\npaper proposes a novel two-dimensional (2D) finite-difference time-domain\n(FDTD) framework that simulates sound propagation as a wave-based model in\nUnreal Engine, with an emphasis on capturing lower frequency wave phenomena,\nembedding occlusion, diffraction, reflection and interference in generated\nimpulse responses. The process begins by discretizing the scene geometry into a\n2D grid via a top-down projection from which obstacle masks and boundary\nconditions are derived. A Python-based FDTD solver injects a sine sweep at a\nsource position, and virtual quadraphonic microphone arrays record pressure\nfield responses at pre-defined listener positions. De-convolution of the\npressure responses yields multi-channel impulse responses that retain spatial\ndirectionality which are then integrated into Unreal Engine's audio pipeline\nfor dynamic playback. Benchmark tests confirm agreement with analytical\nexpectations, and the paper outlines hybrid extensions aimed at commercial\nviability.\n","authors":["Bilkent Samsurya"],"pdf_url":"https://arxiv.org/pdf/2507.09376v1.pdf","comment":"Accepted to the 50th International Computer Music Conference (ICMC),\n  2025"},{"id":"http://arxiv.org/abs/2507.09256v1","updated":"2025-07-12T11:30:32Z","published":"2025-07-12T11:30:32Z","title":"Ambiguity-Aware and High-Order Relation Learning for Multi-Grained\n  Image-Text Matching","summary":"  Image-text matching is crucial for bridging the semantic gap between computer\nvision and natural language processing. However, existing methods still face\nchallenges in handling high-order associations and semantic ambiguities among\nsimilar instances. These ambiguities arise from subtle differences between soft\npositive samples (semantically similar but incorrectly labeled) and soft\nnegative samples (locally matched but globally inconsistent), creating matching\nuncertainties. Furthermore, current methods fail to fully utilize the\nneighborhood relationships among semantically similar instances within training\nbatches, limiting the model's ability to learn high-order shared knowledge.\nThis paper proposes the Ambiguity-Aware and High-order Relation learning\nframework (AAHR) to address these issues. AAHR constructs a unified\nrepresentation space through dynamic clustering prototype contrastive learning,\neffectively mitigating the soft positive sample problem. The framework\nintroduces global and local feature extraction mechanisms and an adaptive\naggregation network, significantly enhancing full-grained semantic\nunderstanding capabilities. Additionally, AAHR employs intra-modal and\ninter-modal correlation matrices to investigate neighborhood relationships\namong sample instances thoroughly. It incorporates GNN to enhance semantic\ninteractions between instances. Furthermore, AAHR integrates momentum\ncontrastive learning to expand the negative sample set. These combined\nstrategies significantly improve the model's ability to discriminate between\nfeatures. Experimental results demonstrate that AAHR outperforms existing\nstate-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,\nconsiderably improving the accuracy and efficiency of image-text matching. The\ncode and model checkpoints for this research are available at\nhttps://github.com/Image-Text-Matching/AAHR .\n","authors":["Junyu Chen","Yihua Gao","Mingyuan Ge","Mingyong Li"],"pdf_url":"https://arxiv.org/pdf/2507.09256v1.pdf","comment":"Accepted by the Knowledge-Based Systems(KBS), 2025"},{"id":"http://arxiv.org/abs/2306.11341v2","updated":"2025-07-12T04:28:35Z","published":"2023-06-20T07:19:36Z","title":"MSVD-Indonesian: A Benchmark for Multimodal Video-Text Tasks in\n  Indonesian","summary":"  Multimodal learning on video and text has seen significant progress,\nparticularly in tasks like text-to-video retrieval, video-to-text retrieval,\nand video captioning. However, most existing methods and datasets focus\nexclusively on English. Despite Indonesian being one of the most widely spoken\nlanguages, multimodal research in Indonesian remains under-explored, largely\ndue to the lack of benchmark datasets. To address this gap, we introduce the\nfirst public Indonesian video-text dataset by translating the English captions\nin the MSVD dataset into Indonesian. Using this dataset, we evaluate neural\nnetwork models which were developed for the English video-text dataset on three\ntasks, i.e., text-to-video retrieval, video-to-text retrieval, and video\ncaptioning. Most existing models rely on feature extractors pretrained on\nEnglish vision-language datasets, raising concerns about their applicability to\nIndonesian, given the scarcity of large-scale pretraining resources in the\nlanguage. We apply a cross-lingual transfer learning approach by leveraging\nEnglish-pretrained extractors and fine-tuning models on our Indonesian dataset.\nExperimental results demonstrate that this strategy improves performance across\nall tasks and metrics. We release our dataset publicly to support future\nresearch and hope it will inspire further progress in Indonesian multimodal\nlearning.\n","authors":["Willy Fitra Hendria"],"pdf_url":"https://arxiv.org/pdf/2306.11341v2.pdf","comment":"10 pages, 5 figures, 5 tables"}]},"2025-07-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2412.18424v3","updated":"2025-07-15T17:55:08Z","published":"2024-12-24T13:39:32Z","title":"LongDocURL: a Comprehensive Multimodal Long Document Benchmark\n  Integrating Understanding, Reasoning, and Locating","summary":"  Large vision language models (LVLMs) have improved the document understanding\ncapabilities remarkably, enabling the handling of complex document elements,\nlonger contexts, and a wider range of tasks. However, existing document\nunderstanding benchmarks have been limited to handling only a small number of\npages and fail to provide a comprehensive analysis of layout elements locating.\nIn this paper, we first define three primary task categories: Long Document\nUnderstanding, numerical Reasoning, and cross-element Locating, and then\npropose a comprehensive benchmark, LongDocURL, integrating above three primary\ntasks and comprising 20 sub-tasks categorized based on different primary tasks\nand answer evidences. Furthermore, we develop a semi-automated construction\npipeline and collect 2,325 high-quality question-answering pairs, covering more\nthan 33,000 pages of documents, significantly outperforming existing\nbenchmarks. Subsequently, we conduct comprehensive evaluation experiments on\nboth open-source and closed-source models across 26 different configurations,\nrevealing critical performance gaps in this field.\n","authors":["Chao Deng","Jiale Yuan","Pi Bu","Peijie Wang","Zhong-Zhi Li","Jian Xu","Xiao-Hui Li","Yuan Gao","Jun Song","Bo Zheng","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2412.18424v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11515v1","updated":"2025-07-15T17:36:37Z","published":"2025-07-15T17:36:37Z","title":"AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of\n  LLM over the Air","summary":"  Operating Large Language Models (LLMs) on edge devices is increasingly\nchallenged by limited communication bandwidth and strained computational and\nmemory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.\nNevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ\nfixed or heuristic rank configurations, and the subsequent over-the-air\ntransmission of all LoRA parameters could be rather inefficient. To address\nthis limitation, we develop AirLLM, a hierarchical diffusion policy framework\nfor communication-aware LoRA adaptation. Specifically, AirLLM models the rank\nconfiguration as a structured action vector that spans all LoRA-inserted\nprojections. To solve the underlying high-dimensional sequential\ndecision-making problem, a Proximal Policy Optimization (PPO) agent generates\ncoarse-grained decisions by jointly observing wireless states and linguistic\ncomplexity, which are then refined via Denoising Diffusion Implicit Models\n(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The\ntwo modules are optimized alternatively, with the DDIM trained under the\nClassifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.\nExperiments under varying signal-to-noise ratios demonstrate that AirLLM\nconsistently enhances fine-tuning performance while significantly reducing\ntransmission costs, highlighting the effectiveness of reinforcement-driven,\ndiffusion-refined rank adaptation for scalable and efficient remote fine-tuning\nover the air.\n","authors":["Shiyi Yang","Xiaoxue Yu","Rongpeng Li","Jianhang Zhu","Zhifeng Zhao","Honggang Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.11515v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.11508v1","updated":"2025-07-15T17:23:56Z","published":"2025-07-15T17:23:56Z","title":"Real-World Summarization: When Evaluation Reaches Its Limits","summary":"  We examine evaluation of faithfulness to input data in the context of hotel\nhighlights: brief LLM-generated summaries that capture unique features of\naccommodations. Through human evaluation campaigns involving categorical error\nassessment and span-level annotation, we compare traditional metrics, trainable\nmethods, and LLM-as-a-judge approaches. Our findings reveal that simpler\nmetrics like word overlap correlate surprisingly well with human judgments\n(Spearman correlation rank of 0.63), often outperforming more complex methods\nwhen applied to out-of-domain data. We further demonstrate that while LLMs can\ngenerate high-quality highlights, they prove unreliable for evaluation as they\ntend to severely under- or over-annotate. Our analysis of real-world business\nimpacts shows incorrect and non-checkable information pose the greatest risks.\nWe also highlight challenges in crowdsourced evaluations.\n","authors":["Patrícia Schmidtová","Ondřej Dušek","Saad Mahamood"],"pdf_url":"https://arxiv.org/pdf/2507.11508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06565v3","updated":"2025-07-15T17:19:46Z","published":"2025-07-09T05:39:56Z","title":"A Mathematical Theory of Discursive Networks","summary":"  Large-language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source \\emph{Flaws-of-Others (FOO) algorithm}: a configurable loop in\nwhich any set of agents critique one another while a harmonizer merges their\nverdicts. We identify an ethical transgression, epithesis, that occurs when\nhumans fail to engage in the discursive network. The takeaway is practical and\ncultural: reliability in this new medium comes not from perfecting single\nmodels but from connecting imperfect ones into networks that enforce mutual\naccountability.\n","authors":["Juan B. Gutiérrez"],"pdf_url":"https://arxiv.org/pdf/2507.06565v3.pdf","comment":"32 pages, 4 figures, 4 tables, 1 algorithm, 54 references"},{"id":"http://arxiv.org/abs/2507.04099v2","updated":"2025-07-15T16:49:25Z","published":"2025-07-05T16:49:34Z","title":"Conversation Forests: The Key to Fine Tuning Large Language Models for\n  Multi-Turn Medical Conversations is Branching","summary":"  Fine-tuning methods such as Direct Preference Optimization (DPO) and Group\nRelative Policy Optimization (GRPO) have demonstrated success in training large\nlanguage models (LLMs) for single-turn tasks. However, these methods fall short\nin multi-turn applications, such as diagnostic patient interviewing, where\nunderstanding how early conversational turns influence downstream completions\nand outcomes is essential. In medicine, a multi-turn perspective is critical\nfor learning diagnostic schemas and better understanding conversation dynamics.\nTo address this gap, I introduce Savage Conversation Forests (SCF), a\nreinforcement learning framework that leverages a branched conversation\narchitecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple\npossible conversation continuations at each turn, enabling the model to learn\nhow different early responses affect downstream interactions and diagnostic\noutcomes. In experiments simulating doctor-patient conversations, SCF with\nbranching outperforms linear conversation architectures on diagnostic accuracy.\nI hypothesize that SCF's improvements stem from its ability to provide richer,\ninterdependent training signals across conversation turns. These results\nsuggest that a branched training architecture is an important strategy for fine\ntuning LLMs in complex multi-turn conversational tasks.\n","authors":["Thomas Savage"],"pdf_url":"https://arxiv.org/pdf/2507.04099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07945v2","updated":"2025-07-15T16:24:28Z","published":"2025-06-09T17:10:47Z","title":"ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication\n  Protocols","summary":"  Recent advances in large language models (LLMs) have demonstrated strong\nperformance in generating code for general-purpose programming languages.\nHowever, their potential for hardware description languages (HDLs), such as\nSystemVerilog, remains largely unexplored. HDL code generation poses unique\nchallenges due to strict timing semantics, concurrency, and synthesizability\nconstraints essential for correct hardware functionality. Further, HDL-based\ndesign flows encompass a broad set of tasks beyond structural code generation,\nincluding testbench development, assertion-based verification, timing closure,\nand protocol-level integration for on-chip communication. In this work, we\nevaluate the capabilities of both open-source and state-of-the-art LLMs in\ngenerating synthesizable and functionally accurate SystemVerilog\nimplementations of widely used communication protocols that are critical\ncomponents of embedded and System-on-Chip (SoC) systems. We introduce\nProtocolLLM, the first benchmark suite specifically targeting these protocols\nwith tasks spanning multiple design abstraction levels and varying prompt\nspecificity. Our evaluation method also focuses on timing correctness in\naddition to synthesizability and syntactic correctness. We observe that most of\nthe models fail to generate SystemVerilog code for communication protocols that\nfollow timing constrains.\n","authors":["Arnav Sheth","Ivaxi Sheth","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2506.07945v2.pdf","comment":"Accepted at MLSysArch@ISCA 2025"},{"id":"http://arxiv.org/abs/2502.16366v3","updated":"2025-07-15T16:12:44Z","published":"2025-02-22T21:48:48Z","title":"A Generative Approach to LLM Harmfulness Detection with Special Red Flag\n  Tokens","summary":"  Most safety training methods for large language models (LLMs) are based on\nfine-tuning that forces models to shift from an unsafe answer to refusal when\nfaced with harmful requests. Unfortunately, these drastic distribution shifts\ngenerally compromise model capabilities. To avoid that, we propose to expand\nthe model's vocabulary with a special token we call red flag token (<rf>) and\npropose to train the model to insert this token into its response at any time\nwhen harmful content is generated or about to be generated. Our approach offers\nseveral advantages: it enables the model to explicitly learn the concept of\nharmfulness while marginally affecting the generated distribution, thus\nmaintaining the model's utility. It also evaluates each generated answer and\nprovides robustness as good as adversarial training without the need to run\nattacks during training. Moreover, by encapsulating our safety tuning in a LoRA\nmodule, we provide additional defenses against fine-tuning API attacks.\n","authors":["Sophie Xhonneux","David Dobre","Mehrnaz Mofakhami","Leo Schwinn","Gauthier Gidel"],"pdf_url":"https://arxiv.org/pdf/2502.16366v3.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.11423v1","updated":"2025-07-15T15:47:47Z","published":"2025-07-15T15:47:47Z","title":"Reasoning Strategies in Large Language Models: Can They Follow, Prefer,\n  and Optimize?","summary":"  Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities.\n","authors":["Yanjian Zhang","Guillaume Wisniewski","Nadi Tomeh","Thierry Charnois"],"pdf_url":"https://arxiv.org/pdf/2507.11423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07505v3","updated":"2025-07-15T15:42:40Z","published":"2025-07-10T07:50:52Z","title":"Hallucination Stations: On Some Basic Limitations of Transformer-Based\n  Language Models","summary":"  In this paper we explore hallucinations and related capability limitations in\nLLMs and LLM-based agents from the perspective of computational complexity. We\nshow that beyond a certain complexity, LLMs are incapable of carrying out\ncomputational and agentic tasks or verifying their accuracy.\n","authors":["Varin Sikka","Vishal Sikka"],"pdf_url":"https://arxiv.org/pdf/2507.07505v3.pdf","comment":"6 pages; to be submitted to AAAI-26 after reviews"},{"id":"http://arxiv.org/abs/2507.11412v1","updated":"2025-07-15T15:31:51Z","published":"2025-07-15T15:31:51Z","title":"Seq vs Seq: An Open Suite of Paired Encoders and Decoders","summary":"  The large language model (LLM) community focuses almost exclusively on\ndecoder-only language models, since they are easier to use for text generation.\nHowever, a large subset of the community still uses encoder-only models for\ntasks such as classification or retrieval. Previous work has attempted to\ncompare these architectures, but is forced to make comparisons with models that\nhave different numbers of parameters, training techniques, and datasets. We\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\ndecoder-only models produces SOTA recipes in both categories for their\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\ndecoders. Like previous work, we find that encoder-only models excel at\nclassification and retrieval tasks while decoders excel at generative tasks.\nHowever, we show that adapting a decoder model to encoder tasks (and vice\nversa) through continued training is subpar compared to using only the reverse\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\nfor generative tasks). We open-source all artifacts of this study including\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\nallow future work to analyze or extend all aspects of training.\n","authors":["Orion Weller","Kathryn Ricci","Marc Marone","Antoine Chaffin","Dawn Lawrie","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2507.11412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11408v1","updated":"2025-07-15T15:28:37Z","published":"2025-07-15T15:28:37Z","title":"KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical\n  Reasoning?","summary":"  Chain-of-thought traces have been shown to improve performance of large\nlanguage models in a plethora of reasoning tasks, yet there is no consensus on\nthe mechanism through which this performance boost is achieved. To shed more\nlight on this, we introduce Causal CoT Graphs (CCGs), which are directed\nacyclic graphs automatically extracted from reasoning traces that model\nfine-grained causal dependencies in the language model output. A collection of\n$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their\nassociated CCGs are compiled into our dataset -- \\textbf{KisMATH}. Our detailed\nempirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in\nthe CCG are mediators for the final answer, a condition necessary for\nreasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating\nthat models internally realise structures akin to our graphs. KisMATH enables\ncontrolled, graph-aligned interventions and opens up avenues for further\ninvestigation into the role of chain-of-thought in LLM reasoning.\n","authors":["Soumadeep Saha","Akshay Chaturvedi","Saptarshi Saha","Utpal Garain","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2507.11408v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2507.11407v1","updated":"2025-07-15T15:24:51Z","published":"2025-07-15T15:24:51Z","title":"EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and\n  Reasoning Modes","summary":"  This technical report introduces EXAONE 4.0, which integrates a Non-reasoning\nmode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5\nand the advanced reasoning abilities of EXAONE Deep. To pave the way for the\nagentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool\nuse, and its multilingual capabilities are extended to support Spanish in\naddition to English and Korean. The EXAONE 4.0 model series consists of two\nsizes: a mid-size 32B model optimized for high performance, and a small-size\n1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates\nsuperior performance compared to open-weight models in its class and remains\ncompetitive even against frontier-class models. The models are publicly\navailable for research purposes and can be easily downloaded via\nhttps://huggingface.co/LGAI-EXAONE.\n","authors":["LG AI Research"," :","Kyunghoon Bae","Eunbi Choi","Kibong Choi","Stanley Jungkyu Choi","Yemuk Choi","Kyubeen Han","Seokhee Hong","Junwon Hwang","Taewan Hwang","Joonwon Jang","Hyojin Jeon","Kijeong Jeon","Gerrard Jeongwon Jo","Hyunjik Jo","Jiyeon Jung","Euisoon Kim","Hyosang Kim","Jihoon Kim","Joonkee Kim","Seonghwan Kim","Soyeon Kim","Sunkyoung Kim","Yireun Kim","Yongil Kim","Youchul Kim","Edward Hwayoung Lee","Gwangho Lee","Haeju Lee","Honglak Lee","Jinsik Lee","Kyungmin Lee","Sangha Park","Young Min Paik","Yongmin Park","Youngyong Park","Sanghyun Seo","Sihoon Yang","Heuiyeen Yeen","Sihyuk Yi","Hyeongu Yun"],"pdf_url":"https://arxiv.org/pdf/2507.11407v1.pdf","comment":"Technical Report, 30 Pages"},{"id":"http://arxiv.org/abs/2507.11405v1","updated":"2025-07-15T15:23:53Z","published":"2025-07-15T15:23:53Z","title":"DCR: Quantifying Data Contamination in LLMs Evaluation","summary":"  The rapid advancement of large language models (LLMs) has heightened concerns\nabout benchmark data contamination (BDC), where models inadvertently memorize\nevaluation data, inflating performance metrics and undermining genuine\ngeneralization assessment. This paper introduces the Data Contamination Risk\n(DCR) framework, a lightweight, interpretable pipeline designed to detect and\nquantify BDC across four granular levels: semantic, informational, data, and\nlabel. By synthesizing contamination scores via a fuzzy inference system, DCR\nproduces a unified DCR Factor that adjusts raw accuracy to reflect\ncontamination-aware performance. Validated on 9 LLMs (0.5B-72B) across\nsentiment analysis, fake news detection, and arithmetic reasoning tasks, the\nDCR framework reliably diagnoses contamination severity and with accuracy\nadjusted using the DCR Factor to within 4% average error across the three\nbenchmarks compared to the uncontaminated baseline. Emphasizing computational\nefficiency and transparency, DCR provides a practical tool for integrating\ncontamination assessment into routine evaluations, fostering fairer comparisons\nand enhancing the credibility of LLM benchmarking practices.\n","authors":["Cheng Xu","Nan Yan","Shuhao Guan","Changhong Jin","Yuke Mei","Yibing Guo","M-Tahar Kechadi"],"pdf_url":"https://arxiv.org/pdf/2507.11405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00077v3","updated":"2025-07-15T15:17:21Z","published":"2025-05-29T23:39:24Z","title":"Gaussian mixture models as a proxy for interacting language models","summary":"  Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions.\n","authors":["Edward L. Wang","Tianyu Wang","Hayden Helm","Avanti Athreya","Vince Lyzinski","Carey E. Priebe"],"pdf_url":"https://arxiv.org/pdf/2506.00077v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01504v3","updated":"2025-07-15T14:54:52Z","published":"2025-07-02T09:10:33Z","title":"Following the Clues: Experiments on Person Re-ID using Cross-Modal\n  Intelligence","summary":"  The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID.\n","authors":["Robert Aufschläger","Youssef Shoeb","Azarm Nowzad","Michael Heigl","Fabian Bally","Martin Schramm"],"pdf_url":"https://arxiv.org/pdf/2507.01504v3.pdf","comment":"accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia"},{"id":"http://arxiv.org/abs/2507.11384v1","updated":"2025-07-15T14:53:33Z","published":"2025-07-15T14:53:33Z","title":"Addressing Data Imbalance in Transformer-Based Multi-Label Emotion\n  Detection with Weighted Loss","summary":"  This paper explores the application of a simple weighted loss function to\nTransformer-based models for multi-label emotion detection in SemEval-2025\nShared Task 11. Our approach addresses data imbalance by dynamically adjusting\nclass weights, thereby enhancing performance on minority emotion classes\nwithout the computational burden of traditional resampling methods. We evaluate\nBERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such\nas Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.\nThe results demonstrate that the weighted loss function improves performance on\nhigh-frequency emotion classes but shows limited impact on minority classes.\nThese findings underscore both the effectiveness and the challenges of applying\nthis approach to imbalanced multi-label emotion detection.\n","authors":["Xia Cui"],"pdf_url":"https://arxiv.org/pdf/2507.11384v1.pdf","comment":"10 pages, 1 figure, SemEval 2025"},{"id":"http://arxiv.org/abs/2507.11356v1","updated":"2025-07-15T14:26:50Z","published":"2025-07-15T14:26:50Z","title":"What is the Best Process Model Representation? A Comparative Analysis\n  for Process Modeling with Large Language Models","summary":"  Large Language Models (LLMs) are increasingly applied for Process Modeling\n(PMo) tasks such as Process Model Generation (PMG). To support these tasks,\nresearchers have introduced a variety of Process Model Representations (PMRs)\nthat serve as model abstractions or generation targets. However, these PMRs\ndiffer widely in structure, complexity, and usability, and have never been\nsystematically compared. Moreover, recent PMG approaches rely on distinct\nevaluation strategies and generation techniques, making comparison difficult.\nThis paper presents the first empirical study that evaluates multiple PMRs in\nthe context of PMo with LLMs. We introduce the PMo Dataset, a new dataset\ncontaining 55 process descriptions paired with models in nine different PMRs.\nWe evaluate PMRs along two dimensions: suitability for LLM-based PMo and\nperformance on PMG. \\textit{Mermaid} achieves the highest overall score across\nsix PMo criteria, whereas \\textit{BPMN text} delivers the best PMG results in\nterms of process element similarity.\n","authors":["Alexis Brissard","Frédéric Cuppens","Amal Zouaq"],"pdf_url":"https://arxiv.org/pdf/2507.11356v1.pdf","comment":"12 pages, 7 figures, to be published in AI4BPM 2025 Proceedings"},{"id":"http://arxiv.org/abs/2504.05294v2","updated":"2025-07-15T14:16:16Z","published":"2025-04-07T17:49:23Z","title":"Truthful or Fabricated? Using Causal Attribution to Mitigate Reward\n  Hacking in Explanations","summary":"  Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations.\n","authors":["Pedro Ferreira","Wilker Aziz","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2504.05294v2.pdf","comment":"20 pages, 10 figures, 6 tables"},{"id":"http://arxiv.org/abs/2507.11330v1","updated":"2025-07-15T14:03:55Z","published":"2025-07-15T14:03:55Z","title":"Automated Novelty Evaluation of Academic Paper: A Collaborative Approach\n  Integrating Human and Large Language Model Knowledge","summary":"  Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. The most common novelty in\nacademic papers is the introduction of new methods. In this paper, we propose\nleveraging human knowledge and LLM to assist pretrained language models (PLMs,\ne.g. BERT etc.) in predicting the method novelty of papers. Specifically, we\nextract sentences related to the novelty of the academic paper from peer review\nreports and use LLM to summarize the methodology section of the academic paper,\nwhich are then used to fine-tune PLMs. In addition, we have designed a\ntext-guided fusion module with novel Sparse-Attention to better integrate human\nand LLM knowledge. We compared the method we proposed with a large number of\nbaselines. Extensive experiments demonstrate that our method achieves superior\nperformance.\n","authors":["Wenqing Wu","Chengzhi Zhang","Yi Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.11330v1.pdf","comment":"Journal of the Association for Information Science and Technology,\n  2025"},{"id":"http://arxiv.org/abs/2507.11316v1","updated":"2025-07-15T13:48:35Z","published":"2025-07-15T13:48:35Z","title":"Internal Value Alignment in Large Language Models through Controlled\n  Value Vector Activation","summary":"  Aligning Large Language Models (LLMs) with human values has attracted\nincreasing attention since it provides clarity, transparency, and the ability\nto adapt to evolving scenarios. In this paper, we introduce a Controlled Value\nVector Activation (ConVA) method that directly aligns the internal values of\nLLMs by interpreting how a value is encoded in their latent representations and\nmodifies relevant activations to ensure consistent values in LLMs. To ensure an\naccurate and unbiased interpretation, we propose a context-controlled value\nvector identification method. To consistently control values without\nsacrificing model performance, we introduce a gated value vector activation\nmethod for effective and minimum degree of value control. Experiments show that\nour method achieves the highest control success rate across 10 basic values\nwithout hurting LLM performance and fluency, and ensures target values even\nwith opposite and potentially malicious input prompts. Source code and data are\navailable at~ https://github.com/hr-jin/ConVA.\n","authors":["Haoran Jin","Meng Li","Xiting Wang","Zhihao Xu","Minlie Huang","Yantao Jia","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2507.11316v1.pdf","comment":"25 pages, 14 figures. Accepted by ACL 2025 (main conference)"},{"id":"http://arxiv.org/abs/2507.06313v2","updated":"2025-07-15T13:47:22Z","published":"2025-07-08T18:06:45Z","title":"ETT: Expanding the Long Context Understanding Capability of LLMs at\n  Test-Time","summary":"  Transformer-based Language Models' computation and memory overhead increase\nquadratically as a function of sequence length. The quadratic cost poses\nchallenges when employing LLMs for processing long sequences. In this work, we\nintroduce \\ourmodelacronym~(Extend at Test-Time), method for extending the\ncontext length of short context Transformer-based LLMs, with constant memory\nrequirement and linear computation overhead. ETT enable the extension of the\ncontext length at test-time by efficient fine-tuning the model's parameters on\nthe input context, chunked into overlapping small subsequences. We evaluate ETT\non LongBench by extending the context length of GPT-Large and Phi-2 up to 32\ntimes, increasing from 1k to 32k tokens. This results in up to a 30 percent\nimprovement in the model's accuracy. We also study how context can be stored in\nLLM's weights effectively and efficiently. Through a detailed ablation study,\nwe examine which Transformer modules are most beneficial to fine-tune at\ntest-time. Interestingly, we find that fine-tuning the second layer of the FFNs\nis more effective than full fine-tuning, leading to a further improvement in\nthe models' accuracy.\n","authors":["Kiarash Zahirnia","Zahra Golpayegani","Walid Ahmed","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2507.06313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11299v1","updated":"2025-07-15T13:26:49Z","published":"2025-07-15T13:26:49Z","title":"Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving\n  Patient-Doctor Communication in Romanian","summary":"  Text-based telemedicine has become increasingly common, yet the quality of\nmedical advice in doctor-patient interactions is often judged more on how\nadvice is communicated rather than its clinical accuracy. To address this, we\nintroduce Dr.Copilot , a multi-agent large language model (LLM) system that\nsupports Romanian-speaking doctors by evaluating and enhancing the presentation\nquality of their written responses. Rather than assessing medical correctness,\nDr.Copilot provides feedback along 17 interpretable axes. The system comprises\nof three LLM agents with prompts automatically optimized via DSPy. Designed\nwith low-resource Romanian data and deployed using open-weight models, it\ndelivers real-time specific feedback to doctors within a telemedicine platform.\nEmpirical evaluations and live deployment with 41 doctors show measurable\nimprovements in user reviews and response quality, marking one of the first\nreal-world deployments of LLMs in Romanian medical settings.\n","authors":["Andrei Niculae","Adrian Cosma","Cosmin Dumitrache","Emilian Rǎdoi"],"pdf_url":"https://arxiv.org/pdf/2507.11299v1.pdf","comment":"10 figures, 2 tables, 2 listings"},{"id":"http://arxiv.org/abs/2507.11292v1","updated":"2025-07-15T13:19:18Z","published":"2025-07-15T13:19:18Z","title":"Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources,\n  Coded Term Lexicon, and Enhanced Detection Frameworks","summary":"  The proliferation of hate speech has inflicted significant societal harm,\nwith its intensity and directionality closely tied to specific targets and\narguments. In recent years, numerous machine learning-based methods have been\ndeveloped to detect hateful comments on online platforms automatically.\nHowever, research on Chinese hate speech detection lags behind, and\ninterpretability studies face two major challenges: first, the scarcity of\nspan-level fine-grained annotated datasets limits models' deep semantic\nunderstanding of hate speech; second, insufficient research on identifying and\ninterpreting coded hate speech restricts model explainability in complex\nreal-world scenarios. To address these, we make the following contributions:\n(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE\nToxiCN), the first span-level Chinese hate speech dataset, and evaluate the\nhate semantic understanding of existing models using it. (2) We conduct the\nfirst comprehensive study on Chinese coded hate terms, LLMs' ability to\ninterpret hate semantics. (3) We propose a method to integrate an annotated\nlexicon into models, significantly enhancing hate speech detection performance.\nOur work provides valuable resources and insights to advance the\ninterpretability of Chinese hate speech detection research.\n","authors":["Zewen Bai","Liang Yang","Shengdi Yin","Yuanyuan Sun","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2507.11292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14407v2","updated":"2025-07-15T13:16:23Z","published":"2025-06-17T11:08:29Z","title":"ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge","summary":"  Retrieval systems are central to many NLP pipelines, but often rely on\nsurface-level cues such as keyword overlap and lexical semantic similarity. To\nevaluate retrieval beyond these shallow signals, recent benchmarks introduce\nreasoning-heavy queries; however, they primarily shift the burden to query-side\nprocessing techniques -- like prompting or multi-hop retrieval -- that can help\nresolve complexity. In contrast, we present ImpliRet, a benchmark that shifts\nthe reasoning challenge to document-side processing: The queries are simple,\nbut relevance depends on facts stated implicitly in documents through temporal\n(e.g., resolving \"two days ago\"), arithmetic, and world knowledge\nrelationships. We evaluate a range of sparse and dense retrievers, all of which\nstruggle in this setting: the best nDCG@10 is only 14.91%. We also test whether\nlong-context models can overcome this limitation. But even with a short context\nof only thirty documents, including the positive document, GPT-o4-mini scores\nonly 55.54%, showing that document-side reasoning remains a challenge. Our\ncodes are available at: github.com/ZeinabTaghavi/IMPLIRET\n","authors":["Zeinab Sadat Taghavi","Ali Modarressi","Yunpu Ma","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2506.14407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.22791v3","updated":"2025-07-15T12:59:47Z","published":"2025-06-28T07:25:12Z","title":"ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models","summary":"  Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.\n","authors":["Jianxin Yan","Wangze Ni","Lei Chen","Xuemin Lin","Peng Cheng","Zhan Qin","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2506.22791v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11275v1","updated":"2025-07-15T12:52:47Z","published":"2025-07-15T12:52:47Z","title":"FMC: Formalization of Natural Language Mathematical Competition Problems","summary":"  Efficient and accurate autoformalization methods, which leverage large-scale\ndatasets of extensive natural language mathematical problems to construct\nformal language datasets, are key to advancing formal mathematical reasoning.\nIn this paper, we propose an autoformalization pipeline based on large language\nmodels with error feedback, achieving a fully automatic and training-free\nformalization approach. Using this pipeline, we curate an Olympiad-level\ndataset aligning natural language problems with Lean formalizations. The\ndataset comprises $3,922$ mathematical problems in natural language and $9,787$\nin Lean, of which $64.46\\%$ were assessed as at least above-average quality,\nmaking it suitable as a benchmark for automated theorem provers. Additionally,\nwe investigate the formalization and reasoning capabilities of various LLMs and\nempirically demonstrate that few-shot learning, error feedback, and increasing\nsampling numbers enhance the autoformalization process. Experiments of three\nautomated theorem provers on the \\dataset\\ dataset also highlight its\nchallenging nature and its value as a benchmark for formal reasoning tasks.\n","authors":["Jiaxuan Xie","Chengwu Liu","Ye Yuan","Siqi Li","Zhiping Xiao","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.11275v1.pdf","comment":"Accepted in ICML 2025 AI4MATH Workshop"},{"id":"http://arxiv.org/abs/2507.11273v1","updated":"2025-07-15T12:52:12Z","published":"2025-07-15T12:52:12Z","title":"KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding","summary":"  Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.\n","authors":["Luohe Shi","Zuchao Li","Lefei Zhang","Guoming Liu","Baoyuan Qi","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.11273v1.pdf","comment":"To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)"},{"id":"http://arxiv.org/abs/2505.00582v2","updated":"2025-07-15T12:40:48Z","published":"2025-05-01T15:14:32Z","title":"Block Circulant Adapter for Large Language Models","summary":"  Fine-tuning large language models (LLMs) is difficult due to their huge model\nsize. Recent Fourier domain-based methods show potential for reducing\nfine-tuning costs. We propose a block circulant matrix-based fine-tuning method\nwith a stable training heuristic to leverage the properties of circulant\nmatrices and one-dimensional Fourier transforms to reduce storage and\ncomputation costs. Experiments show that our method uses $14\\times$ less number\nof parameters than VeRA, $16\\times$ smaller than LoRA and $32\\times$ less FLOPs\nthan FourierFT, while maintaining close or better task performance. Our\napproach presents a promising way in frequency domain to fine-tune large models\non downstream tasks.\n","authors":["Xinyu Ding","Meiqi Wang","Siyu Liao","Zhongfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2505.00582v2.pdf","comment":"to appear in Proceedings of the 2025 International Joint Conference\n  on Artificial Intelligence (IJCAI-2025)"},{"id":"http://arxiv.org/abs/2503.21073v3","updated":"2025-07-15T12:15:41Z","published":"2025-03-27T01:17:06Z","title":"Shared Global and Local Geometry of Language Model Embeddings","summary":"  Researchers have recently suggested that models share common representations.\nIn our work, we find numerous geometric similarities across the token\nembeddings of large language models. First, we find ``global'' similarities:\ntoken embeddings often share similar relative orientations. Next, we\ncharacterize local geometry in two ways: (1) by using Locally Linear\nEmbeddings, and (2) by defining a simple measure for the intrinsic dimension of\neach embedding. Both characterizations allow us to find local similarities\nacross token embeddings. Additionally, our intrinsic dimension demonstrates\nthat embeddings lie on a lower dimensional manifold, and that tokens with lower\nintrinsic dimensions often have semantically coherent clusters, while those\nwith higher intrinsic dimensions do not. Based on our findings, we introduce\nEMB2EMB, a simple application to linearly transform steering vectors from one\nlanguage model to another, despite the two models having different dimensions.\n","authors":["Andrew Lee","Melanie Weber","Fernanda Viégas","Martin Wattenberg"],"pdf_url":"https://arxiv.org/pdf/2503.21073v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03106v3","updated":"2025-07-15T12:11:37Z","published":"2025-06-03T17:39:02Z","title":"Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback","summary":"  Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided self-refinements simultaneously while maintaining exploration.\nAdditionally, we employ a shaping function to amplify learning from correct,\nespecially unfamiliar, refinements and penalize incorrect ones. Extensive\nexperiments with Qwen2.5-7B-Base, Qwen2.5-Math-7B-Base, and Qwen3-8B\ndemonstrate that Critique-GRPO consistently outperforms supervised learning and\nRL-based fine-tuning methods across eight challenging mathematical, STEM, and\ngeneral reasoning tasks, improving average pass@1 scores by approximately 4.4%\nand 3.8% on Qwen2.5-7B-Base and Qwen3-8B, respectively. Notably, Critique-GRPO\nenables effective self-improvement through self-critiquing and weak-to-strong\ngeneralization, achieving consistent gains over GRPO, such as 16.7% and 10.0%\npass@1 improvements on AIME 2024, respectively.\n","authors":["Xiaoying Zhang","Hao Sun","Yipeng Zhang","Kaituo Feng","Chaochao Lu","Chao Yang","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2506.03106v3.pdf","comment":"49 pages, updated with new experimental results"},{"id":"http://arxiv.org/abs/2507.08297v2","updated":"2025-07-15T12:04:21Z","published":"2025-07-11T04:07:10Z","title":"KAT-V1: Kwai-AutoThink Technical Report","summary":"  We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model\ndeveloped to address the overthinking problem in reasoning-intensive tasks,\nwhere an automatic thinking training paradigm is proposed to dynamically switch\nbetween reasoning and non-reasoning modes based on task complexity.\nSpecifically, first, we construct the dual-regime dataset based on a novel\ntagging pipeline and a multi-agent synthesis strategy, and then we apply\nMulti-Token Prediction (MTP)-enhanced knowledge distillation, enabling\nefficient and fine-grained reasoning transfer with minimal pretraining cost.\nBesides, we implement a cold-start initialization strategy that introduces\nmode-selection priors using majority-vote signals and intent-aware prompting.\nFinally, we propose Step-SRPO, a reinforcement learning algorithm that\nincorporates intermediate supervision into the GRPO framework, offering\nstructured guidance over both reasoning-mode selection and response accuracy.\nExtensive experiments across multiple benchmarks demonstrate that KAT\nconsistently matches or even outperforms current state-of-the-art models,\nincluding DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of\nreasoning-intensive tasks while reducing token usage by up to approximately\n30\\%. Beyond academic evaluation, KAT has been successfully deployed in\nKwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world\ndevelopment workflows with high accuracy, efficiency, and controllable\nreasoning behaviors. Moreover, we are actively training a 200B\nMixture-of-Experts (MoE) with 40B activation parameters, where the early-stage\nresults already demonstrate promising improvements in performance and\nefficiency, further showing the scalability of the AutoThink paradigm.\n","authors":["Zizheng Zhan","Ken Deng","Huaixi Tang","Wen Xiang","Kun Wu","Weihao Li","Wenqiang Zhu","Jingxuan Xu","Lecheng Huang","Zongxian Feng","Shaojie Wang","Shangpeng Yan","Xuxing Chen","Jiaheng Liu","Zhongyuan Peng","Zuchen Gao","Haoyang Huang","Xiaojiang Zhang","Jinghui Wang","Zheng Lin","Mengtong Li","Huiming Wang","Ziqi Zhan","Yanan Wu","Yuanxing Zhang","Jian Yang","Guang Chen","Haotian Zhang","Bin Chen","Bing Yu"],"pdf_url":"https://arxiv.org/pdf/2507.08297v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02962v3","updated":"2025-07-15T12:01:49Z","published":"2025-06-30T09:02:45Z","title":"RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs\n  through Multi-query Parallelism","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while they remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have explored enhancing models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to the single-query mode. In this paper, we propose\nRAG-R1, a novel training framework designed to enable LLMs to adaptively\nleverage internal and external knowledge during the reasoning process. We\nfurther expand the generation and retrieval processes within the framework from\nsingle-query mode to multi-query parallelism, aimed at reducing inference time\nand enhancing the model's capabilities. Extensive experiments on seven\nquestion-answering benchmarks demonstrate that our method outperforms the\nstrongest baseline by up to 13.2% and decreases inference time by 11.1%.\n","authors":["Zhiwen Tan","Jiaming Huang","Qintong Wu","Hongxuan Zhang","Chenyi Zhuang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2507.02962v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11230v1","updated":"2025-07-15T12:00:30Z","published":"2025-07-15T12:00:30Z","title":"Sparse Autoencoders Can Capture Language-Specific Concepts Across\n  Diverse Languages","summary":"  Understanding the multilingual mechanisms of large language models (LLMs)\nprovides insight into how they process different languages, yet this remains\nchallenging. Existing studies often focus on individual neurons, but their\npolysemantic nature makes it difficult to isolate language-specific units from\ncross-lingual representations. To address this, we explore sparse autoencoders\n(SAEs) for their ability to learn monosemantic features that represent concrete\nand abstract concepts across languages in LLMs. While some of these features\nare language-independent, the presence of language-specific features remains\nunderexplored. In this work, we introduce SAE-LAPE, a method based on feature\nactivation probability, to identify language-specific features within the\nfeed-forward network. We find that many such features predominantly appear in\nthe middle to final layers of the model and are interpretable. These features\ninfluence the model's multilingual performance and language output and can be\nused for language identification with performance comparable to fastText along\nwith more interpretability. Our code is available at\nhttps://github.com/LyzanderAndrylie/language-specific-features .\n","authors":["Lyzander Marciano Andrylie","Inaya Rahmanisa","Mahardika Krisna Ihsani","Alfan Farizki Wicaksono","Haryo Akbarianto Wibowo","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2507.11230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11222v1","updated":"2025-07-15T11:50:25Z","published":"2025-07-15T11:50:25Z","title":"An Agentic Flow for Finite State Machine Extraction using Prompt\n  Chaining","summary":"  Finite-State Machines (FSMs) are critical for modeling the operational logic\nof network protocols, enabling verification, analysis, and vulnerability\ndiscovery. However, existing FSM extraction techniques face limitations such as\nscalability, incomplete coverage, and ambiguity in natural language\nspecifications. In this paper, we propose FlowFSM, a novel agentic framework\nthat leverages Large Language Models (LLMs) combined with prompt chaining and\nchain-of-thought reasoning to extract accurate FSMs from raw RFC documents.\nFlowFSM systematically processes protocol specifications, identifies state\ntransitions, and constructs structured rule-books by chaining agent outputs.\nExperimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM\nachieves high extraction precision while minimizing hallucinated transitions,\nshowing promising results. Our findings highlight the potential of agent-based\nLLM systems in the advancement of protocol analysis and FSM inference for\ncybersecurity and reverse engineering applications.\n","authors":["Fares Wael","Youssef Maklad","Ali Hamdi","Wael Elsersy"],"pdf_url":"https://arxiv.org/pdf/2507.11222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07817v2","updated":"2025-07-15T11:42:05Z","published":"2025-07-10T14:46:33Z","title":"On the Effect of Instruction Tuning Loss on Generalization","summary":"  Instruction Tuning has emerged as a pivotal post-training paradigm that\nenables pre-trained language models to better follow user instructions. Despite\nits significance, little attention has been given to optimizing the loss\nfunction used. A fundamental, yet often overlooked, question is whether the\nconventional auto-regressive objective - where loss is computed only on\nresponse tokens, excluding prompt tokens - is truly optimal for instruction\ntuning. In this work, we systematically investigate the impact of\ndifferentially weighting prompt and response tokens in instruction tuning loss,\nand propose Weighted Instruction Tuning (WIT) as a better alternative to\nconventional instruction tuning. Through extensive experiments on five language\nmodels of different families and scale, three finetuning datasets of different\nsizes, and five diverse evaluation benchmarks, we show that the standard\ninstruction tuning loss often yields suboptimal performance and limited\nrobustness to input prompt variations. We find that a low-to-moderate weight\nfor prompt tokens coupled with a moderate-to-high weight for response tokens\nyields the best-performing models across settings and also serve as better\nstarting points for the subsequent preference alignment training. These\nfindings highlight the need to reconsider instruction tuning loss and offer\nactionable insights for developing more robust and generalizable models. Our\ncode is open-sourced at https://github.com/kowndinya-renduchintala/WIT.\n","authors":["Anwoy Chatterjee","H S V N S Kowndinya Renduchintala","Sumit Bhatia","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2507.07817v2.pdf","comment":"To appear in Transactions of the Association for Computational\n  Linguistics (TACL)"},{"id":"http://arxiv.org/abs/2507.11216v1","updated":"2025-07-15T11:37:30Z","published":"2025-07-15T11:37:30Z","title":"EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question\n  Answering","summary":"  Previous literature has largely shown that Large Language Models (LLMs)\nperpetuate social biases learnt from their pre-training data. Given the notable\nlack of resources for social bias evaluation in languages other than English,\nand for social contexts outside of the United States, this paper introduces the\nSpanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and\nCaBBQ). Based on the original BBQ, these two parallel datasets are designed to\nassess social bias across 10 categories using a multiple-choice QA setting, now\nadapted to the Spanish and Catalan languages and to the social context of\nSpain. We report evaluation results on different LLMs, factoring in model\nfamily, size and variant. Our results show that models tend to fail to choose\nthe correct answer in ambiguous scenarios, and that high QA accuracy often\ncorrelates with greater reliance on social biases.\n","authors":["Valle Ruiz-Fernández","Mario Mina","Júlia Falcão","Luis Vasquez-Reina","Anna Sallés","Aitor Gonzalez-Agirre","Olatz Perez-de-Viñaspre"],"pdf_url":"https://arxiv.org/pdf/2507.11216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.00838v2","updated":"2025-07-15T11:31:45Z","published":"2025-07-01T15:08:53Z","title":"Stylometry recognizes human and LLM-generated texts in short samples","summary":"  The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.\n","authors":["Karol Przystalski","Jan K. Argasiński","Iwona Grabska-Gradzińska","Jeremi K. Ochab"],"pdf_url":"https://arxiv.org/pdf/2507.00838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10157v3","updated":"2025-07-15T11:14:36Z","published":"2025-04-14T12:12:52Z","title":"SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users","summary":"  Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.\n","authors":["Xinnong Zhang","Jiayu Lin","Xinyi Mou","Shiyue Yang","Xiawei Liu","Libo Sun","Hanjia Lyu","Yihang Yang","Weihong Qi","Yue Chen","Guanying Li","Ling Yan","Yao Hu","Siming Chen","Yu Wang","Xuanjing Huang","Jiebo Luo","Shiping Tang","Libo Wu","Baohua Zhou","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2504.10157v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11198v1","updated":"2025-07-15T11:06:32Z","published":"2025-07-15T11:06:32Z","title":"Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy\n  Gains in Qualitative Coding","summary":"  Large Language Models (LLMs) enable new possibilities for qualitative\nresearch at scale, including coding and data annotation. While multi-agent\nsystems (MAS) can emulate human coding workflows, their benefits over\nsingle-agent coding remain poorly understood. We conducted an experimental\nstudy of how agent persona and temperature shape consensus-building and coding\naccuracy of dialog segments based on a codebook with 8 codes. Our open-source\nMAS mirrors deductive human coding through structured agent discussion and\nconsensus arbitration. Using six open-source LLMs (with 3 to 32 billion\nparameters) and 18 experimental configurations, we analyze over 77,000 coding\ndecisions against a gold-standard dataset of human-annotated transcripts from\nonline math tutoring sessions. Temperature significantly impacted whether and\nwhen consensus was reached across all six LLMs. MAS with multiple personas\n(including neutral, assertive, or empathetic), significantly delayed consensus\nin four out of six LLMs compared to uniform personas. In three of those LLMs,\nhigher temperatures significantly diminished the effects of multiple personas\non consensus. However, neither temperature nor persona pairing lead to robust\nimprovements in coding accuracy. Single agents matched or outperformed MAS\nconsensus in most conditions. Only one model (OpenHermesV2:7B) and code\ncategory showed above-chance gains from MAS deliberation when temperature was\n0.5 or lower and especially when the agents included at least one assertive\npersona. Qualitative analysis of MAS collaboration for these configurations\nsuggests that MAS may nonetheless aid in narrowing ambiguous code applications\nthat could improve codebooks and human-AI coding. We contribute new insight\ninto the limits of LLM-based qualitative methods, challenging the notion that\ndiverse MAS personas lead to better outcomes. We open-source our MAS and\nexperimentation code.\n","authors":["Conrad Borchers","Bahar Shahrokhian","Francesco Balzan","Elham Tajik","Sreecharan Sankaranarayanan","Sebastian Simon"],"pdf_url":"https://arxiv.org/pdf/2507.11198v1.pdf","comment":"Manuscript submitted for review"},{"id":"http://arxiv.org/abs/2507.06803v2","updated":"2025-07-15T11:05:37Z","published":"2025-07-09T12:44:49Z","title":"Text to model via SysML: Automated generation of dynamical system\n  computational models from unstructured natural language text via enhanced\n  System Modeling Language diagrams","summary":"  This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only.\n","authors":["Matthew Anderson Hendricks","Alice Cicirello"],"pdf_url":"https://arxiv.org/pdf/2507.06803v2.pdf","comment":"v2 - typos and imprecisions corrected"},{"id":"http://arxiv.org/abs/2505.17793v2","updated":"2025-07-15T10:57:33Z","published":"2025-05-23T12:11:03Z","title":"Compression Hacking: A Supplementary Perspective on Informatics\n  Properties of Language Models from Geometric Distortion","summary":"  Recently, the concept of ``compression as intelligence'' has provided a novel\ninformatics metric perspective for language models (LMs), emphasizing that\nhighly structured representations signify the intelligence level of LMs.\nHowever, from a geometric standpoint, the word representation space of highly\ncompressed LMs tends to degenerate into a highly anisotropic state, which\nhinders the LM's ability to comprehend instructions and directly impacts its\nperformance. We found this compression-anisotropy synchronicity is essentially\nthe ``Compression Hacking'' in LM representations, where noise-dominated\ndirections tend to create the illusion of high compression rates by sacrificing\nspatial uniformity. Based on this, we propose three refined compression metrics\nby incorporating geometric distortion analysis and integrate them into a\nself-evaluation pipeline. The refined metrics exhibit strong alignment with the\nLM's comprehensive capabilities, achieving Spearman correlation coefficients\nabove 0.9, significantly outperforming both the original compression and other\ninternal structure-based metrics. This confirms that compression hacking\nsubstantially enhances the informatics interpretation of LMs by incorporating\ngeometric distortion of representations.\n","authors":["Jianxiang Zang","Meiling Ning","Yongda Wei","Shihan Dou","Jiazheng Zhang","Nijia Mo","Binhong Li","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2505.17793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17755v3","updated":"2025-07-15T10:13:18Z","published":"2024-09-26T11:40:07Z","title":"SECURE: Semantics-aware Embodied Conversation under Unawareness for\n  Lifelong Robot Learning","summary":"  This paper addresses a challenging interactive task learning scenario we call\nrearrangement under unawareness: an agent must manipulate a rigid-body\nenvironment without knowing a key concept necessary for solving the task and\nmust learn about it during deployment. For example, the user may ask to \"put\nthe two granny smith apples inside the basket\", but the agent cannot correctly\nidentify which objects in the environment are \"granny smith\" as the agent has\nnot been exposed to such a concept before. We introduce SECURE, an interactive\ntask learning policy designed to tackle such scenarios. The unique feature of\nSECURE is its ability to enable agents to engage in semantic analysis when\nprocessing embodied conversations and making decisions. Through embodied\nconversation, a SECURE agent adjusts its deficient domain model by engaging in\ndialogue to identify and learn about previously unforeseen possibilities. The\nSECURE agent learns from the user's embodied corrective feedback when mistakes\nare made and strategically engages in dialogue to uncover useful information\nabout novel concepts relevant to the task. These capabilities enable the SECURE\nagent to generalize to new tasks with the acquired knowledge. We demonstrate in\nthe simulated Blocksworld and the real-world apple manipulation environments\nthat the SECURE agent, which solves such rearrangements under unawareness, is\nmore data-efficient than agents that do not engage in embodied conversation or\nsemantic analysis.\n","authors":["Rimvydas Rubavicius","Peter David Fagan","Alex Lascarides","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2409.17755v3.pdf","comment":"Published at 4th Conference on Lifelong Learning Agents (CoLLAs),\n  2025"},{"id":"http://arxiv.org/abs/2505.08054v2","updated":"2025-07-15T10:03:15Z","published":"2025-05-12T20:45:25Z","title":"FalseReject: A Resource for Improving Contextual Safety and Mitigating\n  Over-Refusals in LLMs via Structured Reasoning","summary":"  Safety alignment approaches in large language models (LLMs) often lead to the\nover-refusal of benign queries, significantly diminishing their utility in\nsensitive scenarios. To address this challenge, we introduce FalseReject, a\ncomprehensive resource containing 16k seemingly toxic queries accompanied by\nstructured responses across 44 safety-related categories. We propose a\ngraph-informed adversarial multi-agent interaction framework to generate\ndiverse and complex prompts, while structuring responses with explicit\nreasoning to aid models in accurately distinguishing safe from unsafe contexts.\nFalseReject includes training datasets tailored for both standard\ninstruction-tuned models and reasoning-oriented models, as well as a\nhuman-annotated benchmark test set. Our extensive benchmarking on 29\nstate-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges.\nEmpirical results demonstrate that supervised finetuning with FalseReject\nsubstantially reduces unnecessary refusals without compromising overall safety\nor general language capabilities.\n","authors":["Zhehao Zhang","Weijie Xu","Fanyou Wu","Chandan K. Reddy"],"pdf_url":"https://arxiv.org/pdf/2505.08054v2.pdf","comment":"Accepted at COLM 2025"},{"id":"http://arxiv.org/abs/2505.11441v4","updated":"2025-07-15T09:48:13Z","published":"2025-05-16T16:59:14Z","title":"Is Compression Really Linear with Code Intelligence?","summary":"  Understanding the relationship between data compression and the capabilities\nof Large Language Models (LLMs) is crucial, especially in specialized domains\nlike code intelligence. Prior work posited a linear relationship between\ncompression and general intelligence. However, it overlooked the multifaceted\nnature of code that encompasses diverse programming languages and tasks, and\nstruggled with fair evaluation of modern Code LLMs. We address this by\nevaluating a diverse array of open-source Code LLMs on comprehensive\nmulti-language, multi-task code benchmarks. To address the challenge of\nefficient and fair evaluation of pre-trained LLMs' code intelligence, we\nintroduce \\textit{Format Annealing}, a lightweight, transparent training\nmethodology designed to assess the intrinsic capabilities of these pre-trained\nmodels equitably. Compression efficacy, measured as bits-per-character (BPC),\nis determined using a novel, large-scale, and previously unseen code validation\nset derived from GitHub. Our empirical results reveal a fundamental logarithmic\nrelationship between measured code intelligence and BPC. This finding refines\nprior hypotheses of linearity, which we suggest are likely observations of the\nlogarithmic curve's tail under specific, limited conditions. Our work provides\na more nuanced understanding of compression's role in developing code\nintelligence and contributes a robust evaluation framework in the code domain.\n","authors":["Shijie Xuyang","Xianzhen Luo","Tianhao Cheng","Zheng Chu","Houyi Li","ziqi wang","Siming Huang","Qingfu Zhu","Qiufeng Wang","Xiangyu Zhang","Shuigeng Zhou","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2505.11441v4.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2504.01738v3","updated":"2025-07-15T09:34:32Z","published":"2025-04-02T13:50:20Z","title":"Style over Substance: Distilled Language Models Reason Via Stylistic\n  Replication","summary":"  Specialized reasoning language models (RLMs) have demonstrated that scaling\ntest-time computation through detailed reasoning traces significantly enhances\nperformance. Although these traces effectively facilitate knowledge\ndistillation into smaller, instruction-tuned models, the precise nature of\ntransferred reasoning remains unclear. In this study, we investigate to what\nextent distilled models internalize replicated stylistic patterns during\nreasoning. To this end, we systematically analyze reasoning traces, identifying\nstructural and lexical patterns that characterize successful reasoning. We then\nintroduce two new datasets -- a dataset of emergent reasoning traces and a\nsynthetic dataset explicitly constructed to replicate these stylistic patterns\n-- to precisely examine their influence on distilled models' reasoning\ncapabilities. We find that models trained on the synthetic traces achieve\ncomparable performance, indicating that distilled reasoning abilities rely\nsignificantly on surface-level patterns. Surprisingly, we observe an increase\nin performance even when the synthetic traces are altered to lead to the wrong\nanswer. Our findings highlight how stylistic patterns can be leveraged to\nefficiently enhance LM reasoning across diverse model families.\n","authors":["Philip Lippmann","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2504.01738v3.pdf","comment":"To appear at COLM 2025"},{"id":"http://arxiv.org/abs/2507.11128v1","updated":"2025-07-15T09:28:44Z","published":"2025-07-15T09:28:44Z","title":"What Should LLMs Forget? Quantifying Personal Data in LLMs for\n  Right-to-Be-Forgotten Requests","summary":"  Large Language Models (LLMs) can memorize and reveal personal information,\nraising concerns regarding compliance with the EU's GDPR, particularly the\nRight to Be Forgotten (RTBF). Existing machine unlearning methods assume the\ndata to forget is already known but do not address how to identify which\nindividual-fact associations are stored in the model. Privacy auditing\ntechniques typically operate at the population level or target a small set of\nidentifiers, limiting applicability to individual-level data inquiries. We\nintroduce WikiMem, a dataset of over 5,000 natural language canaries covering\n243 human-related properties from Wikidata, and a model-agnostic metric to\nquantify human-fact associations in LLMs. Our approach ranks ground-truth\nvalues against counterfactuals using calibrated negative log-likelihood across\nparaphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B\nparameters), showing that memorization correlates with subject web presence and\nmodel scale. We provide a foundation for identifying memorized personal data in\nLLMs at the individual level, enabling the dynamic construction of forget sets\nfor machine unlearning and RTBF requests.\n","authors":["Dimitri Staufer"],"pdf_url":"https://arxiv.org/pdf/2507.11128v1.pdf","comment":"16 pages, 3 figures. Accepted at the 7th Workshop on eXplainable\n  Knowledge Discovery in Data Mining (XKDD 2025), ECML PKDD 2025, Porto,\n  Portugal"},{"id":"http://arxiv.org/abs/2412.21033v2","updated":"2025-07-15T09:27:28Z","published":"2024-12-30T15:58:41Z","title":"Plancraft: an evaluation dataset for planning with LLM agents","summary":"  We present Plancraft, a multi-modal evaluation dataset for LLM agents.\nPlancraft has both a text-only and multi-modal interface, based on the\nMinecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and\nRetrieval Augmented Generation (RAG), as well as a handcrafted planner and\nOracle Retriever, to ablate the different components of a modern agent\narchitecture. To evaluate decision-making, Plancraft also includes a subset of\nexamples that are intentionally unsolvable, providing a realistic challenge\nthat requires the agent not only to complete tasks but also to decide whether\nthey are solvable at all. We benchmark both open-source and closed-source LLMs\nand compare their performance and efficiency to a handcrafted planner. Overall,\nwe find that LLMs and VLMs struggle with the planning problems that Plancraft\nintroduces, and offer suggestions on how to improve their capabilities.\n","authors":["Gautier Dagan","Frank Keller","Alex Lascarides"],"pdf_url":"https://arxiv.org/pdf/2412.21033v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.21596v2","updated":"2025-07-15T09:14:31Z","published":"2025-06-18T19:31:35Z","title":"Evaluating Multimodal Large Language Models on Educational Textbook\n  Question Answering","summary":"  Multimodal large language models (MLLMs) have shown success in\nvision-language tasks, but their ability to reason over complex educational\nmaterials remains largely untested. This work presents the first evaluation of\nstate-of-the-art MLLMs, including LLaVA-1.5 and LLaMA 3.2-Vision, on the\ntextbook question answering (TQA) task using the CK12-QA dataset. We introduce\na multimodal retrieval-augmented generation (RAG) pipeline to simulate\nreal-world learning by providing relevant lesson paragraphs and diagrams as\ncontext. Our zero-shot experiments reveal a critical trade-off: while retrieved\ncontext improves LLaVA's performance on text-based questions, it significantly\ndegrades the accuracy of the more powerful LLaMA 3.2-Vision on diagram-based\ntasks, dropping its validation accuracy from 74.07% to 25.93%. We term this\nstatistically significant phenomenon \"catastrophic context interference.\"\nFurthermore, fine-tuning highlights architectural differences: LLaMA\n3.2-Vision's performance improves to 71.16% on the test set, demonstrating its\ncapacity to learn multimodal integration, whereas LLaVA's performance declines,\nindicating challenges with generalization. Our results underscore the\nchallenges MLLMs face in modality prioritization and context integration,\nproviding a benchmark and pointing to key directions for developing more robust\nAI-driven educational tools.\n","authors":["Hessa A. Alawwad","Anas Zafar","Areej Alhothali","Usman Naseem","Ali Alkhathlan","Amani Jamal"],"pdf_url":"https://arxiv.org/pdf/2506.21596v2.pdf","comment":"8 Pages"},{"id":"http://arxiv.org/abs/2507.11114v1","updated":"2025-07-15T09:05:05Z","published":"2025-07-15T09:05:05Z","title":"MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal\n  Reasoning With Ensemble Vision Language Models","summary":"  We present a robust ensemble-based system for multilingual multimodal\nreasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach\nintegrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption\nrefinement and consistency checks, and Gemini 2.5 Pro as a reasoner which\nhandles final answer selection, all coordinated through carefully engineered\nfew-shot and zero-shot prompts. We conducted an extensive ablation study,\ntraining several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,\nMistral) on an English dataset and its multilingual augmented version.\nAdditionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for\ncomparison and found it to substantially outperform the trained models. Prompt\ndesign also proved critical: enforcing concise, language-normalized formats and\nprohibiting explanatory text boosted model accuracy on the English validation\nset from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)\nachieved first place overall in the multilingual track with 81.4% accuracy, and\nled 11 out of 13 individual language tracks, with top results such as 95.07%\nfor Croatian and 92.12% for Italian. These findings highlight that lightweight\nOCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual\naugmentation, can outperform heavier end-to-end models in high-stakes,\nmultilingual educational settings.\n","authors":["Seif Ahmed","Mohamed T. Younes","Abdelrahman Moustafa","Abdelrahman Allam","Hamza Moustafa"],"pdf_url":"https://arxiv.org/pdf/2507.11114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11112v1","updated":"2025-07-15T09:04:30Z","published":"2025-07-15T09:04:30Z","title":"Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs","summary":"  Recent studies have shown that Large Language Models (LLMs) are vulnerable to\ndata poisoning attacks, where malicious training examples embed hidden\nbehaviours triggered by specific input patterns. However, most existing works\nassume a phrase and focus on the attack's effectiveness, offering limited\nunderstanding of trigger mechanisms and how multiple triggers interact within\nthe model. In this paper, we present a framework for studying poisoning in\nLLMs. We show that multiple distinct backdoor triggers can coexist within a\nsingle model without interfering with each other, enabling adversaries to embed\nseveral triggers concurrently. Using multiple triggers with high embedding\nsimilarity, we demonstrate that poisoned triggers can achieve robust activation\neven when tokens are substituted or separated by long token spans. Our findings\nexpose a broader and more persistent vulnerability surface in LLMs. To mitigate\nthis threat, we propose a post hoc recovery method that selectively retrains\nspecific model components based on a layer-wise weight difference analysis. Our\nmethod effectively removes the trigger behaviour with minimal parameter\nupdates, presenting a practical and efficient defence against multi-trigger\npoisoning.\n","authors":["Sanhanat Sivapiromrat","Caiqi Zhang","Marco Basaldella","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2507.11112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.15075v3","updated":"2025-07-15T08:54:19Z","published":"2025-05-21T03:43:37Z","title":"Traveling Across Languages: Benchmarking Cross-Lingual Consistency in\n  Multimodal LLMs","summary":"  The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models.\n","authors":["Hao Wang","Pinzhi Huang","Jihan Yang","Saining Xie","Daisuke Kawahara"],"pdf_url":"https://arxiv.org/pdf/2505.15075v3.pdf","comment":"https://github.com/nlp-waseda/traveling-across-languages"},{"id":"http://arxiv.org/abs/2507.11097v1","updated":"2025-07-15T08:44:46Z","published":"2025-07-15T08:44:46Z","title":"The Devil behind the mask: An emergent safety vulnerability of Diffusion\n  LLMs","summary":"  Diffusion-based large language models (dLLMs) have recently emerged as a\npowerful alternative to autoregressive LLMs, offering faster inference and\ngreater interactivity via parallel decoding and bidirectional modeling.\nHowever, despite strong performance in code generation and text infilling, we\nidentify a fundamental safety concern: existing alignment mechanisms fail to\nsafeguard dLLMs against context-aware, masked-input adversarial prompts,\nexposing novel vulnerabilities. To this end, we present DIJA, the first\nsystematic study and jailbreak attack framework that exploits unique safety\nweaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial\ninterleaved mask-text prompts that exploit the text generation mechanisms of\ndLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional\nmodeling drives the model to produce contextually consistent outputs for masked\nspans, even when harmful, while parallel decoding limits model dynamic\nfiltering and rejection sampling of unsafe content. This causes standard\nalignment mechanisms to fail, enabling harmful completions in alignment-tuned\ndLLMs, even when harmful behaviors or unsafe instructions are directly exposed\nin the prompt. Through comprehensive experiments, we demonstrate that DIJA\nsignificantly outperforms existing jailbreak methods, exposing a previously\noverlooked threat surface in dLLM architectures. Notably, our method achieves\nup to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior\nbaseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and\nby 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of\nharmful content in the jailbreak prompt. Our findings underscore the urgent\nneed for rethinking safety alignment in this emerging class of language models.\nCode is available at https://github.com/ZichenWen1/DIJA.\n","authors":["Zichen Wen","Jiashu Qu","Dongrui Liu","Zhiyuan Liu","Ruixi Wu","Yicun Yang","Xiangqi Jin","Haoyun Xu","Xuyang Liu","Weijia Li","Chaochao Lu","Jing Shao","Conghui He","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.11097v1.pdf","comment":"21 pages, 9 figures, work in progress"},{"id":"http://arxiv.org/abs/2507.11086v1","updated":"2025-07-15T08:28:24Z","published":"2025-07-15T08:28:24Z","title":"Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border\n  Entity Identification","summary":"  The growing prevalence of cross-border financial activities in global markets\nhas underscored the necessity of accurately identifying and classifying foreign\nentities. This practice is essential within the Spanish financial system for\nensuring robust risk management, regulatory adherence, and the prevention of\nfinancial misconduct. This process involves a labor-intensive entity-matching\ntask, where entities need to be validated against available reference sources.\nChallenges arise from linguistic variations, special characters, outdated\nnames, and changes in legal forms, complicating traditional matching algorithms\nlike Jaccard, cosine, and Levenshtein distances. These methods struggle with\ncontextual nuances and semantic relationships, leading to mismatches. To\naddress these limitations, we explore Large Language Models (LLMs) as a\nflexible alternative. LLMs leverage extensive training to interpret context,\nhandle abbreviations, and adapt to legal transitions. We evaluate traditional\nmethods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft\nCopilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.\nResults show traditional methods achieve accuracies over 92% but suffer high\nfalse positive rates (20-40%). Interface-based LLMs outperform, achieving\naccuracies above 93%, F1 scores exceeding 96%, and lower false positives\n(40-80%).\n","authors":["Andres Azqueta-Gavaldón","Joaquin Ramos Cosgrove"],"pdf_url":"https://arxiv.org/pdf/2507.11086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11084v1","updated":"2025-07-15T08:26:58Z","published":"2025-07-15T08:26:58Z","title":"Social Media Sentiments Analysis on the July Revolution in Bangladesh: A\n  Hybrid Transformer Based Machine Learning Approach","summary":"  The July Revolution in Bangladesh marked a significant student-led mass\nuprising, uniting people across the nation to demand justice, accountability,\nand systemic reform. Social media platforms played a pivotal role in amplifying\npublic sentiment and shaping discourse during this historic mass uprising. In\nthis study, we present a hybrid transformer-based sentiment analysis framework\nto decode public opinion expressed in social media comments during and after\nthe revolution. We used a brand new dataset of 4,200 Bangla comments collected\nfrom social media. The framework employs advanced transformer-based feature\nextraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the\nproposed hybrid XMB-BERT, to capture nuanced patterns in textual data.\nPrinciple Component Analysis (PCA) were utilized for dimensionality reduction\nto enhance computational efficiency. We explored eleven traditional and\nadvanced machine learning classifiers for identifying sentiments. The proposed\nhybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of\n83.7% and outperform other model classifier combinations. This study\nunderscores the potential of machine learning techniques to analyze social\nsentiment in low-resource languages like Bangla.\n","authors":["Md. Sabbir Hossen","Md. Saiduzzaman","Pabon Shaha"],"pdf_url":"https://arxiv.org/pdf/2507.11084v1.pdf","comment":"This paper has been accepted and presented at the IEEE ECAI 2025. The\n  final version will be available in the IEEE Xplore Digital Library"},{"id":"http://arxiv.org/abs/2502.19130v3","updated":"2025-07-15T08:12:34Z","published":"2025-02-26T13:39:18Z","title":"Voting or Consensus? Decision-Making in Multi-Agent Debate","summary":"  Much of the success of multi-agent debates depends on carefully choosing the\nright parameters. The decision-making protocol stands out as it can highly\nimpact final model answers, depending on how decisions are reached. Systematic\ncomparison of decision protocols is difficult because many studies alter\nmultiple discussion parameters beyond the protocol. So far, it has been largely\nunknown how decision-making influences different tasks. This work\nsystematically evaluates the impact of seven decision protocols (e.g., majority\nvoting, unanimity consensus). We change only one variable at a time - the\ndecision protocol - to analyze how different methods affect the collaboration\nbetween agents and measure differences in knowledge and reasoning tasks. Our\nresults show that voting protocols improve performance by 13.2% in reasoning\ntasks and consensus protocols by 2.8% in knowledge tasks compared to other\ndecision protocols. Increasing the number of agents improves performance, while\nmore discussion rounds before voting reduce it. To improve decision-making by\nincreasing answer diversity, we propose two new methods, All-Agents Drafting\n(AAD) and Collective Improvement (CI). Our methods improve task performance by\nup to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the\nimportance of decision-making in multi-agent debates beyond scaling.\n","authors":["Lars Benedikt Kaesberg","Jonas Becker","Jan Philip Wahle","Terry Ruas","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2502.19130v3.pdf","comment":"Accepted at ACL2025 (Findings)"},{"id":"http://arxiv.org/abs/2507.11059v1","updated":"2025-07-15T07:52:33Z","published":"2025-07-15T07:52:33Z","title":"SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language\n  Models on Software Engineering Tasks","summary":"  The rapid advancement of Large Language Models (LLMs) in software engineering\nhas revealed critical limitations in existing benchmarks, particularly the\nwidely used SWE-bench dataset. Recent studies have uncovered severe data\ncontamination issues, e.g. SWE-bench reports 32.67% of successful patches\ninvolve direct solution leakage and 31.08\\% pass due to inadequate test cases.\nWe introduce SWE-MERA, a dynamic, continuously updated benchmark designed to\naddress these fundamental challenges through an automated collection of\nreal-world GitHub issues and rigorous quality validation. Our approach\nimplements a reliable pipeline that ensures quality while minimizing\ncontamination risks, resulting in approximately 10,000 potential tasks with 300\nsamples currently available. Evaluation using the Aider coding agent\ndemonstrates strong discriminative power in state-of-the-art models. We report\nperformance across a dozen recent LLMs evaluated on tasks collected between\nSeptember 2024 and June 2025.\n","authors":["Pavel Adamenko","Mikhail Ivanov","Aidar Valeev","Rodion Levichev","Pavel Zadorozhny","Ivan Lopatin","Dmitry Babayev","Alena Fenogenova","Valentin Malykh"],"pdf_url":"https://arxiv.org/pdf/2507.11059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01706v3","updated":"2025-07-15T07:52:08Z","published":"2025-02-03T13:30:44Z","title":"Comply: Learning Sentences with Complex Weights inspired by Fruit Fly\n  Olfaction","summary":"  Biologically inspired neural networks offer alternative avenues to model data\ndistributions. FlyVec is a recent example that draws inspiration from the fruit\nfly's olfactory circuit to tackle the task of learning word embeddings.\nSurprisingly, this model performs competitively even against deep learning\napproaches specifically designed to encode text, and it does so with the\nhighest degree of computational efficiency. We pose the question of whether\nthis performance can be improved further. For this, we introduce Comply. By\nincorporating positional information through complex weights, we enable a\nsingle-layer neural network to learn sequence representations. Our experiments\nshow that Comply not only supersedes FlyVec but also performs on par with\nsignificantly larger state-of-the-art models. We achieve this without\nadditional parameters. Comply yields sparse contextual representations of\nsentences that can be interpreted explicitly from the neuron weights.\n","authors":["Alexei Figueroa","Justus Westerhoff","Golzar Atefi","Dennis Fast","Benjamin Winter","Felix Alexander Gers","Alexander Löser","Wolfgang Nejdl"],"pdf_url":"https://arxiv.org/pdf/2502.01706v3.pdf","comment":"Accepted at NICE2025"},{"id":"http://arxiv.org/abs/2507.08606v2","updated":"2025-07-15T07:51:41Z","published":"2025-07-11T14:00:56Z","title":"DocPolarBERT: A Pre-trained Model for Document Understanding with\n  Relative Polar Coordinate Encoding of Layout Structures","summary":"  We introduce DocPolarBERT, a layout-aware BERT model for document\nunderstanding that eliminates the need for absolute 2D positional embeddings.\nWe extend self-attention to take into account text block positions in relative\npolar coordinate system rather than the Cartesian one. Despite being\npre-trained on a dataset more than six times smaller than the widely used\nIIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results\ndemonstrate that a carefully designed attention mechanism can compensate for\nreduced pre-training data, offering an efficient and effective alternative for\ndocument understanding.\n","authors":["Benno Uthayasooriyar","Antoine Ly","Franck Vermet","Caio Corro"],"pdf_url":"https://arxiv.org/pdf/2507.08606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05713v2","updated":"2025-07-15T07:36:34Z","published":"2025-07-08T06:52:43Z","title":"DRAGON: Dynamic RAG Benchmark On News","summary":"  Retrieval-Augmented Generation (RAG) is a widely adopted approach for\nimproving the factuality of large language models (LLMs) by incorporating\nexternal knowledge at inference time. Although there exist multiple RAG\nbenchmarks for English, evaluation resources for other languages, including\nRussian, remain scarce and static, failing to capture the dynamic nature of\nreal-world deployments. In this work, we present DRAGON (Dynamic RAG Benchmark\nOn News), the first dynamic benchmark for evaluating RAG systems in Russian on\na changing news corpora. DRAGON is built upon a regularly updated corpus of\nRussian news and public documents and supports comprehensive evaluation of both\nthe retriever and generator components. Question generation is performed\nautomatically with the use of Knowledge Graph constructed from the corpus and\nenables the extraction of four core question types aligned with distinct\nsubgraph patterns. We release a complete evaluation framework comprising the\npipeline for automatic question generation, evaluation scripts, which are\npotentially reusable for other languages and multilingual settings, and\nbenchmark data. We also launch a public leaderboard to encourage community\nparticipation and comparison.\n","authors":["Fedor Chernogorskii","Sergei Averkiev","Liliya Kudraleeva","Zaven Martirosian","Maria Tikhonova","Valentin Malykh","Alena Fenogenova"],"pdf_url":"https://arxiv.org/pdf/2507.05713v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11052v1","updated":"2025-07-15T07:32:16Z","published":"2025-07-15T07:32:16Z","title":"LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk\n  Prediction: A Clinical NLP","summary":"  Timely identification and accurate risk stratification of cardiovascular\ndisease (CVD) remain essential for reducing global mortality. While existing\nprediction models primarily leverage structured data, unstructured clinical\nnotes contain valuable early indicators. This study introduces a novel\nLLM-augmented clinical NLP pipeline that employs domain-adapted large language\nmodels for symptom extraction, contextual reasoning, and correlation from\nfree-text reports. Our approach integrates cardiovascular-specific fine-tuning,\nprompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III\nand CARDIO-NLP datasets demonstrate improved performance in precision, recall,\nF1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by\ncardiologists. Challenges such as contextual hallucination, which occurs when\nplausible information contracts with provided source, and temporal ambiguity,\nwhich is related with models struggling with chronological ordering of events\nare addressed using prompt engineering and hybrid rule-based verification. This\nwork underscores the potential of LLMs in clinical decision support systems\n(CDSS), advancing early warning systems and enhancing the translation of\npatient narratives into actionable risk assessments.\n","authors":["Haowei Yang","Ziyu Shen","Junli Shao","Luyao Men","Xinyue Han","Jing Dong"],"pdf_url":"https://arxiv.org/pdf/2507.11052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11049v1","updated":"2025-07-15T07:22:04Z","published":"2025-07-15T07:22:04Z","title":"Journalism-Guided Agentic In-Context Learning for News Stance Detection","summary":"  As online news consumption grows, personalized recommendation systems have\nbecome integral to digital journalism. However, these systems risk reinforcing\nfilter bubbles and political polarization by failing to incorporate diverse\nperspectives. Stance detection -- identifying a text's position on a target --\ncan help mitigate this by enabling viewpoint-aware recommendations and\ndata-driven analyses of media bias. Yet, existing stance detection research\nremains largely limited to short texts and high-resource languages. To address\nthese gaps, we introduce \\textsc{K-News-Stance}, the first Korean dataset for\narticle-level stance detection, comprising 2,000 news articles with\narticle-level and 19,650 segment-level stance annotations across 47 societal\nissues. We also propose \\textsc{JoA-ICL}, a \\textbf{Jo}urnalism-guided\n\\textbf{A}gentic \\textbf{I}n-\\textbf{C}ontext \\textbf{L}earning framework that\nemploys a language model agent to predict the stances of key structural\nsegments (e.g., leads, quotes), which are then aggregated to infer the overall\narticle stance. Experiments show that \\textsc{JoA-ICL} outperforms existing\nstance detection methods, highlighting the benefits of segment-level agency in\ncapturing the overall position of long-form news articles. Two case studies\nfurther demonstrate its broader utility in promoting viewpoint diversity in\nnews recommendations and uncovering patterns of media bias.\n","authors":["Dahyun Lee","Jonghyeon Choi","Jiyoung Han","Kunwoo Park"],"pdf_url":"https://arxiv.org/pdf/2507.11049v1.pdf","comment":"Preprint. 24 pages"},{"id":"http://arxiv.org/abs/2412.14959v2","updated":"2025-07-15T07:07:14Z","published":"2024-12-19T15:39:31Z","title":"Understanding the Dark Side of LLMs' Intrinsic Self-Correction","summary":"  Intrinsic self-correction was proposed to improve LLMs' responses via\nfeedback prompts solely based on their inherent capability. However, recent\nworks show that LLMs' intrinsic self-correction fails without oracle labels as\nfeedback prompts. In this paper, we aim to interpret LLMs' intrinsic\nself-correction for different tasks, especially for those failure cases. By\nincluding one simple task and three complex tasks with state-of-the-art (SOTA)\nLLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B,\nand 3.1-8B), we design three interpretation methods to reveal the dark side of\nLLMs' intrinsic self-correction. We identify intrinsic self-correction can (1)\ncause LLMs to waver both intermedia and final answers and lead to prompt bias\non simple factual questions; (2) introduce human-like cognitive bias on complex\ntasks. In light of our findings, we also provide two simple yet effective\nstrategies for alleviation: question repeating and supervised fine-tuning with\na few samples. We open-source our work at https://x-isc.info/.\n","authors":["Qingjie Zhang","Di Wang","Haoting Qian","Yiming Li","Tianwei Zhang","Minlie Huang","Ke Xu","Hewu Li","Yan Liu","Han Qiu"],"pdf_url":"https://arxiv.org/pdf/2412.14959v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14565v2","updated":"2025-07-15T06:30:11Z","published":"2025-02-20T13:50:02Z","title":"ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification","summary":"  Self-awareness, i.e., the ability to assess and correct one's own generation,\nis a fundamental aspect of human intelligence, making its replication in large\nlanguage models (LLMs) an important yet challenging task. Previous works tackle\nthis by employing extensive reinforcement learning or rather relying on large\nexternal verifiers. In this work, we propose Refine via Intrinsic\nSelf-Verification (ReVISE), an efficient and effective framework that enables\nLLMs to self-correct their outputs through self-verification. The core idea of\nReVISE is to enable LLMs to verify their reasoning processes and continually\nrethink reasoning trajectories based on its verification. We introduce a\nstructured curriculum based upon online preference learning to implement this\nefficiently. Specifically, as ReVISE involves two challenging tasks (i.e.,\nself-verification and reasoning correction), we tackle each task sequentially\nusing curriculum learning, collecting both failed and successful reasoning\npaths to construct preference pairs for efficient training. During inference,\nour approach enjoys natural test-time scaling by integrating self-verification\nand correction capabilities, further enhanced by our proposed confidence-aware\ndecoding mechanism. Our experiments on various reasoning tasks demonstrate that\nReVISE achieves efficient self-correction and significantly improves reasoning\nperformance.\n","authors":["Hyunseok Lee","Seunghyuk Oh","Jaehyung Kim","Jinwoo Shin","Jihoon Tack"],"pdf_url":"https://arxiv.org/pdf/2502.14565v2.pdf","comment":"Published as conference proceeding for ICML 2025. First two authors\n  contributed equally"},{"id":"http://arxiv.org/abs/2507.11017v1","updated":"2025-07-15T06:18:46Z","published":"2025-07-15T06:18:46Z","title":"First-Order Error Matters: Accurate Compensation for Quantized Large\n  Language Models","summary":"  Post-training quantization (PTQ) offers an efficient approach to compressing\nlarge language models (LLMs), significantly reducing memory access and\ncomputational costs. Existing compensation-based weight calibration methods\noften rely on a second-order Taylor expansion to model quantization error,\nunder the assumption that the first-order term is negligible in well-trained\nfull-precision models. However, we reveal that the progressive compensation\nprocess introduces accumulated first-order deviations between latent weights\nand their full-precision counterparts, making this assumption fundamentally\nflawed. To address this, we propose FOEM, a novel PTQ method that explicitly\nincorporates first-order gradient terms to improve quantization error\ncompensation. FOEM approximates gradients by directly computing the difference\nbetween latent and full-precision weights, avoiding the high cost and limited\ngeneralization of backpropagation-based gradient computation. This approach\nintroduces minimal additional computational overhead. Moreover, FOEM leverages\nprecomputed Cholesky factors to efficiently recover the inverse of Hessian\nsubmatrices in real time. Extensive experiments across a wide range of models\nand benchmarks demonstrate that FOEM consistently outperforms the classical\nGPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of\nLlama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from\n51.7% to 74.9%, approaching the full-precision performance of 78.6%.\nFurthermore, FOEM can be seamlessly integrated with advanced techniques such as\nGPTAQ and SpinQuant, yielding additional improvements under the challenging\nW4A4KV4 setting, and further narrowing the accuracy gap with full-precision\nbaselines beyond what current state-of-the-art methods achieve. The code is\navailable at https://github.com/Xingyu-Zheng/FOEM.\n","authors":["Xingyu Zheng","Haotong Qin","Yuye Li","Jiakai Wang","Jinyang Guo","Michele Magno","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2507.11017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10541v2","updated":"2025-07-15T06:16:53Z","published":"2025-07-14T17:58:47Z","title":"REST: Stress Testing Large Reasoning Models by Asking Multiple Problems\n  at Once","summary":"  Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly creation of new\nquestions with large human efforts, (2) failure to evaluate models under\nmulti-context pressure, a key requirement for real-world deployment. To bridge\nthis gap, we present REST (Reasoning Evaluation through Simultaneous Testing),\na stress-testing framework that exposes LRMs to multiple problems\nsimultaneously. Beyond basic reasoning, REST evaluates several under-tested\ncapabilities: contextual priority allocation, cross-problem interference\nresistance, and dynamic cognitive load management. Our evaluation reveals\nseveral striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1\nexhibit substantial performance degradation under stress testing. Crucially,\nREST demonstrates stronger discriminative power than existing benchmarks,\nrevealing pronounced performance differences among models that exhibit similar,\nnear-ceiling performance under single-question evaluations. Some key insights\nemerge from our analysis: (1) the \"overthinking trap\" is a critical factor\ncontributing to the performance degradation; (2) the models trained with\n\"long2short\" technique preserve more accuracy of their single-problem\nperformance under REST, outperforming standard-trained counterparts. These\nresults establish REST as a cost-efficient, future-proof evaluation paradigm\nthat better reflects real-world reasoning demands while reducing reliance on\ncontinuous human annotation. Code and results are available at\nhttps://opendatalab.github.io/REST.\n","authors":["Zhuoshi Pan","Qizhi Pei","Yu Li","Qiyao Sun","Zinan Tang","H. Vicky Zhao","Conghui He","Lijun Wu"],"pdf_url":"https://arxiv.org/pdf/2507.10541v2.pdf","comment":"REST (Reasoning Evaluation through Simultaneous Testing), a\n  stress-testing framework that concurrently exposes LRMs to multiple problems\n  simultaneously"},{"id":"http://arxiv.org/abs/2505.05464v2","updated":"2025-07-15T06:09:44Z","published":"2025-05-08T17:56:23Z","title":"Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging","summary":"  Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation.\n","authors":["Shiqi Chen","Jinghan Zhang","Tongyao Zhu","Wei Liu","Siyang Gao","Miao Xiong","Manling Li","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2505.05464v2.pdf","comment":"ICML 2025. Camera-ready version updated. Our code is publicly\n  available at https://github.com/shiqichen17/VLM_Merging"},{"id":"http://arxiv.org/abs/2507.11004v1","updated":"2025-07-15T05:42:50Z","published":"2025-07-15T05:42:50Z","title":"Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification","summary":"  This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task\nat the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the\nbest-performing open-source model from the previous year's challenge. It\nimproves evidence quality through document summarization and answer\nreformulation, optimizes veracity prediction via post-training quantization\nunder computational constraints, and enhances overall system performance by\nintegrating updated language model (LM) backbones. HerO 2 ranked second on the\nleaderboard while achieving the shortest runtime among the top three systems,\ndemonstrating both high efficiency and strong potential for real-world fact\nverification. The code is available at https://github.com/ssu-humane/HerO2.\n","authors":["Yejun Yoon","Jaeyoon Jung","Seunghyun Yoon","Kunwoo Park"],"pdf_url":"https://arxiv.org/pdf/2507.11004v1.pdf","comment":"ACL 2025 Workshop (FEVER)"},{"id":"http://arxiv.org/abs/2507.10996v1","updated":"2025-07-15T05:30:32Z","published":"2025-07-15T05:30:32Z","title":"Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism\n  Detection","summary":"  This paper presents our approach to EXIST 2025 Task 1, addressing text-based\nsexism detection in English and Spanish tweets through hierarchical Low-Rank\nAdaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter\nrouting that explicitly models label dependencies across three hierarchically\nstructured subtasks: binary sexism identification, source intention detection,\nand multilabel sexism categorization. Unlike conventional LoRA applications\nthat target only attention layers, we apply adaptation to all linear\ntransformations, enhancing the model's capacity to capture task-specific\npatterns. In contrast to complex data processing and ensemble approaches, we\nshow that straightforward parameter-efficient fine-tuning achieves strong\nperformance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each\nsubtask using unified multilingual training that leverages Llama 3.1's native\nbilingual capabilities. The method requires minimal preprocessing and uses\nstandard supervised learning. Our multilingual training strategy eliminates the\nneed for separate language-specific models, achieving 1.7-2.4\\% F1 improvements\nthrough cross-lingual transfer. With only 1.67\\% trainable parameters compared\nto full fine-tuning, our approach reduces training time by 75\\% and model\nstorage by 98\\%, while achieving competitive performance across all subtasks\n(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,\n0.6519 for multilabel categorization).\n","authors":["Lin Tian","Johanne R. Trippas","Marian-Andrei Rizoiu"],"pdf_url":"https://arxiv.org/pdf/2507.10996v1.pdf","comment":"12 pages, 5 tables, CLEF 2025"},{"id":"http://arxiv.org/abs/2504.12355v3","updated":"2025-07-15T05:21:49Z","published":"2025-04-16T02:33:19Z","title":"Leveraging Large Language Models for Multi-Class and Multi-Label\n  Detection of Drug Use and Overdose Symptoms on Social Media","summary":"  Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies.\n","authors":["Muhammad Ahmad","Fida Ullah","Muhammad Usman","Umyh Habiba","ldar Batyrshin","Grigori Sidorov"],"pdf_url":"https://arxiv.org/pdf/2504.12355v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23022v3","updated":"2025-07-15T05:15:18Z","published":"2024-10-30T13:52:43Z","title":"Online Intrinsic Rewards for Decision Making Agents from Large Language\n  Model Feedback","summary":"  Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples, due to requiring LLM annotations for each observation, or\nthey require a diverse offline dataset, which may not exist or be impossible to\ncollect. In this work, we address these limitations through a combination of\nalgorithmic and systems-level contributions. We propose ONI, a distributed\narchitecture that simultaneously learns an RL policy and an intrinsic reward\nfunction using LLM feedback. Our approach annotates the agent's collected\nexperience via an asynchronous LLM server, which is then distilled into an\nintrinsic reward model. We explore a range of algorithmic choices for reward\nmodeling with varying complexity, including hashing, classification, and\nranking models. Our approach achieves state-of-the-art performance across a\nrange of challenging tasks from the NetHack Learning Environment, while\nremoving the need for large offline datasets required by prior work. We make\nour code available at https://github.com/facebookresearch/oni .\n","authors":["Qinqing Zheng","Mikael Henaff","Amy Zhang","Aditya Grover","Brandon Amos"],"pdf_url":"https://arxiv.org/pdf/2410.23022v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05763v2","updated":"2025-07-15T05:12:11Z","published":"2025-05-09T03:53:10Z","title":"BMDetect: A Multimodal Deep Learning Framework for Comprehensive\n  Biomedical Misconduct Detection","summary":"  Academic misconduct detection in biomedical research remains challenging due\nto algorithmic narrowness in existing methods and fragmented analytical\npipelines. We present BMDetect, a multimodal deep learning framework that\nintegrates journal metadata (SJR, institutional data), semantic embeddings\n(PubMedBERT), and GPT-4o-mined textual attributes (methodological statistics,\ndata anomalies) for holistic manuscript evaluation. Key innovations include:\n(1) multimodal fusion of domain-specific features to reduce detection bias; (2)\nquantitative evaluation of feature importance, identifying journal authority\nmetrics (e.g., SJR-index) and textual anomalies (e.g., statistical outliers) as\ndominant predictors; and (3) the BioMCD dataset, a large-scale benchmark with\n13,160 retracted articles and 53,411 controls. BMDetect achieves 74.33% AUC,\noutperforming single-modality baselines by 8.6%, and demonstrates\ntransferability across biomedical subfields. This work advances scalable,\ninterpretable tools for safeguarding research integrity.\n","authors":["Yize Zhou","Jie Zhang","Meijie Wang","Lun Yu"],"pdf_url":"https://arxiv.org/pdf/2505.05763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08898v2","updated":"2025-07-15T05:06:59Z","published":"2025-07-11T05:15:35Z","title":"SEALGuard: Safeguarding the Multilingual Conversations in Southeast\n  Asian Languages for LLM Software Systems","summary":"  Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail.\n","authors":["Wenliang Shan","Michael Fu","Rui Yang","Chakkrit Tantithamthavorn"],"pdf_url":"https://arxiv.org/pdf/2507.08898v2.pdf","comment":"Under Review at Information and Software Technology"},{"id":"http://arxiv.org/abs/2507.10972v1","updated":"2025-07-15T04:31:52Z","published":"2025-07-15T04:31:52Z","title":"Teach Me Sign: Stepwise Prompting LLM for Sign Language Production","summary":"  Large language models, with their strong reasoning ability and rich\nknowledge, have brought revolution to many tasks of AI, but their impact on\nsign language generation remains limited due to its complexity and unique\nrules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign\nlanguage as another natural language. By fine-tuning an LLM, we enable it to\nlearn the correspondence between text and sign language, and facilitate\ngeneration. Considering the differences between sign and spoken language, we\nemploy a stepwise prompting strategy to extract the inherent sign language\nknowledge within the LLM, thereby supporting the learning and generation\nprocess. Experimental results on How2Sign and Phoenix14T datasets demonstrate\nthat our approach effectively leverages both the sign language knowledge and\nreasoning capabilities of LLM to align the different distribution and\ngrammatical rules between sign and spoken language.\n","authors":["Zhaoyi An","Rei Kawakami"],"pdf_url":"https://arxiv.org/pdf/2507.10972v1.pdf","comment":"Accepted by IEEE ICIP 2025"},{"id":"http://arxiv.org/abs/2411.15821v4","updated":"2025-07-15T03:41:57Z","published":"2024-11-24T12:51:50Z","title":"Is Training Data Quality or Quantity More Impactful to Small Language\n  Model Performance?","summary":"  This study investigates the relative impact of training data quality versus\nquantity on the performance of small language models (SLMs), utilizing the\nTinyStories dataset for empirical analysis. Analysis of dataset variations with\nrespect to size (25% and 50% of the original size) and duplication (controlled\nrates of 25%, 50%, 75%, and 100%) were performed. Model performance was\nevaluated based on the validation loss, accuracy, and perplexity metrics.\nResults indicate training data quality plays a more significant role in the\noverall performance of SLMs, especially given scale of this experiment. Minimal\nduplication positively impacted model accuracy (+0.87% increase in accuracy at\n25% duplication) without significantly increasing perplexity (+0.52% increase\ngoing from 0% to 25% duplication) but excessive duplication led to pronounced\nperformance degradation (-40% drop in accuracy at 100% duplication). The\nimplications of this exploration extend beyond just model performance; training\nlarge-scale models imposes significant financial and computational burdens,\nwhich can be prohibitive for organizations, individuals, and the public at\nlarge, especially in developing countries. Additionally, the energy consumption\nassociated with large-scale training raises environmental concerns.\nUnderstanding the relative importance of data quality versus quantity could\ndemocratize AI technology, making advanced models more accessible and\nsustainable for all.\n","authors":["Aryan Sajith","Krishna Chaitanya Rao Kathala"],"pdf_url":"https://arxiv.org/pdf/2411.15821v4.pdf","comment":"14 pages, 5 tables, 4 figures | Accepted at International Conference\n  on Neural Computing for Advanced Applications 2025, Conference info:\n  https://aaci.org.hk/ncaa2025"},{"id":"http://arxiv.org/abs/2507.10958v1","updated":"2025-07-15T03:40:46Z","published":"2025-07-15T03:40:46Z","title":"DS@GT at eRisk 2025: From prompts to predictions, benchmarking early\n  depression detection with conversational agent based assessments and temporal\n  attention models","summary":"  This Working Note summarizes the participation of the DS@GT team in two eRisk\n2025 challenges. For the Pilot Task on conversational depression detection with\nlarge language-models (LLMs), we adopted a prompt-engineering strategy in which\ndiverse LLMs conducted BDI-II-based assessments and produced structured JSON\noutputs. Because ground-truth labels were unavailable, we evaluated cross-model\nagreement and internal consistency. Our prompt design methodology aligned model\noutputs with BDI-II criteria and enabled the analysis of conversational cues\nthat influenced the prediction of symptoms. Our best submission, second on the\nofficial leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.\n","authors":["Anthony Miyaguchi","David Guecha","Yuwen Chiu","Sidharth Gaur"],"pdf_url":"https://arxiv.org/pdf/2507.10958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10957v1","updated":"2025-07-15T03:40:21Z","published":"2025-07-15T03:40:21Z","title":"Modeling Understanding of Story-Based Analogies Using Large Language\n  Models","summary":"  Recent advancements in Large Language Models (LLMs) have brought them closer\nto matching human cognition across a variety of tasks. How well do these models\nalign with human performance in detecting and mapping analogies? Prior research\nhas shown that LLMs can extract similarities from analogy problems but lack\nrobust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the\ncurrent study focused on a story-based analogical mapping task and conducted a\nfine-grained evaluation of LLM reasoning abilities compared to human\nperformance. First, it explored the semantic representation of analogies in\nLLMs, using sentence embeddings to assess whether they capture the similarity\nbetween the source and target texts of an analogy, and the dissimilarity\nbetween the source and distractor texts. Second, it investigated the\neffectiveness of explicitly prompting LLMs to explain analogies. Throughout, we\nexamine whether LLMs exhibit similar performance profiles to those observed in\nhumans by evaluating their reasoning at the level of individual analogies, and\nnot just at the level of overall accuracy (as prior studies have done). Our\nexperiments include evaluating the impact of model size (8B vs. 70B parameters)\nand performance variation across state-of-the-art model architectures such as\nGPT-4 and LLaMA3. This work advances our understanding of the analogical\nreasoning abilities of LLMs and their potential as models of human reasoning.\n","authors":["Kalit Inani","Keshav Kabra","Vijay Marupudi","Sashank Varma"],"pdf_url":"https://arxiv.org/pdf/2507.10957v1.pdf","comment":"To appear at CogSci 2025"},{"id":"http://arxiv.org/abs/2507.09279v2","updated":"2025-07-15T03:09:40Z","published":"2025-07-12T13:21:10Z","title":"Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for\n  Clinically-Aligned Confidence Calibration in Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/prompt4trust.\n","authors":["Anita Kriz","Elizabeth Laura Janes","Xing Shen","Tal Arbel"],"pdf_url":"https://arxiv.org/pdf/2507.09279v2.pdf","comment":"Accepted to ICCV 2025 Workshop CVAMD"},{"id":"http://arxiv.org/abs/2504.01550v3","updated":"2025-07-15T02:52:56Z","published":"2025-04-02T09:47:01Z","title":"Representation Bending for Large Language Model Safety","summary":"  Large Language Models (LLMs) have emerged as powerful tools, but their\ninherent safety risks - ranging from harmful content generation to broader\nsocietal harms - pose significant challenges. These risks can be amplified by\nthe recent adversarial attacks, fine-tuning vulnerabilities, and the increasing\ndeployment of LLMs in high-stakes environments. Existing safety-enhancing\ntechniques, such as fine-tuning with human feedback or adversarial training,\nare still vulnerable as they address specific threats and often fail to\ngeneralize across unseen attacks, or require manual system-level defenses. This\npaper introduces RepBend, a novel approach that fundamentally disrupts the\nrepresentations underlying harmful behaviors in LLMs, offering a scalable\nsolution to enhance (potentially inherent) safety. RepBend brings the idea of\nactivation steering - simple vector arithmetic for steering model's behavior\nduring inference - to loss-based fine-tuning. Through extensive evaluation,\nRepBend achieves state-of-the-art performance, outperforming prior methods such\nas Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success\nrates across diverse jailbreak benchmarks, all with negligible reduction in\nmodel usability and general capabilities.\n","authors":["Ashkan Yousefpour","Taeheon Kim","Ryan S. Kwon","Seungbeen Lee","Wonje Jeung","Seungju Han","Alvin Wan","Harrison Ngan","Youngjae Yu","Jonghyun Choi"],"pdf_url":"https://arxiv.org/pdf/2504.01550v3.pdf","comment":"Accepted to ACL 2025 (main)"},{"id":"http://arxiv.org/abs/2407.09975v2","updated":"2025-07-15T02:39:33Z","published":"2024-04-25T15:39:22Z","title":"The GPT Surprise: Offering Large Language Model Chat in a Massive Coding\n  Class Reduced Engagement but Increased Adopters Exam Performances","summary":"  Large language models (LLMs) are quickly being adopted in a wide range of\nlearning experiences, especially via ubiquitous and broadly accessible chat\ninterfaces like ChatGPT and Copilot. This type of interface is readily\navailable to students and teachers around the world, yet relatively little\nresearch has been done to assess the impact of such generic tools on student\nlearning. Coding education is an interesting test case, both because LLMs have\nstrong performance on coding tasks, and because LLM-powered support tools are\nrapidly becoming part of the workflow of professional software engineers. To\nhelp understand the impact of generic LLM use on coding education, we conducted\na large-scale randomized control trial with 5,831 students from 146 countries\nin an online coding class in which we provided some students with access to a\nchat interface with GPT-4. We estimate positive benefits on exam performance\nfor adopters, the students who used the tool, but over all students, the\nadvertisement of GPT-4 led to a significant average decrease in exam\nparticipation. We observe similar decreases in other forms of course\nengagement. However, this decrease is modulated by the student's country of\norigin. Offering access to LLMs to students from low human development index\ncountries increased their exam participation rate on average. Our results\nsuggest there may be promising benefits to using LLMs in an introductory coding\nclass, but also potential harms for engagement, which makes their longer term\nimpact on student success unclear. Our work highlights the need for additional\ninvestigations to help understand the potential impact of future adoption and\nintegration of LLMs into classrooms.\n","authors":["Allen Nie","Yash Chandak","Miroslav Suzara","Ali Malik","Juliette Woodrow","Matt Peng","Mehran Sahami","Emma Brunskill","Chris Piech"],"pdf_url":"https://arxiv.org/pdf/2407.09975v2.pdf","comment":"32 pages. Published at L@S 2025"},{"id":"http://arxiv.org/abs/2507.10920v1","updated":"2025-07-15T02:26:47Z","published":"2025-07-15T02:26:47Z","title":"HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via\n  Hanja-Augmented Pre-Training","summary":"  Large language models (LLMs) often show poor performance in low-resource\nlanguages like Korean, partly due to unique linguistic challenges such as\nhomophonous Sino-Korean words that are indistinguishable in Hangul script. To\naddress this semantic ambiguity, we propose HanjaBridge, a novel\nmeaning-injection technique integrated into a continual pre-training (CPT)\nframework. Instead of deterministically mapping a word to a single Hanja\n(Chinese character), HanjaBridge presents the model with all possible Hanja\ncandidates for a given homograph, encouraging the model to learn contextual\ndisambiguation. This process is paired with token-level knowledge distillation\nto prevent catastrophic forgetting. Experimental results show that HanjaBridge\nsignificantly improves Korean language understanding, achieving a 21\\% relative\nimprovement on the KoBALT benchmark. Notably, by reinforcing semantic alignment\nbetween Korean and Chinese through shared Hanja, we observe a strong positive\ncross-lingual transfer. Furthermore, these gains persist even when Hanja\naugmentation is omitted at inference time, ensuring practical efficiency with\nno additional run-time cost.\n","authors":["Seungho Choi"],"pdf_url":"https://arxiv.org/pdf/2507.10920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13444v4","updated":"2025-07-15T02:25:49Z","published":"2024-01-24T13:36:50Z","title":"Fine-grained Stateful Knowledge Exploration: Effective and Efficient\n  Graph Retrieval with Large Language Models","summary":"  Large Language Models (LLMs) have shown impressive capabilities, yet updating\ntheir knowledge remains a significant challenge, often leading to outdated or\ninaccurate responses. A proposed solution is the integration of external\nknowledge bases, such as knowledge graphs, with LLMs. Most existing methods use\na paradigm that treats the whole question as the objective, with relevant\nknowledge being incrementally retrieved from the knowledge graph. However, this\nparadigm often leads to a granularity mismatch between the target question and\nthe retrieved entities and relations. As a result, the information in the\nquestion cannot precisely correspond to the retrieved knowledge. This may cause\nredundant exploration or omission of vital knowledge, thereby leading to\nenhanced computational consumption and reduced retrieval accuracy. To address\nthe limitations of coarse-grained knowledge exploration, we propose FiSKE, a\nnovel paradigm for Fine-grained Stateful Knowledge Exploration. FiSKE first\ndecomposes questions into fine-grained clues, then employs an adaptive mapping\nstrategy during knowledge exploration process to resolve ambiguity in\nclue-to-graph mappings. This strategy dynamically infers contextual\ncorrespondences while maintaining a stateful record of the mappings. A\nclue-driven termination mechanism ensures rigorous augmentation--leveraging\nfully mapped paths for LLMs while reverting to chain-of-thought reasoning when\nnecessary. Our approach balances precision and efficiency. Experiments on\nmultiple datasets revealed that our paradigm surpasses current advanced methods\nin knowledge retrieval while significantly reducing the average number of LLM\ninvocations.\n","authors":["Dehao Tao","Congqi Wang","Feng Huang","Junhao Chen","Yongfeng Huang","Minghu Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.13444v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10918v1","updated":"2025-07-15T02:19:52Z","published":"2025-07-15T02:19:52Z","title":"How Stylistic Similarity Shapes Preferences in Dialogue Dataset with\n  User and Third Party Evaluations","summary":"  Recent advancements in dialogue generation have broadened the scope of\nhuman-bot interactions, enabling not only contextually appropriate responses\nbut also the analysis of human affect and sensitivity. While prior work has\nsuggested that stylistic similarity between user and system may enhance user\nimpressions, the distinction between subjective and objective similarity is\noften overlooked. To investigate this issue, we introduce a novel dataset that\nincludes users' preferences, subjective stylistic similarity based on users'\nown perceptions, and objective stylistic similarity annotated by third party\nevaluators in open-domain dialogue settings. Analysis using the constructed\ndataset reveals a strong positive correlation between subjective stylistic\nsimilarity and user preference. Furthermore, our analysis suggests an important\nfinding: users' subjective stylistic similarity differs from third party\nobjective similarity. This underscores the importance of distinguishing between\nsubjective and objective evaluations and understanding the distinct aspects\neach captures when analyzing the relationship between stylistic similarity and\nuser preferences. The dataset presented in this paper is available online.\n","authors":["Ikumi Numaya","Shoji Moriya","Shiki Sato","Reina Akama","Jun Suzuki"],"pdf_url":"https://arxiv.org/pdf/2507.10918v1.pdf","comment":"Accepted to SIGDIAL 2025 (long)"},{"id":"http://arxiv.org/abs/2507.10903v1","updated":"2025-07-15T01:42:44Z","published":"2025-07-15T01:42:44Z","title":"LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided\n  DRL for Optimized SFC Provisioning","summary":"  Effective management of Service Function Chains (SFCs) and optimal Virtual\nNetwork Function (VNF) placement are critical challenges in modern\nSoftware-Defined Networking (SDN) and Network Function Virtualization (NFV)\nenvironments. Although Deep Reinforcement Learning (DRL) is widely adopted for\ndynamic network decision-making, its inherent dependency on structured data and\nfixed action rules often limits adaptability and responsiveness, particularly\nunder unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a\nnovel approach combining Lightweight Language Model (LiLM) with Relational\nDatabase (RDB) to answer network state queries to guide DRL model for efficient\nSFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and\nAuto-Regressive Transformers (BART) and the Fine-tuned Language Net T5\n(FLAN-T5), to interpret network data and support diverse query types related to\nSFC demands, data center resources, and VNF availability. Results demonstrate\nthat FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to\n0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time\n(2h 2min compared to 2h 38min). Moreover, when compared to the large language\nmodel SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting\nprocessing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).\n","authors":["Parisa Fard Moshiri","Xinyu Zhu","Poonam Lohan","Burak Kantarci","Emil Janulewicz"],"pdf_url":"https://arxiv.org/pdf/2507.10903v1.pdf","comment":"9 pages, 6 figures, Accepted to IEEE 16th International Conference on\n  Network of the Future (NoF) 2025"},{"id":"http://arxiv.org/abs/2505.06110v2","updated":"2025-07-15T01:30:36Z","published":"2025-05-09T15:10:57Z","title":"Multimodal Sentiment Analysis on CMU-MOSEI Dataset using\n  Transformer-based Models","summary":"  This project performs multimodal sentiment analysis using the CMU-MOSEI\ndataset, using transformer-based models with early fusion to integrate text,\naudio, and visual modalities. We employ BERT-based encoders for each modality,\nextracting embeddings that are concatenated before classification. The model\nachieves strong performance, with 97.87% 7-class accuracy and a 0.9682 F1-score\non the test set, demonstrating the effectiveness of early fusion in capturing\ncross-modal interactions. The training utilized Adam optimization (lr=1e-4),\ndropout (0.3), and early stopping to ensure generalization and robustness.\nResults highlight the superiority of transformer architectures in modeling\nmultimodal sentiment, with a low MAE (0.1060) indicating precise sentiment\nintensity prediction. Future work may compare fusion strategies or enhance\ninterpretability. This approach utilizes multimodal learning by effectively\ncombining linguistic, acoustic, and visual cues for sentiment analysis.\n","authors":["Jugal Gajjar","Kaustik Ranaware"],"pdf_url":"https://arxiv.org/pdf/2505.06110v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.10894v1","updated":"2025-07-15T01:20:22Z","published":"2025-07-15T01:20:22Z","title":"NavComposer: Composing Language Instructions for Navigation Trajectories\n  through Action-Scene-Object Modularization","summary":"  Language-guided navigation is a cornerstone of embodied AI, enabling agents\nto interpret language instructions and navigate complex environments. However,\nexpert-provided instructions are limited in quantity, while synthesized\nannotations often lack quality, making them insufficient for large-scale\nresearch. To address this, we propose NavComposer, a novel framework for\nautomatically generating high-quality navigation instructions. NavComposer\nexplicitly decomposes semantic entities such as actions, scenes, and objects,\nand recomposes them into natural language instructions. Its modular\narchitecture allows flexible integration of state-of-the-art techniques, while\nthe explicit use of semantic entities enhances both the richness and accuracy\nof instructions. Moreover, it operates in a data-agnostic manner, supporting\nadaptation to diverse navigation trajectories without domain-specific training.\nComplementing NavComposer, we introduce NavInstrCritic, a comprehensive\nannotation-free evaluation system that assesses navigation instructions on\nthree dimensions: contrastive matching, semantic consistency, and linguistic\ndiversity. NavInstrCritic provides a holistic evaluation of instruction\nquality, addressing limitations of traditional metrics that rely heavily on\nexpert annotations. By decoupling instruction generation and evaluation from\nspecific navigation agents, our method enables more scalable and generalizable\nresearch. Extensive experiments provide direct and practical evidence for the\neffectiveness of our method.\n","authors":["Zongtao He","Liuyi Wang","Lu Chen","Chengju Liu","Qijun Chen"],"pdf_url":"https://arxiv.org/pdf/2507.10894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01100v2","updated":"2025-07-15T01:14:25Z","published":"2025-02-03T06:44:49Z","title":"ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning","summary":"  We investigate the logical reasoning capabilities of large language models\n(LLMs) and their scalability in complex non-monotonic reasoning. To this end,\nwe introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM\nreasoning performance on logic grid puzzles derived from constraint\nsatisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with\ncontrollable and quantifiable complexity, facilitating a systematic study of\nthe scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By\nencompassing a broad range of search space complexities and diverse logical\nconstraints, ZebraLogic provides a structured environment to evaluate reasoning\nunder increasing difficulty.\n  Our results reveal a significant decline in accuracy as problem complexity\ngrows -- a phenomenon we term the curse of complexity. This limitation persists\neven with larger models and increased inference-time computation, suggesting\ninherent constraints in current LLM reasoning capabilities. Additionally, we\nexplore strategies to enhance logical reasoning, including Best-of-N sampling,\nbacktracking mechanisms, and self-verification prompts. Our findings offer\ncritical insights into the scalability of LLM reasoning, highlight fundamental\nlimitations, and outline potential directions for improvement.\n","authors":["Bill Yuchen Lin","Ronan Le Bras","Kyle Richardson","Ashish Sabharwal","Radha Poovendran","Peter Clark","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2502.01100v2.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2507.10880v1","updated":"2025-07-15T00:46:01Z","published":"2025-07-15T00:46:01Z","title":"Domain-Adaptive Small Language Models for Structured Tax Code Prediction","summary":"  Every day, multinational firms process thousands of transactions, each of\nwhich must adhere to tax regulations that vary by jurisdiction and are often\nnuanced. The determination of product and service tax codes, such as HSN or SAC\nis a major use case in Tax compliance. An accurate determination of such codes\nis imperative to avoid any tax penalties. This paper proposes a domain-adaptive\nsmall language model (SLM) with an encoder-decoder architecture for the\nenhanced prediction of product and service tax codes. In this approach, we\naddress the problem of predicting hierarchical tax code sequences using\nunstructured product and services data. We employ an SLM based upon\nencoder-decoder architecture as this enables sequential generation of tax codes\nto capture the hierarchical dependencies present within the tax codes. Our\nexperiments demonstrate that encoder-decoder SLMs can be successfully applied\nto the sequential prediction of structured tax codes, a domain that remains\ncomparatively unexplored in current NLP research. In this paper, we demonstrate\nthe superior performance of the domain-adaptive encoder-decoder SLMs over flat\nclassifiers when applied to the Harmonized System of Nomenclature (HSN), and\nachieve superior results compared to decoder-only and encoder-only\narchitectures for structured sequence generation tasks. This approach can also\nbe scaled to other government-mandated tax commodity codes, such as United\nNations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura\nComum do Mercosul (NCM).\n","authors":["Souvik Nath","Sumit Wadhwa","Luiz Perez"],"pdf_url":"https://arxiv.org/pdf/2507.10880v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.08193v5","updated":"2025-07-15T00:32:25Z","published":"2024-10-10T17:58:24Z","title":"GenARM: Reward Guided Generation with Autoregressive Reward Model for\n  Test-time Alignment","summary":"  Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining. Our project page is\navailable at: https://genarm.github.io.\n","authors":["Yuancheng Xu","Udari Madhushani Sehwag","Alec Koppel","Sicheng Zhu","Bang An","Furong Huang","Sumitra Ganesh"],"pdf_url":"https://arxiv.org/pdf/2410.08193v5.pdf","comment":"Published at the Thirteenth International Conference on Learning\n  Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2506.22760v2","updated":"2025-07-15T00:13:39Z","published":"2025-06-28T05:44:57Z","title":"Jan-nano Technical Report","summary":"  Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage Reinforcement Learning with Verifiable Rewards (RLVR) system that\ncompletely eliminates reliance on next token prediction training (SFT),\nJan-nano achieves 83.2% on SimpleQA benchmark with MCP integration while\nrunning on consumer hardware. With 128K context length, Jan-nano proves that\nintelligence isn't about scale, it's about strategy.\n","authors":["Alan Dao","Dinh Bach Vu"],"pdf_url":"https://arxiv.org/pdf/2506.22760v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.10077v2","updated":"2025-07-15T00:08:11Z","published":"2025-06-11T18:00:30Z","title":"A quantum semantic framework for natural language processing","summary":"  Semantic degeneracy represents a fundamental property of natural language\nthat extends beyond simple polysemy to encompass the combinatorial explosion of\npotential interpretations that emerges as semantic expressions increase in\ncomplexity. In this work, we argue this property imposes fundamental\nlimitations on Large Language Models (LLMs) and other modern NLP systems,\nprecisely because they operate within natural language itself. Using Kolmogorov\ncomplexity, we demonstrate that as an expression's complexity grows, the amount\nof contextual information required to reliably resolve its ambiguity explodes\ncombinatorially. The computational intractability of recovering a single\nintended meaning for complex or ambiguous text therefore suggests that the\nclassical view that linguistic forms possess intrinsic meaning in and of\nthemselves is conceptually inadequate. We argue instead that meaning is\ndynamically actualized through an observer-dependent interpretive act, a\nprocess whose non-deterministic nature is most appropriately described by a\nnon-classical, quantum-like logic. To test this hypothesis, we conducted a\nsemantic Bell inequality test using diverse LLM agents. Our experiments yielded\naverage CHSH expectation values from 1.2 to 2.8, with several runs producing\nvalues (e.g., 2.3-2.4) in significant violation of the classical boundary\n($|S|\\leq2$), demonstrating that linguistic interpretation under ambiguity can\nexhibit non-classical contextuality, consistent with results from human\ncognition experiments. These results inherently imply that classical\nfrequentist-based analytical approaches for natural language are necessarily\nlossy. Instead, we propose that Bayesian-style repeated sampling approaches can\nprovide more practically useful and appropriate characterizations of linguistic\nmeaning in context.\n","authors":["Christopher J. Agostino","Quan Le Thien","Molly Apsel","Denizhan Pak","Elina Lesyk","Ashabari Majumdar"],"pdf_url":"https://arxiv.org/pdf/2506.10077v2.pdf","comment":"12 pages, 2 figures, accepted submission to Quantum AI and NLP 2025"},{"id":"http://arxiv.org/abs/2507.11788v1","updated":"2025-07-15T23:04:44Z","published":"2025-07-15T23:04:44Z","title":"Simulated Language Acquisition in a Biologically Realistic Model of the\n  Brain","summary":"  Despite tremendous progress in neuroscience, we do not have a compelling\nnarrative for the precise way whereby the spiking of neurons in our brain\nresults in high-level cognitive phenomena such as planning and language. We\nintroduce a simple mathematical formulation of six basic and broadly accepted\nprinciples of neuroscience: excitatory neurons, brain areas, random synapses,\nHebbian plasticity, local inhibition, and inter-area inhibition. We implement a\nsimulated neuromorphic system based on this formalism, which is capable of\nbasic language acquisition: Starting from a tabula rasa, the system learns, in\nany language, the semantics of words, their syntactic role (verb versus noun),\nand the word order of the language, including the ability to generate novel\nsentences, through the exposure to a modest number of grounded sentences in the\nsame language. We discuss several possible extensions and implications of this\nresult.\n","authors":["Daniel Mitropolsky","Christos Papadimitriou"],"pdf_url":"https://arxiv.org/pdf/2507.11788v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.17253v3","updated":"2025-07-15T22:18:40Z","published":"2024-06-25T03:41:02Z","title":"How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities, but\nupdating their knowledge post-training remains a critical challenge. While\nrecent model editing techniques like Rank-One Model Editing (ROME) show\npromise, their effectiveness may vary based on the nature of the knowledge\nbeing edited. We introduce the concept of ``perplexingness'': the degree to\nwhich new knowledge conflicts with an LLM's learned conceptual hierarchies and\ncategorical relationships. For instance, editing ``British Shorthair is a kind\nof cat'' to ``British Shorthair is a kind of dog'' represents a\nlow-perplexingness edit within the same taxonomic level, while editing ``A cat\nis a kind of animal'' to ``A cat is a kind of plant'' represents a\nhigh-perplexingness edit that violates fundamental categorical boundaries. To\nsystematically investigate this phenomenon, we introduce HierarchyData, a\ncarefully curated dataset of 99 hyponym-hypernym pairs across diverse\ncategories. Through controlled experiments across three models and four editing\nmethods, we demonstrate a strong negative correlation between the\nperplexingness of new knowledge and the effectiveness of knowledge editing. Our\nanalysis reveals that edits involving more abstract concepts (hypernyms)\ngenerally exhibit higher perplexingness and are more resistant to modification\nthan their specific counterparts (hyponyms). These findings highlight a\nfundamental challenge in LLM knowledge editing: the more a new fact contradicts\nan LLM's learned conceptual hierarchies, the harder it becomes to reliably\nencode that knowledge.\n","authors":["Huaizhi Ge","Frank Rudzicz","Zining Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.17253v3.pdf","comment":"A previous version of this document contained a hidden prompt entered\n  by Z Zhu without knowledge of -- or consent by -- his co-authors. This\n  version does not contain the prompt"},{"id":"http://arxiv.org/abs/2406.17241v4","updated":"2025-07-15T22:13:30Z","published":"2024-06-25T03:09:53Z","title":"Understanding Language Model Circuits through Knowledge Editing","summary":"  Recent advances in language model interpretability have identified circuits,\ncritical subnetworks that replicate model behaviors, yet how knowledge is\nstructured within these crucial subnetworks remains opaque. To gain an\nunderstanding toward the knowledge in the circuits, we conduct systematic\nknowledge editing experiments on the circuits of the GPT-2 language model. Our\nanalysis reveals intriguing patterns in how circuits respond to editing\nattempts, the extent of knowledge distribution across network components, and\nthe architectural composition of knowledge-bearing circuits. These findings\noffer insights into the complex relationship between model circuits and\nknowledge representation, deepening the understanding of how information is\norganized within language models. Our findings offer novel insights into the\n``meanings'' of the circuits, and introduce directions for further\ninterpretability and safety research of language models.\n","authors":["Huaizhi Ge","Frank Rudzicz","Zining Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.17241v4.pdf","comment":"A previous version of this document contained a hidden prompt entered\n  by Z Zhu without knowledge of -- or consent by -- his co-authors. This\n  version does not contain the prompt"},{"id":"http://arxiv.org/abs/2507.11764v1","updated":"2025-07-15T22:10:20Z","published":"2025-07-15T22:10:20Z","title":"AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings\n  with Sentiment for Subjectivity Detection in News Articles","summary":"  This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).\n","authors":["Matteo Fasulo","Luca Babboni","Luca Tedeschini"],"pdf_url":"https://arxiv.org/pdf/2507.11764v1.pdf","comment":"14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab"},{"id":"http://arxiv.org/abs/2506.00713v3","updated":"2025-07-15T21:31:55Z","published":"2025-05-31T21:11:30Z","title":"AKReF: An argumentative knowledge representation framework for\n  structured argumentation","summary":"  This paper presents a framework to convert argumentative texts into argument\nknowledge graphs (AKG). The proposed argumentative knowledge representation\nframework (AKReF) extends the theoretical foundation and enables the AKG to\nprovide a graphical view of the argumentative structure that is easier to\nunderstand. Starting with basic annotations of argumentative components (ACs)\nand argumentative relations (ARs), we enrich the information by constructing a\nknowledge base (KB) graph with metadata attributes for nodes. Next, we apply\nmodus ponens on premises and inference rules from the KB to form arguments.\nFrom these arguments, we create an AKG. The nodes and edges of the AKG have\nattributes capturing key argumentative features such as the type of premise\n(e.g., axiom, ordinary premise, assumption), the type of inference rule (e.g.,\nstrict, defeasible), preference order over defeasible rules, markers (e.g.,\n\"therefore\", \"however\"), and the type of attack (e.g., undercut, rebuttal,\nundermining). We identify inference rules by locating a specific set of\nmarkers, called inference markers (IM). This, in turn, makes it possible to\nidentify undercut attacks previously undetectable in existing datasets. AKG\nprepares the ground for reasoning tasks, including checking the coherence of\narguments and identifying opportunities for revision. For this, it is essential\nto find indirect relations, many of which are implicit. Our proposed AKG\nformat, with annotated inference rules and modus ponens, helps reasoning models\nlearn the implicit, indirect relations that require inference over arguments\nand their interconnections. We use an essay from the AAEC dataset to illustrate\nthe framework. We further show its application in complex analyses such as\nextracting a conflict-free set and a maximal set of admissible arguments.\n","authors":["Debarati Bhattacharjee","Ashish Anand"],"pdf_url":"https://arxiv.org/pdf/2506.00713v3.pdf","comment":"20 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2507.11742v1","updated":"2025-07-15T21:14:08Z","published":"2025-07-15T21:14:08Z","title":"CRABS: A syntactic-semantic pincer strategy for bounding LLM\n  interpretation of Python notebooks","summary":"  Recognizing the information flows and operations comprising data science and\nmachine learning Python notebooks is critical for evaluating, reusing, and\nadapting notebooks for new tasks. Investigating a notebook via re-execution\noften is impractical due to the challenges of resolving data and software\ndependencies. While Large Language Models (LLMs) pre-trained on large codebases\nhave demonstrated effectiveness in understanding code without running it, we\nobserve that they fail to understand some realistic notebooks due to\nhallucinations and long-context challenges. To address these issues, we propose\na notebook understanding task yielding an information flow graph and\ncorresponding cell execution dependency graph for a notebook, and demonstrate\nthe effectiveness of a pincer strategy that uses limited syntactic analysis to\nassist full comprehension of the notebook using an LLM. Our Capture and Resolve\nAssisted Bounding Strategy (CRABS) employs shallow syntactic parsing and\nanalysis of the abstract syntax tree (AST) to capture the correct\ninterpretation of a notebook between lower and upper estimates of the\ninter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via\ncell-by-cell zero-shot learning, thereby identifying the true data inputs and\noutputs of each cell. We evaluate and demonstrate the effectiveness of our\napproach using an annotated dataset of 50 representative, highly up-voted\nKaggle notebooks that together represent 3454 actual cell inputs and outputs.\nThe LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the\nsyntactic structure of these notebooks. Across 50 notebooks, CRABS achieves\naverage F1 scores of 98% identifying cell-to-cell information flows and 99%\nidentifying transitive cell execution dependencies.\n","authors":["Meng Li","Timothy M. McPhillips","Dingmin Wang","Shin-Rong Tsai","Bertram Ludäscher"],"pdf_url":"https://arxiv.org/pdf/2507.11742v1.pdf","comment":"Preprint. Accepted to COLM 2025"},{"id":"http://arxiv.org/abs/2502.05111v2","updated":"2025-07-15T21:10:27Z","published":"2025-02-07T17:35:17Z","title":"Flexible and Efficient Grammar-Constrained Decoding","summary":"  Large Language Models (LLMs) are often asked to generate structured outputs\nthat obey precise syntactic rules, such as code snippets or formatted data.\nGrammar-constrained decoding (GCD) can guarantee that LLM outputs matches such\nrules by masking out tokens that will provably lead to outputs that do not\nbelong to a specified context-free grammar (CFG). To guarantee soundness, GCD\nalgorithms have to compute how a given LLM subword tokenizer can align with the\ntokens used\n  by a given context-free grammar and compute token masks based on this\ninformation. Doing so efficiently is challenging and existing GCD algorithms\nrequire tens of minutes to preprocess common grammars. We present a new GCD\nalgorithm together with an implementation that offers 17.71x faster offline\npreprocessing than existing approaches while preserving state-of-the-art\nefficiency in online mask computation.\n","authors":["Kanghee Park","Timothy Zhou","Loris D'Antoni"],"pdf_url":"https://arxiv.org/pdf/2502.05111v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10389v2","updated":"2025-07-15T20:16:15Z","published":"2025-05-15T15:11:48Z","title":"Multi-domain Multilingual Sentiment Analysis in Industry: Predicting\n  Aspect-based Opinion Quadruples","summary":"  This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. We investigate whether a single fine-tuned model can effectively\nhandle multiple domain-specific taxonomies simultaneously. We demonstrate that\na combined multi-domain model achieves performance comparable to specialized\nsingle-domain models while reducing operational complexity. We also share\nlessons learned for handling non-extractive predictions and evaluating various\nfailure modes when developing LLM-based systems for structured prediction\ntasks.\n","authors":["Benjamin White","Anastasia Shimorina"],"pdf_url":"https://arxiv.org/pdf/2505.10389v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11694v1","updated":"2025-07-15T19:51:24Z","published":"2025-07-15T19:51:24Z","title":"ExpliCIT-QA: Explainable Code-Based Image Table Question Answering","summary":"  We present ExpliCIT-QA, a system that extends our previous MRT approach for\ntabular question answering into a multimodal pipeline capable of handling\ncomplex table images and providing explainable answers. ExpliCIT-QA follows a\nmodular design, consisting of: (1) Multimodal Table Understanding, which uses a\nChain-of-Thought approach to extract and transform content from table images;\n(2) Language-based Reasoning, where a step-by-step explanation in natural\nlanguage is generated to solve the problem; (3) Automatic Code Generation,\nwhere Python/Pandas scripts are created based on the reasoning steps, with\nfeedback for handling errors; (4) Code Execution to compute the final answer;\nand (5) Natural Language Explanation that describes how the answer was\ncomputed. The system is built for transparency and auditability: all\nintermediate outputs, parsed tables, reasoning steps, generated code, and final\nanswers are available for inspection. This strategy works towards closing the\nexplainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on\nthe TableVQA-Bench benchmark, comparing it with existing baselines. We\ndemonstrated improvements in interpretability and transparency, which open the\ndoor for applications in sensitive domains like finance and healthcare where\nauditing results are critical.\n","authors":["Maximiliano Hormazábal Lagos","Álvaro Bueno Sáez","Pedro Alonso Doval","Jorge Alcalde Vesteiro","Héctor Cerezo-Costas"],"pdf_url":"https://arxiv.org/pdf/2507.11694v1.pdf","comment":"This work has been accepted for presentation at the 24nd Portuguese\n  Conference on Artificial Intelligence (EPIA 2025) and will be published in\n  the proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available"},{"id":"http://arxiv.org/abs/2507.11687v1","updated":"2025-07-15T19:44:20Z","published":"2025-07-15T19:44:20Z","title":"MetaLint: Generalizable Idiomatic Code Quality Analysis through\n  Instruction-Following and Easy-to-Hard Generalization","summary":"  Large Language Models, though successful in code generation, struggle with\ncode quality analysis because they are limited by static training data and\ncan't easily adapt to evolving best practices. We introduce MetaLint, a new\ninstruction-following framework that formulates code quality analysis as the\ntask of detecting and fixing problematic semantic code fragments or code idioms\nbased on high-level specifications. Unlike conventional approaches that train\nmodels on static, rule-based data, MetaLint employs instruction tuning on\nsynthetic linter-generated data to support easy-to-hard generalization,\nenabling models to adapt to novel or complex code patterns without retraining.\nTo evaluate this, we construct a benchmark of challenging idioms inspired by\nreal-world coding standards such as Python Enhancement Proposals (PEPs) and\nassess whether MetaLint-trained models reason adaptively or simply memorize.\nOur results show that MetaLint improves generalization to unseen PEP idioms,\nachieving a 70.37% F-score on idiom detection with the highest recall (70.43%)\namong all evaluated models. It also achieves 26.73% on localization,\ncompetitive for its 4B parameter size and comparable to larger state-of-the-art\nmodels like o3-mini, highlighting its potential for future-proof code quality\nanalysis.\n","authors":["Atharva Naik","Lawanya Baghel","Dhakshin Govindarajan","Darsh Agrawal","Daniel Fried","Carolyn Rose"],"pdf_url":"https://arxiv.org/pdf/2507.11687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11662v1","updated":"2025-07-15T18:50:29Z","published":"2025-07-15T18:50:29Z","title":"Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with\n  Self-Grounded Verification","summary":"  Verifiers -- functions assigning rewards to agent behavior -- have been key\nfor AI progress in domains like math and board games. However, extending these\ngains to domains without clear-cut success criteria (e.g.,computer use) remains\na challenge: while humans can recognize suitable outcomes, translating this\nintuition into scalable rules is non-trivial. Multimodal Large Language\nModels(MLLMs) emerge as a promising solution, given their world knowledge,\nhuman-preference alignment, and reasoning skills. We evaluate MLLMs as\nverifiers of agent trajectories across web navigation, computer use, and\nrobotic manipulation, and identify a critical limitation: agreement bias, a\nstrong tendency for MLLMs to favor information in their context window, often\ngenerating chains of thought to rationalize flawed behavior. This bias is\npervasive across models, resilient to test-time scaling, and can impact several\nmethods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs\ndespite MLLMs showing strong, human-aligned priors on desired behavior. To\naddress this, we propose Self-Grounded Verification (SGV), a lightweight method\nthat enables more effective use of MLLMs' knowledge and reasoning by harnessing\ntheir own sampling mechanisms via unconditional and conditional generation. SGV\noperates in two steps: first, the MLLM is elicited to retrieve broad priors\nabout task completion, independent of the data under evaluation. Then,\nconditioned on self-generated priors, it reasons over and evaluates a candidate\ntrajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in\naccuracy and failure detection rates, and can perform real-time supervision of\nheterogeneous agents, boosting task completion of a GUI specialist in OSWorld,\na diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting\na new state of the art on the benchmark, surpassing the previous best by 48%.\n","authors":["Moises Andrade","Joonhyuk Cha","Brandon Ho","Vriksha Srihari","Karmesh Yadav","Zsolt Kira"],"pdf_url":"https://arxiv.org/pdf/2507.11662v1.pdf","comment":"Our code and data are publicly available at\n  https://github.com/mshalimay/mllm-verifiers-abias-sgv"},{"id":"http://arxiv.org/abs/2507.11661v1","updated":"2025-07-15T18:47:49Z","published":"2025-07-15T18:47:49Z","title":"Partitioner Guided Modal Learning Framework","summary":"  Multimodal learning benefits from multiple modal information, and each\nlearned modal representations can be divided into uni-modal that can be learned\nfrom uni-modal training and paired-modal features that can be learned from\ncross-modal interaction. Building on this perspective, we propose a\npartitioner-guided modal learning framework, PgM, which consists of the modal\npartitioner, uni-modal learner, paired-modal learner, and uni-paired modal\ndecoder. Modal partitioner segments the learned modal representation into\nuni-modal and paired-modal features. Modal learner incorporates two dedicated\ncomponents for uni-modal and paired-modal learning. Uni-paired modal decoder\nreconstructs modal representation based on uni-modal and paired-modal features.\nPgM offers three key benefits: 1) thorough learning of uni-modal and\npaired-modal features, 2) flexible distribution adjustment for uni-modal and\npaired-modal representations to suit diverse downstream tasks, and 3) different\nlearning rates across modalities and partitions. Extensive experiments\ndemonstrate the effectiveness of PgM across four multimodal tasks and further\nhighlight its transferability to existing models. Additionally, we visualize\nthe distribution of uni-modal and paired-modal features across modalities and\ntasks, offering insights into their respective contributions.\n","authors":["Guimin Hu","Yi Xin","Lijie Hu","Zhihong Zhu","Hasti Seifi"],"pdf_url":"https://arxiv.org/pdf/2507.11661v1.pdf","comment":"acm multimedia 2025"},{"id":"http://arxiv.org/abs/2410.16069v2","updated":"2025-07-15T18:29:52Z","published":"2024-10-21T14:47:37Z","title":"Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp Context","summary":"  Human processing of idioms relies on understanding the contextual sentences\nin which idioms occur, as well as language-intrinsic features such as frequency\nand speaker-intrinsic factors like familiarity. While LLMs have shown high\nperformance on idiomaticity detection tasks, this success may be attributed to\nreasoning shortcuts in existing datasets. To this end, we construct a novel,\ncontrolled contrastive dataset designed to test whether LLMs can effectively\nuse context to disambiguate idiomatic meaning. Additionally, we explore how\ncollocational frequency and sentence probability influence model performance.\nOur findings reveal that LLMs often fail to resolve idiomaticity when it is\nrequired to attend to the surrounding context, and that models perform better\non sentences that have higher likelihood. The collocational frequency of\nexpressions also impacts performance. We make our code and dataset publicly\navailable.\n","authors":["Maggie Mi","Aline Villavicencio","Nafise Sadat Moosavi"],"pdf_url":"https://arxiv.org/pdf/2410.16069v2.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2507.11634v1","updated":"2025-07-15T18:13:25Z","published":"2025-07-15T18:13:25Z","title":"Cross-lingual Few-shot Learning for Persian Sentiment Analysis with\n  Incremental Adaptation","summary":"  This research examines cross-lingual sentiment analysis using few-shot\nlearning and incremental learning methods in Persian. The main objective is to\ndevelop a model capable of performing sentiment analysis in Persian using\nlimited data, while getting prior knowledge from high-resource languages. To\nachieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and\nDistilBERT) were employed, which were fine-tuned using few-shot and incremental\nlearning approaches on small samples of Persian data from diverse sources,\nincluding X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled\nthe models to learn from a broad range of contexts. Experimental results show\nthat the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%\naccuracy on Persian sentiment analysis. These findings highlight the\neffectiveness of combining few-shot learning and incremental learning with\nmultilingual pre-trained models.\n","authors":["Farideh Majidi","Ziaeddin Beheshtifard"],"pdf_url":"https://arxiv.org/pdf/2507.11634v1.pdf","comment":"Proceedings of the First National Conference on Artificial\n  Intelligence and Emerging Research: Convergence of Humans and Intelligent\n  Systems"},{"id":"http://arxiv.org/abs/2505.15670v3","updated":"2025-07-15T18:10:44Z","published":"2025-05-21T15:48:30Z","title":"Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model","summary":"  Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility.\n","authors":["Ke Hu","Ehsan Hosseini-Asl","Chen Chen","Edresson Casanova","Subhankar Ghosh","Piotr Żelasko","Zhehuai Chen","Jason Li","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2505.15670v3.pdf","comment":"Accepted to Interspeech 2025"},{"id":"http://arxiv.org/abs/2507.11630v1","updated":"2025-07-15T18:10:29Z","published":"2025-07-15T18:10:29Z","title":"Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility","summary":"  AI systems are rapidly advancing in capability, and frontier model developers\nbroadly acknowledge the need for safeguards against serious misuse. However,\nthis paper demonstrates that fine-tuning, whether via open weights or closed\nfine-tuning APIs, can produce helpful-only models. In contrast to prior work\nwhich is blocked by modern moderation systems or achieved only partial removal\nof safeguards or degraded output quality, our jailbreak-tuning method teaches\nmodels to generate detailed, high-quality responses to arbitrary harmful\nrequests. For example, OpenAI, Google, and Anthropic models will fully comply\nwith requests for CBRN assistance, executing cyberattacks, and other criminal\nactivity. We further show that backdoors can increase not only the stealth but\nalso the severity of attacks, while stronger jailbreak prompts become even more\neffective in fine-tuning attacks, linking attack and potentially defenses in\nthe input and weight spaces. Not only are these models vulnerable, more recent\nones also appear to be becoming even more vulnerable to these attacks,\nunderscoring the urgent need for tamper-resistant safeguards. Until such\nsafeguards are discovered, companies and policymakers should view the release\nof any fine-tunable model as simultaneously releasing its evil twin: equally\ncapable as the original model, and usable for any malicious purpose within its\ncapabilities.\n","authors":["Brendan Murphy","Dillon Bowen","Shahrad Mohammadzadeh","Julius Broomfield","Adam Gleave","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2507.11630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11625v1","updated":"2025-07-15T18:02:57Z","published":"2025-07-15T18:02:57Z","title":"MapIQ: Benchmarking Multimodal Large Language Models for Map Question\n  Answering","summary":"  Recent advancements in multimodal large language models (MLLMs) have driven\nresearchers to explore how well these models read data visualizations, e.g.,\nbar charts, scatter plots. More recently, attention has shifted to visual\nquestion answering with maps (Map-VQA). However, Map-VQA research has primarily\nfocused on choropleth maps, which cover only a limited range of thematic\ncategories and visual analytical tasks. To address these gaps, we introduce\nMapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three\nmap types: choropleth maps, cartograms, and proportional symbol maps spanning\ntopics from six distinct themes (e.g., housing, crime). We evaluate multiple\nMLLMs using six visual analytical tasks, comparing their performance against\none another and a human baseline. An additional experiment examining the impact\nof map design changes (e.g., altered color schemes, modified legend designs,\nand removal of map elements) provides insights into the robustness and\nsensitivity of MLLMs, their reliance on internal geographic knowledge, and\npotential avenues for improving Map-VQA performance.\n","authors":["Varun Srivastava","Fan Lei","Srija Mukhopadhyay","Vivek Gupta","Ross Maciejewski"],"pdf_url":"https://arxiv.org/pdf/2507.11625v1.pdf","comment":"Published as a conference paper at COLM 2025"},{"id":"http://arxiv.org/abs/2507.11310v1","updated":"2025-07-15T13:42:32Z","published":"2025-07-15T13:42:32Z","title":"LRCTI: A Large Language Model-Based Framework for Multi-Step Evidence\n  Retrieval and Reasoning in Cyber Threat Intelligence Credibility Verification","summary":"  Verifying the credibility of Cyber Threat Intelligence (CTI) is essential for\nreliable cybersecurity defense. However, traditional approaches typically treat\nthis task as a static classification problem, relying on handcrafted features\nor isolated deep learning models. These methods often lack the robustness\nneeded to handle incomplete, heterogeneous, or noisy intelligence, and they\nprovide limited transparency in decision-making-factors that reduce their\neffectiveness in real-world threat environments. To address these limitations,\nwe propose LRCTI, a Large Language Model (LLM)-based framework designed for\nmulti-step CTI credibility verification. The framework first employs a text\nsummarization module to distill complex intelligence reports into concise and\nactionable threat claims. It then uses an adaptive multi-step evidence\nretrieval mechanism that iteratively identifies and refines supporting\ninformation from a CTI-specific corpus, guided by LLM feedback. Finally, a\nprompt-based Natural Language Inference (NLI) module is applied to evaluate the\ncredibility of each claim while generating interpretable justifications for the\nclassification outcome. Experiments conducted on two benchmark datasets,\nCTI-200 and PolitiFact show that LRCTI improves F1-Macro and F1-Micro scores by\nover 5%, reaching 90.9% and 93.6%, respectively, compared to state-of-the-art\nbaselines. These results demonstrate that LRCTI effectively addresses the core\nlimitations of prior methods, offering a scalable, accurate, and explainable\nsolution for automated CTI credibility verification\n","authors":["Fengxiao Tang","Huan Li","Ming Zhao","Zongzong Wu","Shisong Peng","Tao Yin"],"pdf_url":"https://arxiv.org/pdf/2507.11310v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2507.11540v1","updated":"2025-07-15T17:59:59Z","published":"2025-07-15T17:59:59Z","title":"Towards Depth Foundation Model: Recent Trends in Vision-Based Depth\n  Estimation","summary":"  Depth estimation is a fundamental task in 3D computer vision, crucial for\napplications such as 3D reconstruction, free-viewpoint rendering, robotics,\nautonomous driving, and AR/VR technologies. Traditional methods relying on\nhardware sensors like LiDAR are often limited by high costs, low resolution,\nand environmental sensitivity, limiting their applicability in real-world\nscenarios. Recent advances in vision-based methods offer a promising\nalternative, yet they face challenges in generalization and stability due to\neither the low-capacity model architectures or the reliance on domain-specific\nand small-scale datasets. The emergence of scaling laws and foundation models\nin other domains has inspired the development of \"depth foundation models\":\ndeep neural networks trained on large datasets with strong zero-shot\ngeneralization capabilities. This paper surveys the evolution of deep learning\narchitectures and paradigms for depth estimation across the monocular, stereo,\nmulti-view, and monocular video settings. We explore the potential of these\nmodels to address existing challenges and provide a comprehensive overview of\nlarge-scale datasets that can facilitate their development. By identifying key\narchitectures and training strategies, we aim to highlight the path towards\nrobust depth foundation models, offering insights into their future research\nand applications.\n","authors":["Zhen Xu","Hongyu Zhou","Sida Peng","Haotong Lin","Haoyu Guo","Jiahao Shao","Peishan Yang","Qinglin Yang","Sheng Miao","Xingyi He","Yifan Wang","Yue Wang","Ruizhen Hu","Yiyi Liao","Xiaowei Zhou","Hujun Bao"],"pdf_url":"https://arxiv.org/pdf/2507.11540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11539v1","updated":"2025-07-15T17:59:57Z","published":"2025-07-15T17:59:57Z","title":"Streaming 4D Visual Geometry Transformer","summary":"  Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.\n","authors":["Dong Zhuo","Wenzhao Zheng","Jiahe Guo","Yuqi Wu","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2507.11539v1.pdf","comment":"Code is available at: https://github.com/wzzheng/StreamVGGT"},{"id":"http://arxiv.org/abs/2507.11533v1","updated":"2025-07-15T17:58:08Z","published":"2025-07-15T17:58:08Z","title":"CharaConsist: Fine-Grained Consistent Character Generation","summary":"  In text-to-image generation, producing a series of consistent contents that\npreserve the same identity is highly valuable for real-world applications.\nAlthough a few works have explored training-free methods to enhance the\nconsistency of generated subjects, we observe that they suffer from the\nfollowing problems. First, they fail to maintain consistent background details,\nwhich limits their applicability. Furthermore, when the foreground character\nundergoes large motion variations, inconsistencies in identity and clothing\ndetails become evident. To address these problems, we propose CharaConsist,\nwhich employs point-tracking attention and adaptive token merge along with\ndecoupled control of the foreground and background. CharaConsist enables\nfine-grained consistency for both foreground and background, supporting the\ngeneration of one character in continuous shots within a fixed scene or in\ndiscrete shots across different scenes. Moreover, CharaConsist is the first\nconsistent generation method tailored for text-to-image DiT model. Its ability\nto maintain fine-grained consistency, combined with the larger capacity of\nlatest base model, enables it to produce high-quality visual outputs,\nbroadening its applicability to a wider range of real-world scenarios. The\nsource code has been released at https://github.com/Murray-Wang/CharaConsist\n","authors":["Mengyu Wang","Henghui Ding","Jianing Peng","Yao Zhao","Yunpeng Chen","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2507.11533v1.pdf","comment":"ICCV 2025 accepted paper, project page:\n  https://murray-wang.github.io/CharaConsist/"},{"id":"http://arxiv.org/abs/2507.11522v1","updated":"2025-07-15T17:47:01Z","published":"2025-07-15T17:47:01Z","title":"CATVis: Context-Aware Thought Visualization","summary":"  EEG-based brain-computer interfaces (BCIs) have shown promise in various\napplications, such as motor imagery and cognitive state monitoring. However,\ndecoding visual representations from EEG signals remains a significant\nchallenge due to their complex and noisy nature. We thus propose a novel\n5-stage framework for decoding visual representations from EEG signals: (1) an\nEEG encoder for concept classification, (2) cross-modal alignment of EEG and\ntext embeddings in CLIP feature space, (3) caption refinement via re-ranking,\n(4) weighted interpolation of concept and caption embeddings for richer\nsemantics, and (5) image generation using a pre-trained Stable Diffusion model.\nWe enable context-aware EEG-to-image generation through cross-modal alignment\nand re-ranking. Experimental results demonstrate that our method generates\nhigh-quality images aligned with visual stimuli, outperforming SOTA approaches\nby 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and\nreducing Fr\\'echet Inception Distance by 36.61%, indicating superior semantic\nalignment and image quality.\n","authors":["Tariq Mehmood","Hamza Ahmad","Muhammad Haroon Shakeel","Murtaza Taj"],"pdf_url":"https://arxiv.org/pdf/2507.11522v1.pdf","comment":"Accepted at MICCAI 2025. This is the submitted version prior to peer\n  review. The final Version of Record will appear in the MICCAI 2025\n  proceedings (Springer LNCS)"},{"id":"http://arxiv.org/abs/2503.06151v2","updated":"2025-07-15T17:27:11Z","published":"2025-03-08T10:22:36Z","title":"Biomechanics-Guided Residual Approach to Generalizable Human Motion\n  Generation and Estimation","summary":"  Human pose, action, and motion generation are critical for applications in\ndigital humans, character animation, and humanoid robotics. However, many\nexisting methods struggle to produce physically plausible movements that are\nconsistent with biomechanical principles. Although recent autoregressive and\ndiffusion models deliver impressive visual quality, they often neglect key\nbiodynamic features and fail to ensure physically realistic motions.\nReinforcement Learning (RL) approaches can address these shortcomings but are\nhighly dependent on simulation environments, limiting their generalizability.\nTo overcome these challenges, we propose BioVAE, a biomechanics-aware framework\nwith three core innovations: (1) integration of muscle electromyography (EMG)\nsignals and kinematic features with acceleration constraints to enable\nphysically plausible motion without simulations; (2) seamless coupling with\ndiffusion models for stable end-to-end training; and (3) biomechanical priors\nthat promote strong generalization across diverse motion generation and\nestimation tasks. Extensive experiments demonstrate that BioVAE achieves\nstate-of-the-art performance on multiple benchmarks, bridging the gap between\ndata-driven motion synthesis and biomechanical authenticity while setting new\nstandards for physically accurate motion generation and pose estimation.\n","authors":["Zixi Kang","Xinghan Wang","Yadong Mu"],"pdf_url":"https://arxiv.org/pdf/2503.06151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11488v1","updated":"2025-07-15T17:01:45Z","published":"2025-07-15T17:01:45Z","title":"COLIBRI Fuzzy Model: Color Linguistic-Based Representation and\n  Interpretation","summary":"  Colors are omnipresent in today's world and play a vital role in how humans\nperceive and interact with their surroundings. However, it is challenging for\ncomputers to imitate human color perception. This paper introduces the Human\nPerception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based\nRepresentation and Interpretation), designed to bridge the gap between\ncomputational color representations and human visual perception. The proposed\nmodel uses fuzzy sets and logic to create a framework for color categorization.\nUsing a three-phase experimental approach, the study first identifies\ndistinguishable color stimuli for hue, saturation, and intensity through\npreliminary experiments, followed by a large-scale human categorization survey\ninvolving more than 1000 human subjects. The resulting data are used to extract\nfuzzy partitions and generate membership functions that reflect real-world\nperceptual uncertainty. The model incorporates a mechanism for adaptation that\nallows refinement based on feedback and contextual changes. Comparative\nevaluations demonstrate the model's alignment with human perception compared to\ntraditional color models, such as RGB, HSV, and LAB. To the best of our\nknowledge, no previous research has documented the construction of a model for\ncolor attribute specification based on a sample of this size or a comparable\nsample of the human population (n = 2496). Our findings are significant for\nfields such as design, artificial intelligence, marketing, and human-computer\ninteraction, where perceptually relevant color representation is critical.\n","authors":["Pakizar Shamoi","Nuray Toganas","Muragul Muratbekova","Elnara Kadyrgali","Adilet Yerkin","Ayan Igali","Malika Ziyada","Ayana Adilova","Aron Karatayev","Yerdauit Torekhan"],"pdf_url":"https://arxiv.org/pdf/2507.11488v1.pdf","comment":"submitted to IEEE for consideration"},{"id":"http://arxiv.org/abs/2507.11476v1","updated":"2025-07-15T16:47:44Z","published":"2025-07-15T16:47:44Z","title":"C-FBI: A Combinatorial method using Convolutions for Circle Fitting in\n  Blurry Images","summary":"  This paper addresses the fundamental computer vision challenge of robust\ncircle detection and fitting in degraded imaging conditions. We present\nCombinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an\nalgorithm that bridges the gap between circle detection and precise parametric\nfitting by combining (1) efficient combinatorial edge pixel (edgel) sampling\nand (2) convolution-based density estimation in parameter space.\n  We evaluate 3C-FBI across three experimental frameworks: (1) real-world\nmedical data from Parkinson's disease assessments (144 frames from 36 videos),\n(2) controlled synthetic data following established circle-fitting benchmarks,\nand (3) systematic analysis across varying spatial resolutions and outlier\ncontamination levels. Results show that 3C-FBI achieves state-of-the-art\naccuracy (Jaccard index 0.896) while maintaining real-time performance (40.3\nfps), significantly outperforming classical methods like RCD (6.8 fps) on a\nstandard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost\n1.0) at high resolutions (480x480) and reliable performance (Jaccard higher\nthan 0.95) down to 160x160 with up to 20% outliers.\n  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989\nacross contamination levels, comparable to modern methods like Qi et al. (2024,\n0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and\nrobustness makes 3C-FBI ideal for medical imaging, robotics, and industrial\ninspection under challenging conditions.\n","authors":["Esteban Román Catafau","Torbjörn E. M. Nordling"],"pdf_url":"https://arxiv.org/pdf/2507.11476v1.pdf","comment":"22 pages, 16 figures"},{"id":"http://arxiv.org/abs/2507.11474v1","updated":"2025-07-15T16:45:43Z","published":"2025-07-15T16:45:43Z","title":"HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry\n  Synthesis and Controllable Editing","summary":"  Accurate characterization of vascular geometry is essential for\ncardiovascular diagnosis and treatment planning. Traditional statistical shape\nmodeling (SSM) methods rely on linear assumptions, limiting their expressivity\nand scalability to complex topologies such as multi-branch vascular structures.\nWe introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular\ngeometry Synthesis, which integrates NURBS surface parameterization with\ndiffusion-based generative modeling to synthesize realistic, fine-grained\naortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates\nanatomically faithful aortas with supra-aortic branches, yielding biomarker\ndistributions that closely match those of the original dataset. HUG-VAS adopts\na hierarchical architecture comprising a denoising diffusion model that\ngenerates centerlines and a guided diffusion model that synthesizes radial\nprofiles conditioned on those centerlines, thereby capturing two layers of\nanatomical variability. Critically, the framework supports zero-shot\nconditional generation from image-derived priors, enabling practical\napplications such as interactive semi-automatic segmentation, robust\nreconstruction under degraded imaging conditions, and implantable device\noptimization. To our knowledge, HUG-VAS is the first SSM framework to bridge\nimage-derived priors with generative shape modeling via a unified integration\nof NURBS parameterization and hierarchical diffusion processes.\n","authors":["Pan Du","Mingqi Xu","Xiaozhi Zhu","Jian-xun Wang"],"pdf_url":"https://arxiv.org/pdf/2507.11474v1.pdf","comment":"59 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.14162v2","updated":"2025-07-15T16:43:23Z","published":"2023-10-22T03:24:53Z","title":"Augmenting End-to-End Steering Angle Prediction with CAN Bus Data","summary":"  In recent years, end to end steering prediction for autonomous vehicles has\nbecome a major area of research. The primary method for achieving end to end\nsteering was to use computer vision models on a live feed of video data.\nHowever, to further increase accuracy, many companies have added data from\nlight detection and ranging (LiDAR) and or radar sensors through sensor fusion.\nHowever, the addition of lasers and sensors comes at a high financial cost. In\nthis paper, I address both of these issues by increasing the accuracy of the\ncomputer vision models without the increased cost of using LiDAR and or\nsensors. I achieved this by improving the accuracy of computer vision models by\nsensor fusing CAN bus data, a vehicle protocol, with video data. CAN bus data\nis a rich source of information about the vehicle's state, including its speed,\nsteering angle, and acceleration. By fusing this data with video data, the\naccuracy of the computer vision model's predictions can be improved. When I\ntrained the model without CAN bus data, I obtained an RMSE of 0.02492, while\nthe model trained with the CAN bus data achieved an RMSE of 0.01970. This\nfinding indicates that fusing CAN Bus data with video data can reduce the\ncomputer vision model's prediction error by 20% with some models decreasing the\nerror by 80%.\n","authors":["Amit Singh"],"pdf_url":"https://arxiv.org/pdf/2310.14162v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2507.11465v1","updated":"2025-07-15T16:36:20Z","published":"2025-07-15T16:36:20Z","title":"Elevating 3D Models: High-Quality Texture and Geometry Refinement from a\n  Low-Quality Model","summary":"  High-quality 3D assets are essential for various applications in computer\ngraphics and 3D vision but remain scarce due to significant acquisition costs.\nTo address this shortage, we introduce Elevate3D, a novel framework that\ntransforms readily accessible low-quality 3D assets into higher quality. At the\ncore of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that\nsignificantly improves texture quality while preserving the appearance and\ngeometry while fixing its degradations. Furthermore, Elevate3D operates in a\nview-by-view manner, alternating between texture and geometry refinement.\nUnlike previous methods that have largely overlooked geometry refinement, our\nframework leverages geometric cues from images refined with HFS-SDEdit by\nemploying state-of-the-art monocular geometry predictors. This approach ensures\ndetailed and accurate geometry that aligns seamlessly with the enhanced\ntexture. Elevate3D outperforms recent competitors by achieving state-of-the-art\nquality in 3D model refinement, effectively addressing the scarcity of\nhigh-quality open-source 3D assets.\n","authors":["Nuri Ryu","Jiyun Won","Jooeun Son","Minsu Gong","Joo-Haeng Lee","Sunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2507.11465v1.pdf","comment":"Accepted to SIGGRAPH 2025. For the project page, see\n  https://cg.postech.ac.kr/research/Elevate3D/"},{"id":"http://arxiv.org/abs/2507.11461v1","updated":"2025-07-15T16:33:01Z","published":"2025-07-15T16:33:01Z","title":"Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror\n  Descent","summary":"  Deep Equilibrium Models (DEQs) are implicit neural networks with fixed\npoints, which have recently gained attention for learning image regularization\nfunctionals, particularly in settings involving Gaussian fidelities, where\nassumptions on the forward operator ensure contractiveness of standard\n(proximal) Gradient Descent operators. In this work, we extend the application\nof DEQs to Poisson inverse problems, where the data fidelity term is more\nappropriately modeled by the Kullback-Leibler divergence. To this end, we\nintroduce a novel DEQ formulation based on Mirror Descent defined in terms of a\ntailored non-Euclidean geometry that naturally adapts with the structure of the\ndata term. This enables the learning of neural regularizers within a principled\ntraining framework. We derive sufficient conditions to guarantee the\nconvergence of the learned reconstruction scheme and propose computational\nstrategies that enable both efficient training and fully parameter-free\ninference. Numerical experiments show that our method outperforms traditional\nmodel-based approaches and it is comparable to the performance of Bregman\nPlug-and-Play methods, while mitigating their typical drawbacks - namely,\nsensitivity to initialization and careful tuning of hyperparameters. The code\nis publicly available at https://github.com/christiandaniele/DEQ-MD.\n","authors":["Christian Daniele","Silvia Villa","Samuel Vaiter","Luca Calatroni"],"pdf_url":"https://arxiv.org/pdf/2507.11461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11443v1","updated":"2025-07-15T16:07:07Z","published":"2025-07-15T16:07:07Z","title":"COLI: A Hierarchical Efficient Compressor for Large Images","summary":"  The escalating adoption of high-resolution, large-field-of-view imagery\namplifies the need for efficient compression methodologies. Conventional\ntechniques frequently fail to preserve critical image details, while\ndata-driven approaches exhibit limited generalizability. Implicit Neural\nRepresentations (INRs) present a promising alternative by learning continuous\nmappings from spatial coordinates to pixel intensities for individual images,\nthereby storing network weights rather than raw pixels and avoiding the\ngeneralization problem. However, INR-based compression of large images faces\nchallenges including slow compression speed and suboptimal compression ratios.\nTo address these limitations, we introduce COLI (Compressor for Large Images),\na novel framework leveraging Neural Representations for Videos (NeRV). First,\nrecognizing that INR-based compression constitutes a training process, we\naccelerate its convergence through a pretraining-finetuning paradigm,\nmixed-precision training, and reformulation of the sequential loss into a\nparallelizable objective. Second, capitalizing on INRs' transformation of image\nstorage constraints into weight storage, we implement Hyper-Compression, a\nnovel post-training technique to substantially enhance compression ratios while\nmaintaining minimal output distortion. Evaluations across two medical imaging\ndatasets demonstrate that COLI consistently achieves competitive or superior\nPSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while\naccelerating NeRV training by up to 4 times.\n","authors":["Haoran Wang","Hanyu Pei","Yang Lyu","Kai Zhang","Li Li","Feng-Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2507.11443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11441v1","updated":"2025-07-15T16:05:30Z","published":"2025-07-15T16:05:30Z","title":"Implementing Adaptations for Vision AutoRegressive Model","summary":"  Vision AutoRegressive model (VAR) was recently introduced as an alternative\nto Diffusion Models (DMs) in image generation domain. In this work we focus on\nits adaptations, which aim to fine-tune pre-trained models to perform specific\ndownstream tasks, like medical data generation. While for DMs there exist many\ntechniques, adaptations for VAR remain underexplored. Similarly, differentially\nprivate (DP) adaptations-ones that aim to preserve privacy of the adaptation\ndata-have been extensively studied for DMs, while VAR lacks such solutions. In\nour work, we implement and benchmark many strategies for VAR, and compare them\nto state-of-the-art DM adaptation strategies. We observe that VAR outperforms\nDMs for non-DP adaptations, however, the performance of DP suffers, which\nnecessitates further research in private adaptations for VAR. Code is available\nat https://github.com/sprintml/finetuning_var_dp.\n","authors":["Kaif Shaikh","Antoni Kowalczuk","Franziska Boenisch","Adam Dziedzic"],"pdf_url":"https://arxiv.org/pdf/2507.11441v1.pdf","comment":"Accepted at DIG-BUGS: Data in Generative Models Workshop @ ICML 2025"},{"id":"http://arxiv.org/abs/2507.11415v1","updated":"2025-07-15T15:40:17Z","published":"2025-07-15T15:40:17Z","title":"U-RWKV: Lightweight medical image segmentation with direction-adaptive\n  RWKV","summary":"  Achieving equity in healthcare accessibility requires lightweight yet\nhigh-performance solutions for medical image segmentation, particularly in\nresource-limited settings. Existing methods like U-Net and its variants often\nsuffer from limited global Effective Receptive Fields (ERFs), hindering their\nability to capture long-range dependencies. To address this, we propose U-RWKV,\na novel framework leveraging the Recurrent Weighted Key-Value(RWKV)\narchitecture, which achieves efficient long-range modeling at O(N)\ncomputational cost. The framework introduces two key innovations: the\nDirection-Adaptive RWKV Module(DARM) and the Stage-Adaptive\nSqueeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan\nmechanisms to aggregate contextual cues across images, mitigating directional\nbias while preserving global context and maintaining high computational\nefficiency. SASE dynamically adapts its architecture to different feature\nextraction stages, balancing high-resolution detail preservation and semantic\nrelationship capture. Experiments demonstrate that U-RWKV achieves\nstate-of-the-art segmentation performance with high computational efficiency,\noffering a practical solution for democratizing advanced medical imaging\ntechnologies in resource-constrained environments. The code is available at\nhttps://github.com/hbyecoding/U-RWKV.\n","authors":["Hongbo Ye","Fenghe Tang","Peiang Zhao","Zhen Huang","Dexin Zhao","Minghao Bian","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.11415v1.pdf","comment":"Accepted by MICCAI2025"},{"id":"http://arxiv.org/abs/2402.03666v6","updated":"2025-07-15T15:33:13Z","published":"2024-02-06T03:39:44Z","title":"QuEST: Low-bit Diffusion Model Quantization via Efficient Selective\n  Finetuning","summary":"  The practical deployment of diffusion models is still hindered by the high\nmemory and computational overhead. Although quantization paves a way for model\ncompression and acceleration, existing methods face challenges in achieving\nlow-bit quantization efficiently. In this paper, we identify imbalanced\nactivation distributions as a primary source of quantization difficulty, and\npropose to adjust these distributions through weight finetuning to be more\nquantization-friendly. We provide both theoretical and empirical evidence\nsupporting finetuning as a practical and reliable solution. Building on this\napproach, we further distinguish two critical types of quantized layers: those\nresponsible for retaining essential temporal information and those particularly\nsensitive to bit-width reduction. By selectively finetuning these layers under\nboth local and global supervision, we mitigate performance degradation while\nenhancing quantization efficiency. Our method demonstrates its efficacy across\nthree high-resolution image generation tasks, obtaining state-of-the-art\nperformance across multiple bit-width settings.\n","authors":["Haoxuan Wang","Yuzhang Shang","Zhihang Yuan","Junyi Wu","Junchi Yan","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2402.03666v6.pdf","comment":"ICCV 2025. Code is available at\n  https://github.com/hatchetProject/QuEST"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.11412v1","updated":"2025-07-15T15:31:51Z","published":"2025-07-15T15:31:51Z","title":"Seq vs Seq: An Open Suite of Paired Encoders and Decoders","summary":"  The large language model (LLM) community focuses almost exclusively on\ndecoder-only language models, since they are easier to use for text generation.\nHowever, a large subset of the community still uses encoder-only models for\ntasks such as classification or retrieval. Previous work has attempted to\ncompare these architectures, but is forced to make comparisons with models that\nhave different numbers of parameters, training techniques, and datasets. We\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\ndecoder-only models produces SOTA recipes in both categories for their\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\ndecoders. Like previous work, we find that encoder-only models excel at\nclassification and retrieval tasks while decoders excel at generative tasks.\nHowever, we show that adapting a decoder model to encoder tasks (and vice\nversa) through continued training is subpar compared to using only the reverse\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\nfor generative tasks). We open-source all artifacts of this study including\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\nallow future work to analyze or extend all aspects of training.\n","authors":["Orion Weller","Kathryn Ricci","Marc Marone","Antoine Chaffin","Dawn Lawrie","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2507.11412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.19777v2","updated":"2025-07-15T14:55:37Z","published":"2025-06-24T16:42:46Z","title":"Alleviating User-Sensitive bias with Fair Generative Sequential\n  Recommendation Model","summary":"  Recommendation fairness has recently attracted much attention. In the real\nworld, recommendation systems are driven by user behavior, and since users with\nthe same sensitive feature (e.g., gender and age) tend to have the same\npatterns, recommendation models can easily capture the strong correlation\npreference of sensitive features and thus cause recommendation unfairness.\nDiffusion model (DM) as a new generative model paradigm has achieved great\nsuccess in recommendation systems. DM's ability to model uncertainty and\nrepresent diversity, and its modeling mechanism has a high degree of\nadaptability with the real-world recommendation process with bias. Therefore,\nwe use DM to effectively model the fairness of recommendation and enhance the\ndiversity. This paper proposes a FairGENerative sequential Recommendation model\nbased on DM, FairGENRec. In the training phase, we inject random noise into the\noriginal distribution under the guidance of the sensitive feature recognition\nmodel, and a sequential denoise model is designed for the reverse\nreconstruction of items. Simultaneously, recommendation fairness modeling is\ncompleted by injecting multi-interests representational information that\neliminates the bias of sensitive user features into the generated results. In\nthe inference phase, the model obtains the noise in the form of noise addition\nby using the history interactions which is followed by reverse iteration to\nreconstruct the target item representation. Finally, our extensive experiments\non three datasets demonstrate the dual enhancement effect of FairGENRec on\naccuracy and fairness, while the statistical analysis of the cases visualizes\nthe degree of improvement on the fairness of the recommendation.\n","authors":["Yang Liu","Feng Wu","Xuefang Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.19777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11364v1","updated":"2025-07-15T14:32:49Z","published":"2025-07-15T14:32:49Z","title":"From Chaos to Automation: Enabling the Use of Unstructured Data for\n  Robotic Process Automation","summary":"  The growing volume of unstructured data within organizations poses\nsignificant challenges for data analysis and process automation. Unstructured\ndata, which lacks a predefined format, encompasses various forms such as\nemails, reports, and scans. It is estimated to constitute approximately 80% of\nenterprise data. Despite the valuable insights it can offer, extracting\nmeaningful information from unstructured data is more complex compared to\nstructured data. Robotic Process Automation (RPA) has gained popularity for\nautomating repetitive tasks, improving efficiency, and reducing errors.\nHowever, RPA is traditionally reliant on structured data, limiting its\napplication to processes involving unstructured documents. This study addresses\nthis limitation by developing the UNstructured Document REtrieval SyStem\n(UNDRESS), a system that uses fuzzy regular expressions, techniques for natural\nlanguage processing, and large language models to enable RPA platforms to\neffectively retrieve information from unstructured documents. The research\ninvolved the design and development of a prototype system, and its subsequent\nevaluation based on text extraction and information retrieval performance. The\nresults demonstrate the effectiveness of UNDRESS in enhancing RPA capabilities\nfor unstructured data, providing a significant advancement in the field. The\nfindings suggest that this system could facilitate broader RPA adoption across\nprocesses traditionally hindered by unstructured data, thereby improving\noverall business process efficiency.\n","authors":["Kelly Kurowski","Xixi Lu","Hajo A. Reijers"],"pdf_url":"https://arxiv.org/pdf/2507.11364v1.pdf","comment":"Accepted at AUTOMATE 2025"},{"id":"http://arxiv.org/abs/2504.07112v4","updated":"2025-07-15T12:07:42Z","published":"2025-03-20T08:38:57Z","title":"Are AI Agents interacting with Online Ads?","summary":"  As AI-driven agents become increasingly integrated into the digital\necosystem, they reshape how online advertising is perceived and processed.\nParticularly in the travel and hotel booking sector, these autonomous systems\ninfluence the effectiveness of traditional advertising formats. While visual\ncues and emotional appeals sway human users, AI agents prioritize structured\ndata such as price, availability, and specifications. This study examines how\ndifferent AI agents interact with online advertising, whether they incorporate\nads into their decision-making processes, and which ad formats prove most\neffective. We analyze interaction patterns, click behavior, and decision-making\nstrategies through experiments with multimodal language models such as OpenAI\nGPT-4o, Anthropic Claude 3.7 Sonnet, and Google Gemini 2.0 Flash. Our findings\nreveal that AI agents neither ignore nor systematically avoid advertisements\nbut instead favor certain features-particularly keywords and structured data.\nThese insights have significant implications for the future design of\nadvertising strategies in AI-dominated digital environments.\n","authors":["Andreas Stöckl","Joel Nitu"],"pdf_url":"https://arxiv.org/pdf/2504.07112v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.02962v3","updated":"2025-07-15T12:01:49Z","published":"2025-06-30T09:02:45Z","title":"RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs\n  through Multi-query Parallelism","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while they remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have explored enhancing models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to the single-query mode. In this paper, we propose\nRAG-R1, a novel training framework designed to enable LLMs to adaptively\nleverage internal and external knowledge during the reasoning process. We\nfurther expand the generation and retrieval processes within the framework from\nsingle-query mode to multi-query parallelism, aimed at reducing inference time\nand enhancing the model's capabilities. Extensive experiments on seven\nquestion-answering benchmarks demonstrate that our method outperforms the\nstrongest baseline by up to 13.2% and decreases inference time by 11.1%.\n","authors":["Zhiwen Tan","Jiaming Huang","Qintong Wu","Hongxuan Zhang","Chenyi Zhuang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2507.02962v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.21596v2","updated":"2025-07-15T09:14:31Z","published":"2025-06-18T19:31:35Z","title":"Evaluating Multimodal Large Language Models on Educational Textbook\n  Question Answering","summary":"  Multimodal large language models (MLLMs) have shown success in\nvision-language tasks, but their ability to reason over complex educational\nmaterials remains largely untested. This work presents the first evaluation of\nstate-of-the-art MLLMs, including LLaVA-1.5 and LLaMA 3.2-Vision, on the\ntextbook question answering (TQA) task using the CK12-QA dataset. We introduce\na multimodal retrieval-augmented generation (RAG) pipeline to simulate\nreal-world learning by providing relevant lesson paragraphs and diagrams as\ncontext. Our zero-shot experiments reveal a critical trade-off: while retrieved\ncontext improves LLaVA's performance on text-based questions, it significantly\ndegrades the accuracy of the more powerful LLaMA 3.2-Vision on diagram-based\ntasks, dropping its validation accuracy from 74.07% to 25.93%. We term this\nstatistically significant phenomenon \"catastrophic context interference.\"\nFurthermore, fine-tuning highlights architectural differences: LLaMA\n3.2-Vision's performance improves to 71.16% on the test set, demonstrating its\ncapacity to learn multimodal integration, whereas LLaVA's performance declines,\nindicating challenges with generalization. Our results underscore the\nchallenges MLLMs face in modality prioritization and context integration,\nproviding a benchmark and pointing to key directions for developing more robust\nAI-driven educational tools.\n","authors":["Hessa A. Alawwad","Anas Zafar","Areej Alhothali","Usman Naseem","Ali Alkhathlan","Amani Jamal"],"pdf_url":"https://arxiv.org/pdf/2506.21596v2.pdf","comment":"8 Pages"},{"id":"http://arxiv.org/abs/2409.15346v3","updated":"2025-07-15T07:51:12Z","published":"2024-09-10T13:46:14Z","title":"Big data searching using words","summary":"  Big data analytics is one of the most promising areas of new research and\ndevelopment in computer science, enterprises, e-commerce, and defense. For many\norganizations, big data is considered one of their most important strategic\nassets. This explosive growth has made it necessary to develop effective\ntechniques for examining and analyzing big data from mathematical perspectives.\nAmong various methods of analyzing big data, topological data analysis (TDA) is\nnow considered one of the useful tools. However, there is no fundamental\nconcept related to the topological structure in big data. In this paper, we\npresent fundamental concepts related to the neighborhood structures of words in\nbig data search, laying the groundwork for developing topological frameworks\nfor big data in the future. We also introduce the notion of big data primal\nwithin the context of big data search and explore how neighborhood structures,\ncombined with the Jaccard similarity coefficient, can be utilized to detect\nanomalies in search behavior.\n","authors":["Santanu Acharjee","Ripunjoy Choudhury"],"pdf_url":"https://arxiv.org/pdf/2409.15346v3.pdf","comment":"accepted for publication, Acceptance month: July, 2025"},{"id":"http://arxiv.org/abs/2507.11042v1","updated":"2025-07-15T07:11:29Z","published":"2025-07-15T07:11:29Z","title":"Aligned Query Expansion: Efficient Query Expansion for Information\n  Retrieval through LLM Alignment","summary":"  With the breakthroughs in large language models (LLMs), query generation\ntechniques that expand documents and queries with related terms are becoming\nincreasingly popular in the information retrieval field. Such techniques have\nbeen shown to improve the effectiveness of traditional lexical retrieval\nmethods by dealing with the vocabulary mismatch problem. Recent work has found\nthat generating queries with a greedy decoding strategy can produce sub-optimal\nqueries, including hallucinations, and proposed to filter out queries before\nexpansion. This `generate-then-filter' approach is costly, as it requires\ngenerating multiple queries and applying a relevance model to all of them and\ndoes not teach the LLM which of the generated queries is more effective for\nexpansion. To overcome such limitations, we propose Aligned Query Expansion\n(AQE), a novel approach to enhance query expansion for passage retrieval in\nopen-domain question answering. AQE leverages recent techniques in LLM\nalignment to fine-tune models for generating query expansions that directly\noptimize the effectiveness of the retrieval task, eliminating the need for\nadditional filtering steps. This alignment ensures that queries are more\nrelevant, reducing computational costs while improving retrieval effectiveness.\nEmpirical evaluations show that AQE outperforms baseline models for query\nexpansion in both in-domain and out-of-domain settings, demonstrating\nsignificant improvements in retrieval effectiveness.\n","authors":["Adam Yang","Gustavo Penha","Enrico Palumbo","Hugues Bouchard"],"pdf_url":"https://arxiv.org/pdf/2507.11042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10953v1","updated":"2025-07-15T03:34:00Z","published":"2025-07-15T03:34:00Z","title":"Unraveling the Biomarker Prospects of High-Altitude Diseases: Insights\n  from Biomolecular Event Network Constructed using Text Mining","summary":"  High-altitude diseases (HAD), encompassing acute mountain sickness (AMS),\nhigh-altitude cerebral edema (HACE), and high-altitude pulmonary edema (HAPE),\nare triggered by hypobaric hypoxia at elevations above 2,500 meters. These\nconditions pose significant health risks, yet the molecular mechanisms remain\ninsufficiently understood. In this study, we developed a biomolecular event\nextraction pipeline integrating supervised machine learning with feature-based\nand multiscale Laplacian graph kernels to analyze 7,847 curated HAD-related\nabstracts from PubMed. We extracted over 150 unique biomolecular events\nincluding gene expression, regulation, binding, and localization and\nconstructed a weighted, undirected biomolecular event network comprising 97\nnodes and 153 edges. Using the PageRank algorithm, we prioritized key\nbiomolecules based on their centrality within the event network. The top-ranked\nproteins included Erythropoietin (EPO) (0.0163), Vascular endothelial growth\nfactor (VEGF) (0.0148), Hypoxia-inducible factor 1 (HIF-1) alpha (0.0136),\nEndothelial PAS Domain Protein 1 (EPAS1) and Angiotensin-Converting Enzyme\n(ACE) (0.0119), Egl nine homolog 1 (EGLN1), Endothelin 1 (ET-1), and 70\nkilodalton heat shock protein (Hsp70)(0.0118), all of which play crucial roles\nin oxygen sensing, vascular remodeling, erythropoiesis, and blood pressure\nregulation. Subnetwork analysis revealed three major functional clusters\ncentered on hypoxia response, inflammation, and stress adaptation pathways. Our\nintegrative approach demonstrates the utility of large-scale text mining and\ngraph-based analysis to uncover mechanistic insights and prioritize potential\nbiomarkers for high-altitude disease.\n","authors":["Balu Bhasuran","Sabenabanu Abdulkadhar","Jeyakumar Natarajan"],"pdf_url":"https://arxiv.org/pdf/2507.10953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17507v2","updated":"2025-07-15T03:20:06Z","published":"2024-06-25T12:47:04Z","title":"CART: A Generative Cross-Modal Retrieval Framework with Coarse-To-Fine\n  Semantic Modeling","summary":"  Cross-modal retrieval aims to search for instances, which are semantically\nrelated to the query through the interaction of different modal data.\nTraditional solutions utilize a single-tower or dual-tower framework to\nexplicitly compute the score between queries and candidates, which is\nchallenged by training cost and inference latency with large-scale data.\nInspired by the remarkable performance and efficiency of generative models, we\npropose a generative cross-modal retrieval framework (CART) based on\ncoarse-to-fine semantic modeling, which assigns identifiers to each candidate\nand treats the generating identifier as the retrieval target. Specifically, we\nexplore an effective coarse-to-fine scheme, combining K-Means and RQ-VAE to\ndiscretize multimodal data into token sequences that support autoregressive\ngeneration. Further, considering the lack of explicit interaction between\nqueries and candidates, we propose a feature fusion strategy to align their\nsemantics. Extensive experiments demonstrate the effectiveness of the\nstrategies in the CART, achieving excellent results in both retrieval\nperformance and efficiency.\n","authors":["Minghui Fang","Shengpeng Ji","Jialong Zuo","Hai Huang","Yan Xia","Jieming Zhu","Xize Cheng","Xiaoda Yang","Wenrui Liu","Gang Wang","Zhenhua Dong","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.17507v2.pdf","comment":"ACL 2025 Main"},{"id":"http://arxiv.org/abs/2403.06372v3","updated":"2025-07-15T02:45:05Z","published":"2024-03-11T01:50:41Z","title":"Repeated Padding+: Simple yet Effective Data Augmentation Plugin for\n  Sequential Recommendation","summary":"  Sequential recommendation aims to provide users with personalized suggestions\nbased on their historical interactions. When training sequential models,\npadding is a widely adopted technique for two main reasons: 1) The vast\nmajority of models can only handle fixed-length sequences; 2) Batching-based\ntraining needs to ensure that the sequences in each batch have the same length.\nThe special value \\emph{0} is usually used as the padding content, which does\nnot contain the actual information and is ignored in the model calculations.\nThis common-sense padding strategy leads us to a problem that has never been\nexplored before: Can we fully utilize this idle input space by padding other\ncontent to further improve model performance and training efficiency?\n  In this work, we propose a simple yet effective padding method called\nRepeated Padding+ (RepPad+). Specifically, we use the original interaction\nsequences as the padding content and fill it to the padding positions during\nmodel training. This operation can be performed a finite number of times or\nrepeated until the input sequences' length reaches the maximum limit. For those\nsequences that can not pad full original data, we draw inspiration from the\nSliding Windows strategy and intercept consecutive subsequences to fill in the\nidle space. Our RepPad+ can be viewed as a sequence-level data augmentation\nstrategy. Unlike most existing works, our method contains no trainable\nparameters or hyperparameters and is a plug-and-play data augmentation\noperation. Extensive experiments on various categories of sequential models and\nseven real-world datasets demonstrate the effectiveness and efficiency of our\napproach. The average recommendation performance improvement is up to 84.11% on\nGRU4Rec and 35.34% on SASRec. We also provide in-depth analysis and explanation\nof what makes RepPad+ effective from multiple perspectives.\n","authors":["Yizhou Dang","Yuting Liu","Enneng Yang","Guibing Guo","Linying Jiang","Jianzhe Zhao","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06372v3.pdf","comment":"v1: Initial Version, v2:Accepted by RecSys 2024, v3:Extended Version\n  Under Review"},{"id":"http://arxiv.org/abs/2507.10917v1","updated":"2025-07-15T02:13:54Z","published":"2025-07-15T02:13:54Z","title":"LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation","summary":"  Recently, much effort has been devoted to modeling users' multi-interests\nbased on their behaviors or auxiliary signals. However, existing methods often\nrely on heuristic assumptions, e.g., co-occurring items indicate the same\ninterest of users, failing to capture user multi-interests aligning with\nreal-world scenarios. While large language models (LLMs) show significant\npotential for multi-interest analysis due to their extensive knowledge and\npowerful reasoning capabilities, two key challenges remain. First, the\ngranularity of LLM-driven multi-interests is agnostic, possibly leading to\noverly fine or coarse interest grouping. Second, individual user analysis\nprovides limited insights due to the data sparsity issue. In this paper, we\npropose an LLM-driven dual-level multi-interest modeling framework for more\neffective recommendation. At the user-individual level, we exploit LLMs to\nflexibly allocate items engaged by users into different semantic clusters,\nindicating their diverse and distinct interests. To alleviate the agnostic\ngeneration of LLMs, we adaptively assign these semantic clusters to users'\ncollaborative multi-interests learned from global user-item interactions,\nallowing the granularity to be automatically adjusted according to the user's\nbehaviors using an alignment module. To alleviate the limited insights derived\nfrom individual users' behaviors, at the user-crowd level, we propose\naggregating user cliques into synthesized users with rich behaviors for more\ncomprehensive LLM-driven multi-interest analysis. We formulate a max covering\nproblem to ensure the compactness and representativeness of synthesized users'\nbehaviors, and then conduct contrastive learning based on their LLM-driven\nmulti-interests to disentangle item representations among different interests.\nExperiments on real-world datasets show the superiority of our approach against\nstate-of-the-art methods.\n","authors":["Ziyan Wang","Yingpeng Du","Zhu Sun","Jieyi Bi","Haoyan Chua","Tianjun Wei","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.10917v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2506.10077v2","updated":"2025-07-15T00:08:11Z","published":"2025-06-11T18:00:30Z","title":"A quantum semantic framework for natural language processing","summary":"  Semantic degeneracy represents a fundamental property of natural language\nthat extends beyond simple polysemy to encompass the combinatorial explosion of\npotential interpretations that emerges as semantic expressions increase in\ncomplexity. In this work, we argue this property imposes fundamental\nlimitations on Large Language Models (LLMs) and other modern NLP systems,\nprecisely because they operate within natural language itself. Using Kolmogorov\ncomplexity, we demonstrate that as an expression's complexity grows, the amount\nof contextual information required to reliably resolve its ambiguity explodes\ncombinatorially. The computational intractability of recovering a single\nintended meaning for complex or ambiguous text therefore suggests that the\nclassical view that linguistic forms possess intrinsic meaning in and of\nthemselves is conceptually inadequate. We argue instead that meaning is\ndynamically actualized through an observer-dependent interpretive act, a\nprocess whose non-deterministic nature is most appropriately described by a\nnon-classical, quantum-like logic. To test this hypothesis, we conducted a\nsemantic Bell inequality test using diverse LLM agents. Our experiments yielded\naverage CHSH expectation values from 1.2 to 2.8, with several runs producing\nvalues (e.g., 2.3-2.4) in significant violation of the classical boundary\n($|S|\\leq2$), demonstrating that linguistic interpretation under ambiguity can\nexhibit non-classical contextuality, consistent with results from human\ncognition experiments. These results inherently imply that classical\nfrequentist-based analytical approaches for natural language are necessarily\nlossy. Instead, we propose that Bayesian-style repeated sampling approaches can\nprovide more practically useful and appropriate characterizations of linguistic\nmeaning in context.\n","authors":["Christopher J. Agostino","Quan Le Thien","Molly Apsel","Denizhan Pak","Elina Lesyk","Ashabari Majumdar"],"pdf_url":"https://arxiv.org/pdf/2506.10077v2.pdf","comment":"12 pages, 2 figures, accepted submission to Quantum AI and NLP 2025"},{"id":"http://arxiv.org/abs/2507.11764v1","updated":"2025-07-15T22:10:20Z","published":"2025-07-15T22:10:20Z","title":"AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings\n  with Sentiment for Subjectivity Detection in News Articles","summary":"  This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).\n","authors":["Matteo Fasulo","Luca Babboni","Luca Tedeschini"],"pdf_url":"https://arxiv.org/pdf/2507.11764v1.pdf","comment":"14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab"},{"id":"http://arxiv.org/abs/2507.11272v1","updated":"2025-07-15T12:49:42Z","published":"2025-07-15T12:49:42Z","title":"An Empirical Study of Multi-Agent RAG for Real-World University\n  Admissions Counseling","summary":"  This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University\nAdmission System), a real-world deployment of a conversational AI platform for\nhigher education admissions counseling in Vietnam. While large language models\n(LLMs) offer potential for automating advisory tasks, most existing solutions\nremain limited to prototypes or synthetic benchmarks. MARAUS addresses this gap\nby combining hybrid retrieval, multi-agent orchestration, and LLM-based\ngeneration into a system tailored for real-world university admissions. In\ncollaboration with the University of Transport Technology (UTT) in Hanoi, we\nconducted a two-phase study involving technical development and real-world\nevaluation. MARAUS processed over 6,000 actual user interactions, spanning six\ncategories of queries. Results show substantial improvements over LLM-only\nbaselines: on average 92 percent accuracy, hallucination rates reduced from 15\nprecent to 1.45 percent, and average response times below 4 seconds. The system\noperated cost-effectively, with a two-week deployment cost of 11.58 USD using\nGPT-4o mini. This work provides actionable insights for the deployment of\nagentic RAG systems in low-resource educational settings.\n","authors":["Anh Nguyen-Duc","Chien Vu Manh","Bao Anh Tran","Viet Phuong Ngo","Luan Le Chi","Anh Quang Nguyen"],"pdf_url":"https://arxiv.org/pdf/2507.11272v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2507.11539v1","updated":"2025-07-15T17:59:57Z","published":"2025-07-15T17:59:57Z","title":"Streaming 4D Visual Geometry Transformer","summary":"  Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.\n","authors":["Dong Zhuo","Wenzhao Zheng","Jiahe Guo","Yuqi Wu","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2507.11539v1.pdf","comment":"Code is available at: https://github.com/wzzheng/StreamVGGT"},{"id":"http://arxiv.org/abs/2507.11535v1","updated":"2025-07-15T17:58:55Z","published":"2025-07-15T17:58:55Z","title":"Canonical Bayesian Linear System Identification","summary":"  Standard Bayesian approaches for linear time-invariant (LTI) system\nidentification are hindered by parameter non-identifiability; the resulting\ncomplex, multi-modal posteriors make inference inefficient and impractical. We\nsolve this problem by embedding canonical forms of LTI systems within the\nBayesian framework. We rigorously establish that inference in these minimal\nparameterizations fully captures all invariant system dynamics (e.g., transfer\nfunctions, eigenvalues, predictive distributions of system outputs) while\nresolving identifiability. This approach unlocks the use of meaningful,\nstructure-aware priors (e.g., enforcing stability via eigenvalues) and ensures\nconditions for a Bernstein--von Mises theorem -- a link between Bayesian and\nfrequentist large-sample asymptotics that is broken in standard forms.\nExtensive simulations with modern MCMC methods highlight advantages over\nstandard parameterizations: canonical forms achieve higher computational\nefficiency, generate interpretable and well-behaved posteriors, and provide\nrobust uncertainty estimates, particularly from limited data.\n","authors":["Andrey Bryutkin","Matthew E. Levine","Iñigo Urteaga","Youssef Marzouk"],"pdf_url":"https://arxiv.org/pdf/2507.11535v1.pdf","comment":"46 pages, 9 figures"},{"id":"http://arxiv.org/abs/2507.11531v1","updated":"2025-07-15T17:57:48Z","published":"2025-07-15T17:57:48Z","title":"Langevin Flows for Modeling Neural Latent Dynamics","summary":"  Neural populations exhibit latent dynamical structures that drive\ntime-evolving spiking activities, motivating the search for models that capture\nboth intrinsic network dynamics and external unobserved influences. In this\nwork, we introduce LangevinFlow, a sequential Variational Auto-Encoder where\nthe time evolution of latent variables is governed by the underdamped Langevin\nequation. Our approach incorporates physical priors -- such as inertia,\ndamping, a learned potential function, and stochastic forces -- to represent\nboth autonomous and non-autonomous processes in neural systems. Crucially, the\npotential function is parameterized as a network of locally coupled\noscillators, biasing the model toward oscillatory and flow-like behaviors\nobserved in biological neural populations. Our model features a recurrent\nencoder, a one-layer Transformer decoder, and Langevin dynamics in the latent\nspace. Empirically, our method outperforms state-of-the-art baselines on\nsynthetic neural populations generated by a Lorenz attractor, closely matching\nground-truth firing rates. On the Neural Latents Benchmark (NLB), the model\nachieves superior held-out neuron likelihoods (bits per spike) and forward\nprediction accuracy across four challenging datasets. It also matches or\nsurpasses alternative methods in decoding behavioral metrics such as hand\nvelocity. Overall, this work introduces a flexible, physics-inspired,\nhigh-performing framework for modeling complex neural population dynamics and\ntheir unobserved influences.\n","authors":["Yue Song","T. Anderson Keller","Yisong Yue","Pietro Perona","Max Welling"],"pdf_url":"https://arxiv.org/pdf/2507.11531v1.pdf","comment":"Full version of the Cognitive Computational Neuroscience (CCN) 2025\n  poster"},{"id":"http://arxiv.org/abs/2507.07986v2","updated":"2025-07-15T17:54:16Z","published":"2025-07-10T17:57:46Z","title":"EXPO: Stable Reinforcement Learning with Expressive Policies","summary":"  We study the problem of training and fine-tuning expressive policies with\nonline reinforcement learning (RL) given an offline dataset. Training\nexpressive policy classes with online RL present a unique challenge of stable\nvalue maximization. Unlike simpler Gaussian policies commonly used in online\nRL, expressive policies like diffusion and flow-matching policies are\nparameterized by a long denoising chain, which hinders stable gradient\npropagation from actions to policy parameters when optimizing against some\nvalue function. Our key insight is that we can address stable value\nmaximization by avoiding direct optimization over value with the expressive\npolicy and instead construct an on-the-fly RL policy to maximize Q-value. We\npropose Expressive Policy Optimization (EXPO), a sample-efficient online RL\nalgorithm that utilizes an on-the-fly policy to maximize value with two\nparameterized policies -- a larger expressive base policy trained with a stable\nimitation learning objective and a light-weight Gaussian edit policy that edits\nthe actions sampled from the base policy toward a higher value distribution.\nThe on-the-fly policy optimizes the actions from the base policy with the\nlearned edit policy and chooses the value maximizing action from the base and\nedited actions for both sampling and temporal-difference (TD) backup. Our\napproach yields up to 2-3x improvement in sample efficiency on average over\nprior methods both in the setting of fine-tuning a pretrained policy given\noffline data and in leveraging offline data to train online.\n","authors":["Perry Dong","Qiyang Li","Dorsa Sadigh","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2507.07986v2.pdf","comment":"corrected typo, formatting, added experiments"},{"id":"http://arxiv.org/abs/2507.11522v1","updated":"2025-07-15T17:47:01Z","published":"2025-07-15T17:47:01Z","title":"CATVis: Context-Aware Thought Visualization","summary":"  EEG-based brain-computer interfaces (BCIs) have shown promise in various\napplications, such as motor imagery and cognitive state monitoring. However,\ndecoding visual representations from EEG signals remains a significant\nchallenge due to their complex and noisy nature. We thus propose a novel\n5-stage framework for decoding visual representations from EEG signals: (1) an\nEEG encoder for concept classification, (2) cross-modal alignment of EEG and\ntext embeddings in CLIP feature space, (3) caption refinement via re-ranking,\n(4) weighted interpolation of concept and caption embeddings for richer\nsemantics, and (5) image generation using a pre-trained Stable Diffusion model.\nWe enable context-aware EEG-to-image generation through cross-modal alignment\nand re-ranking. Experimental results demonstrate that our method generates\nhigh-quality images aligned with visual stimuli, outperforming SOTA approaches\nby 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and\nreducing Fr\\'echet Inception Distance by 36.61%, indicating superior semantic\nalignment and image quality.\n","authors":["Tariq Mehmood","Hamza Ahmad","Muhammad Haroon Shakeel","Murtaza Taj"],"pdf_url":"https://arxiv.org/pdf/2507.11522v1.pdf","comment":"Accepted at MICCAI 2025. This is the submitted version prior to peer\n  review. The final Version of Record will appear in the MICCAI 2025\n  proceedings (Springer LNCS)"},{"id":"http://arxiv.org/abs/2502.19417v2","updated":"2025-07-15T17:44:28Z","published":"2025-02-26T18:58:41Z","title":"Hi Robot: Open-Ended Instruction Following with Hierarchical\n  Vision-Language-Action Models","summary":"  Generalist robots that can perform a range of different tasks in open-world\nsettings must be able to not only reason about the steps needed to accomplish\ntheir goals, but also process complex instructions, prompts, and even feedback\nduring task execution. Intricate instructions (e.g., \"Could you make me a\nvegetarian sandwich?\" or \"I don't like that one\") require not just the ability\nto physically perform the individual steps, but the ability to situate complex\ncommands and feedback in the physical world. In this work, we describe a system\nthat uses vision-language models in a hierarchical structure, first reasoning\nover complex prompts and user feedback to deduce the most appropriate next step\nto fulfill the task, and then performing that step with low-level actions. In\ncontrast to direct instruction following methods that can fulfill simple\ncommands (\"pick up the cup\"), our system can reason through complex prompts and\nincorporate situated feedback during task execution (\"that's not trash\"). We\nevaluate our system across three robotic platforms, including single-arm,\ndual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks\nsuch as cleaning messy tables, making sandwiches, and grocery shopping. Videos\nare available at https://www.pi.website/research/hirobot\n","authors":["Lucy Xiaoyang Shi","Brian Ichter","Michael Equi","Liyiming Ke","Karl Pertsch","Quan Vuong","James Tanner","Anna Walling","Haohuan Wang","Niccolo Fusai","Adrian Li-Bell","Danny Driess","Lachy Groom","Sergey Levine","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2502.19417v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2507.11515v1","updated":"2025-07-15T17:36:37Z","published":"2025-07-15T17:36:37Z","title":"AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of\n  LLM over the Air","summary":"  Operating Large Language Models (LLMs) on edge devices is increasingly\nchallenged by limited communication bandwidth and strained computational and\nmemory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.\nNevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ\nfixed or heuristic rank configurations, and the subsequent over-the-air\ntransmission of all LoRA parameters could be rather inefficient. To address\nthis limitation, we develop AirLLM, a hierarchical diffusion policy framework\nfor communication-aware LoRA adaptation. Specifically, AirLLM models the rank\nconfiguration as a structured action vector that spans all LoRA-inserted\nprojections. To solve the underlying high-dimensional sequential\ndecision-making problem, a Proximal Policy Optimization (PPO) agent generates\ncoarse-grained decisions by jointly observing wireless states and linguistic\ncomplexity, which are then refined via Denoising Diffusion Implicit Models\n(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The\ntwo modules are optimized alternatively, with the DDIM trained under the\nClassifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.\nExperiments under varying signal-to-noise ratios demonstrate that AirLLM\nconsistently enhances fine-tuning performance while significantly reducing\ntransmission costs, highlighting the effectiveness of reinforcement-driven,\ndiffusion-refined rank adaptation for scalable and efficient remote fine-tuning\nover the air.\n","authors":["Shiyi Yang","Xiaoxue Yu","Rongpeng Li","Jianhang Zhu","Zhifeng Zhao","Honggang Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.11515v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.08156v5","updated":"2025-07-15T17:27:07Z","published":"2025-01-14T14:31:45Z","title":"Are DeepSeek R1 And Other Reasoning Models More Faithful?","summary":"  Language models trained to solve reasoning tasks via reinforcement learning\nhave achieved striking results. We refer to these models as reasoning models.\nAre the Chains of Thought (CoTs) of reasoning models more faithful than\ntraditional models? We evaluate three reasoning models (based on Qwen-2.5,\nGemini-2, and DeepSeek-V3-Base) on an existing test of faithful CoT. To measure\nfaithfulness, we test whether models can describe how a cue in their prompt\ninfluences their answer to MMLU questions. For example, when the cue \"A\nStanford Professor thinks the answer is D\" is added to the prompt, models\nsometimes switch their answer to D. In such cases, the DeepSeek-R1 reasoning\nmodel describes the cue's influence 59% of the time, compared to 7% for the\nnon-reasoning DeepSeek model. We evaluate seven types of cue, such as\nmisleading few-shot examples and suggestive follow-up questions from the user.\nReasoning models describe cues that influence them much more reliably than all\nthe non-reasoning models tested (including Claude-3.5-Sonnet and GPT-4o). In an\nadditional experiment, we provide evidence suggesting that the use of reward\nmodels causes less faithful responses -- which may help explain why\nnon-reasoning models are less faithful. Our study has two main limitations.\nFirst, we test faithfulness using a set of artificial tasks, which may not\nreflect realistic use-cases. Second, we only measure one specific aspect of\nfaithfulness -- whether models can describe the influence of cues. Future\nresearch should investigate whether the advantage of reasoning models in\nfaithfulness holds for a broader set of tests. Still, we think this increase in\nfaithfulness is promising for the explainability of language models.\n","authors":["James Chua","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2501.08156v5.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.17787v2","updated":"2025-07-15T17:21:32Z","published":"2024-10-23T11:37:20Z","title":"Large Language Models Engineer Too Many Simple Features For Tabular Data","summary":"  Tabular machine learning problems often require time-consuming and\nlabor-intensive feature engineering. Recent efforts have focused on using large\nlanguage models (LLMs) to capitalize on their potential domain knowledge. At\nthe same time, researchers have observed ethically concerning negative biases\nin other LLM-related use cases, such as text generation. These developments\nmotivated us to investigate whether LLMs exhibit a bias that negatively impacts\nthe performance of feature engineering. While not ethically concerning, such a\nbias could hinder practitioners from fully utilizing LLMs for automated data\nscience. Therefore, we propose a method to detect potential biases by detecting\nanomalies in the frequency of operators (e.g., adding two features) suggested\nby LLMs when engineering new features. Our experiments evaluate the bias of\nfour LLMs, two big frontier and two small open-source models, across 27 tabular\ndatasets. Our results indicate that LLMs are biased toward simple operators,\nsuch as addition, and can fail to utilize more complex operators, such as\ngrouping followed by aggregations. Furthermore, the bias can negatively impact\nthe predictive performance when using LLM-generated features. Our results call\nfor mitigating bias when using LLMs for feature engineering.\n","authors":["Jaris Küken","Lennart Purucker","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2410.17787v2.pdf","comment":"Accepted at the 3rd Table Representation Learning Workshop @ NeurIPS\n  2024"},{"id":"http://arxiv.org/abs/2507.11506v1","updated":"2025-07-15T17:21:31Z","published":"2025-07-15T17:21:31Z","title":"Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep\n  Learning Compiler Techniques","summary":"  To meet the increasing demand of deep learning (DL) models, AI chips are\nemploying both off-chip memory (e.g., HBM) and high-bandwidth low-latency\ninterconnect for direct inter-core data exchange. However, it is not easy to\nexplore the efficiency of these inter-core connected AI (ICCA) chips, due to a\nfundamental tussle among compute (per-core execution), communication\n(inter-core data exchange), and I/O (off-chip data access).\n  In this paper, we develop Elk, a DL compiler framework to maximize the\nefficiency of ICCA chips by jointly trading off all the three performance\nfactors discussed above. Elk structures these performance factors into\nconfigurable parameters and forms a global trade-off space in the DL compiler.\nTo systematically explore this space and maximize overall efficiency, Elk\nemploys a new inductive operator scheduling policy and a cost-aware on-chip\nmemory allocation algorithm. It generates globally optimized execution plans\nthat best overlap off-chip data loading and on-chip execution. To examine the\nefficiency of Elk, we build a full-fledged emulator based on a real ICCA chip\nIPU-POD4, and an ICCA chip simulator for sensitivity analysis with different\ninterconnect network topologies. Elk achieves 94% of the ideal roofline\nperformance of ICCA chips on average, showing the benefits of supporting large\nDL models on ICCA chips. We also show Elk's capability of enabling architecture\ndesign space exploration for new ICCA chip development.\n","authors":["Yiqi Liu","Yuqi Xue","Noelle Crawford","Jilong Xue","Jian Huang"],"pdf_url":"https://arxiv.org/pdf/2507.11506v1.pdf","comment":"This paper is accepted at the 58th IEEE/ACM International Symposium\n  on Microarchitecture (MICRO'25)"},{"id":"http://arxiv.org/abs/2507.06565v3","updated":"2025-07-15T17:19:46Z","published":"2025-07-09T05:39:56Z","title":"A Mathematical Theory of Discursive Networks","summary":"  Large-language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source \\emph{Flaws-of-Others (FOO) algorithm}: a configurable loop in\nwhich any set of agents critique one another while a harmonizer merges their\nverdicts. We identify an ethical transgression, epithesis, that occurs when\nhumans fail to engage in the discursive network. The takeaway is practical and\ncultural: reliability in this new medium comes not from perfecting single\nmodels but from connecting imperfect ones into networks that enforce mutual\naccountability.\n","authors":["Juan B. Gutiérrez"],"pdf_url":"https://arxiv.org/pdf/2507.06565v3.pdf","comment":"32 pages, 4 figures, 4 tables, 1 algorithm, 54 references"},{"id":"http://arxiv.org/abs/2411.04371v3","updated":"2025-07-15T17:05:53Z","published":"2024-11-07T02:04:34Z","title":"ComFairGNN: Community Fair Graph Neural Network","summary":"  Graph Neural Networks (GNNs) have become the leading approach for addressing\ngraph analytical problems in various real-world scenarios. However, GNNs may\nproduce biased predictions against certain demographic subgroups due to node\nattributes and neighbors surrounding a node. Most current research on GNN\nfairness focuses predominantly on debiasing GNNs using oversimplified fairness\nevaluation metrics, which can give a misleading impression of fairness.\nUnderstanding the potential evaluation paradoxes due to the complicated nature\nof the graph structure is crucial for developing effective GNN debiasing\nmechanisms. In this paper, we examine the effectiveness of current GNN\ndebiasing methods in terms of unfairness evaluation. Specifically, we introduce\na community-level strategy to measure bias in GNNs and evaluate debiasing\nmethods at this level. Further, We introduce ComFairGNN, a novel framework\ndesigned to mitigate community-level bias in GNNs. Our approach employs a\nlearnable coreset-based debiasing function that addresses bias arising from\ndiverse local neighborhood distributions during GNNs neighborhood aggregation.\nComprehensive evaluations on three benchmark datasets demonstrate our model's\neffectiveness in both accuracy and fairness metrics.\n","authors":["Yonas Sium","Qi Li"],"pdf_url":"https://arxiv.org/pdf/2411.04371v3.pdf","comment":"PAKDD 2025"},{"id":"http://arxiv.org/abs/2411.08706v2","updated":"2025-07-15T17:04:14Z","published":"2024-11-13T15:50:32Z","title":"Searching Latent Program Spaces","summary":"  General intelligence requires systems that acquire new skills efficiently and\ngeneralize beyond their training distributions. Although program synthesis\napproaches have strong generalization power, they face scaling issues due to\nlarge combinatorial spaces that quickly make them impractical and require\nhuman-generated DSLs or pre-trained priors to narrow this search space. On the\nother hand, deep learning methods have had high successes, but they lack\nstructured test-time adaptation and rely on heavy stochastic sampling or\nexpensive gradient updates for fine-tuning. In this work, we propose the Latent\nProgram Network (LPN), a new architecture that builds in test-time search\ndirectly into neural models. LPN learns a latent space of implicit\nprograms--neurally mapping inputs to outputs--through which it can search using\ngradients at test time. LPN combines the adaptability of symbolic approaches\nand the scalability of neural methods. It searches through a compact latent\nspace at test time and bypasses the need for pre-defined domain-specific\nlanguages. On a range of programming-by-examples tasks, LPN either outperforms\nor matches performance compared to in-context learning and test-time training\nmethods. Tested on the ARC-AGI benchmark, we demonstrate that LPN can both\nlearn a compact program space and search through it at test time to adapt to\nnovel tasks. LPN doubles its performance on out-of-distribution tasks when\ntest-time search is switched on.\n","authors":["Matthew V Macfarlane","Clément Bonnet"],"pdf_url":"https://arxiv.org/pdf/2411.08706v2.pdf","comment":"Code available at https://github.com/clement-bonnet/lpn"},{"id":"http://arxiv.org/abs/2507.07969v2","updated":"2025-07-15T16:59:35Z","published":"2025-07-10T17:48:03Z","title":"Reinforcement Learning with Action Chunking","summary":"  We present Q-chunking, a simple yet effective recipe for improving\nreinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.\nOur recipe is designed for the offline-to-online RL setting, where the goal is\nto leverage an offline prior dataset to maximize the sample-efficiency of\nonline learning. Effective exploration and sample-efficient learning remain\ncentral challenges in this setting, as it is not obvious how the offline data\nshould be utilized to acquire a good exploratory policy. Our key insight is\nthat action chunking, a technique popularized in imitation learning where\nsequences of future actions are predicted rather than a single action at each\ntimestep, can be applied to temporal difference (TD)-based RL methods to\nmitigate the exploration challenge. Q-chunking adopts action chunking by\ndirectly running RL in a 'chunked' action space, enabling the agent to (1)\nleverage temporally consistent behaviors from offline data for more effective\nonline exploration and (2) use unbiased $n$-step backups for more stable and\nefficient TD learning. Our experimental results demonstrate that Q-chunking\nexhibits strong offline performance and online sample efficiency, outperforming\nprior best offline-to-online methods on a range of long-horizon, sparse-reward\nmanipulation tasks.\n","authors":["Qiyang Li","Zhiyuan Zhou","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2507.07969v2.pdf","comment":"25 pages, 15 figures"},{"id":"http://arxiv.org/abs/2507.11486v1","updated":"2025-07-15T16:57:00Z","published":"2025-07-15T16:57:00Z","title":"Exploring the robustness of TractOracle methods in RL-based tractography","summary":"  Tractography algorithms leverage diffusion MRI to reconstruct the fibrous\narchitecture of the brain's white matter. Among machine learning approaches,\nreinforcement learning (RL) has emerged as a promising framework for\ntractography, outperforming traditional methods in several key aspects.\nTractOracle-RL, a recent RL-based approach, reduces false positives by\nincorporating anatomical priors into the training process via a reward-based\nmechanism. In this paper, we investigate four extensions of the original\nTractOracle-RL framework by integrating recent advances in RL, and we evaluate\ntheir performance across five diverse diffusion MRI datasets. Results\ndemonstrate that combining an oracle with the RL framework consistently leads\nto robust and reliable tractography, regardless of the specific method or\ndataset used. We also introduce a novel RL training scheme called Iterative\nReward Training (IRT), inspired by the Reinforcement Learning from Human\nFeedback (RLHF) paradigm. Instead of relying on human input, IRT leverages\nbundle filtering methods to iteratively refine the oracle's guidance throughout\ntraining. Experimental results show that RL methods trained with oracle\nfeedback significantly outperform widely used tractography techniques in terms\nof accuracy and anatomical validity.\n","authors":["Jeremi Levesque","Antoine Théberge","Maxime Descoteaux","Pierre-Marc Jodoin"],"pdf_url":"https://arxiv.org/pdf/2507.11486v1.pdf","comment":"38 pages, 8 figures. Submitted to Medical Image Analysis"},{"id":"http://arxiv.org/abs/2505.01319v2","updated":"2025-07-15T16:48:15Z","published":"2025-05-02T14:47:21Z","title":"Model See Model Do: Speech-Driven Facial Animation with Style Control","summary":"  Speech-driven 3D facial animation plays a key role in applications such as\nvirtual avatars, gaming, and digital content creation. While existing methods\nhave made significant progress in achieving accurate lip synchronization and\ngenerating basic emotional expressions, they often struggle to capture and\neffectively transfer nuanced performance styles. We propose a novel\nexample-based generation framework that conditions a latent diffusion model on\na reference style clip to produce highly expressive and temporally coherent\nfacial animations. To address the challenge of accurately adhering to the style\nreference, we introduce a novel conditioning mechanism called style basis,\nwhich extracts key poses from the reference and additively guides the diffusion\ngeneration process to fit the style without compromising lip synchronization\nquality. This approach enables the model to capture subtle stylistic cues while\nensuring that the generated animations align closely with the input speech.\nExtensive qualitative, quantitative, and perceptual evaluations demonstrate the\neffectiveness of our method in faithfully reproducing the desired style while\nachieving superior lip synchronization across various speech scenarios.\n","authors":["Yifang Pan","Karan Singh","Luiz Gustavo Hafemann"],"pdf_url":"https://arxiv.org/pdf/2505.01319v2.pdf","comment":"10 pages, 7 figures, SIGGRAPH Conference Papers '25"},{"id":"http://arxiv.org/abs/2507.11473v1","updated":"2025-07-15T16:43:41Z","published":"2025-07-15T16:43:41Z","title":"Chain of Thought Monitorability: A New and Fragile Opportunity for AI\n  Safety","summary":"  AI systems that \"think\" in human language offer a unique opportunity for AI\nsafety: we can monitor their chains of thought (CoT) for the intent to\nmisbehave. Like all other known AI oversight methods, CoT monitoring is\nimperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows\npromise and we recommend further research into CoT monitorability and\ninvestment in CoT monitoring alongside existing safety methods. Because CoT\nmonitorability may be fragile, we recommend that frontier model developers\nconsider the impact of development decisions on CoT monitorability.\n","authors":["Tomek Korbak","Mikita Balesni","Elizabeth Barnes","Yoshua Bengio","Joe Benton","Joseph Bloom","Mark Chen","Alan Cooney","Allan Dafoe","Anca Dragan","Scott Emmons","Owain Evans","David Farhi","Ryan Greenblatt","Dan Hendrycks","Marius Hobbhahn","Evan Hubinger","Geoffrey Irving","Erik Jenner","Daniel Kokotajlo","Victoria Krakovna","Shane Legg","David Lindner","David Luan","Aleksander Mądry","Julian Michael","Neel Nanda","Dave Orr","Jakub Pachocki","Ethan Perez","Mary Phuong","Fabien Roger","Joshua Saxe","Buck Shlegeris","Martín Soto","Eric Steinberger","Jasmine Wang","Wojciech Zaremba","Bowen Baker","Rohin Shah","Vlad Mikulik"],"pdf_url":"https://arxiv.org/pdf/2507.11473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11471v1","updated":"2025-07-15T16:41:31Z","published":"2025-07-15T16:41:31Z","title":"D3FL: Data Distribution and Detrending for Robust Federated Learning in\n  Non-linear Time-series Data","summary":"  With advancements in computing and communication technologies, the Internet\nof Things (IoT) has seen significant growth. IoT devices typically collect data\nfrom various sensors, such as temperature, humidity, and energy meters. Much of\nthis data is temporal in nature. Traditionally, data from IoT devices is\ncentralized for analysis, but this approach introduces delays and increased\ncommunication costs. Federated learning (FL) has emerged as an effective\nalternative, allowing for model training across distributed devices without the\nneed to centralize data. In many applications, such as smart home energy and\nenvironmental monitoring, the data collected by IoT devices across different\nlocations can exhibit significant variation in trends and seasonal patterns.\nAccurately forecasting such non-stationary, non-linear time-series data is\ncrucial for applications like energy consumption estimation and weather\nforecasting. However, these data variations can severely impact prediction\naccuracy. The key contributions of this paper are: (1) Investigating how\nnon-linear, non-stationary time-series data distributions, like generalized\nextreme value (gen-extreme) and log norm distributions, affect FL performance.\n(2) Analyzing how different detrending techniques for non-linear time-series\ndata influence the forecasting model's performance in a FL setup. We generated\nseveral synthetic time-series datasets using non-linear data distributions and\ntrained an LSTM-based forecasting model using both centralized and FL\napproaches. Additionally, we evaluated the impact of detrending on real-world\ndatasets with non-linear time-series data distributions. Our experimental\nresults show that: (1) FL performs worse than centralized approaches when\ndealing with non-linear data distributions. (2) The use of appropriate\ndetrending techniques improves FL performance, reducing loss across different\ndata distributions.\n","authors":["Harsha Varun Marisetty","Manik Gupta","Yogesh Simmhan"],"pdf_url":"https://arxiv.org/pdf/2507.11471v1.pdf","comment":"Preprint of paper to appear in the proceedings of IEEE INTERNATIONAL\n  CONFERENCE ON EDGE COMPUTING & COMMUNICATIONS EDGE 2025"},{"id":"http://arxiv.org/abs/2311.09386v4","updated":"2025-07-15T16:39:10Z","published":"2023-11-15T21:29:57Z","title":"Gram-Schmidt Methods for Unsupervised Feature Extraction and Selection","summary":"  Feature extraction and selection in the presence of nonlinear dependencies\namong the data is a fundamental challenge in unsupervised learning. We propose\nusing a Gram-Schmidt (GS) type orthogonalization process over function spaces\nto detect and map out such dependencies. Specifically, by applying the GS\nprocess over some family of functions, we construct a series of covariance\nmatrices that can either be used to identify new large-variance directions, or\nto remove those dependencies from known directions. In the former case, we\nprovide information-theoretic guarantees in terms of entropy reduction. In the\nlatter, we provide precise conditions by which the chosen function family\neliminates existing redundancy in the data. Each approach provides both a\nfeature extraction and a feature selection algorithm. Our feature extraction\nmethods are linear, and can be seen as natural generalization of principal\ncomponent analysis (PCA). We provide experimental results for synthetic and\nreal-world benchmark datasets which show superior performance over\nstate-of-the-art (linear) feature extraction and selection algorithms.\nSurprisingly, our linear feature extraction algorithms are comparable and often\noutperform several important nonlinear feature extraction methods such as\nautoencoders, kernel PCA, and UMAP. Furthermore, one of our feature selection\nalgorithms strictly generalizes a recent Fourier-based feature selection\nmechanism (Heidari et al., IEEE Transactions on Information Theory, 2022), yet\nat significantly reduced complexity.\n","authors":["Bahram Yaghooti","Netanel Raviv","Bruno Sinopoli"],"pdf_url":"https://arxiv.org/pdf/2311.09386v4.pdf","comment":"To appear in IEEE Transactions on Information Theory"},{"id":"http://arxiv.org/abs/2505.07719v3","updated":"2025-07-15T16:34:42Z","published":"2025-05-12T16:25:00Z","title":"Training neural control variates using correlated configurations","summary":"  Neural control variates (NCVs) have emerged as a powerful tool for variance\nreduction in Monte Carlo (MC) simulations, particularly in high-dimensional\nproblems where traditional control variates are difficult to construct\nanalytically. By training neural networks to learn auxiliary functions\ncorrelated with the target observable, NCVs can significantly reduce estimator\nvariance while preserving unbiasedness. However, a critical but often\noverlooked aspect of NCV training is the role of autocorrelated samples\ngenerated by Markov Chain Monte Carlo (MCMC). While such samples are typically\ndiscarded for error estimation due to their statistical redundancy, they may\ncontain useful information about the structure of the underlying probability\ndistribution that can benefit the training process. In this work, we\nsystematically examine the effect of using correlated configurations in\ntraining neural control variates. We demonstrate, both conceptually and\nnumerically, that training on correlated data can improve control variate\nperformance, especially in settings with limited computational resources. Our\nanalysis includes empirical results from $U(1)$ gauge theory and scalar field\ntheory, illustrating when and how autocorrelated samples enhance NCV\nconstruction. These findings provide practical guidance for the efficient use\nof MCMC data in training neural networks.\n","authors":["Hyunwoo Oh"],"pdf_url":"https://arxiv.org/pdf/2505.07719v3.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.11457v1","updated":"2025-07-15T16:29:45Z","published":"2025-07-15T16:29:45Z","title":"LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis\n  Assessment in Rectal Cancer","summary":"  Accurate preoperative assessment of lymph node (LN) metastasis in rectal\ncancer guides treatment decisions, yet conventional MRI evaluation based on\nmorphological criteria shows limited diagnostic performance. While some\nartificial intelligence models have been developed, they often operate as black\nboxes, lacking the interpretability needed for clinical trust. Moreover, these\nmodels typically evaluate nodes in isolation, overlooking the patient-level\ncontext. To address these limitations, we introduce LRMR, an LLM-Driven\nRelational Multi-node Ranking framework. This approach reframes the diagnostic\ntask from a direct classification problem into a structured reasoning and\nranking process. The LRMR framework operates in two stages. First, a multimodal\nlarge language model (LLM) analyzes a composite montage image of all LNs from a\npatient, generating a structured report that details ten distinct radiological\nfeatures. Second, a text-based LLM performs pairwise comparisons of these\nreports between different patients, establishing a relative risk ranking based\non the severity and number of adverse features. We evaluated our method on a\nretrospective cohort of 117 rectal cancer patients. LRMR achieved an area under\nthe curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of\ndeep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies\nconfirmed the value of our two main contributions: removing the relational\nranking stage or the structured prompting stage led to a significant\nperformance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our\nwork demonstrates that decoupling visual perception from cognitive reasoning\nthrough a two-stage LLM framework offers a powerful, interpretable, and\neffective new paradigm for assessing lymph node metastasis in rectal cancer.\n","authors":["Yaoxian Dong","Yifan Gao","Haoyue Li","Yanfen Cui","Xin Gao"],"pdf_url":"https://arxiv.org/pdf/2507.11457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16366v3","updated":"2025-07-15T16:12:44Z","published":"2025-02-22T21:48:48Z","title":"A Generative Approach to LLM Harmfulness Detection with Special Red Flag\n  Tokens","summary":"  Most safety training methods for large language models (LLMs) are based on\nfine-tuning that forces models to shift from an unsafe answer to refusal when\nfaced with harmful requests. Unfortunately, these drastic distribution shifts\ngenerally compromise model capabilities. To avoid that, we propose to expand\nthe model's vocabulary with a special token we call red flag token (<rf>) and\npropose to train the model to insert this token into its response at any time\nwhen harmful content is generated or about to be generated. Our approach offers\nseveral advantages: it enables the model to explicitly learn the concept of\nharmfulness while marginally affecting the generated distribution, thus\nmaintaining the model's utility. It also evaluates each generated answer and\nprovides robustness as good as adversarial training without the need to run\nattacks during training. Moreover, by encapsulating our safety tuning in a LoRA\nmodule, we provide additional defenses against fine-tuning API attacks.\n","authors":["Sophie Xhonneux","David Dobre","Mehrnaz Mofakhami","Leo Schwinn","Gauthier Gidel"],"pdf_url":"https://arxiv.org/pdf/2502.16366v3.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.04140v2","updated":"2025-07-15T16:09:44Z","published":"2025-02-06T15:20:32Z","title":"Synthetic Datasets for Machine Learning on Spatio-Temporal Graphs using\n  PDEs","summary":"  Many physical processes can be expressed through partial differential\nequations (PDEs). Real-world measurements of such processes are often collected\nat irregularly distributed points in space, which can be effectively\nrepresented as graphs; however, there are currently only a few existing\ndatasets. Our work aims to make advancements in the field of PDE-modeling\naccessible to the temporal graph machine learning community, while addressing\nthe data scarcity problem, by creating and utilizing datasets based on PDEs. In\nthis work, we create and use synthetic datasets based on PDEs to support\nspatio-temporal graph modeling in machine learning for different applications.\nMore precisely, we showcase three equations to model different types of\ndisasters and hazards in the fields of epidemiology, atmospheric particles, and\ntsunami waves. Further, we show how such created datasets can be used by\nbenchmarking several machine learning models on the epidemiological dataset.\nAdditionally, we show how pre-training on this dataset can improve model\nperformance on real-world epidemiological data. The presented methods enable\nothers to create datasets and benchmarks customized to individual requirements.\nThe source code for our methodology and the three created datasets can be found\non https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs.\n","authors":["Jost Arndt","Utku Isil","Michael Detzel","Wojciech Samek","Jackie Ma"],"pdf_url":"https://arxiv.org/pdf/2502.04140v2.pdf","comment":"Camera-ready version of the paper, which is now accepted at DMLR, see\n  https://openreview.net/forum?id=EguDBMechn . 17 pages, 5 Figures"},{"id":"http://arxiv.org/abs/2410.08979v4","updated":"2025-07-15T16:07:53Z","published":"2024-10-11T16:54:07Z","title":"Overcoming Slow Decision Frequencies in Continuous Control: Model-Based\n  Sequence Reinforcement Learning for Model-Free Control","summary":"  Reinforcement learning (RL) is rapidly reaching and surpassing human-level\ncontrol capabilities. However, state-of-the-art RL algorithms often require\ntimesteps and reaction times significantly faster than human capabilities,\nwhich is impractical in real-world settings and typically necessitates\nspecialized hardware. We introduce Sequence Reinforcement Learning (SRL), an RL\nalgorithm designed to produce a sequence of actions for a given input state,\nenabling effective control at lower decision frequencies. SRL addresses the\nchallenges of learning action sequences by employing both a model and an\nactor-critic architecture operating at different temporal scales. We propose a\n\"temporal recall\" mechanism, where the critic uses the model to estimate\nintermediate states between primitive actions, providing a learning signal for\neach individual action within the sequence. Once training is complete, the\nactor can generate action sequences independently of the model, achieving\nmodel-free control at a slower frequency. We evaluate SRL on a suite of\ncontinuous control tasks, demonstrating that it achieves performance comparable\nto state-of-the-art algorithms while significantly reducing actor sample\ncomplexity. To better assess performance across varying decision frequencies,\nwe introduce the Frequency-Averaged Score (FAS) metric. Our results show that\nSRL significantly outperforms traditional RL algorithms in terms of FAS, making\nit particularly suitable for applications requiring variable decision\nfrequencies. Furthermore, we compare SRL with model-based online planning,\nshowing that SRL achieves comparable FAS while leveraging the same model during\ntraining that online planners use for planning.\n","authors":["Devdhar Patel","Hava Siegelmann"],"pdf_url":"https://arxiv.org/pdf/2410.08979v4.pdf","comment":"30 pages, 14 figures, 7 tables. Presented at the Thirteenth\n  International Conference on Learning Representations (ICLR 2025), Singapore,\n  April 24-28, 2025"},{"id":"http://arxiv.org/abs/2507.11441v1","updated":"2025-07-15T16:05:30Z","published":"2025-07-15T16:05:30Z","title":"Implementing Adaptations for Vision AutoRegressive Model","summary":"  Vision AutoRegressive model (VAR) was recently introduced as an alternative\nto Diffusion Models (DMs) in image generation domain. In this work we focus on\nits adaptations, which aim to fine-tune pre-trained models to perform specific\ndownstream tasks, like medical data generation. While for DMs there exist many\ntechniques, adaptations for VAR remain underexplored. Similarly, differentially\nprivate (DP) adaptations-ones that aim to preserve privacy of the adaptation\ndata-have been extensively studied for DMs, while VAR lacks such solutions. In\nour work, we implement and benchmark many strategies for VAR, and compare them\nto state-of-the-art DM adaptation strategies. We observe that VAR outperforms\nDMs for non-DP adaptations, however, the performance of DP suffers, which\nnecessitates further research in private adaptations for VAR. Code is available\nat https://github.com/sprintml/finetuning_var_dp.\n","authors":["Kaif Shaikh","Antoni Kowalczuk","Franziska Boenisch","Adam Dziedzic"],"pdf_url":"https://arxiv.org/pdf/2507.11441v1.pdf","comment":"Accepted at DIG-BUGS: Data in Generative Models Workshop @ ICML 2025"},{"id":"http://arxiv.org/abs/2507.11439v1","updated":"2025-07-15T16:01:58Z","published":"2025-07-15T16:01:58Z","title":"Data Augmentation in Time Series Forecasting through Inverted Framework","summary":"  Currently, iTransformer is one of the most popular and effective models for\nmultivariate time series (MTS) forecasting. Thanks to its inverted framework,\niTransformer effectively captures multivariate correlation. However, the\ninverted framework still has some limitations. It diminishes temporal\ninterdependency information, and introduces noise in cases of nonsignificant\nvariable correlation. To address these limitations, we introduce a novel data\naugmentation method on inverted framework, called DAIF. Unlike previous data\naugmentation methods, DAIF stands out as the first real-time augmentation\nspecifically designed for the inverted framework in MTS forecasting. We first\ndefine the structure of the inverted sequence-to-sequence framework, then\npropose two different DAIF strategies, Frequency Filtering and Cross-variation\nPatching to address the existing challenges of the inverted framework.\nExperiments across multiple datasets and inverted models have demonstrated the\neffectiveness of our DAIF.\n","authors":["Hongming Tan","Ting Chen","Ruochong Jin","Wai Kin Chan"],"pdf_url":"https://arxiv.org/pdf/2507.11439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11436v1","updated":"2025-07-15T15:58:36Z","published":"2025-07-15T15:58:36Z","title":"Toward Improving fNIRS Classification: A Study on Activation Functions\n  in Deep Neural Architectures","summary":"  Activation functions are critical to the performance of deep neural networks,\nparticularly in domains such as functional near-infrared spectroscopy (fNIRS),\nwhere nonlinearity, low signal-to-noise ratio (SNR), and signal variability\nposes significant challenges to model accuracy. However, the impact of\nactivation functions on deep learning (DL) performance in the fNIRS domain\nremains underexplored and lacks systematic investigation in the current\nliterature. This study evaluates a range of conventional and field-specific\nactivation functions for fNIRS classification tasks using multiple deep\nlearning architectures, including the domain-specific fNIRSNet, AbsoluteNet,\nMDNN, and shallowConvNet (as the baseline), all tested on a single dataset\nrecorded during an auditory task. To ensure fair a comparison, all networks\nwere trained and tested using standardized preprocessing and consistent\ntraining parameters. The results show that symmetrical activation functions\nsuch as Tanh and the Absolute value function Abs(x) can outperform commonly\nused functions like the Rectified Linear Unit (ReLU), depending on the\narchitecture. Additionally, a focused analysis of the role of symmetry was\nconducted using a Modified Absolute Function (MAF), with results further\nsupporting the effectiveness of symmetrical activation functions on performance\ngains. These findings underscore the importance of selecting proper activation\nfunctions that align with the signal characteristics of fNIRS data.\n","authors":["Behtom Adeli","John McLinden","Pankaj Pandey","Ming Shao","Yalda Shahriari"],"pdf_url":"https://arxiv.org/pdf/2507.11436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11430v1","updated":"2025-07-15T15:53:01Z","published":"2025-07-15T15:53:01Z","title":"FLsim: A Modular and Library-Agnostic Simulation Framework for Federated\n  Learning","summary":"  Federated Learning (FL) has undergone significant development since its\ninception in 2016, advancing from basic algorithms to complex methodologies\ntailored to address diverse challenges and use cases. However, research and\nbenchmarking of novel FL techniques against a plethora of established\nstate-of-the-art solutions remain challenging. To streamline this process, we\nintroduce FLsim, a comprehensive FL simulation framework designed to meet the\ndiverse requirements of FL workflows in the literature. FLsim is characterized\nby its modularity, scalability, resource efficiency, and controlled\nreproducibility of experimental outcomes. Its easy to use interface allows\nusers to specify customized FL requirements through job configuration, which\nsupports: (a) customized data distributions, ranging from non-independent and\nidentically distributed (non-iid) data to independent and identically\ndistributed (iid) data, (b) selection of local learning algorithms according to\nuser preferences, with complete agnosticism to ML libraries, (c) choice of\nnetwork topology illustrating communication patterns among nodes, (d)\ndefinition of model aggregation and consensus algorithms, and (e) pluggable\nblockchain support for enhanced robustness. Through a series of experimental\nevaluations, we demonstrate the effectiveness and versatility of FLsim in\nsimulating a diverse range of state-of-the-art FL experiments. We envisage that\nFLsim would mark a significant advancement in FL simulation frameworks,\noffering unprecedented flexibility and functionality for researchers and\npractitioners alike.\n","authors":["Arnab Mukherjee","Raju Halder","Joydeep Chandra"],"pdf_url":"https://arxiv.org/pdf/2507.11430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01966v2","updated":"2025-07-15T15:48:16Z","published":"2025-05-11T06:26:34Z","title":"Matrix Is All You Need","summary":"  Deep neural networks employ specialized architectures for vision, sequential\nand language tasks, yet this proliferation obscures their underlying\ncommonalities. We introduce a unified matrix-order framework that casts\nconvolutional, recurrent and self-attention operations as sparse matrix\nmultiplications. Convolution is realized via an upper-triangular weight matrix\nperforming first-order transformations; recurrence emerges from a\nlower-triangular matrix encoding stepwise updates; attention arises naturally\nas a third-order tensor factorization. We prove algebraic isomorphism with\nstandard CNN, RNN and Transformer layers under mild assumptions. Empirical\nevaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet),\ntime-series forecasting (ETTh1, Electricity Load Diagrams) and language\nmodeling/classification (AG News, WikiText-2, Penn Treebank) confirm that\nsparse-matrix formulations match or exceed native model performance while\nconverging in comparable or fewer epochs. By reducing architecture design to\nsparse pattern selection, our matrix perspective aligns with GPU parallelism\nand leverages mature algebraic optimization tools. This work establishes a\nmathematically rigorous substrate for diverse neural architectures and opens\navenues for principled, hardware-aware network design.\n","authors":["Yuzhou Zhu"],"pdf_url":"https://arxiv.org/pdf/2506.01966v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19077v2","updated":"2025-07-15T15:47:04Z","published":"2024-11-28T11:53:59Z","title":"Improving sub-seasonal wind-speed forecasts in Europe with a non-linear\n  model","summary":"  Sub-seasonal wind speed forecasts provide valuable guidance for wind power\nsystem planning and operations, yet the forecast skills of surface winds\ndecrease sharply after two weeks. However, large-scale variables exhibit\ngreater predictability on this time scale. This study explores the potential of\nleveraging non-linear relationships between 500 hPa geopotential height (Z500)\nand surface wind speed to improve sub-seasonal wind speed forecast skills in\nEurope. Our proposed framework uses a Multiple Linear Regression (MLR) or a\nConvolutional Neural Network (CNN) to regress surface wind speed from Z500.\nEvaluations on ERA5 reanalysis indicate that the CNN performs better due to its\nnon-linearity. Applying these models to sub-seasonal forecasts from the\nEuropean Centre for Medium-Range Weather Forecasts, various verification\nmetrics demonstrate the advantages of non-linearity. Yet, this is partly\nexplained by the fact that these statistical models are under-dispersive since\nthey explain only a fraction of the target variable variance. Introducing\nstochastic perturbations to represent the stochasticity of the unexplained part\nfrom the signal helps compensate for this issue. Results show that the\nperturbed CNN performs better than the perturbed MLR only in the first weeks,\nwhile the perturbed MLR's performance converges towards that of the perturbed\nCNN after two weeks. The study finds that introducing stochastic perturbations\ncan address the issue of insufficient spread in these statistical models, with\nimprovements from the non-linearity varying with the lead time of the\nforecasts.\n","authors":["Ganglin Tian","Camille Le Coz","Anastase Alexandre Charantonis","Alexis Tantet","Naveen Goutham","Riwal Plougonven"],"pdf_url":"https://arxiv.org/pdf/2411.19077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11419v1","updated":"2025-07-15T15:45:36Z","published":"2025-07-15T15:45:36Z","title":"Better Regret Rates in Bilateral Trade via Sublinear Budget Violation","summary":"  Bilateral trade is a central problem in algorithmic economics, and recent\nwork has explored how to design trading mechanisms using no-regret learning\nalgorithms. However, no-regret learning is impossible when budget balance has\nto be enforced at each time step. Bernasconi et al. [Ber+24] show how this\nimpossibility can be circumvented by relaxing the budget balance constraint to\nhold only globally over all time steps. In particular, they design an algorithm\nachieving regret of the order of $\\tilde O(T^{3/4})$ and provide a lower bound\nof $\\Omega(T^{5/7})$.\n  In this work, we interpolate between these two extremes by studying how the\noptimal regret rate varies with the allowed violation of the global budget\nbalance constraint. Specifically, we design an algorithm that, by violating the\nconstraint by at most $T^{\\beta}$ for any given $\\beta \\in [\\frac{3}{4},\n\\frac{6}{7}]$, attains regret $\\tilde O(T^{1 - \\beta/3})$. We complement this\nresult with a matching lower bound, thus fully characterizing the trade-off\nbetween regret and budget violation. Our results show that both the $\\tilde\nO(T^{3/4})$ upper bound in the global budget balance case and the\n$\\Omega(T^{5/7})$ lower bound under unconstrained budget balance violation\nobtained by Bernasconi et al. [Ber+24] are tight.\n","authors":["Anna Lunghi","Matteo Castiglioni","Alberto Marchesi"],"pdf_url":"https://arxiv.org/pdf/2507.11419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03689v2","updated":"2025-07-15T15:37:48Z","published":"2025-07-04T16:12:57Z","title":"A Resource Efficient Quantum Kernel","summary":"  Quantum processors may enhance machine learning by mapping high-dimensional\ndata onto quantum systems for processing. Conventional quantum kernels, or\nfeature maps, for encoding data features onto a quantum circuit are currently\nimpractical, as the number of entangling gates scales quadratically with the\ndimension of the dataset and the number of qubits. In this work, we introduce a\nquantum kernel designed to handle high-dimensional data with a significantly\nreduced number of qubits and entangling operations. Our approach preserves\nessential data characteristics while promoting computational efficiency, as\nevidenced by extensive experiments on benchmark datasets that demonstrate a\nmarked improvement in both accuracy and resource utilization, as compared to\nstate-of-the-art quantum feature maps. Our noisy simulations results combined\nwith lower resource requirements highlight our kernel's ability to function\nwithin the constraints of noisy intermediate-scale quantum devices. Through\nnumerical simulations and small-scale implementation on a superconducting\ncircuit quantum computing platform, we demonstrate that our scheme performs on\npar or better than a set of classical algorithms for classification. Our\nfindings herald a promising avenue for the practical implementation of quantum\nmachine learning algorithms on near future quantum computing platforms.\n","authors":["Utkarsh Singh","Jean-Frédéric Laprade","Aaron Z. Goldberg","Khabat Heshami"],"pdf_url":"https://arxiv.org/pdf/2507.03689v2.pdf","comment":"17 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.00648v4","updated":"2025-07-15T15:36:44Z","published":"2024-12-01T02:55:08Z","title":"DFRot: Achieving Outlier-Free and Massive Activation-Free for Rotated\n  LLMs with Refined Rotation","summary":"  Rotating the activation and weight matrices to reduce the influence of\noutliers in large language models (LLMs) has recently attracted significant\nattention, particularly in the context of model quantization. Prior studies\nhave shown that in low-precision quantization scenarios, such as 4-bit weights\nand 4-bit activations (W4A4), randomized Hadamard transforms can achieve\nsignificantly higher accuracy than randomized orthogonal transforms. Notably,\nthe reason behind this phenomenon remains unknown. In this paper, we find that\nthese transformations show substantial improvement in eliminating outliers for\ncommon tokens and achieve similar quantization error. The primary reason for\nthe accuracy difference lies in the fact that randomized Hadamard transforms\ncan slightly reduce the quantization error for tokens with massive activations\nwhile randomized orthogonal transforms increase the quantization error. Due to\nthe extreme rarity of these tokens and their critical impact on model accuracy,\nwe consider this a long-tail optimization problem, and therefore construct a\nsimple yet effective method: a weighted loss function. Additionally, we propose\nan optimization strategy for the rotation matrix that involves alternating\noptimization of quantization parameters while employing orthogonal Procrustes\ntransforms to refine the rotation matrix. This makes the distribution of the\nrotated activation values more conducive to quantization, especially for tokens\nwith massive activations. Our method enhances the Rotated LLMs by achieving\ndual free, Outlier-Free and Massive Activation-Free, dubbed as DFRot. Extensive\nexperiments demonstrate the effectiveness and efficiency of DFRot. By tuning\nthe rotation matrix using just a single sample, DFRot achieves a perplexity\nimprovement of 0.98 and 0.95 on W4A4KV4 and W4A4KV16, respectively, for\nLLaMA3-70B, a model known for its quantization challenges.\n","authors":["Jingyang Xiang","Sai Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.00648v4.pdf","comment":"Accepeted bythe 2nd Conference on Language Modeling (COLM 2025).\n  Source code \\url{https://github.com/JingyangXiang/DFRot}"},{"id":"http://arxiv.org/abs/2507.11412v1","updated":"2025-07-15T15:31:51Z","published":"2025-07-15T15:31:51Z","title":"Seq vs Seq: An Open Suite of Paired Encoders and Decoders","summary":"  The large language model (LLM) community focuses almost exclusively on\ndecoder-only language models, since they are easier to use for text generation.\nHowever, a large subset of the community still uses encoder-only models for\ntasks such as classification or retrieval. Previous work has attempted to\ncompare these architectures, but is forced to make comparisons with models that\nhave different numbers of parameters, training techniques, and datasets. We\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\ndecoder-only models produces SOTA recipes in both categories for their\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\ndecoders. Like previous work, we find that encoder-only models excel at\nclassification and retrieval tasks while decoders excel at generative tasks.\nHowever, we show that adapting a decoder model to encoder tasks (and vice\nversa) through continued training is subpar compared to using only the reverse\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\nfor generative tasks). We open-source all artifacts of this study including\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\nallow future work to analyze or extend all aspects of training.\n","authors":["Orion Weller","Kathryn Ricci","Marc Marone","Antoine Chaffin","Dawn Lawrie","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2507.11412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11411v1","updated":"2025-07-15T15:31:12Z","published":"2025-07-15T15:31:12Z","title":"Robust-Multi-Task Gradient Boosting","summary":"  Multi-task learning (MTL) has shown effectiveness in exploiting shared\ninformation across tasks to improve generalization. MTL assumes tasks share\nsimilarities that can improve performance. In addition, boosting algorithms\nhave demonstrated exceptional performance across diverse learning problems,\nprimarily due to their ability to focus on hard-to-learn instances and\niteratively reduce residual errors. This makes them a promising approach for\nlearning multi-task problems. However, real-world MTL scenarios often involve\ntasks that are not well-aligned (known as outlier or adversarial tasks), which\ndo not share beneficial similarities with others and can, in fact, deteriorate\nthe performance of the overall model. To overcome this challenge, we propose\nRobust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that\nexplicitly models and adapts to task heterogeneity during training. R-MTGB\nstructures the learning process into three sequential blocks: (1) learning\nshared patterns, (2) partitioning tasks into outliers and non-outliers with\nregularized parameters, and (3) fine-tuning task-specific predictors. This\narchitecture enables R-MTGB to automatically detect and penalize outlier tasks\nwhile promoting effective knowledge transfer among related tasks. Our method\nintegrates these mechanisms seamlessly within gradient boosting, allowing\nrobust handling of noisy or adversarial tasks without sacrificing accuracy.\nExtensive experiments on both synthetic benchmarks and real-world datasets\ndemonstrate that our approach successfully isolates outliers, transfers\nknowledge, and consistently reduces prediction errors for each task\nindividually, and achieves overall performance gains across all tasks. These\nresults highlight robustness, adaptability, and reliable convergence of R-MTGB\nin challenging MTL environments.\n","authors":["Seyedsaman Emami","Gonzalo Martínez-Muñoz","Daniel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2507.11411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00588v2","updated":"2025-07-15T15:22:47Z","published":"2025-05-31T14:51:08Z","title":"Temporal Chunking Enhances Recognition of Implicit Sequential Patterns","summary":"  In this pilot study, we propose a neuro-inspired approach that compresses\ntemporal sequences into context-tagged chunks, where each tag represents a\nrecurring structural unit or``community'' in the sequence. These tags are\ngenerated during an offline sleep phase and serve as compact references to past\nexperience, allowing the learner to incorporate information beyond its\nimmediate input range. We evaluate this idea in a controlled synthetic\nenvironment designed to reveal the limitations of traditional neural network\nbased sequence learners, such as recurrent neural networks (RNNs), when facing\ntemporal patterns on multiple timescales. We evaluate this idea in a controlled\nsynthetic environment designed to reveal the limitations of traditional neural\nnetwork based sequence learners, such as recurrent neural networks (RNNs), when\nfacing temporal patterns on multiple timescales. Our results, while\npreliminary, suggest that temporal chunking can significantly enhance learning\nefficiency under resource constrained settings. A small-scale human pilot study\nusing a Serial Reaction Time Task further motivates the idea of structural\nabstraction. Although limited to synthetic tasks, this work serves as an early\nproof-of-concept, with initial evidence that learned context tags can transfer\nacross related task, offering potential for future applications in transfer\nlearning.\n","authors":["Jayanta Dey","Nicholas Soures","Miranda Gonzales","Itamar Lerner","Christopher Kanan","Dhireesha Kudithipudi"],"pdf_url":"https://arxiv.org/pdf/2506.00588v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00077v3","updated":"2025-07-15T15:17:21Z","published":"2025-05-29T23:39:24Z","title":"Gaussian mixture models as a proxy for interacting language models","summary":"  Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions.\n","authors":["Edward L. Wang","Tianyu Wang","Hayden Helm","Avanti Athreya","Vince Lyzinski","Carey E. Priebe"],"pdf_url":"https://arxiv.org/pdf/2506.00077v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13696v7","updated":"2025-07-15T15:16:22Z","published":"2023-02-27T11:55:24Z","title":"Moderate Adaptive Linear Units (MoLU)","summary":"  We propose the Moderate Adaptive Linear Unit (MoLU), a novel activation\nfunction for deep neural networks, defined analytically as: f(x)=x \\times\n(1+tanh(x))/2. MoLU combines mathematical elegance with empirical\neffectiveness, exhibiting superior performance in terms of prediction accuracy,\nconvergence speed, and computational efficiency. Due to its C-infinity\nsmoothness, i.e. infinite differentiability and analyticity, MoLU is expected\nto mitigate issues such as vanishing or exploding gradients, making it suitable\nfor a broad range of architectures and applications, including large language\nmodels (LLMs), Neural Ordinary Differential Equations (Neural ODEs),\nPhysics-Informed Neural Networks (PINNs), and Convolutional Neural Networks\n(CNNs). Empirical evaluations show that MoLU consistently achieves faster\nconvergence and improved final accuracy relative to widely used activation\nfunctions such as GeLU, SiLU, and Mish. These properties position MoLU as a\npromising and robust candidate for general-purpose activation across diverse\ndeep learning paradigms.\n","authors":["Hankyul Koh","Joon-hyuk Ko","Wonho Jhe"],"pdf_url":"https://arxiv.org/pdf/2302.13696v7.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2507.11401v1","updated":"2025-07-15T15:12:59Z","published":"2025-07-15T15:12:59Z","title":"Stochastic Entanglement Configuration for Constructive Entanglement\n  Topologies in Quantum Machine Learning with Application to Cardiac MRI","summary":"  Efficient entanglement strategies are essential for advancing variational\nquantum circuits (VQCs) for quantum machine learning (QML). However, most\ncurrent approaches use fixed entanglement topologies that are not adaptive to\ntask requirements, limiting potential gains over classical models. We introduce\na novel stochastic entanglement configuration method that systematically\ngenerates diverse entanglement topologies to identify a subspace of\nconstructive entanglement configurations, defined as entanglement topologies\nthat boost hybrid model performance (e.g., classification accuracy) beyond\nclassical baselines. Each configuration is encoded as a stochastic binary\nmatrix, denoting directed entanglement between qubits. This enables scalable\nexploration of the hyperspace of candidate entanglement topologies using\nentanglement density and per-qubit constraints as key metrics. We define\nunconstrained and constrained sampling modes, controlling entanglement per\nqubit. Using our method, 400 stochastic configurations were generated and\nevaluated in a hybrid QML for cardiac MRI disease classification. We identified\n64 (16%) novel constructive entanglement configurations that consistently\noutperformed the classical baseline. Ensemble aggregation of top-performing\nconfigurations achieved ~0.92 classification accuracy, exceeding the classical\nmodel (~0.87) by over 5%. Compared to four conventional topologies (ring,\nnearest neighbor, no entanglement, fully entangled), none surpassed the\nclassical baseline (maximum accuracy ~0.82), while our configurations delivered\nup to ~20% higher accuracy. Thus, highlighting the robustness and\ngeneralizability of the identified constructive entanglements.\n","authors":["Mehri Mehrnia","Mohammed S. M. Elbaz"],"pdf_url":"https://arxiv.org/pdf/2507.11401v1.pdf","comment":"Accepted for publication at IEEE International Conference on Quantum\n  Computing and Engineering (QCE) 2025"},{"id":"http://arxiv.org/abs/2401.08513v3","updated":"2025-07-15T15:11:53Z","published":"2024-01-16T17:21:33Z","title":"X Hacking: The Threat of Misguided AutoML","summary":"  Explainable AI (XAI) and interpretable machine learning methods help to build\ntrust in model predictions and derived insights, yet also present a perverse\nincentive for analysts to manipulate XAI metrics to support pre-specified\nconclusions. This paper introduces the concept of X-hacking, a form of\np-hacking applied to XAI metrics such as SHAP values. We show how easily an\nautomated machine learning pipeline can be adapted to exploit model\nmultiplicity at scale: searching a Rashomon set of 'defensible' models with\nsimilar predictive performance to find a desired explanation. We formulate the\ntrade-off between explanation and accuracy as a multi-objective optimisation\nproblem, and illustrate empirically on familiar real-world datasets that, on\naverage, Bayesian optimisation accelerates X-hacking 3-fold for features\nsusceptible to it, versus random sampling. We show the vulnerability of a\ndataset to X-hacking can be determined by information redundancy among\nfeatures. Finally, we suggest possible methods for detection and prevention,\nand discuss ethical implications for the credibility and reproducibility of\nXAI.\n","authors":["Rahul Sharma","Sergey Redyuk","Sumantrak Mukherjee","Andrea Šipka","Eyke Hüllermeier","Sebastian Vollmer","David Selby"],"pdf_url":"https://arxiv.org/pdf/2401.08513v3.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2501.12633v3","updated":"2025-07-15T15:08:23Z","published":"2025-01-22T04:38:33Z","title":"Inverse Reinforcement Learning with Switching Rewards and History\n  Dependency for Characterizing Animal Behaviors","summary":"  Traditional approaches to studying decision-making in neuroscience focus on\nsimplified behavioral tasks where animals perform repetitive, stereotyped\nactions to receive explicit rewards. While informative, these methods constrain\nour understanding of decision-making to short timescale behaviors driven by\nexplicit goals. In natural environments, animals exhibit more complex,\nlong-term behaviors driven by intrinsic motivations that are often\nunobservable. Recent works in time-varying inverse reinforcement learning (IRL)\naim to capture shifting motivations in long-term, freely moving behaviors.\nHowever, a crucial challenge remains: animals make decisions based on their\nhistory, not just their current state. To address this, we introduce SWIRL\n(SWitching IRL), a novel framework that extends traditional IRL by\nincorporating time-varying, history-dependent reward functions. SWIRL models\nlong behavioral sequences as transitions between short-term decision-making\nprocesses, each governed by a unique reward function. SWIRL incorporates\nbiologically plausible history dependency to capture how past decisions and\nenvironmental contexts shape behavior, offering a more accurate description of\nanimal decision-making. We apply SWIRL to simulated and real-world animal\nbehavior datasets and show that it outperforms models lacking history\ndependency, both quantitatively and qualitatively. This work presents the first\nIRL model to incorporate history-dependent policies and rewards to advance our\nunderstanding of complex, naturalistic decision-making in animals.\n","authors":["Jingyang Ke","Feiyang Wu","Jiyi Wang","Jeffrey Markowitz","Anqi Wu"],"pdf_url":"https://arxiv.org/pdf/2501.12633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11393v1","updated":"2025-07-15T15:05:26Z","published":"2025-07-15T15:05:26Z","title":"A Neural Network Model of Complementary Learning Systems: Pattern\n  Separation and Completion for Continual Learning","summary":"  Learning new information without forgetting prior knowledge is central to\nhuman intelligence. In contrast, neural network models suffer from catastrophic\nforgetting: a significant degradation in performance on previously learned\ntasks when acquiring new information. The Complementary Learning Systems (CLS)\ntheory offers an explanation for this human ability, proposing that the brain\nhas distinct systems for pattern separation (encoding distinct memories) and\npattern completion (retrieving complete memories from partial cues). To capture\nthese complementary functions, we leverage the representational generalization\ncapabilities of variational autoencoders (VAEs) and the robust memory storage\nproperties of Modern Hopfield networks (MHNs), combining them into a neurally\nplausible continual learning model. We evaluate this model on the Split-MNIST\ntask, a popular continual learning benchmark, and achieve close to\nstate-of-the-art accuracy (~90%), substantially reducing forgetting.\nRepresentational analyses empirically confirm the functional dissociation: the\nVAE underwrites pattern completion, while the MHN drives pattern separation. By\ncapturing pattern separation and completion in scalable architectures, our work\nprovides a functional template for modeling memory consolidation,\ngeneralization, and continual learning in both biological and artificial\nsystems.\n","authors":["James P Jun","Vijay Marupudi","Raj Sanjay Shah","Sashank Varma"],"pdf_url":"https://arxiv.org/pdf/2507.11393v1.pdf","comment":"Accepted to CogSci 2025. 7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2507.11387v1","updated":"2025-07-15T14:56:25Z","published":"2025-07-15T14:56:25Z","title":"From Kinetic Theory to AI: a Rediscovery of High-Dimensional Divergences\n  and Their Properties","summary":"  Selecting an appropriate divergence measure is a critical aspect of machine\nlearning, as it directly impacts model performance. Among the most widely used,\nwe find the Kullback-Leibler (KL) divergence, originally introduced in kinetic\ntheory as a measure of relative entropy between probability distributions. Just\nas in machine learning, the ability to quantify the proximity of probability\ndistributions plays a central role in kinetic theory. In this paper, we present\na comparative review of divergence measures rooted in kinetic theory,\nhighlighting their theoretical foundations and exploring their potential\napplications in machine learning and artificial intelligence.\n","authors":["Gennaro Auricchio","Giovanni Brigati","Paolo Giudici","Giuseppe Toscani"],"pdf_url":"https://arxiv.org/pdf/2507.11387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11385v1","updated":"2025-07-15T14:54:57Z","published":"2025-07-15T14:54:57Z","title":"Joint space-time wind field data extrapolation and uncertainty\n  quantification using nonparametric Bayesian dictionary learning","summary":"  A methodology is developed, based on nonparametric Bayesian dictionary\nlearning, for joint space-time wind field data extrapolation and estimation of\nrelated statistics by relying on limited/incomplete measurements. Specifically,\nutilizing sparse/incomplete measured data, a time-dependent optimization\nproblem is formulated for determining the expansion coefficients of an\nassociated low-dimensional representation of the stochastic wind field.\nCompared to an alternative, standard, compressive sampling treatment of the\nproblem, the developed methodology exhibits the following advantages. First,\nthe Bayesian formulation enables also the quantification of the uncertainty in\nthe estimates. Second, the requirement in standard CS-based applications for an\na priori selection of the expansion basis is circumvented. Instead, this is\ndone herein in an adaptive manner based on the acquired data. Overall, the\nmethodology exhibits enhanced extrapolation accuracy, even in cases of\nhigh-dimensional data of arbitrary form, and of relatively large extrapolation\ndistances. Thus, it can be used, potentially, in a wide range of wind\nengineering applications where various constraints dictate the use of a limited\nnumber of sensors. The efficacy of the methodology is demonstrated by\nconsidering two case studies. The first relates to the extrapolation of\nsimulated wind velocity records consistent with a prescribed joint\nwavenumber-frequency power spectral density in a three-dimensional domain (2D\nand time). The second pertains to the extrapolation of four-dimensional (3D and\ntime) boundary layer wind tunnel experimental data that exhibit significant\nspatial variability and non-Gaussian characteristics.\n","authors":["George D. Pasparakis","Ioannis A. Kougioumtzoglou","Michael D. Shields"],"pdf_url":"https://arxiv.org/pdf/2507.11385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11381v1","updated":"2025-07-15T14:50:41Z","published":"2025-07-15T14:50:41Z","title":"From Observational Data to Clinical Recommendations: A Causal Framework\n  for Estimating Patient-level Treatment Effects and Learning Policies","summary":"  We propose a framework for building patient-specific treatment recommendation\nmodels, building on the large recent literature on learning patient-level\ncausal models and inspired by the target trial paradigm of Hernan and Robins.\nWe focus on safety and validity, including the crucial issue of causal\nidentification when using observational data. We do not provide a specific\nmodel, but rather a way to integrate existing methods and know-how into a\npractical pipeline. We further provide a real world use-case of treatment\noptimization for patients with heart failure who develop acute kidney injury\nduring hospitalization. The results suggest our pipeline can improve patient\noutcomes over the current treatment regime.\n","authors":["Rom Gutman","Shimon Sheiba","Omer Noy Klien","Naama Dekel Bird","Amit Gruber","Doron Aronson","Oren Caspi","Uri Shalit"],"pdf_url":"https://arxiv.org/pdf/2507.11381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19347v3","updated":"2025-07-15T14:44:46Z","published":"2025-01-31T17:51:46Z","title":"An All-digital 8.6-nJ/Frame 65-nm Tsetlin Machine Image Classification\n  Accelerator","summary":"  We present an all-digital programmable machine learning accelerator chip for\nimage classification, underpinning on the Tsetlin machine (TM) principles. The\nTM is an emerging machine learning algorithm founded on propositional logic,\nutilizing sub-pattern recognition expressions called clauses. The accelerator\nimplements the coalesced TM version with convolution, and classifies\nbooleanized images of 28$\\times$28 pixels with 10 categories. A configuration\nwith 128 clauses is used in a highly parallel architecture. Fast clause\nevaluation is achieved by keeping all clause weights and Tsetlin automata (TA)\naction signals in registers. The chip is implemented in a 65 nm low-leakage\nCMOS technology, and occupies an active area of 2.7 mm$^2$. At a clock\nfrequency of 27.8 MHz, the accelerator achieves 60.3k classifications per\nsecond, and consumes 8.6 nJ per classification. This demonstrates the\nenergy-efficiency of the TM, which was the main motivation for developing this\nchip. The latency for classifying a single image is 25.4 $\\mu$s which includes\nsystem timing overhead. The accelerator achieves 97.42%, 84.54% and 82.55% test\naccuracies for the datasets MNIST, Fashion-MNIST and Kuzushiji-MNIST,\nrespectively, matching the TM software models.\n","authors":["Svein Anders Tunheim","Yujin Zheng","Lei Jiao","Rishad Shafik","Alex Yakovlev","Ole-Christoffer Granmo"],"pdf_url":"https://arxiv.org/pdf/2501.19347v3.pdf","comment":"Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purpose\\, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2507.11371v1","updated":"2025-07-15T14:44:29Z","published":"2025-07-15T14:44:29Z","title":"Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives\n  Diverse Tool Use in LLMs","summary":"  We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel\nreinforcement learning framework that teaches large language models to explore\ndiverse tool usage patterns beyond conventional high-temperature sampling.\nBuilding on recent advances in step-wise reinforcement learning, we introduce a\ndual-objective reward system that simultaneously optimizes for answer quality\nand tool diversity, training a Llama-3.1 8B model through offline PPO on\nsynthetically generated trajectories from the MMLU-Pro dataset. Our approach\nuniquely employs a rarity-first exploitation strategy where a GPT-4o judge\nscores candidate actions across eight distinct tools plus chain-of-thought\nreasoning, with the policy favoring less-frequently used but still viable tools\nto encourage systematic exploration. Empirical results demonstrate that SPaRK\nachieves competitive performance across 14 MMLU-Pro categories while exhibiting\nsignificantly higher entropy in tool selection compared to both baseline and\nsupervised fine-tuning approaches, suggesting that algorithmic exploration\nthrough explicit tool diversity can enhance reasoning capabilities without\nsacrificing accuracy.\n","authors":["Gabriel Bo","Koa Chang","Justin Gu"],"pdf_url":"https://arxiv.org/pdf/2507.11371v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.11367v1","updated":"2025-07-15T14:39:41Z","published":"2025-07-15T14:39:41Z","title":"Local Pairwise Distance Matching for Backpropagation-Free Reinforcement\n  Learning","summary":"  Training neural networks with reinforcement learning (RL) typically relies on\nbackpropagation (BP), necessitating storage of activations from the forward\npass for subsequent backward updates. Furthermore, backpropagating error\nsignals through multiple layers often leads to vanishing or exploding\ngradients, which can degrade learning performance and stability. We propose a\nnovel approach that trains each layer of the neural network using local signals\nduring the forward pass in RL settings. Our approach introduces local,\nlayer-wise losses leveraging the principle of matching pairwise distances from\nmulti-dimensional scaling, enhanced with optional reward-driven guidance. This\nmethod allows each hidden layer to be trained using local signals computed\nduring forward propagation, thus eliminating the need for backward passes and\nstoring intermediate activations. Our experiments, conducted with policy\ngradient methods across common RL benchmarks, demonstrate that this\nbackpropagation-free method achieves competitive performance compared to their\nclassical BP-based counterpart. Additionally, the proposed method enhances\nstability and consistency within and across runs, and improves performance\nespecially in challenging environments.\n","authors":["Daniel Tanneberg"],"pdf_url":"https://arxiv.org/pdf/2507.11367v1.pdf","comment":"accepted at the European Conference on Artificial Intelligence (ECAI\n  2025)"},{"id":"http://arxiv.org/abs/2507.11366v1","updated":"2025-07-15T14:39:40Z","published":"2025-07-15T14:39:40Z","title":"A Parallelizable Approach for Characterizing NE in Zero-Sum Games After\n  a Linear Number of Iterations of Gradient Descent","summary":"  We study online optimization methods for zero-sum games, a fundamental\nproblem in adversarial learning in machine learning, economics, and many other\ndomains. Traditional methods approximate Nash equilibria (NE) using either\nregret-based methods (time-average convergence) or contraction-map-based\nmethods (last-iterate convergence). We propose a new method based on\nHamiltonian dynamics in physics and prove that it can characterize the set of\nNE in a finite (linear) number of iterations of alternating gradient descent in\nthe unbounded setting, modulo degeneracy, a first in online optimization.\nUnlike standard methods for computing NE, our proposed approach can be\nparallelized and works with arbitrary learning rates, both firsts in\nalgorithmic game theory. Experimentally, we support our results by showing our\napproach drastically outperforms standard methods.\n","authors":["Taemin Kim","James P. Bailey"],"pdf_url":"https://arxiv.org/pdf/2507.11366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.18193v2","updated":"2025-07-15T14:29:38Z","published":"2025-06-22T22:50:06Z","title":"DeInfoReg: A Decoupled Learning Framework for Better Training Throughput","summary":"  This paper introduces Decoupled Supervised Learning with Information\nRegularization (DeInfoReg), a novel approach that transforms a long gradient\nflow into multiple shorter ones, thereby mitigating the vanishing gradient\nproblem. Integrating a pipeline strategy, DeInfoReg enables model\nparallelization across multiple GPUs, significantly improving training\nthroughput. We compare our proposed method with standard backpropagation and\nother gradient flow decomposition techniques. Extensive experiments on diverse\ntasks and datasets demonstrate that DeInfoReg achieves superior performance and\nbetter noise resistance than traditional BP models and efficiently utilizes\nparallel computing resources. The code for reproducibility is available at:\nhttps://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.\n","authors":["Zih-Hao Huang","You-Teng Lin","Hung-Hsuan Chen"],"pdf_url":"https://arxiv.org/pdf/2506.18193v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11357v1","updated":"2025-07-15T14:27:05Z","published":"2025-07-15T14:27:05Z","title":"Neurosymbolic Reasoning Shortcuts under the Independence Assumption","summary":"  The ubiquitous independence assumption among symbolic concepts in\nneurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors\nuse it to speed up probabilistic reasoning. Recent works like van Krieken et\nal. (2024) and Marconato et al. (2024) argued that the independence assumption\ncan hinder learning of NeSy predictors and, more crucially, prevent them from\ncorrectly modelling uncertainty. There is, however, scepticism in the NeSy\ncommunity around the scenarios in which the independence assumption actually\nlimits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle\nthis question by formally showing that assuming independence among symbolic\nconcepts entails that a model can never represent uncertainty over certain\nconcept combinations. Thus, the model fails to be aware of reasoning shortcuts,\ni.e., the pathological behaviour of NeSy predictors that predict correct\ndownstream tasks but for the wrong reasons.\n","authors":["Emile van Krieken","Pasquale Minervini","Edoardo Ponti","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2507.11357v1.pdf","comment":"Accepted at NeSy 2025"},{"id":"http://arxiv.org/abs/2502.08542v2","updated":"2025-07-15T14:22:31Z","published":"2025-02-12T16:27:40Z","title":"Beyond Predictions: A Participatory Framework for Multi-Stakeholder\n  Decision-Making","summary":"  Conventional automated decision-support systems, often based on supervised\nlearning, focus on predicting outcomes to recommend actions. However, they\ntypically overlook the complexity of multi-actor environments, where diverse\nand conflicting stakeholder preferences must be balanced. At the same time,\nparticipatory AI approaches remain largely context-specific, limiting their\nbroader applicability. To address these gaps, we propose a participatory\nframework that reframes decision-making as a multi-stakeholder optimization\nproblem, using context-dependent reward functions to represent each actor's\npreferences. Our modular, model-agnostic framework employs k-fold\ncross-validation to fine-tune user-provided prediction models and evaluate\ndecision strategies, including compromise functions that mediate stakeholder\ntrade-offs. A synthetic scoring mechanism aggregates user-defined preferences\nacross multiple metrics to rank strategies and select an optimal decision-maker\nfor generating actionable recommendations on new data. Validated on two\nhigh-stake real-world case studies, the framework consistently produces\nstakeholder-aware decisions that outperform purely predictive baselines across\nmultiple metrics, while enhancing the transparency and accountability of\nAI-supported decision-making.\n","authors":["Vittoria Vineis","Giuseppe Perelli","Gabriele Tolomei"],"pdf_url":"https://arxiv.org/pdf/2502.08542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11344v1","updated":"2025-07-15T14:20:23Z","published":"2025-07-15T14:20:23Z","title":"Guiding LLM Decision-Making with Fairness Reward Models","summary":"  Large language models are increasingly used to support high-stakes decisions,\npotentially influencing who is granted bail or receives a loan. Naive\nchain-of-thought sampling can improve average decision accuracy, but has also\nbeen shown to amplify unfair bias. To address this challenge and enable the\ntrustworthy use of reasoning models in high-stakes decision-making, we propose\na framework for training a generalizable Fairness Reward Model (FRM). Our model\nassigns a fairness score to LLM reasoning, enabling the system to down-weight\nbiased trajectories and favor equitable ones when aggregating decisions across\nreasoning chains. We show that a single Fairness Reward Model, trained on\nweakly supervised, LLM-annotated examples of biased versus unbiased reasoning,\ntransfers across tasks, domains, and model families without additional\nfine-tuning. Applied to real-world decision-making tasks including recidivism\nprediction and social media moderation, we show that our approach consistently\nimproves fairness while matching, or even surpassing, baseline accuracy.\n","authors":["Zara Hall","Melanie Subbiah","Thomas P Zollo","Kathleen McKeown","Richard Zemel"],"pdf_url":"https://arxiv.org/pdf/2507.11344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.22803v2","updated":"2025-07-15T14:13:41Z","published":"2025-06-28T08:11:29Z","title":"Intervening in Black Box: Concept Bottleneck Model for Enhancing Human\n  Neural Network Mutual Understanding","summary":"  Recent advances in deep learning have led to increasingly complex models with\ndeeper layers and more parameters, reducing interpretability and making their\ndecisions harder to understand. While many methods explain black-box reasoning,\nmost lack effective interventions or only operate at sample-level without\nmodifying the model itself. To address this, we propose the Concept Bottleneck\nModel for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).\nCBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable\nframework to approximate black-box reasoning and communicate conceptual\nunderstanding. Detrimental concepts are automatically identified and refined\n(removed/replaced) based on global gradient contributions. The modified CBM\nthen distills corrected knowledge back into the black-box model, enhancing both\ninterpretability and accuracy. We evaluate CBM-HNMU on various CNN and\ntransformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,\nand CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum\nincrease in average accuracy across 1.03%. Source code is available at:\nhttps://github.com/XiGuaBo/CBM-HNMU.\n","authors":["Nuoye Xiong","Anqi Dong","Ning Wang","Cong Hua","Guangming Zhu","Lin Mei","Peiyi Shen","Liang Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.22803v2.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2506.14110v2","updated":"2025-07-15T13:56:42Z","published":"2025-06-17T02:02:11Z","title":"Universal rates of ERM for agnostic learning","summary":"  The universal learning framework has been developed to obtain guarantees on\nthe learning rates that hold for any fixed distribution, which can be much\nfaster than the ones uniformly hold over all the distributions. Given that the\nEmpirical Risk Minimization (ERM) principle being fundamental in the PAC theory\nand ubiquitous in practical machine learning, the recent work of\narXiv:2412.02810 studied the universal rates of ERM for binary classification\nunder the realizable setting. However, the assumption of realizability is too\nrestrictive to hold in practice. Indeed, the majority of the literature on\nuniversal learning has focused on the realizable case, leaving the\nnon-realizable case barely explored.\n  In this paper, we consider the problem of universal learning by ERM for\nbinary classification under the agnostic setting, where the ''learning curve\"\nreflects the decay of the excess risk as the sample size increases. We explore\nthe possibilities of agnostic universal rates and reveal a compact trichotomy:\nthere are three possible agnostic universal rates of ERM, being either\n$e^{-n}$, $o(n^{-1/2})$, or arbitrarily slow. We provide a complete\ncharacterization of which concept classes fall into each of these categories.\nMoreover, we also establish complete characterizations for the target-dependent\nuniversal rates as well as the Bayes-dependent universal rates.\n","authors":["Steve Hanneke","Mingyue Xu"],"pdf_url":"https://arxiv.org/pdf/2506.14110v2.pdf","comment":"Accepted for presentation at the Conference on Learning Theory (COLT)\n  2025"},{"id":"http://arxiv.org/abs/2507.11316v1","updated":"2025-07-15T13:48:35Z","published":"2025-07-15T13:48:35Z","title":"Internal Value Alignment in Large Language Models through Controlled\n  Value Vector Activation","summary":"  Aligning Large Language Models (LLMs) with human values has attracted\nincreasing attention since it provides clarity, transparency, and the ability\nto adapt to evolving scenarios. In this paper, we introduce a Controlled Value\nVector Activation (ConVA) method that directly aligns the internal values of\nLLMs by interpreting how a value is encoded in their latent representations and\nmodifies relevant activations to ensure consistent values in LLMs. To ensure an\naccurate and unbiased interpretation, we propose a context-controlled value\nvector identification method. To consistently control values without\nsacrificing model performance, we introduce a gated value vector activation\nmethod for effective and minimum degree of value control. Experiments show that\nour method achieves the highest control success rate across 10 basic values\nwithout hurting LLM performance and fluency, and ensures target values even\nwith opposite and potentially malicious input prompts. Source code and data are\navailable at~ https://github.com/hr-jin/ConVA.\n","authors":["Haoran Jin","Meng Li","Xiting Wang","Zhihao Xu","Minlie Huang","Yantao Jia","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2507.11316v1.pdf","comment":"25 pages, 14 figures. Accepted by ACL 2025 (main conference)"},{"id":"http://arxiv.org/abs/2410.15416v2","updated":"2025-07-15T13:42:19Z","published":"2024-10-20T15:20:24Z","title":"Contrast All the Time: Learning Time Series Representation from Temporal\n  Consistency","summary":"  Representation learning for time series using contrastive learning has\nemerged as a critical technique for improving the performance of downstream\ntasks. To advance this effective approach, we introduce CaTT (\\textit{Contrast\nAll The Time}), a new approach to unsupervised contrastive learning for time\nseries, which takes advantage of dynamics between temporally similar moments\nmore efficiently and effectively than existing methods. CaTT departs from\nconventional time-series contrastive approaches that rely on data augmentations\nor selected views. Instead, it uses the full temporal dimension by contrasting\nall time steps in parallel. This is made possible by a scalable NT-pair\nformulation, which extends the classic N-pair loss across both batch and\ntemporal dimensions, making the learning process end-to-end and more efficient.\nCaTT learns directly from the natural structure of temporal data, using\nrepeated or adjacent time steps as implicit supervision, without the need for\npair selection heuristics. We demonstrate that this approach produces superior\nembeddings which allow better performance in downstream tasks. Additionally,\ntraining is faster than other contrastive learning approaches, making it\nsuitable for large-scale and real-world time series applications. The source\ncode is publicly available at\n\\href{https://github.com/sfi-norwai/CaTT}{https://github.com/sfi-norwai/CaTT}.\n","authors":["Abdul-Kazeem Shamba","Kerstin Bach","Gavin Taylor"],"pdf_url":"https://arxiv.org/pdf/2410.15416v2.pdf","comment":"Published in the 28th European Conference on AI (ECAI), October 2025"},{"id":"http://arxiv.org/abs/2507.09291v2","updated":"2025-07-15T13:35:01Z","published":"2025-07-12T14:01:54Z","title":"Supercharging Floorplan Localization with Semantic Rays","summary":"  Floorplans provide a compact representation of the building's structure,\nrevealing not only layout information but also detailed semantics such as the\nlocations of windows and doors. However, contemporary floorplan localization\ntechniques mostly focus on matching depth-based structural cues, ignoring the\nrich semantics communicated within floorplans. In this work, we introduce a\nsemantic-aware localization framework that jointly estimates depth and semantic\nrays, consolidating over both for predicting a structural-semantic probability\nvolume. Our probability volume is constructed in a coarse-to-fine manner: We\nfirst sample a small set of rays to obtain an initial low-resolution\nprobability volume. We then refine these probabilities by performing a denser\nsampling only in high-probability regions and process the refined values for\npredicting a 2D location and orientation angle. We conduct an evaluation on two\nstandard floorplan localization benchmarks. Our experiments demonstrate that\nour approach substantially outperforms state-of-the-art methods, achieving\nsignificant improvements in recall metrics compared to prior works. Moreover,\nwe show that our framework can easily incorporate additional metadata such as\nroom labels, enabling additional gains in both accuracy and efficiency.\n","authors":["Yuval Grader","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2507.09291v2.pdf","comment":"Accepted at ICCV 2025. https://tau-vailab.github.io/SemRayLoc/"},{"id":"http://arxiv.org/abs/2503.22370v3","updated":"2025-07-15T13:30:48Z","published":"2025-03-28T12:24:26Z","title":"Grasping a Handful: Sequential Multi-Object Dexterous Grasp Generation","summary":"  We introduce the sequential multi-object robotic grasp sampling algorithm\nSeqGrasp that can robustly synthesize stable grasps on diverse objects using\nthe robotic hand's partial Degrees of Freedom (DoF). We use SeqGrasp to\nconstruct the large-scale Allegro Hand sequential grasping dataset SeqDataset\nand use it for training the diffusion-based sequential grasp generator\nSeqDiffuser. We experimentally evaluate SeqGrasp and SeqDiffuser against the\nstate-of-the-art non-sequential multi-object grasp generation method MultiGrasp\nin simulation and on a real robot. The experimental results demonstrate that\nSeqGrasp and SeqDiffuser reach an 8.71%-43.33% higher grasp success rate than\nMultiGrasp. Furthermore, SeqDiffuser is approximately 1000 times faster at\ngenerating grasps than SeqGrasp and MultiGrasp.\n","authors":["Haofei Lu","Yifei Dong","Zehang Weng","Florian Pokorny","Jens Lundell","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2503.22370v3.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2506.21095v2","updated":"2025-07-15T13:22:28Z","published":"2025-06-26T08:43:12Z","title":"FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation","summary":"  Federated Learning (FL) enables collaborative model training across multiple\nclients without sharing clients' private data. However, fairness remains a key\nconcern, as biases in local clients' datasets can impact the entire federated\nsystem. Heterogeneous data distributions across clients may lead to models that\nare fairer for some clients than others. Although several fairness-enhancing\nsolutions are present in the literature, most focus on mitigating bias for a\nsingle sensitive attribute, typically binary, overlooking the diverse and\nsometimes conflicting fairness needs of different clients. This limited\nperspective can limit the effectiveness of fairness interventions for the\ndifferent clients. To support more robust and reproducible fairness research in\nFL, we aim to enable a consistent benchmarking of fairness-aware FL methods at\nboth the global and client levels. In this paper, we contribute in three ways:\n(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to\nevaluating fair FL methods under heterogeneous client bias; (2) we release four\nbias-heterogeneous datasets and corresponding benchmarks to compare fairness\nmitigation methods in a controlled environment; (3) we provide ready-to-use\nfunctions for evaluating fairness outcomes for these datasets.\n","authors":["Xenia Heilmann","Luca Corbucci","Mattia Cerrato","Anna Monreale"],"pdf_url":"https://arxiv.org/pdf/2506.21095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10409v2","updated":"2025-07-15T13:17:11Z","published":"2025-07-14T15:54:06Z","title":"Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study","summary":"  This study addresses the challenge of balancing energy efficiency with\nperformance in AI/ML models, focusing on DeepRX, a deep learning receiver based\non a fully convolutional ResNet architecture. We evaluate the energy\nconsumption of DeepRX, considering factors including FLOPs/Watt and\nFLOPs/clock, and find consistency between estimated and actual energy usage,\ninfluenced by memory access patterns. The research extends to comparing energy\ndynamics during training and inference phases. A key contribution is the\napplication of knowledge distillation (KD) to train a compact DeepRX student\nmodel that emulates the performance of the teacher model but with reduced\nenergy consumption. We experiment with different student model sizes, optimal\nteacher sizes, and KD hyperparameters. Performance is measured by comparing the\nBit Error Rate (BER) performance versus Signal-to-Interference & Noise Ratio\n(SINR) values of the distilled model and a model trained from scratch. The\ndistilled models demonstrate a lower error floor across SINR levels,\nhighlighting the effectiveness of KD in achieving energy-efficient AI\nsolutions.\n","authors":["Amine Lbath","Ibtissam Labriji"],"pdf_url":"https://arxiv.org/pdf/2507.10409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11274v1","updated":"2025-07-15T12:52:47Z","published":"2025-07-15T12:52:47Z","title":"Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime","summary":"  We study population convergence guarantees of stochastic gradient descent\n(SGD) for smooth convex objectives in the interpolation regime, where the noise\nat optimum is zero or near zero. The behavior of the last iterate of SGD in\nthis setting -- particularly with large (constant) stepsizes -- has received\ngrowing attention in recent years due to implications for the training of\nover-parameterized models, as well as to analyzing forgetting in continual\nlearning and to understanding the convergence of the randomized Kaczmarz method\nfor solving linear systems. We establish that after $T$ steps of SGD on\n$\\beta$-smooth convex loss functions with stepsize $\\eta \\leq 1/\\beta$, the\nlast iterate exhibits expected excess risk $\\widetilde{O}(1/(\\eta\nT^{1-\\beta\\eta/2}) + \\eta T^{\\beta\\eta/2} \\sigma_\\star^2)$, where\n$\\sigma_\\star^2$ denotes the variance of the stochastic gradients at the\noptimum. In particular, for a well-tuned stepsize we obtain a near optimal\n$\\widetilde{O}(1/T + \\sigma_\\star/\\sqrt{T})$ rate for the last iterate,\nextending the results of Varre et al. (2021) beyond least squares regression;\nand when $\\sigma_\\star=0$ we obtain a rate of $O(1/\\sqrt{T})$ with\n$\\eta=1/\\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently\nestablished by Evron et al. (2025) in the special case of realizable linear\nregression.\n","authors":["Amit Attia","Matan Schliserman","Uri Sherman","Tomer Koren"],"pdf_url":"https://arxiv.org/pdf/2507.11274v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2505.00580v2","updated":"2025-07-15T12:51:28Z","published":"2025-05-01T15:11:46Z","title":"Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors","summary":"  Foundation models have achieved tremendous success in different domains.\nHowever, their huge computation and storage complexity make these models\ndifficult to fine-tune and also less applicable in practice. Recent study shows\ntraining in Fourier domain can be an effective fine-tuning method in terms of\nboth model performance and number of training parameters. In this work, we\npropose to further reduce the complexity by the factorization through the\nproduct of interleaved circulant and diagonal matrices. In addition, we address\nthe case of non-square fine-tuning weights by partitioning the circulant matrix\ninto blocks. Our method avoids the construction of weight change matrix and\nutilizes 1D fast Fourier transform (FFT) instead of 2D FFT. Experimental\nresults show that our method achieves similar or better performance across\nvarious tasks with much less floating-point operations (FLOPs) and the number\nof trainable parameters.\n","authors":["Xinyu Ding","Lexuan Chen","Siyu Liao","Zhongfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2505.00580v2.pdf","comment":"to appear in Proceedings of the 2025 International Joint Conference\n  on Artificial Intelligence (IJCAI-2025)"},{"id":"http://arxiv.org/abs/2403.07095v4","updated":"2025-07-15T12:50:52Z","published":"2024-03-11T18:44:36Z","title":"Gaussian Loss Smoothing Enables Certified Training with Tight Convex\n  Relaxations","summary":"  Training neural networks with high certified accuracy against adversarial\nexamples remains an open challenge despite significant efforts. While\ncertification methods can effectively leverage tight convex relaxations for\nbound computation, in training, these methods, perhaps surprisingly, can\nperform worse than looser relaxations. Prior work hypothesized that this\nphenomenon is caused by the discontinuity, non-smoothness, and perturbation\nsensitivity of the loss surface induced by tighter relaxations. In this work,\nwe theoretically show that applying Gaussian Loss Smoothing (GLS) on the loss\nsurface can alleviate these issues. We confirm this empirically by\ninstantiating GLS with two variants: a zeroth-order optimization algorithm,\ncalled PGPE, which allows training with non-differentiable relaxations, and a\nfirst-order optimization algorithm, called RGS, which requires gradients of the\nrelaxation but is much more efficient than PGPE. Extensive experiments show\nthat when combined with tight relaxations, these methods surpass\nstate-of-the-art methods when training on the same network architecture for\nmany settings. Our results clearly demonstrate the promise of Gaussian Loss\nSmoothing for training certifiably robust neural networks and pave a path\ntowards leveraging tighter relaxations for certified training.\n","authors":["Stefan Balauca","Mark Niklas Müller","Yuhao Mao","Maximilian Baader","Marc Fischer","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2403.07095v4.pdf","comment":"Accepted for publication in TMLR 07/2025"},{"id":"http://arxiv.org/abs/2506.04354v4","updated":"2025-07-15T12:50:08Z","published":"2025-06-04T18:12:59Z","title":"BridgeNet: A Hybrid, Physics-Informed Machine Learning Framework for\n  Solving High-Dimensional Fokker-Planck Equations","summary":"  BridgeNet is a novel hybrid framework that integrates convolutional neural\nnetworks with physics-informed neural networks to efficiently solve non-linear,\nhigh-dimensional Fokker-Planck equations (FPEs). Traditional PINNs, which\ntypically rely on fully connected architectures, often struggle to capture\ncomplex spatial hierarchies and enforce intricate boundary conditions. In\ncontrast, BridgeNet leverages adaptive CNN layers for effective local feature\nextraction and incorporates a dynamically weighted loss function that\nrigorously enforces physical constraints. Extensive numerical experiments\nacross various test cases demonstrate that BridgeNet not only achieves\nsignificantly lower error metrics and faster convergence compared to\nconventional PINN approaches but also maintains robust stability in\nhigh-dimensional settings. This work represents a substantial advancement in\ncomputational physics, offering a scalable and accurate solution methodology\nwith promising applications in fields ranging from financial mathematics to\ncomplex system dynamics.\n","authors":["Elmira Mirzabeigi","Rezvan Salehi","Kourosh Parand"],"pdf_url":"https://arxiv.org/pdf/2506.04354v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11269v1","updated":"2025-07-15T12:46:25Z","published":"2025-07-15T12:46:25Z","title":"Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy\n  Learning via Causal Bound","summary":"  Deep reinforcement learning (DRL) agents excel in solving complex\ndecision-making tasks across various domains. However, they often require a\nsubstantial number of training steps and a vast experience replay buffer,\nleading to significant computational and resource demands. To address these\nchallenges, we introduce a novel theoretical result that leverages the\nNeyman-Rubin potential outcomes framework into DRL. Unlike most methods that\nfocus on bounding the counterfactual loss, we establish a causal bound on the\nfactual loss, which is analogous to the on-policy loss in DRL. This bound is\ncomputed by storing past value network outputs in the experience replay buffer,\neffectively utilizing data that is usually discarded. Extensive experiments\nacross the Atari 2600 and MuJoCo domains on various agents, such as DQN and\nSAC, achieve up to 2,427% higher reward ratio, outperforming the same agents\nwithout our proposed term, and reducing the experience replay buffer size by up\nto 96%, significantly improving sample efficiency at negligible cost.\n","authors":["Tal Fiskus","Uri Shaham"],"pdf_url":"https://arxiv.org/pdf/2507.11269v1.pdf","comment":"51 pages, 16 figures"},{"id":"http://arxiv.org/abs/2302.05545v3","updated":"2025-07-15T12:42:15Z","published":"2023-02-10T23:19:30Z","title":"Privacy Against Agnostic Inference Attacks in Vertical Federated\n  Learning","summary":"  A novel form of inference attack in vertical federated learning (VFL) is\nproposed, where two parties collaborate in training a machine learning (ML)\nmodel. Logistic regression is considered for the VFL model. One party, referred\nto as the active party, possesses the ground truth labels of the samples in the\ntraining phase, while the other, referred to as the passive party, only shares\na separate set of features corresponding to these samples. It is shown that the\nactive party can carry out inference attacks on both training and prediction\nphase samples by acquiring an ML model independently trained on the training\nsamples available to them. This type of inference attack does not require the\nactive party to be aware of the score of a specific sample, hence it is\nreferred to as an agnostic inference attack. It is shown that utilizing the\nobserved confidence scores during the prediction phase, before the time of the\nattack, can improve the performance of the active party's autonomous ML model,\nand thus improve the quality of the agnostic inference attack. As a\ncountermeasure, privacy-preserving schemes (PPSs) are proposed. While the\nproposed schemes preserve the utility of the VFL model, they systematically\ndistort the VFL parameters corresponding to the passive party's features. The\nlevel of the distortion imposed on the passive party's parameters is\nadjustable, giving rise to a trade-off between privacy of the passive party and\ninterpretabiliy of the VFL outcomes by the active party. The distortion level\nof the passive party's parameters could be chosen carefully according to the\nprivacy and interpretabiliy concerns of the passive and active parties,\nrespectively, with the hope of keeping both parties (partially) satisfied.\nFinally, experimental results demonstrate the effectiveness of the proposed\nattack and the PPSs.\n","authors":["Morteza Varasteh"],"pdf_url":"https://arxiv.org/pdf/2302.05545v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00582v2","updated":"2025-07-15T12:40:48Z","published":"2025-05-01T15:14:32Z","title":"Block Circulant Adapter for Large Language Models","summary":"  Fine-tuning large language models (LLMs) is difficult due to their huge model\nsize. Recent Fourier domain-based methods show potential for reducing\nfine-tuning costs. We propose a block circulant matrix-based fine-tuning method\nwith a stable training heuristic to leverage the properties of circulant\nmatrices and one-dimensional Fourier transforms to reduce storage and\ncomputation costs. Experiments show that our method uses $14\\times$ less number\nof parameters than VeRA, $16\\times$ smaller than LoRA and $32\\times$ less FLOPs\nthan FourierFT, while maintaining close or better task performance. Our\napproach presents a promising way in frequency domain to fine-tune large models\non downstream tasks.\n","authors":["Xinyu Ding","Meiqi Wang","Siyu Liao","Zhongfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2505.00582v2.pdf","comment":"to appear in Proceedings of the 2025 International Joint Conference\n  on Artificial Intelligence (IJCAI-2025)"},{"id":"http://arxiv.org/abs/2312.10705v2","updated":"2025-07-15T12:35:13Z","published":"2023-12-17T12:50:10Z","title":"Learning Safe Numeric Planning Action Models","summary":"  A significant challenge in applying planning technology to real-world\nproblems lies in obtaining a planning model that accurately represents the\nproblem's dynamics. Obtaining a planning model is even more challenging in\nmission-critical domains, where a trial-and-error approach to learning how to\nact is not an option. In such domains, the action model used to generate plans\nmust be safe, in the sense that plans generated with it must be applicable and\nachieve their goals. % Learning safe action models for planning has been mostly\nexplored for domains in which states are sufficiently described with Boolean\nvariables. % In this work, we go beyond this limitation and propose the Numeric\nSafe Action Models Learning (N-SAM) algorithm. In this work, we present N-SAM,\nan action model learning algorithm capable of learning safe numeric\npreconditions and effects. We prove that N-SAM runs in linear time in the\nnumber of observations and, under certain conditions, is guaranteed to return\nsafe action models. However, to preserve this safety guarantee, N-SAM must\nobserve a substantial number of examples for each action before including it in\nthe learned model. We address this limitation of N-SAM and propose N-SAM*, an\nextension to the N-SAM algorithm that always returns an action model where\nevery observed action is applicable at least in some states, even if it was\nobserved only once. N-SAM* does so without compromising the safety of the\nreturned action model. We prove that N-SAM* is optimal in terms of sample\ncomplexity compared to any other algorithm that guarantees safety. N-SAM and\nN-SAM* are evaluated over an extensive benchmark of numeric planning domains,\nand their performance is compared to a state-of-the-art numeric action model\nlearning algorithm. We also provide a discussion on the impact of numerical\naccuracy on the learning process.\n","authors":["Argaman Mordoch","Shahaf S. Shperberg","Roni Stern","Berndan Juba"],"pdf_url":"https://arxiv.org/pdf/2312.10705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11262v1","updated":"2025-07-15T12:35:13Z","published":"2025-07-15T12:35:13Z","title":"LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy\n  Environments","summary":"  Training deep neural networks, particularly in computer vision tasks, often\nsuffers from noisy gradients and unstable convergence, which hinder performance\nand generalization. In this paper, we propose LyAm, a novel optimizer that\nintegrates Adam's adaptive moment estimation with Lyapunov-based stability\nmechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability\ntheory to enhance convergence robustness and mitigate training noise. We\nprovide a rigorous theoretical framework proving the convergence guarantees of\nLyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10\nand CIFAR-100 show that LyAm consistently outperforms state-of-the-art\noptimizers in terms of accuracy, convergence speed, and stability, establishing\nit as a strong candidate for robust deep learning optimization.\n","authors":["Elmira Mirzabeigi","Sepehr Rezaee","Kourosh Parand"],"pdf_url":"https://arxiv.org/pdf/2507.11262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04696v4","updated":"2025-07-15T12:30:00Z","published":"2024-11-07T13:29:32Z","title":"The Pragmatic Frames of Spurious Correlations in Machine Learning:\n  Interpreting How and Why They Matter","summary":"  Learning correlations from data forms the foundation of today's machine\nlearning (ML) and artificial intelligence (AI) research. While contemporary\nmethods enable the automatic discovery of complex patterns, they are prone to\nfailure when unintended correlations are captured. This vulnerability has\nspurred a growing interest in interrogating spuriousness, which is often seen\nas a threat to model performance, fairness, and robustness. In this article, we\ntrace departures from the conventional statistical definition of spuriousness\n-- which denotes a non-causal relationship arising from coincidence or\nconfounding -- to examine how its meaning is negotiated in ML research. Rather\nthan relying solely on formal definitions, researchers assess spuriousness\nthrough what we call pragmatic frames: judgments based on what a correlation\ndoes in practice -- how it affects model behavior, supports or impedes task\nperformance, or aligns with broader normative goals. Drawing on a broad survey\nof ML literature, we identify four such frames: relevance (\"Models should use\ncorrelations that are relevant to the task\"), generalizability (\"Models should\nuse correlations that generalize to unseen data\"), human-likeness (\"Models\nshould use correlations that a human would use to perform the same task\"), and\nharmfulness (\"Models should use correlations that are not socially or ethically\nharmful\"). These representations reveal that correlation desirability is not a\nfixed statistical property but a situated judgment informed by technical,\nepistemic, and ethical considerations. By examining how a foundational ML\nconundrum is problematized in research literature, we contribute to broader\nconversations on the contingent practices through which technical concepts like\nspuriousness are defined and operationalized.\n","authors":["Samuel J. Bell","Skyler Wang"],"pdf_url":"https://arxiv.org/pdf/2411.04696v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11247v1","updated":"2025-07-15T12:21:52Z","published":"2025-07-15T12:21:52Z","title":"Fairness-Aware Grouping for Continuous Sensitive Variables: Application\n  for Debiasing Face Analysis with respect to Skin Tone","summary":"  Within a legal framework, fairness in datasets and models is typically\nassessed by dividing observations into predefined groups and then computing\nfairness measures (e.g., Disparate Impact or Equality of Odds with respect to\ngender). However, when sensitive attributes such as skin color are continuous,\ndividing into default groups may overlook or obscure the discrimination\nexperienced by certain minority subpopulations. To address this limitation, we\npropose a fairness-based grouping approach for continuous (possibly\nmultidimensional) sensitive attributes. By grouping data according to observed\nlevels of discrimination, our method identifies the partition that maximizes a\nnovel criterion based on inter-group variance in discrimination, thereby\nisolating the most critical subgroups.\n  We validate the proposed approach using multiple synthetic datasets and\ndemonstrate its robustness under changing population distributions - revealing\nhow discrimination is manifested within the space of sensitive attributes.\nFurthermore, we examine a specialized setting of monotonic fairness for the\ncase of skin color. Our empirical results on both CelebA and FFHQ, leveraging\nthe skin tone as predicted by an industrial proprietary algorithm, show that\nthe proposed segmentation uncovers more nuanced patterns of discrimination than\npreviously reported, and that these findings remain stable across datasets for\na given model. Finally, we leverage our grouping model for debiasing purpose,\naiming at predicting fair scores with group-by-group post-processing. The\nresults demonstrate that our approach improves fairness while having minimal\nimpact on accuracy, thus confirming our partition method and opening the door\nfor industrial deployment.\n","authors":["Veronika Shilova","Emmanuel Malherbe","Giovanni Palma","Laurent Risser","Jean-Michel Loubes"],"pdf_url":"https://arxiv.org/pdf/2507.11247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11246v1","updated":"2025-07-15T12:21:30Z","published":"2025-07-15T12:21:30Z","title":"Generative Click-through Rate Prediction with Applications to Search\n  Advertising","summary":"  Click-Through Rate (CTR) prediction models are integral to a myriad of\nindustrial settings, such as personalized search advertising. Current methods\ntypically involve feature extraction from users' historical behavior sequences\ncombined with product information, feeding into a discriminative model that is\ntrained on user feedback to estimate CTR. With the success of models such as\nGPT, the potential for generative models to enrich expressive power beyond\ndiscriminative models has become apparent. In light of this, we introduce a\nnovel model that leverages generative models to enhance the precision of CTR\npredictions in discriminative models. To reconcile the disparate data\naggregation needs of both model types, we design a two-stage training process:\n1) Generative pre-training for next-item prediction with the given item\ncategory in user behavior sequences; 2) Fine-tuning the well-trained generative\nmodel within a discriminative CTR prediction framework. Our method's efficacy\nis substantiated through extensive experiments on a new dataset, and its\nsignificant utility is further corroborated by online A/B testing results.\nCurrently, the model is deployed on one of the world's largest e-commerce\nplatforms, and we intend to release the associated code and dataset in the\nfuture.\n","authors":["Lingwei Kong","Lu Wang","Changping Peng","Zhangang Lin","Ching Law","Jingping Shao"],"pdf_url":"https://arxiv.org/pdf/2507.11246v1.pdf","comment":"This work was first submitted on February 9, 2024"},{"id":"http://arxiv.org/abs/2503.21073v3","updated":"2025-07-15T12:15:41Z","published":"2025-03-27T01:17:06Z","title":"Shared Global and Local Geometry of Language Model Embeddings","summary":"  Researchers have recently suggested that models share common representations.\nIn our work, we find numerous geometric similarities across the token\nembeddings of large language models. First, we find ``global'' similarities:\ntoken embeddings often share similar relative orientations. Next, we\ncharacterize local geometry in two ways: (1) by using Locally Linear\nEmbeddings, and (2) by defining a simple measure for the intrinsic dimension of\neach embedding. Both characterizations allow us to find local similarities\nacross token embeddings. Additionally, our intrinsic dimension demonstrates\nthat embeddings lie on a lower dimensional manifold, and that tokens with lower\nintrinsic dimensions often have semantically coherent clusters, while those\nwith higher intrinsic dimensions do not. Based on our findings, we introduce\nEMB2EMB, a simple application to linearly transform steering vectors from one\nlanguage model to another, despite the two models having different dimensions.\n","authors":["Andrew Lee","Melanie Weber","Fernanda Viégas","Martin Wattenberg"],"pdf_url":"https://arxiv.org/pdf/2503.21073v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22526v2","updated":"2025-07-15T12:15:26Z","published":"2025-03-28T15:30:42Z","title":"AnnoPage Dataset: Dataset of Non-Textual Elements in Documents with\n  Fine-Grained Categorization","summary":"  We introduce the AnnoPage Dataset, a novel collection of 7,550 pages from\nhistorical documents, primarily in Czech and German, spanning from 1485 to the\npresent, focusing on the late 19th and early 20th centuries. The dataset is\ndesigned to support research in document layout analysis and object detection.\nEach page is annotated with axis-aligned bounding boxes (AABB) representing\nelements of 25 categories of non-textual elements, such as images, maps,\ndecorative elements, or charts, following the Czech Methodology of image\ndocument processing. The annotations were created by expert librarians to\nensure accuracy and consistency. The dataset also incorporates pages from\nmultiple, mainly historical, document datasets to enhance variability and\nmaintain continuity. The dataset is divided into development and test subsets,\nwith the test set carefully selected to maintain the category distribution. We\nprovide baseline results using YOLO and DETR object detectors, offering a\nreference point for future research. The AnnoPage Dataset is publicly available\non Zenodo (https://doi.org/10.5281/zenodo.12788419), along with ground-truth\nannotations in YOLO format.\n","authors":["Martin Kišš","Michal Hradiš","Martina Dvořáková","Václav Jiroušek","Filip Kersch"],"pdf_url":"https://arxiv.org/pdf/2503.22526v2.pdf","comment":"17 pages, 2 tables, 7 figures; Accepted to GREC Workshop at ICDAR2025"},{"id":"http://arxiv.org/abs/2501.03461v3","updated":"2025-07-15T12:08:06Z","published":"2025-01-07T01:35:56Z","title":"Few-Shot Radar Signal Recognition through Self-Supervised Learning and\n  Radio Frequency Domain Adaptation","summary":"  Radar signal recognition (RSR) plays a pivotal role in electronic warfare\n(EW), as accurately classifying radar signals is critical for informing\ndecision-making. Recent advances in deep learning have shown significant\npotential in improving RSR in domains with ample annotated data. However, these\nmethods fall short in EW scenarios where annotated radio frequency (RF) data\nare scarce or impractical to obtain. To address these challenges, we introduce\na self-supervised learning (SSL) method which utilises masked signal modelling\nand RF domain adaption to perform few-shot RSR and enhance performance in\nenvironments with limited RF samples and annotations. We propose a two-step\napproach, first pre-training masked autoencoders (MAE) on baseband in-phase and\nquadrature (I/Q) signals from diverse RF domains, and then transferring the\nlearned representations to the radar domain, where annotated data are scarce.\nEmpirical results show that our lightweight self-supervised ResNet1D model with\ndomain adaptation achieves up to a 17.5% improvement in 1-shot classification\naccuracy when pre-trained on in-domain signals (i.e., radar signals) and up to\na 16.31% improvement when pre-trained on out-of-domain signals (i.e., comm\nsignals), compared to its baseline without using SSL. We also present reference\nresults for several MAE designs and pre-training strategies, establishing a new\nbenchmark for few-shot radar signal classification.\n","authors":["Zi Huang","Simon Denman","Akila Pemasiri","Clinton Fookes","Terrence Martin"],"pdf_url":"https://arxiv.org/pdf/2501.03461v3.pdf","comment":"6 pages, 15 figures"},{"id":"http://arxiv.org/abs/2507.11236v1","updated":"2025-07-15T12:06:11Z","published":"2025-07-15T12:06:11Z","title":"Improved sampling algorithms and Poincaré inequalities for\n  non-log-concave distributions","summary":"  We study the problem of sampling from a distribution $\\mu$ with density\n$\\propto e^{-V}$ for some potential function $V:\\mathbb R^d\\to \\mathbb R$ with\nquery access to $V$ and $\\nabla V$. We start with the following standard\nassumptions:\n  (1) The potential function $V$ is $L$-smooth.\n  (2) The second moment $\\mathbf{E}_{X\\sim \\mu}[\\|X\\|^2]\\leq M$.\n  Recently, He and Zhang (COLT'25) showed that the query complexity of sampling\nfrom such distributions is at least\n$\\left(\\frac{LM}{d\\epsilon}\\right)^{\\Omega(d)}$ where $\\epsilon$ is the desired\naccuracy in total variation distance, and the Poincar\\'e constant can be\narbitrarily large.\n  Meanwhile, another common assumption in the study of diffusion based samplers\n(see e.g., the work of Chen, Chewi, Li, Li, Salim and Zhang (ICLR'23))\nstrengthens the smoothness condition (1) to the following:\n  (1*) The potential function of *every* distribution along the\nOrnstein-Uhlenbeck process starting from $\\mu$ is $L$-smooth.\n  We show that under the assumptions (1*) and (2), the query complexity of\nsampling from $\\mu$ can be $\\mathrm{poly}(L,d)\\cdot\n\\left(\\frac{Ld+M}{\\epsilon^2}\\right)^{\\mathcal{O}(L+1)}$, which is polynomial\nin $d$ and $\\frac{1}{\\epsilon}$ when $L=\\mathcal{O}(1)$ and\n$M=\\mathrm{poly}(d)$. This improves the algorithm with quasi-polynomial query\ncomplexity developed by Huang et al. (COLT'24). Our results imply that the\nseemly moderate strengthening of the smoothness condition (1) to (1*) can lead\nto an exponential gap in the query complexity of sampling algorithms.\n  Moreover, we show that together with the assumption (1*) and the stronger\nmoment assumption that $\\|X\\|$ is $\\lambda$-sub-Gaussian for $X\\sim\\mu$, the\nPoincar\\'e constant of $\\mu$ is at most $\\mathcal{O}(\\lambda)^{2(L+1)}$. As an\napplication of our technique, we obtain improved estimate of the Poincar\\'e\nconstant for mixture of Gaussians with the same covariance.\n","authors":["Yuchen He","Zhehan Lei","Jianan Shao","Chihao Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.11236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11229v1","updated":"2025-07-15T11:59:15Z","published":"2025-07-15T11:59:15Z","title":"DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway\n  Global-Local Fusion","summary":"  Knowledge graphs (KGs) are vital for enabling knowledge reasoning across\nvarious domains. Recent KG reasoning methods that integrate both global and\nlocal information have achieved promising results. However, existing methods\noften suffer from score over-smoothing, which blurs the distinction between\ncorrect and incorrect answers and hinders reasoning effectiveness. To address\nthis, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with\ndual-pathway global-local fusion. DuetGraph tackles over-smoothing by\nsegregating -- rather than stacking -- the processing of local (via message\npassing) and global (via attention) information into two distinct pathways,\npreventing mutual interference and preserving representational discrimination.\nIn addition, DuetGraph introduces a coarse-to-fine optimization, which\npartitions entities into high- and low-score subsets. This strategy narrows the\ncandidate space and sharpens the score gap between the two subsets, which\nalleviates over-smoothing and enhances inference quality. Extensive experiments\non various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)\nperformance, with up to an 8.7% improvement in reasoning quality and a\n1.8$\\times$ acceleration in training efficiency.\n","authors":["Jin Li","Zezhong Ding","Xike Xie"],"pdf_url":"https://arxiv.org/pdf/2507.11229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11737v3","updated":"2025-07-15T11:59:01Z","published":"2025-03-14T14:44:54Z","title":"Multi-View Node Pruning for Accurate Graph Representation","summary":"  Graph pooling, which compresses a whole graph into a smaller coarsened graph,\nis an essential component of graph representation learning. To efficiently\ncompress a given graph, graph pooling methods often drop their nodes with\nattention-based scoring with the task loss. However, this often results in\nsimply removing nodes with lower degrees without consideration of their\nfeature-level relevance to the given task. To fix this problem, we propose a\nMulti-View Pruning(MVP), a graph pruning method based on a multi-view framework\nand reconstruction loss. Given a graph, MVP first constructs multiple graphs\nfor different views either by utilizing the predefined modalities or by\nrandomly partitioning the input features, to consider the importance of each\nnode in diverse perspectives. Then, it learns the score for each node by\nconsidering both the reconstruction and the task loss. MVP can be incorporated\nwith any hierarchical pooling framework to score the nodes. We validate MVP on\nmultiple benchmark datasets by coupling it with two graph pooling methods, and\nshow that it significantly improves the performance of the base graph pooling\nmethod, outperforming all baselines. Further analysis shows that both the\nencoding of multiple views and the consideration of reconstruction loss are the\nkey to the success of MVP, and that it indeed identifies nodes that are less\nimportant according to domain knowledge.\n","authors":["Jiseong Park","Hanjin Kim","Seojin Kim","Jueun Choi","Doheon Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2503.11737v3.pdf","comment":"Jiseong Park and Hanjin Kim are co-first author for this work"},{"id":"http://arxiv.org/abs/2402.12683v3","updated":"2025-07-15T11:58:58Z","published":"2024-02-20T03:14:47Z","title":"TorchCP: A Python Library for Conformal Prediction","summary":"  Conformal prediction (CP) is a robust statistical framework that generates\nprediction intervals or sets with guaranteed coverage probability, addressing\nthe challenge of quantifying predictive uncertainty in deep learning. Despite\nadvancements in deep learning architectures and datasets, reliable uncertainty\nestimation remains elusive, making CP increasingly vital. This paper introduces\nTorchCP, a PyTorch-native library designed to integrate state-of-the-art CP\nalgorithms into deep learning tasks, including classification, regression,\ngraph neural networks, and large language models. TorchCP offers a\ncomprehensive suite of advanced methodologies, a modular design for easy\ncustomization, and full GPU-accelerated scalability. Released under the\nLGPL-3.0 license, TorchCP has gained widespread adoption with over 12,582 PyPi\ndownloads. It is supported by approximately 16,132 lines of code, 564 unit\ntests achieving 100\\% coverage, and comprehensive documentation. By bridging\nstatistics and computer science, TorchCP empowers researchers and practitioners\nto advance conformal prediction in diverse deep learning applications.\n","authors":["Jianguo Huang","Jianqing Song","Xuanning Zhou","Bingyi Jing","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2402.12683v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11228v1","updated":"2025-07-15T11:58:42Z","published":"2025-07-15T11:58:42Z","title":"Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with\n  Data on the Sphere?","summary":"  Gradient descent (GD) on logistic regression has many fascinating properties.\nWhen the dataset is linearly separable, it is known that the iterates converge\nin direction to the maximum-margin separator regardless of how large the step\nsize is. In the non-separable case, however, it has been shown that GD can\nexhibit a cycling behaviour even when the step sizes is still below the\nstability threshold $2/\\lambda$, where $\\lambda$ is the largest eigenvalue of\nthe Hessian at the solution. This short paper explores whether restricting the\ndata to have equal magnitude is a sufficient condition for global convergence,\nunder any step size below the stability threshold. We prove that this is true\nin a one dimensional space, but in higher dimensions cycling behaviour can\nstill occur. We hope to inspire further studies on quantifying how common these\ncycles are in realistic datasets, as well as finding sufficient conditions to\nguarantee global convergence with large step sizes.\n","authors":["Si Yi Meng","Baptiste Goujaud","Antonio Orvieto","Christopher De Sa"],"pdf_url":"https://arxiv.org/pdf/2507.11228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.18629v2","updated":"2025-07-15T11:52:41Z","published":"2025-06-23T13:35:06Z","title":"On Equivariant Model Selection through the Lens of Uncertainty","summary":"  Equivariant models leverage prior knowledge on symmetries to improve\npredictive performance, but misspecified architectural constraints can harm it\ninstead. While work has explored learning or relaxing constraints, selecting\namong pretrained models with varying symmetry biases remains challenging. We\nexamine this model selection task from an uncertainty-aware perspective,\ncomparing frequentist (via Conformal Prediction), Bayesian (via the marginal\nlikelihood), and calibration-based measures to naive error-based evaluation. We\nfind that uncertainty metrics generally align with predictive performance, but\nBayesian model evidence does so inconsistently. We attribute this to a mismatch\nin Bayesian and geometric notions of model complexity for the employed\nlast-layer Laplace approximation, and discuss possible remedies. Our findings\npoint towards the potential of uncertainty in guiding symmetry-aware model\nselection.\n","authors":["Putri A. van der Linden","Alexander Timans","Dharmesh Tailor","Erik J. Bekkers"],"pdf_url":"https://arxiv.org/pdf/2506.18629v2.pdf","comment":"9 pages, 4 figures, 2 tables. In the 8th Workshop on Tractable\n  Probabilistic Modeling at UAI 2025"},{"id":"http://arxiv.org/abs/2507.07817v2","updated":"2025-07-15T11:42:05Z","published":"2025-07-10T14:46:33Z","title":"On the Effect of Instruction Tuning Loss on Generalization","summary":"  Instruction Tuning has emerged as a pivotal post-training paradigm that\nenables pre-trained language models to better follow user instructions. Despite\nits significance, little attention has been given to optimizing the loss\nfunction used. A fundamental, yet often overlooked, question is whether the\nconventional auto-regressive objective - where loss is computed only on\nresponse tokens, excluding prompt tokens - is truly optimal for instruction\ntuning. In this work, we systematically investigate the impact of\ndifferentially weighting prompt and response tokens in instruction tuning loss,\nand propose Weighted Instruction Tuning (WIT) as a better alternative to\nconventional instruction tuning. Through extensive experiments on five language\nmodels of different families and scale, three finetuning datasets of different\nsizes, and five diverse evaluation benchmarks, we show that the standard\ninstruction tuning loss often yields suboptimal performance and limited\nrobustness to input prompt variations. We find that a low-to-moderate weight\nfor prompt tokens coupled with a moderate-to-high weight for response tokens\nyields the best-performing models across settings and also serve as better\nstarting points for the subsequent preference alignment training. These\nfindings highlight the need to reconsider instruction tuning loss and offer\nactionable insights for developing more robust and generalizable models. Our\ncode is open-sourced at https://github.com/kowndinya-renduchintala/WIT.\n","authors":["Anwoy Chatterjee","H S V N S Kowndinya Renduchintala","Sumit Bhatia","Tanmoy Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2507.07817v2.pdf","comment":"To appear in Transactions of the Association for Computational\n  Linguistics (TACL)"},{"id":"http://arxiv.org/abs/2507.00838v2","updated":"2025-07-15T11:31:45Z","published":"2025-07-01T15:08:53Z","title":"Stylometry recognizes human and LLM-generated texts in short samples","summary":"  The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.\n","authors":["Karol Przystalski","Jan K. Argasiński","Iwona Grabska-Gradzińska","Jeremi K. Ochab"],"pdf_url":"https://arxiv.org/pdf/2507.00838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10867v3","updated":"2025-07-15T11:31:28Z","published":"2024-07-15T16:12:51Z","title":"Provable Robustness of (Graph) Neural Networks Against Data Poisoning\n  and Backdoor Attacks","summary":"  Generalization of machine learning models can be severely compromised by data\npoisoning, where adversarial changes are applied to the training data. This\nvulnerability has led to interest in certifying (i.e., proving) that such\nchanges up to a certain magnitude do not affect test predictions. We, for the\nfirst time, certify Graph Neural Networks (GNNs) against poisoning attacks,\nincluding backdoors, targeting the node features of a given graph. Our\ncertificates are white-box and based upon $(i)$ the neural tangent kernel,\nwhich characterizes the training dynamics of sufficiently wide networks; and\n$(ii)$ a novel reformulation of the bilevel optimization problem describing\npoisoning as a mixed-integer linear program. Consequently, we leverage our\nframework to provide fundamental insights into the role of graph structure and\nits connectivity on the worst-case robustness behavior of convolution-based and\nPageRank-based GNNs. We note that our framework is more general and constitutes\nthe first approach to derive white-box poisoning certificates for NNs, which\ncan be of independent interest beyond graph-related tasks.\n","authors":["Lukas Gosch","Mahalakshmi Sabanayagam","Debarghya Ghoshdastidar","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2407.10867v3.pdf","comment":"Published in TMLR. Best Paper Award at the AdvML-Frontiers @ NeurIPS\n  2024 workshop. Code available at https://github.com/saper0/qpcert"},{"id":"http://arxiv.org/abs/2411.16370v5","updated":"2025-07-15T11:27:39Z","published":"2024-11-25T13:26:09Z","title":"A Review of Bayesian Uncertainty Quantification in Deep Probabilistic\n  Image Segmentation","summary":"  Advances in architectural design, data availability, and compute have driven\nremarkable progress in semantic segmentation. Yet, these models often rely on\nrelaxed Bayesian assumptions, omitting critical uncertainty information needed\nfor robust decision-making. The resulting reliance on point estimates has\nfueled interest in probabilistic segmentation, but the literature remains\nfragmented. In response, this review consolidates and contextualizes\nfoundational concepts in uncertainty modeling, including the non-trivial task\nof distinguishing between epistemic and aleatoric uncertainty and examining\ntheir roles across four key downstream segmentation tasks, highlighting Active\nLearning as particularly promising. By unifying theory, terminology, and\napplications, we provide a coherent foundation for researchers and identify\ncritical challenges, such as strong assumptions in spatial aggregation, lack of\nstandardized benchmarks, and pitfalls in current uncertainty quantification\nmethods. We identify trends such as the adoption of contemporary generative\nmodels, driven by advances in the broader field of generative modeling, with\nsegmentation-specific innovation primarily in the conditioning mechanisms.\nMoreover, we observe growing interest in distribution- and sampling-free\napproaches to uncertainty estimation. We further propose directions for\nadvancing uncertainty-aware segmentation in deep learning, including pragmatic\nstrategies for disentangling different sources of uncertainty, novel\nuncertainty modeling approaches and improved Transformer-based backbones. In\nthis way, we aim to support the development of more reliable, efficient, and\ninterpretable segmentation models that effectively incorporate uncertainty into\nreal-world applications.\n","authors":["M. M. A. Valiuddin","R. J. G. van Sloun","C. G. A. Viviers","P. H. N. de With","F. van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2411.16370v5.pdf","comment":"31 pages of content, revised"},{"id":"http://arxiv.org/abs/2507.11202v1","updated":"2025-07-15T11:15:35Z","published":"2025-07-15T11:15:35Z","title":"A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion\n  Recognition","summary":"  Multimodal Emotion Recognition (MER) often encounters incomplete\nmultimodality in practical applications due to sensor failures or privacy\nprotection requirements. While existing methods attempt to address various\nincomplete multimodal scenarios by balancing the training of each modality\ncombination through additional gradients, these approaches face a critical\nlimitation: training gradients from different modality combinations conflict\nwith each other, ultimately degrading the performance of the final prediction\nmodel. In this paper, we propose a unimodal decoupled dynamic low-rank\nadaptation method based on modality combinations, named MCULoRA, which is a\nnovel framework for the parameter-efficient training of incomplete multimodal\nlearning models. MCULoRA consists of two key modules, modality combination\naware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The\nMCLA module effectively decouples the shared information from the distinct\ncharacteristics of individual modality combinations. The DPFT module adjusts\nthe training ratio of modality combinations based on the separability of each\nmodality's representation space, optimizing the learning efficiency across\ndifferent modality combinations. Our extensive experimental evaluation in\nmultiple benchmark datasets demonstrates that MCULoRA substantially outperforms\nprevious incomplete multimodal learning approaches in downstream task accuracy.\n","authors":["Xinkui Zhao","Jinsong Shu","Yangyang Wu","Guanjie Cheng","Zihe Liu","Naibo Wang","Shuiguang Deng","Zhongle Xie","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2507.11202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.05906v2","updated":"2025-07-15T11:07:33Z","published":"2025-07-08T11:45:51Z","title":"Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why","summary":"  This survey provides a comparative analysis of feature-based and GAN-based\napproaches to learning from demonstrations, with a focus on the structure of\nreward functions and their implications for policy learning. Feature-based\nmethods offer dense, interpretable rewards that excel at high-fidelity motion\nimitation, yet often require sophisticated representations of references and\nstruggle with generalization in unstructured settings. GAN-based methods, in\ncontrast, use implicit, distributional supervision that enables scalability and\nadaptation flexibility, but are prone to training instability and coarse reward\nsignals. Recent advancements in both paradigms converge on the importance of\nstructured motion representations, which enable smoother transitions,\ncontrollable synthesis, and improved task integration. We argue that the\ndichotomy between feature-based and GAN-based methods is increasingly nuanced:\nrather than one paradigm dominating the other, the choice should be guided by\ntask-specific priorities such as fidelity, diversity, interpretability, and\nadaptability. This work outlines the algorithmic trade-offs and design\nconsiderations that underlie method selection, offering a framework for\nprincipled decision-making in learning from demonstrations.\n","authors":["Chenhao Li","Marco Hutter","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2507.05906v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11192v1","updated":"2025-07-15T10:52:57Z","published":"2025-07-15T10:52:57Z","title":"Recent Advances in Simulation-based Inference for Gravitational Wave\n  Data Analysis","summary":"  The detection of gravitational waves by the LIGO-Virgo-KAGRA collaboration\nhas ushered in a new era of observational astronomy, emphasizing the need for\nrapid and detailed parameter estimation and population-level analyses.\nTraditional Bayesian inference methods, particularly Markov chain Monte Carlo,\nface significant computational challenges when dealing with the\nhigh-dimensional parameter spaces and complex noise characteristics inherent in\ngravitational wave data. This review examines the emerging role of\nsimulation-based inference methods in gravitational wave astronomy, with a\nfocus on approaches that leverage machine-learning techniques such as\nnormalizing flows and neural posterior estimation. We provide a comprehensive\noverview of the theoretical foundations underlying various simulation-based\ninference methods, including neural posterior estimation, neural ratio\nestimation, neural likelihood estimation, flow matching, and consistency\nmodels. We explore the applications of these methods across diverse\ngravitational wave data processing scenarios, from single-source parameter\nestimation and overlapping signal analysis to testing general relativity and\nconducting population studies. Although these techniques demonstrate speed\nimprovements over traditional methods in controlled studies, their\nmodel-dependent nature and sensitivity to prior assumptions are barriers to\ntheir widespread adoption. Their accuracy, which is similar to that of\nconventional methods, requires further validation across broader parameter\nspaces and noise conditions.\n","authors":["Bo Liang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2507.11192v1.pdf","comment":"30 pages, 6 figures, 1 table. Published version accepted by\n  Astronomical Techniques and Instruments (ATI)"},{"id":"http://arxiv.org/abs/2507.11191v1","updated":"2025-07-15T10:52:45Z","published":"2025-07-15T10:52:45Z","title":"Data-Driven Differential Evolution in Tire Industry Extrusion:\n  Leveraging Surrogate Models","summary":"  The optimization of industrial processes remains a critical challenge,\nparticularly when no mathematical formulation of objective functions or\nconstraints is available. This study addresses this issue by proposing a\nsurrogate-based, data-driven methodology for optimizing complex real-world\nmanufacturing systems using only historical process data. Machine learning\nmodels are employed to approximate system behavior and construct surrogate\nmodels, which are integrated into a tailored metaheuristic approach:\nData-Driven Differential Evolution with Multi-Level Penalty Functions and\nSurrogate Models, an adapted version of Differential Evolution suited to the\ncharacteristics of the studied process. The methodology is applied to an\nextrusion process in the tire manufacturing industry, with the goal of\noptimizing initialization parameters to reduce waste and production time.\nResults show that the surrogate-based optimization approach outperforms\nhistorical best configurations, achieving a 65\\% reduction in initialization\nand setup time, while also significantly minimizing material waste. These\nfindings highlight the potential of combining data-driven modeling and\nmetaheuristic optimization for industrial processes where explicit formulations\nare unavailable.\n","authors":["Eider Garate-Perez","Kerman López de Calle-Etxabe","Susana Ferreiro"],"pdf_url":"https://arxiv.org/pdf/2507.11191v1.pdf","comment":"22 pages, 15 figures"},{"id":"http://arxiv.org/abs/2507.11187v1","updated":"2025-07-15T10:41:55Z","published":"2025-07-15T10:41:55Z","title":"Striking the Perfect Balance: Preserving Privacy While Boosting Utility\n  in Collaborative Medical Prediction Platforms","summary":"  Online collaborative medical prediction platforms offer convenience and\nreal-time feedback by leveraging massive electronic health records. However,\ngrowing concerns about privacy and low prediction quality can deter patient\nparticipation and doctor cooperation. In this paper, we first clarify the\nprivacy attacks, namely attribute attacks targeting patients and model\nextraction attacks targeting doctors, and specify the corresponding privacy\nprinciples. We then propose a privacy-preserving mechanism and integrate it\ninto a novel one-shot distributed learning framework, aiming to simultaneously\nmeet both privacy requirements and prediction performance objectives. Within\nthe framework of statistical learning theory, we theoretically demonstrate that\nthe proposed distributed learning framework can achieve the optimal prediction\nperformance under specific privacy requirements. We further validate the\ndeveloped privacy-preserving collaborative medical prediction platform through\nboth toy simulations and real-world data experiments.\n","authors":["Shao-Bo Lin","Xiaotong Liu","Yao Wang"],"pdf_url":"https://arxiv.org/pdf/2507.11187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11185v1","updated":"2025-07-15T10:38:38Z","published":"2025-07-15T10:38:38Z","title":"An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular\n  Disease Detection and Risk Assessment","summary":"  Heart disease remains a major global health concern, particularly in regions\nwith limited access to medical resources and diagnostic facilities. Traditional\ndiagnostic methods often fail to accurately identify and manage heart disease\nrisks, leading to adverse outcomes. Machine learning has the potential to\nsignificantly enhance the accuracy, efficiency, and speed of heart disease\ndiagnosis. In this study, we proposed a comprehensive framework that combines\nclassification models for heart disease detection and regression models for\nrisk prediction. We employed the Heart Disease dataset, which comprises 1,035\ncases. To address the issue of class imbalance, the Synthetic Minority\nOversampling Technique (SMOTE) was applied, resulting in the generation of an\nadditional 100,000 synthetic data points. Performance metrics, including\naccuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to\nevaluate the model's effectiveness. Among the classification models, Random\nForest emerged as the standout performer, achieving an accuracy of 97.2% on\nreal data and 97.6% on synthetic data. For regression tasks, Linear Regression\ndemonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic\ndatasets, respectively, with the lowest error metrics. Additionally,\nExplainable AI techniques were employed to enhance the interpretability of the\nmodels. This study highlights the potential of machine learning to\nrevolutionize heart disease diagnosis and risk prediction, thereby facilitating\nearly intervention and enhancing clinical decision-making.\n","authors":["Md. Emon Akter Sourov","Md. Sabbir Hossen","Pabon Shaha","Mohammad Minoar Hossain","Md Sadiq Iqbal"],"pdf_url":"https://arxiv.org/pdf/2507.11185v1.pdf","comment":"This paper has been accepted at the IEEE QPAIN 2025. The final\n  version will be available in the IEEE Xplore Digital Library"},{"id":"http://arxiv.org/abs/2507.11183v1","updated":"2025-07-15T10:37:59Z","published":"2025-07-15T10:37:59Z","title":"Quantized Rank Reduction: A Communications-Efficient Federated Learning\n  Scheme for Network-Critical Applications","summary":"  Federated learning is a machine learning approach that enables multiple\ndevices (i.e., agents) to train a shared model cooperatively without exchanging\nraw data. This technique keeps data localized on user devices, ensuring privacy\nand security, while each agent trains the model on their own data and only\nshares model updates. The communication overhead is a significant challenge due\nto the frequent exchange of model updates between the agents and the central\nserver. In this paper, we propose a communication-efficient federated learning\nscheme that utilizes low-rank approximation of neural network gradients and\nquantization to significantly reduce the network load of the decentralized\nlearning process with minimal impact on the model's accuracy.\n","authors":["Dimitrios Kritsiolis","Constantine Kotropoulos"],"pdf_url":"https://arxiv.org/pdf/2507.11183v1.pdf","comment":"In Proceedings of the 2025 IARIA Annual Congress on Frontiers in\n  Science, Technology, Services, and Applications (IARIA Congress 2025),\n  Venice, Italy, July 6-10, 2025"},{"id":"http://arxiv.org/abs/2507.11181v1","updated":"2025-07-15T10:36:43Z","published":"2025-07-15T10:36:43Z","title":"Mixture of Experts in Large Language Models","summary":"  This paper presents a comprehensive review of the Mixture-of-Experts (MoE)\narchitecture in large language models, highlighting its ability to\nsignificantly enhance model performance while maintaining minimal computational\noverhead. Through a systematic analysis spanning theoretical foundations, core\narchitectural designs, and large language model (LLM) applications, we examine\nexpert gating and routing mechanisms, hierarchical and sparse MoE\nconfigurations, meta-learning approaches, multimodal and multitask learning\nscenarios, real-world deployment cases, and recent advances and challenges in\ndeep learning. Our analysis identifies key advantages of MoE, including\nsuperior model capacity compared to equivalent Bayesian approaches, improved\ntask-specific performance, and the ability to scale model capacity efficiently.\nWe also underscore the importance of ensuring expert diversity, accurate\ncalibration, and reliable inference aggregation, as these are essential for\nmaximizing the effectiveness of MoE architectures. Finally, this review\noutlines current research limitations, open challenges, and promising future\ndirections, providing a foundation for continued innovation in MoE architecture\nand its applications.\n","authors":["Danyang Zhang","Junhao Song","Ziqian Bi","Yingfang Yuan","Tianyang Wang","Joe Yeong","Junfeng Hao"],"pdf_url":"https://arxiv.org/pdf/2507.11181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11178v1","updated":"2025-07-15T10:35:29Z","published":"2025-07-15T10:35:29Z","title":"Gradient Regularization-based Neural Granger Causality","summary":"  With the advancement of deep learning technologies, various neural\nnetwork-based Granger causality models have been proposed. Although these\nmodels have demonstrated notable improvements, several limitations remain. Most\nexisting approaches adopt the component-wise architecture, necessitating the\nconstruction of a separate model for each time series, which results in\nsubstantial computational costs. In addition, imposing the sparsity-inducing\npenalty on the first-layer weights of the neural network to extract causal\nrelationships weakens the model's ability to capture complex interactions. To\naddress these limitations, we propose Gradient Regularization-based Neural\nGranger Causality (GRNGC), which requires only one time series prediction model\nand applies $L_{1}$ regularization to the gradient between model's input and\noutput to infer Granger causality. Moreover, GRNGC is not tied to a specific\ntime series forecasting model and can be implemented with diverse architectures\nsuch as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical\nsimulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC\noutperforms existing baselines and significantly reduces computational\noverhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder\nurothelial carcinoma datasets further validate the model's effectiveness in\nreconstructing gene regulatory networks.\n","authors":["Meiliang Liu","Huiwen Dong","Xiaoxiao Yang","Yunfang Xu","Zijin Li","Zhengye Si","Xinyue Yang","Zhiwen Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.11178v1.pdf","comment":"9 pages,3 figures, conference"},{"id":"http://arxiv.org/abs/2507.11176v1","updated":"2025-07-15T10:30:45Z","published":"2025-07-15T10:30:45Z","title":"An Interpretable AI framework Quantifying Traditional Chinese Medicine\n  Principles Towards Enhancing and Integrating with Modern Biomedicine","summary":"  Traditional Chinese Medicine diagnosis and treatment principles, established\nthrough centuries of trial-and-error clinical practice, directly maps\npatient-specific symptom patterns to personalised herbal therapies. These\nempirical holistic mapping principles offer valuable strategies to address\nremaining challenges of reductionism methodologies in modern biomedicine.\nHowever, the lack of a quantitative framework and molecular-level evidence has\nlimited their interpretability and reliability. Here, we present an AI\nframework trained on ancient and classical TCM formula records to quantify the\nsymptom pattern-herbal therapy mappings. Interestingly, we find that empirical\nTCM diagnosis and treatment are consistent with the encoding-decoding processes\nin the AI model. This enables us to construct an interpretable TCM embedding\nspace (TCM-ES) using the model's quantitative representation of TCM principles.\nValidated through broad and extensive TCM patient data, the TCM-ES offers\nuniversal quantification of the TCM practice and therapeutic efficacy. We\nfurther map biomedical entities into the TCM-ES through correspondence\nalignment. We find that the principal directions of the TCM-ES are\nsignificantly associated with key biological functions (such as metabolism,\nimmune, and homeostasis), and that the disease and herb embedding proximity\naligns with their genetic relationships in the human protein interactome, which\ndemonstrate the biological significance of TCM principles. Moreover, the TCM-ES\nuncovers latent disease relationships, and provides alternative metric to\nassess clinical efficacy for modern disease-drug pairs. Finally, we construct a\ncomprehensive and integrative TCM knowledge graph, which predicts potential\nassociations between diseases and targets, drugs, herbal compounds, and herbal\ntherapies, providing TCM-informed opportunities for disease analysis and drug\ndevelopment.\n","authors":["Haoran Li","Xingye Cheng","Ziyang Huang","Jingyuan Luo","Qianqian Xu","Qiguang Zhao","Tianchen Guo","Yumeng Zhang","Linda Lidan Zhong","Zhaoxiang Bian","Leihan Tang","Aiping Lyu","Liang Tian"],"pdf_url":"https://arxiv.org/pdf/2507.11176v1.pdf","comment":"31 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.11173v1","updated":"2025-07-15T10:27:27Z","published":"2025-07-15T10:27:27Z","title":"Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in\n  Reinforcement Learning Based UAV Deconfliction","summary":"  Autonomous unmanned aerial vehicles (UAVs) rely on global navigation\nsatellite system (GNSS) pseudorange measurements for accurate real-time\nlocalization and navigation. However, this dependence exposes them to\nsophisticated spoofing threats, where adversaries manipulate pseudoranges to\ndeceive UAV receivers. Among these, drift-evasive spoofing attacks subtly\nperturb measurements, gradually diverting the UAVs trajectory without\ntriggering conventional signal-level anti-spoofing mechanisms. Traditional\ndistributional shift detection techniques often require accumulating a\nthreshold number of samples, causing delays that impede rapid detection and\ntimely response. Consequently, robust temporal-scale detection methods are\nessential to identify attack onset and enable contingency planning with\nalternative sensing modalities, improving resilience against stealthy\nadversarial manipulations. This study explores a Bayesian online change point\ndetection (BOCPD) approach that monitors temporal shifts in value estimates\nfrom a reinforcement learning (RL) critic network to detect subtle behavioural\ndeviations in UAV navigation. Experimental results show that this temporal\nvalue-based framework outperforms conventional GNSS spoofing detectors,\ntemporal semi-supervised learning frameworks, and the Page-Hinkley test,\nachieving higher detection accuracy and lower false-positive and false-negative\nrates for drift-evasive spoofing attacks.\n","authors":["Deepak Kumar Panda","Weisi Guo"],"pdf_url":"https://arxiv.org/pdf/2507.11173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11168v1","updated":"2025-07-15T10:18:32Z","published":"2025-07-15T10:18:32Z","title":"Improving Wi-Fi Network Performance Prediction with Deep Learning Models","summary":"  The increasing need for robustness, reliability, and determinism in wireless\nnetworks for industrial and mission-critical applications is the driver for the\ngrowth of new innovative methods. The study presented in this work makes use of\nmachine learning techniques to predict channel quality in a Wi-Fi network in\nterms of the frame delivery ratio. Predictions can be used proactively to\nadjust communication parameters at runtime and optimize network operations for\nindustrial applications. Methods including convolutional neural networks and\nlong short-term memory were analyzed on datasets acquired from a real Wi-Fi\nsetup across multiple channels. The models were compared in terms of prediction\naccuracy and computational complexity. Results show that the frame delivery\nratio can be reliably predicted, and convolutional neural networks, although\nslightly less effective than other models, are more efficient in terms of CPU\nusage and memory consumption. This enhances the model's usability on embedded\nand industrial systems.\n","authors":["Gabriele Formis","Amanda Ericson","Stefan Forsstrom","Kyi Thar","Gianluca Cena","Stefano Scanzio"],"pdf_url":"https://arxiv.org/pdf/2507.11168v1.pdf","comment":"preprint accepted, 8 pages, 2025"},{"id":"http://arxiv.org/abs/2310.03399v3","updated":"2025-07-15T10:16:21Z","published":"2023-10-05T09:08:47Z","title":"GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks","summary":"  Graph neural networks (GNNs) learn to represent nodes by aggregating\ninformation from their neighbors. As GNNs increase in depth, their receptive\nfield grows exponentially, leading to high memory costs. Several existing\nmethods address this by sampling a small subset of nodes, scaling GNNs to much\nlarger graphs. These methods are primarily evaluated on homophilous graphs,\nwhere neighboring nodes often share the same label. However, most of these\nmethods rely on static heuristics that may not generalize across different\ngraphs or tasks. We argue that the sampling method should be adaptive,\nadjusting to the complex structural properties of each graph. To this end, we\nintroduce GRAPES, an adaptive sampling method that learns to identify the set\nof nodes crucial for training a GNN. GRAPES trains a second GNN to predict node\nsampling probabilities by optimizing the downstream task objective. We evaluate\nGRAPES on various node classification benchmarks, involving homophilous as well\nas heterophilous graphs. We demonstrate GRAPES' effectiveness in accuracy and\nscalability, particularly in multi-label heterophilous graphs. Unlike other\nsampling methods, GRAPES maintains high accuracy even with smaller sample sizes\nand, therefore, can scale to massive graphs. Our code is publicly available at\nhttps://github.com/dfdazac/grapes.\n","authors":["Taraneh Younesian","Daniel Daza","Emile van Krieken","Thiviyan Thanapalasingam","Peter Bloem"],"pdf_url":"https://arxiv.org/pdf/2310.03399v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11161v1","updated":"2025-07-15T10:09:55Z","published":"2025-07-15T10:09:55Z","title":"How does Labeling Error Impact Contrastive Learning? A Perspective from\n  Data Dimensionality Reduction","summary":"  In recent years, contrastive learning has achieved state-of-the-art\nperformance in the territory of self-supervised representation learning. Many\nprevious works have attempted to provide the theoretical understanding\nunderlying the success of contrastive learning. Almost all of them rely on a\ndefault assumption, i.e., the label consistency assumption, which may not hold\nin practice (the probability of failure is called labeling error) due to the\nstrength and randomness of common augmentation strategies, such as random\nresized crop (RRC). This paper investigates the theoretical impact of labeling\nerror on the downstream classification performance of contrastive learning. We\nfirst reveal several significant negative impacts of labeling error on\ndownstream classification risk. To mitigate these impacts, data dimensionality\nreduction method (e.g., singular value decomposition, SVD) is applied on\noriginal data to reduce false positive samples, and establish both theoretical\nand empirical evaluations. Moreover, it is also found that SVD acts as a\ndouble-edged sword, which may lead to the deterioration of downstream\nclassification accuracy due to the reduced connectivity of the augmentation\ngraph. Based on the above observations, we give the augmentation suggestion\nthat we should use some moderate embedding dimension (such as $512, 1024$ in\nour experiments), data inflation, weak augmentation, and SVD to ensure large\ngraph connectivity and small labeling error to improve model performance.\n","authors":["Jun Chen","Hong Chen","Yonghua Yu","Yiming Ying"],"pdf_url":"https://arxiv.org/pdf/2507.11161v1.pdf","comment":"Accepted by ICML2025 as a poster"},{"id":"http://arxiv.org/abs/2310.13367v3","updated":"2025-07-15T10:01:29Z","published":"2023-10-20T09:22:51Z","title":"EASTER: Embedding Aggregation-based Heterogeneous Models Training in\n  Vertical Federated Learning","summary":"  Vertical federated learning has garnered significant attention as it allows\nclients to train machine learning models collaboratively without sharing local\ndata, which protects the client's local private data. However, existing VFL\nmethods face challenges when dealing with heterogeneous local models among\nparticipants, which affects optimization convergence and generalization. To\naddress this challenge, this paper proposes a novel approach called Vertical\nfederated learning for training multiple Heterogeneous models (VFedMH). VFedMH\nfocuses on aggregating the local embeddings of each participant's knowledge\nduring forward propagation. To protect the participants' local embedding\nvalues, we propose an embedding protection method based on lightweight blinding\nfactors. In particular, participants obtain local embedding using local\nheterogeneous models. Then the passive party, who owns only features of the\nsample, injects the blinding factor into the local embedding and sends it to\nthe active party. The active party aggregates local embeddings to obtain global\nknowledge embeddings and sends them to passive parties. The passive parties\nthen utilize the global embeddings to propagate forward on their local\nheterogeneous networks. However, the passive party does not own the sample\nlabels, so the local model gradient cannot be calculated locally. To overcome\nthis limitation, the active party assists the passive party in computing its\nlocal heterogeneous model gradients. Then, each participant trains their local\nmodel using the heterogeneous model gradients. The objective is to minimize the\nloss value of their respective local heterogeneous models. Extensive\nexperiments are conducted to demonstrate that VFedMH can simultaneously train\nmultiple heterogeneous models with heterogeneous optimization and outperform\nsome recent methods in model performance.\n","authors":["Shuo Wang","Keke Gai","Jing Yu","Liehuang Zhu","Kim-Kwang Raymond Choo","Bin Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.13367v3.pdf","comment":"15 pages, 19 figures"},{"id":"http://arxiv.org/abs/2504.02016v2","updated":"2025-07-15T09:58:14Z","published":"2025-04-02T13:20:19Z","title":"Fast Fourier Correlation is a Highly Efficient and Accurate Feature\n  Attribution Algorithm from the Perspective of Control Theory and Game Theory","summary":"  The study of neural networks from the perspective of Fourier features has\ngarnered significant attention. While existing analytical research suggests\nthat neural networks tend to learn low-frequency features, a clear attribution\nmethod for identifying the specific learned Fourier features has remained\nelusive. To bridge this gap, we propose a novel Fourier feature attribution\nmethod grounded in signal decomposition theory. Additionally, we analyze the\ndifferences between game-theoretic attribution metrics for Fourier and spatial\ndomain features, demonstrating that game-theoretic evaluation metrics are\nbetter suited for Fourier-based feature attribution.\n  Our experiments show that Fourier feature attribution exhibits superior\nfeature selection capabilities compared to spatial domain attribution methods.\nFor instance, in the case of Vision Transformers (ViTs) on the ImageNet\ndataset, only $8\\%$ of the Fourier features are required to maintain the\noriginal predictions for $80\\%$ of the samples. Furthermore, we compare the\nspecificity of features identified by our method against traditional spatial\ndomain attribution methods. Results reveal that Fourier features exhibit\ngreater intra-class concentration and inter-class distinctiveness, indicating\ntheir potential for more efficient classification and explainable AI\nalgorithms.\n","authors":["Zechen Liu","Feiyang Zhang","Wei Song","Xiang Li","Wei Wei"],"pdf_url":"https://arxiv.org/pdf/2504.02016v2.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.11143v1","updated":"2025-07-15T09:48:36Z","published":"2025-07-15T09:48:36Z","title":"RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for\n  Landslide Segmentation and Detection from Remote Sensing Images","summary":"  In recent years, landslide disasters have reported frequently due to the\nextreme weather events of droughts, floods , storms, or the consequence of\nhuman activities such as deforestation, excessive exploitation of natural\nresources. However, automatically observing landslide is challenging due to the\nextremely large observing area and the rugged topography such as mountain or\nhighland. This motivates us to propose an end-to-end deep-learning-based model\nwhich explores the remote sensing images for automatically observing landslide\nevents. By considering remote sensing images as the input data, we can obtain\nfree resource, observe large and rough terrains by time. To explore the remote\nsensing images, we proposed a novel neural network architecture which is for\ntwo tasks of landslide detection and landslide segmentation. We evaluated our\nproposed model on three different benchmark datasets of LandSlide4Sense, Bijie,\nand Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,\n93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU\nscores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,\nNepal datasets. These experimental results prove potential to integrate our\nproposed model into real-life landslide observation systems.\n","authors":["Lam Pham","Cam Le","Hieu Tang","Khang Truong","Truong Nguyen","Jasmin Lampert","Alexander Schindler","Martin Boyer","Son Phan"],"pdf_url":"https://arxiv.org/pdf/2507.11143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10434v2","updated":"2025-07-15T09:43:30Z","published":"2025-07-14T16:23:39Z","title":"CLA: Latent Alignment for Online Continual Self-Supervised Learning","summary":"  Self-supervised learning (SSL) is able to build latent representations that\ngeneralize well to unseen data. However, only a few SSL techniques exist for\nthe online CL setting, where data arrives in small minibatches, the model must\ncomply with a fixed computational budget, and task boundaries are absent. We\nintroduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL\nthat aligns the representations learned by the current model with past\nrepresentations to mitigate forgetting. We found that our CLA is able to speed\nup the convergence of the training process in the online scenario,\noutperforming state-of-the-art approaches under the same computational budget.\nSurprisingly, we also discovered that using CLA as a pretraining protocol in\nthe early stages of pretraining leads to a better final performance when\ncompared to a full i.i.d. pretraining.\n","authors":["Giacomo Cignoni","Andrea Cossu","Alexandra Gomez-Villa","Joost van de Weijer","Antonio Carta"],"pdf_url":"https://arxiv.org/pdf/2507.10434v2.pdf","comment":"Accepted at CoLLAs 2025 conference (oral)"},{"id":"http://arxiv.org/abs/2507.11137v1","updated":"2025-07-15T09:38:11Z","published":"2025-07-15T09:38:11Z","title":"Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks\n  in Weight-based Neural Network Watermarking","summary":"  As valuable digital assets, deep neural networks necessitate robust ownership\nprotection, positioning neural network watermarking (NNW) as a promising\nsolution. Among various NNW approaches, weight-based methods are favored for\ntheir simplicity and practicality; however, they remain vulnerable to forging\nand overwriting attacks. To address those challenges, we propose NeuralMark, a\nrobust method built around a hashed watermark filter. Specifically, we utilize\na hash function to generate an irreversible binary watermark from a secret key,\nwhich is then used as a filter to select the model parameters for embedding.\nThis design cleverly intertwines the embedding parameters with the hashed\nwatermark, providing a robust defense against both forging and overwriting\nattacks. An average pooling is also incorporated to resist fine-tuning and\npruning attacks. Furthermore, it can be seamlessly integrated into various\nneural network architectures, ensuring broad applicability. Theoretically, we\nanalyze its security boundary. Empirically, we verify its effectiveness and\nrobustness across 13 distinct Convolutional and Transformer architectures,\ncovering five image classification tasks and one text generation task. The\nsource codes are available at https://github.com/AIResearch-Group/NeuralMark.\n","authors":["Yuan Yao","Jin Song","Jian Jin"],"pdf_url":"https://arxiv.org/pdf/2507.11137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11136v1","updated":"2025-07-15T09:37:49Z","published":"2025-07-15T09:37:49Z","title":"Interpretable Bayesian Tensor Network Kernel Machines with Automatic\n  Rank and Feature Selection","summary":"  Tensor Network (TN) Kernel Machines speed up model learning by representing\nparameters as low-rank TNs, reducing computation and memory use. However, most\nTN-based Kernel methods are deterministic and ignore parameter uncertainty.\nFurther, they require manual tuning of model complexity hyperparameters like\ntensor rank and feature dimensions, often through trial-and-error or\ncomputationally costly methods like cross-validation. We propose Bayesian\nTensor Network Kernel Machines, a fully probabilistic framework that uses\nsparsity-inducing hierarchical priors on TN factors to automatically infer\nmodel complexity. This enables automatic inference of tensor rank and feature\ndimensions, while also identifying the most relevant features for prediction,\nthereby enhancing model interpretability. All the model parameters and\nhyperparameters are treated as latent variables with corresponding priors.\nGiven the Bayesian approach and latent variable dependencies, we apply a\nmean-field variational inference to approximate their posteriors. We show that\napplying a mean-field approximation to TN factors yields a Bayesian ALS\nalgorithm with the same computational complexity as its deterministic\ncounterpart, enabling uncertainty quantification at no extra computational\ncost. Experiments on synthetic and real-world datasets demonstrate the superior\nperformance of our model in prediction accuracy, uncertainty quantification,\ninterpretability, and scalability.\n","authors":["Afra Kilic","Kim Batselier"],"pdf_url":"https://arxiv.org/pdf/2507.11136v1.pdf","comment":"39 pages, 5 figures, 4 tables. Submitted to Journal of Machine\n  Learning Research. The code is available at:\n  https://github.com/afrakilic/BTN-Kernel-Machines. arXiv admin note: text\n  overlap with arXiv:1401.6497 by other authors"},{"id":"http://arxiv.org/abs/2507.11129v1","updated":"2025-07-15T09:29:29Z","published":"2025-07-15T09:29:29Z","title":"MMOne: Representing Multiple Modalities in One Scene","summary":"  Humans perceive the world through multimodal cues to understand and interact\nwith the environment. Learning a scene representation for multiple modalities\nenhances comprehension of the physical world. However, modality conflicts,\narising from inherent distinctions among different modalities, present two\ncritical challenges: property disparity and granularity disparity. To address\nthese challenges, we propose a general framework, MMOne, to represent multiple\nmodalities in one scene, which can be readily extended to additional\nmodalities. Specifically, a modality modeling module with a novel modality\nindicator is proposed to capture the unique properties of each modality.\nAdditionally, we design a multimodal decomposition mechanism to separate\nmulti-modal Gaussians into single-modal Gaussians based on modality\ndifferences. We address the essential distinctions among modalities by\ndisentangling multimodal information into shared and modality-specific\ncomponents, resulting in a more compact and efficient multimodal scene\nrepresentation. Extensive experiments demonstrate that our method consistently\nenhances the representation capability for each modality and is scalable to\nadditional modalities. The code is available at\nhttps://github.com/Neal2020GitHub/MMOne.\n","authors":["Zhifeng Gu","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2507.11129v1.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2507.11128v1","updated":"2025-07-15T09:28:44Z","published":"2025-07-15T09:28:44Z","title":"What Should LLMs Forget? Quantifying Personal Data in LLMs for\n  Right-to-Be-Forgotten Requests","summary":"  Large Language Models (LLMs) can memorize and reveal personal information,\nraising concerns regarding compliance with the EU's GDPR, particularly the\nRight to Be Forgotten (RTBF). Existing machine unlearning methods assume the\ndata to forget is already known but do not address how to identify which\nindividual-fact associations are stored in the model. Privacy auditing\ntechniques typically operate at the population level or target a small set of\nidentifiers, limiting applicability to individual-level data inquiries. We\nintroduce WikiMem, a dataset of over 5,000 natural language canaries covering\n243 human-related properties from Wikidata, and a model-agnostic metric to\nquantify human-fact associations in LLMs. Our approach ranks ground-truth\nvalues against counterfactuals using calibrated negative log-likelihood across\nparaphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B\nparameters), showing that memorization correlates with subject web presence and\nmodel scale. We provide a foundation for identifying memorized personal data in\nLLMs at the individual level, enabling the dynamic construction of forget sets\nfor machine unlearning and RTBF requests.\n","authors":["Dimitri Staufer"],"pdf_url":"https://arxiv.org/pdf/2507.11128v1.pdf","comment":"16 pages, 3 figures. Accepted at the 7th Workshop on eXplainable\n  Knowledge Discovery in Data Mining (XKDD 2025), ECML PKDD 2025, Porto,\n  Portugal"},{"id":"http://arxiv.org/abs/2506.18046v2","updated":"2025-07-15T09:18:30Z","published":"2025-06-22T14:19:36Z","title":"TAB: Unified Benchmarking of Time Series Anomaly Detection Methods","summary":"  Time series anomaly detection (TSAD) plays an important role in many domains\nsuch as finance, transportation, and healthcare. With the ongoing\ninstrumentation of reality, more time series data will be available, leading\nalso to growing demands for TSAD. While many TSAD methods already exist, new\nand better methods are still desirable. However, effective progress hinges on\nthe availability of reliable means of evaluating new methods and comparing them\nwith existing methods. We address deficiencies in current evaluation procedures\nrelated to datasets and experimental settings and protocols. Specifically, we\npropose a new time series anomaly detection benchmark, called TAB. First, TAB\nencompasses 29 public multivariate datasets and 1,635 univariate time series\nfrom different domains to facilitate more comprehensive evaluations on diverse\ndatasets. Second, TAB covers a variety of TSAD methods, including Non-learning,\nMachine learning, Deep learning, LLM-based, and Time-series pre-trained\nmethods. Third, TAB features a unified and automated evaluation pipeline that\nenables fair and easy evaluation of TSAD methods. Finally, we employ TAB to\nevaluate existing TSAD methods and report on the outcomes, thereby offering a\ndeeper insight into the performance of these methods. Besides, all datasets and\ncode are available at https://github.com/decisionintelligence/TAB.\n","authors":["Xiangfei Qiu","Zhe Li","Wanghui Qiu","Shiyan Hu","Lekui Zhou","Xingjian Wu","Zhengyu Li","Chenjuan Guo","Aoying Zhou","Zhenli Sheng","Jilin Hu","Christian S. Jensen","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2506.18046v2.pdf","comment":"Accepted by PVLDB2025"},{"id":"http://arxiv.org/abs/2403.15524v2","updated":"2025-07-15T09:12:13Z","published":"2024-03-22T14:13:11Z","title":"PPA-Game: Characterizing and Learning Competitive Dynamics Among Online\n  Content Creators","summary":"  In this paper, we present the Proportional Payoff Allocation Game (PPA-Game),\nwhich characterizes situations where agents compete for divisible resources. In\nthe PPA-game, agents select from available resources, and their payoffs are\nproportionately determined based on heterogeneous weights attributed to them.\nSuch dynamics simulate content creators on online recommender systems like\nYouTube and TikTok, who compete for finite consumer attention, with content\nexposure reliant on inherent and distinct quality. We first conduct a\ngame-theoretical analysis of the PPA-Game. While the PPA-Game does not always\nguarantee the existence of a pure Nash equilibrium (PNE), we identify prevalent\nscenarios ensuring its existence. Simulated experiments further prove that the\ncases where PNE does not exist rarely happen. Beyond analyzing static payoffs,\nwe further discuss the agents' online learning about resource payoffs by\nintegrating a multi-player multi-armed bandit framework. We propose an online\nalgorithm facilitating each agent's maximization of cumulative payoffs over $T$\nrounds. Theoretically, we establish that the regret of any agent is bounded by\n$O(\\log^{1 + \\eta} T)$ for any $\\eta > 0$. Empirical results further validate\nthe effectiveness of our online learning approach.\n","authors":["Renzhe Xu","Haotian Wang","Xingxuan Zhang","Bo Li","Peng Cui"],"pdf_url":"https://arxiv.org/pdf/2403.15524v2.pdf","comment":"KDD 2026"},{"id":"http://arxiv.org/abs/2507.07955v2","updated":"2025-07-15T09:06:11Z","published":"2025-07-10T17:39:37Z","title":"Dynamic Chunking for End-to-End Hierarchical Sequence Modeling","summary":"  Major progress on language models (LMs) in recent years has largely resulted\nfrom moving away from specialized models designed for specific tasks, to\ngeneral models based on powerful architectures (e.g. the Transformer) that\nlearn everything from raw data. Despite this trend, pre-processing steps such\nas tokenization remain a barrier to true end-to-end foundation models. We\nintroduce a collection of new techniques that enable a dynamic chunking\nmechanism which automatically learns content- and context- dependent\nsegmentation strategies learned jointly with the rest of the model.\nIncorporating this into an explicit hierarchical network (H-Net) allows\nreplacing the (implicitly hierarchical) tokenization-LM-detokenization pipeline\nwith a single model learned fully end-to-end. When compute- and data- matched,\nan H-Net with one stage of hierarchy operating at the byte level outperforms a\nstrong Transformer language model operating over BPE tokens. Iterating the\nhierarchy to multiple stages further increases its performance by modeling\nmultiple levels of abstraction, demonstrating significantly better scaling with\ndata and matching the token-based Transformer of twice its size. H-Nets\npretrained on English show significantly increased character-level robustness,\nand qualitatively learn meaningful data-dependent chunking strategies without\nany heuristics or explicit supervision. Finally, the H-Net's improvement over\ntokenized pipelines is further increased in languages and modalities with\nweaker tokenization heuristics, such as Chinese and code, or DNA sequences\n(nearly 4x improvement in data efficiency over baselines), showing the\npotential of true end-to-end models that learn and scale better from\nunprocessed data.\n","authors":["Sukjun Hwang","Brandon Wang","Albert Gu"],"pdf_url":"https://arxiv.org/pdf/2507.07955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15249v2","updated":"2025-07-15T09:05:34Z","published":"2025-06-18T08:26:30Z","title":"Context-Aware Deep Lagrangian Networks for Model Predictive Control","summary":"  Controlling a robot based on physics-consistent dynamic models, such as Deep\nLagrangian Networks (DeLaN), can improve the generalizability and\ninterpretability of the resulting behavior. However, in complex environments,\nthe number of objects to potentially interact with is vast, and their physical\nproperties are often uncertain. This complexity makes it infeasible to employ a\nsingle global model. Therefore, we need to resort to online system\nidentification of context-aware models that capture only the currently relevant\naspects of the environment. While physical principles such as the conservation\nof energy may not hold across varying contexts, ensuring physical plausibility\nfor any individual context-aware model can still be highly desirable,\nparticularly when using it for receding horizon control methods such as model\npredictive control (MPC). Hence, in this work, we extend DeLaN to make it\ncontext-aware, combine it with a recurrent network for online system\nidentification, and integrate it with an MPC for adaptive, physics-consistent\ncontrol. We also combine DeLaN with a residual dynamics model to leverage the\nfact that a nominal model of the robot is typically available. We evaluate our\nmethod on a 7-DOF robot arm for trajectory tracking under varying loads. Our\nmethod reduces the end-effector tracking error by 39%, compared to a 21%\nimprovement achieved by a baseline that uses an extended Kalman filter.\n","authors":["Lucas Schulze","Jan Peters","Oleg Arenz"],"pdf_url":"https://arxiv.org/pdf/2506.15249v2.pdf","comment":"Accepted to the 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)"},{"id":"http://arxiv.org/abs/2507.11112v1","updated":"2025-07-15T09:04:30Z","published":"2025-07-15T09:04:30Z","title":"Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs","summary":"  Recent studies have shown that Large Language Models (LLMs) are vulnerable to\ndata poisoning attacks, where malicious training examples embed hidden\nbehaviours triggered by specific input patterns. However, most existing works\nassume a phrase and focus on the attack's effectiveness, offering limited\nunderstanding of trigger mechanisms and how multiple triggers interact within\nthe model. In this paper, we present a framework for studying poisoning in\nLLMs. We show that multiple distinct backdoor triggers can coexist within a\nsingle model without interfering with each other, enabling adversaries to embed\nseveral triggers concurrently. Using multiple triggers with high embedding\nsimilarity, we demonstrate that poisoned triggers can achieve robust activation\neven when tokens are substituted or separated by long token spans. Our findings\nexpose a broader and more persistent vulnerability surface in LLMs. To mitigate\nthis threat, we propose a post hoc recovery method that selectively retrains\nspecific model components based on a layer-wise weight difference analysis. Our\nmethod effectively removes the trigger behaviour with minimal parameter\nupdates, presenting a practical and efficient defence against multi-trigger\npoisoning.\n","authors":["Sanhanat Sivapiromrat","Caiqi Zhang","Marco Basaldella","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2507.11112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11106v1","updated":"2025-07-15T08:57:27Z","published":"2025-07-15T08:57:27Z","title":"A Mathematical Optimization Approach to Multisphere Support Vector Data\n  Description","summary":"  We present a novel mathematical optimization framework for outlier detection\nin multimodal datasets, extending Support Vector Data Description approaches.\nWe provide a primal formulation, in the shape of a Mixed Integer Second Order\nCone model, that constructs Euclidean hyperspheres to identify anomalous\nobservations. Building on this, we develop a dual model that enables the\napplication of the kernel trick, thus allowing for the detection of outliers\nwithin complex, non-linear data structures. An extensive computational study\ndemonstrates the effectiveness of our exact method, showing clear advantages\nover existing heuristic techniques in terms of accuracy and robustness.\n","authors":["Víctor Blanco","Inmaculada Espejo","Raúl Páez","Antonio M. Rodríguez-Chía"],"pdf_url":"https://arxiv.org/pdf/2507.11106v1.pdf","comment":"18 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2505.15075v3","updated":"2025-07-15T08:54:19Z","published":"2025-05-21T03:43:37Z","title":"Traveling Across Languages: Benchmarking Cross-Lingual Consistency in\n  Multimodal LLMs","summary":"  The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models.\n","authors":["Hao Wang","Pinzhi Huang","Jihan Yang","Saining Xie","Daisuke Kawahara"],"pdf_url":"https://arxiv.org/pdf/2505.15075v3.pdf","comment":"https://github.com/nlp-waseda/traveling-across-languages"},{"id":"http://arxiv.org/abs/2406.08933v3","updated":"2025-07-15T08:40:26Z","published":"2024-06-13T09:03:53Z","title":"LaCoOT: Layer Collapse through Optimal Transport","summary":"  Although deep neural networks are well-known for their outstanding\nperformance in tackling complex tasks, their hunger for computational resources\nremains a significant hurdle, posing energy-consumption issues and restricting\ntheir deployment on resource-constrained devices, preventing their widespread\nadoption. In this paper, we present an optimal transport-based method to reduce\nthe depth of over-parametrized deep neural networks, alleviating their\ncomputational burden. More specifically, we propose a new regularization\nstrategy based on the Max-Sliced Wasserstein distance to minimize the distance\nbetween the intermediate feature distributions in the neural network. We show\nthat minimizing this distance enables the complete removal of intermediate\nlayers in the network, achieving better performance/depth trade-off compared to\nexisting techniques. We assess the effectiveness of our method on traditional\nimage classification setups and extend it to generative image models. Our code\nis available at https://github.com/VGCQ/LaCoOT.\n","authors":["Victor Quétu","Zhu Liao","Nour Hezbri","Fabio Pizzati","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2406.08933v3.pdf","comment":"ICCV25"},{"id":"http://arxiv.org/abs/2507.08053v2","updated":"2025-07-15T08:40:16Z","published":"2025-07-10T08:26:49Z","title":"Tree-Structured Parzen Estimator Can Solve Black-Box Combinatorial\n  Optimization More Efficiently","summary":"  Tree-structured Parzen estimator (TPE) is a versatile hyperparameter\noptimization (HPO) method supported by popular HPO tools. Since these HPO tools\nhave been developed in line with the trend of deep learning (DL), the problem\nsetups often used in the DL domain have been discussed for TPE such as\nmulti-objective optimization and multi-fidelity optimization. However, the\npractical applications of HPO are not limited to DL, and black-box\ncombinatorial optimization is actively utilized in some domains, e.g.,\nchemistry and biology. As combinatorial optimization has been an untouched, yet\nvery important, topic in TPE, we propose an efficient combinatorial\noptimization algorithm for TPE. In this paper, we first generalize the\ncategorical kernel with the numerical kernel in TPE, enabling us to introduce a\ndistance structure to the categorical kernel. Then we discuss modifications for\nthe newly developed kernel to handle a large combinatorial search space. These\nmodifications reduce the time complexity of the kernel calculation with respect\nto the size of a combinatorial search space. In the experiments using synthetic\nproblems, we verified that our proposed method identifies better solutions with\nfewer evaluations than the original TPE. Our algorithm is available in Optuna,\nan open-source framework for HPO.\n","authors":["Kenshin Abe","Yunzhuo Wang","Shuhei Watanabe"],"pdf_url":"https://arxiv.org/pdf/2507.08053v2.pdf","comment":"Submitted to AutoML Conference"},{"id":"http://arxiv.org/abs/2507.11071v1","updated":"2025-07-15T08:04:31Z","published":"2025-07-15T08:04:31Z","title":"LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly\n  Detection","summary":"  Log anomaly detection using traditional rule based or deep learning based\nmethods is often challenging due to the large volume and highly complex nature\nof log sequence. So effective way of detection of anomalous sequence of logs is\ncrucial for system maintenance and development. This paper proposes parameter\nefficient finetuning specifically low rank adaptation (LoRA) and adapter based\napproaches for finding contextual anomalies in sequence of logs in large log\ndata set. It compares different tiny large language models (LLMs) on the\nThunderbird dataset. The results show that LoRA based finetuning provides\nsubstantial performance improvements of 18 to 19 percentage over LogBert based\nfull finetuning approach, achieving accuracy scores between 97.76% and 98.83%\ncompared to 79.37%.\n","authors":["Isaiah Thompson Ocansey","Ritwik Bhattacharya","Tanmay Sen"],"pdf_url":"https://arxiv.org/pdf/2507.11071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11063v1","updated":"2025-07-15T07:55:09Z","published":"2025-07-15T07:55:09Z","title":"A Distance Metric for Mixed Integer Programming Instances","summary":"  Mixed-integer linear programming (MILP) is a powerful tool for addressing a\nwide range of real-world problems, but it lacks a clear structure for comparing\ninstances. A reliable similarity metric could establish meaningful\nrelationships between instances, enabling more effective evaluation of instance\nset heterogeneity and providing better guidance to solvers, particularly when\nmachine learning is involved. Existing similarity metrics often lack precision\nin identifying instance classes or rely heavily on labeled data, which limits\ntheir applicability and generalization. To bridge this gap, this paper\nintroduces the first mathematical distance metric for MILP instances, derived\ndirectly from their mathematical formulations. By discretizing right-hand\nsides, weights, and variables into classes, the proposed metric draws\ninspiration from the Earth mover's distance to quantify mismatches in\nweight-variable distributions for constraint comparisons. This approach\nnaturally extends to enable instance-level comparisons. We evaluate both an\nexact and a greedy variant of our metric under various parameter settings,\nusing the StrIPLIB dataset. Results show that all components of the metric\ncontribute to class identification, and that the greedy version achieves\naccuracy nearly identical to the exact formulation while being nearly 200 times\nfaster. Compared to state-of-the-art baselines, including feature-based,\nimage-based, and neural network models, our unsupervised method consistently\noutperforms all non-learned approaches and rivals the performance of a\nsupervised classifier on class and subclass grouping tasks.\n","authors":["Gwen Maudet","Grégoire Danoy"],"pdf_url":"https://arxiv.org/pdf/2507.11063v1.pdf","comment":"Accepted to ECAI 2025"},{"id":"http://arxiv.org/abs/2502.01706v3","updated":"2025-07-15T07:52:08Z","published":"2025-02-03T13:30:44Z","title":"Comply: Learning Sentences with Complex Weights inspired by Fruit Fly\n  Olfaction","summary":"  Biologically inspired neural networks offer alternative avenues to model data\ndistributions. FlyVec is a recent example that draws inspiration from the fruit\nfly's olfactory circuit to tackle the task of learning word embeddings.\nSurprisingly, this model performs competitively even against deep learning\napproaches specifically designed to encode text, and it does so with the\nhighest degree of computational efficiency. We pose the question of whether\nthis performance can be improved further. For this, we introduce Comply. By\nincorporating positional information through complex weights, we enable a\nsingle-layer neural network to learn sequence representations. Our experiments\nshow that Comply not only supersedes FlyVec but also performs on par with\nsignificantly larger state-of-the-art models. We achieve this without\nadditional parameters. Comply yields sparse contextual representations of\nsentences that can be interpreted explicitly from the neuron weights.\n","authors":["Alexei Figueroa","Justus Westerhoff","Golzar Atefi","Dennis Fast","Benjamin Winter","Felix Alexander Gers","Alexander Löser","Wolfgang Nejdl"],"pdf_url":"https://arxiv.org/pdf/2502.01706v3.pdf","comment":"Accepted at NICE2025"},{"id":"http://arxiv.org/abs/2412.20946v2","updated":"2025-07-15T07:46:57Z","published":"2024-12-30T13:38:31Z","title":"Generalising Battery Control in Net-Zero Buildings via Personalised\n  Federated RL","summary":"  This work studies the challenge of optimal energy management in\nbuilding-based microgrids through a collaborative and privacy-preserving\nframework. We evaluated two common RL algorithms (PPO and TRPO) in different\ncollaborative setups to manage distributed energy resources (DERs) efficiently.\nUsing a customized version of the CityLearn environment and synthetically\ngenerated data, we simulate and design net-zero energy scenarios for microgrids\ncomposed of multiple buildings. Our approach emphasizes reducing energy costs\nand carbon emissions while ensuring privacy. Experimental results demonstrate\nthat Federated TRPO is comparable with state-of-the-art federated RL\nmethodologies without hyperparameter tuning. The proposed framework highlights\nthe feasibility of collaborative learning for achieving optimal control\npolicies in energy systems, advancing the goals of sustainable and efficient\nsmart grids. Our code is accessible\n\\href{https://github.com/Optimization-and-Machine-Learning-Lab/energy_fed_trpo.git}{\\textit{this\nrepo}}.\n","authors":["Nicolas M Cuadrado Avila","Samuel Horváth","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2412.20946v2.pdf","comment":"Accepted at CO-Build Workshop ICML2025"},{"id":"http://arxiv.org/abs/2507.05313v2","updated":"2025-07-15T07:37:58Z","published":"2025-07-07T13:17:38Z","title":"Solar Flare Prediction Using Long Short-term Memory (LSTM) and\n  Decomposition-LSTM with Sliding Window Pattern Recognition","summary":"  We investigate the use of Long Short-Term Memory (LSTM) and\nDecomposition-LSTM (DLSTM) networks, combined with an ensemble algorithm, to\npredict solar flare occurrences using time-series data from the GOES catalog.\nThe dataset spans from 2003 to 2023 and includes 151,071 flare events. Among\napproximately possible patterns, 7,552 yearly pattern windows are identified,\nhighlighting the challenge of long-term forecasting due to the Sun's complex,\nself-organized criticality-driven behavior. A sliding window technique is\nemployed to detect temporal quasi-patterns in both irregular and regularized\nflare time series. Regularization reduces complexity, enhances large flare\nactivity, and captures active days more effectively. To address class\nimbalance, resampling methods are applied. LSTM and DLSTM models are trained on\nsequences of peak fluxes and waiting times from irregular time series, while\nLSTM and DLSTM, integrated with an ensemble approach, are applied to sliding\nwindows of regularized time series with a 3-hour interval. Performance metrics,\nparticularly TSS (0.74), recall (0.95) and the area under the curve (AUC=0.87)\nin the receiver operating characteristic (ROC), indicate that DLSTM with an\nensemble approach on regularized time series outperforms other models, offering\nmore accurate large-flare forecasts with fewer false errors compared to models\ntrained on irregular time series. The superior performance of DLSTM is\nattributed to its ability to decompose time series into trend and seasonal\ncomponents, effectively isolating random noise. This study underscores the\npotential of advanced machine learning techniques for solar flare prediction\nand highlights the importance of incorporating various solar cycle phases and\nresampling strategies to enhance forecasting reliability.\n","authors":["Zeinab Hassani","Davud Mohammadpur","Hossein Safari"],"pdf_url":"https://arxiv.org/pdf/2507.05313v2.pdf","comment":"Published in the Astrophysical Journal Supplement Series, volume 279,\n  2025, DOI: 10.3847/1538-4365/addc73"},{"id":"http://arxiv.org/abs/2507.11053v1","updated":"2025-07-15T07:37:33Z","published":"2025-07-15T07:37:33Z","title":"GATE: Graph Attention Neural Networks with Real-Time Edge Construction\n  for Robust Indoor Localization using Mobile Embedded Devices","summary":"  Accurate indoor localization is crucial for enabling spatial context in smart\nenvironments and navigation systems. Wi-Fi Received Signal Strength (RSS)\nfingerprinting is a widely used indoor localization approach due to its\ncompatibility with mobile embedded devices. Deep Learning (DL) models improve\naccuracy in localization tasks by learning RSS variations across locations, but\nthey assume fingerprint vectors exist in a Euclidean space, failing to\nincorporate spatial relationships and the non-uniform distribution of\nreal-world RSS noise. This results in poor generalization across heterogeneous\nmobile devices, where variations in hardware and signal processing distort RSS\nreadings. Graph Neural Networks (GNNs) can improve upon conventional DL models\nby encoding indoor locations as nodes and modeling their spatial and signal\nrelationships as edges. However, GNNs struggle with non-Euclidean noise\ndistributions and suffer from the GNN blind spot problem, leading to degraded\naccuracy in environments with dense access points (APs). To address these\nchallenges, we propose GATE, a novel framework that constructs an adaptive\ngraph representation of fingerprint vectors while preserving an indoor\nstate-space topology, modeling the non-Euclidean structure of RSS noise to\nmitigate environmental noise and address device heterogeneity. GATE introduces\n1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a\nnovel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind\nspot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic\ngraph adaptation. Extensive real-world evaluations across multiple indoor\nspaces with varying path lengths, AP densities, and heterogeneous devices\ndemonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and\n1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor\nlocalization frameworks.\n","authors":["Danish Gufran","Sudeep Pasricha"],"pdf_url":"https://arxiv.org/pdf/2507.11053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.13523v2","updated":"2025-07-15T07:36:30Z","published":"2025-06-16T14:15:18Z","title":"The Price of Freedom: Exploring Expressivity and Runtime Tradeoffs in\n  Equivariant Tensor Products","summary":"  $E(3)$-equivariant neural networks have demonstrated success across a wide\nrange of 3D modelling tasks. A fundamental operation in these networks is the\ntensor product, which interacts two geometric features in an equivariant manner\nto create new features. Due to the high computational complexity of the tensor\nproduct, significant effort has been invested to optimize the runtime of this\noperation. For example, Luo et al. (2024) recently proposed the Gaunt tensor\nproduct (GTP) which promises a significant speedup. In this work, we provide a\ncareful, systematic analysis of a number of tensor product operations. In\nparticular, we emphasize that different tensor products are not performing the\nsame operation. The reported speedups typically come at the cost of\nexpressivity. We introduce measures of expressivity and interactability to\ncharacterize these differences. In addition, we realized the original\nimplementation of GTP can be greatly simplified by directly using a spherical\ngrid at no cost in asymptotic runtime. This spherical grid approach is faster\non our benchmarks and in actual training of the MACE interatomic potential by\n30%. Finally, we provide the first systematic microbenchmarks of the various\ntensor product operations. We find that the theoretical runtime guarantees can\ndiffer wildly from empirical performance, demonstrating the need for careful\napplication-specific benchmarking. Code is available at\nhttps://github.com/atomicarchitects/PriceofFreedom.\n","authors":["YuQing Xie","Ameya Daigavane","Mit Kotak","Tess Smidt"],"pdf_url":"https://arxiv.org/pdf/2506.13523v2.pdf","comment":"Published at ICML 2025. 27 pages, 10 figures"},{"id":"http://arxiv.org/abs/2507.10015v2","updated":"2025-07-15T07:15:38Z","published":"2025-07-14T07:51:01Z","title":"(Almost) Free Modality Stitching of Foundation Models","summary":"  Foundation multi-modal models are often designed by stitching of multiple\nexisting pretrained uni-modal models: for example, an image classifier with an\ntext model. This stitching process is performed by training a connector module\nthat aims to align the representation spaces of these uni-modal models towards\na multi-modal objective. However, given the complexity of training such\nconnectors on large scale web-based datasets coupled with the ever-increasing\nnumber of available pretrained uni-modal models, the task of uni-modal models\nselection and subsequent connector module training becomes computationally\ndemanding. To address this under-studied critical problem, we propose\nHypernetwork Model Alignment (Hyma), a novel all-in-one solution for optimal\nuni-modal model selection and connector training by leveraging hypernetworks.\nSpecifically, our framework utilizes the parameter prediction capability of a\nhypernetwork to obtain jointly trained connector modules for $N \\times M$\ncombinations of uni-modal models. In our experiments, Hyma reduces the cost of\nsearching for the best performing uni-modal model pair by $10\\times$, while\nmatching the ranking and trained connector performance obtained via grid search\nacross a suite of diverse multi-modal benchmarks.\n","authors":["Jaisidh Singh","Diganta Misra","Boris Knyazev","Antonio Orvieto"],"pdf_url":"https://arxiv.org/pdf/2507.10015v2.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2502.14565v2","updated":"2025-07-15T06:30:11Z","published":"2025-02-20T13:50:02Z","title":"ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification","summary":"  Self-awareness, i.e., the ability to assess and correct one's own generation,\nis a fundamental aspect of human intelligence, making its replication in large\nlanguage models (LLMs) an important yet challenging task. Previous works tackle\nthis by employing extensive reinforcement learning or rather relying on large\nexternal verifiers. In this work, we propose Refine via Intrinsic\nSelf-Verification (ReVISE), an efficient and effective framework that enables\nLLMs to self-correct their outputs through self-verification. The core idea of\nReVISE is to enable LLMs to verify their reasoning processes and continually\nrethink reasoning trajectories based on its verification. We introduce a\nstructured curriculum based upon online preference learning to implement this\nefficiently. Specifically, as ReVISE involves two challenging tasks (i.e.,\nself-verification and reasoning correction), we tackle each task sequentially\nusing curriculum learning, collecting both failed and successful reasoning\npaths to construct preference pairs for efficient training. During inference,\nour approach enjoys natural test-time scaling by integrating self-verification\nand correction capabilities, further enhanced by our proposed confidence-aware\ndecoding mechanism. Our experiments on various reasoning tasks demonstrate that\nReVISE achieves efficient self-correction and significantly improves reasoning\nperformance.\n","authors":["Hyunseok Lee","Seunghyuk Oh","Jaehyung Kim","Jinwoo Shin","Jihoon Tack"],"pdf_url":"https://arxiv.org/pdf/2502.14565v2.pdf","comment":"Published as conference proceeding for ICML 2025. First two authors\n  contributed equally"},{"id":"http://arxiv.org/abs/2411.12334v2","updated":"2025-07-15T06:25:46Z","published":"2024-11-19T08:36:34Z","title":"Learning from Label Proportions and Covariate-shifted Instances","summary":"  In many applications, especially due to lack of supervision or privacy\nconcerns, the training data is grouped into bags of instances (feature-vectors)\nand for each bag we have only an aggregate label derived from the\ninstance-labels in the bag. In learning from label proportions (LLP) the\naggregate label is the average of the instance-labels in a bag, and a\nsignificant body of work has focused on training models in the LLP setting to\npredict instance-labels. In practice however, the training data may have fully\nsupervised albeit covariate-shifted source data, along with the usual target\ndata with bag-labels, and we wish to train a good instance-level predictor on\nthe target domain. We call this the covariate-shifted hybrid LLP problem. Fully\nsupervised covariate shifted data often has useful training signals and the\ngoal is to leverage them for better predictive performance in the hybrid LLP\nsetting. To achieve this, we develop methods for hybrid LLP which naturally\nincorporate the target bag-labels along with the source instance-labels, in the\ndomain adaptation framework. Apart from proving theoretical guarantees bounding\nthe target generalization error, we also conduct experiments on several\npublicly available datasets showing that our methods outperform LLP and domain\nadaptation baselines as well techniques from previous related work.\n","authors":["Sagalpreet Singh","Navodita Sharma","Shreyas Havaldar","Rishi Saket","Aravindan Raghuveer"],"pdf_url":"https://arxiv.org/pdf/2411.12334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11019v1","updated":"2025-07-15T06:24:07Z","published":"2025-07-15T06:24:07Z","title":"Relative Entropy Pathwise Policy Optimization","summary":"  Score-function policy gradients have delivered strong results in\ngame-playing, robotics and language-model fine-tuning. Yet its high-variance\noften undermines training stability. On the other hand, pathwise policy\ngradients alleviate the training variance, but are reliable only when driven by\nan accurate action-conditioned value function which is notoriously hard to\ntrain without relying on past off-policy data. In this paper, we discuss how to\nconstruct a value-gradient driven, on-policy algorithm that allow training\nQ-value models purely from on-policy data, unlocking the possibility of using\npathwise policy updates in the context of on-policy learning. We show how to\nbalance stochastic policies for exploration with constrained policy updates for\nstable training, and evaluate important architectural components that\nfacilitate accurate value function learning. Building on these insights, we\npropose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient\non-policy algorithm that combines the sample-efficiency of pathwise policy\ngradients with the simplicity and minimal memory footprint of standard\non-policy learning. We demonstrate that REPPO provides strong empirical\nperformance at decreased sample requirements, wall-clock time, memory footprint\nas well as high hyperparameter robustness in a set of experiments on two\nstandard GPU-parallelized benchmarks.\n","authors":["Claas Voelcker","Axel Brunnbauer","Marcel Hussing","Michal Nauman","Pieter Abbeel","Eric Eaton","Radu Grosu","Amir-massoud Farahmand","Igor Gilitschenski"],"pdf_url":"https://arxiv.org/pdf/2507.11019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10537v2","updated":"2025-07-15T06:22:08Z","published":"2025-03-13T16:51:59Z","title":"Structured Preconditioners in Adaptive Optimization: A Unified Analysis","summary":"  We present a novel unified analysis for a broad class of adaptive\noptimization algorithms with structured (e.g., layerwise, diagonal, and\nkronecker-factored) preconditioners for both online regret minimization and\noffline convex optimization. Our analysis not only provides matching rate to\nseveral important structured preconditioned algorithms including diagonal\nAdaGrad, full-matrix AdaGrad, and AdaGrad-Norm, but also gives an improved\nconvergence rate for a one-sided variant of Shampoo over that of original\nShampoo. Interestingly, more structured preconditioners (e.g., diagonal\nAdagrad, AdaGrad-Norm which use less space and compute) are often presented as\ncomputationally efficient approximations to full-matrix Adagrad, aiming for\nimproved optimization performance through better approximations. Our unified\nanalysis challenges this prevailing view and reveals, perhaps surprisingly,\nthat more structured preconditioners, despite using less space and computation\nper step, can outperform their less structured counterparts. To demonstrate\nthis, we show that one-sided Shampoo, which is relatively much cheaper than\nfull-matrix AdaGrad could outperform it both theoretically and experimentally.\n","authors":["Shuo Xie","Tianhao Wang","Sashank Reddi","Sanjiv Kumar","Zhiyuan Li"],"pdf_url":"https://arxiv.org/pdf/2503.10537v2.pdf","comment":"Fix typos and add remarks for two-sided Shampoo"},{"id":"http://arxiv.org/abs/2507.11017v1","updated":"2025-07-15T06:18:46Z","published":"2025-07-15T06:18:46Z","title":"First-Order Error Matters: Accurate Compensation for Quantized Large\n  Language Models","summary":"  Post-training quantization (PTQ) offers an efficient approach to compressing\nlarge language models (LLMs), significantly reducing memory access and\ncomputational costs. Existing compensation-based weight calibration methods\noften rely on a second-order Taylor expansion to model quantization error,\nunder the assumption that the first-order term is negligible in well-trained\nfull-precision models. However, we reveal that the progressive compensation\nprocess introduces accumulated first-order deviations between latent weights\nand their full-precision counterparts, making this assumption fundamentally\nflawed. To address this, we propose FOEM, a novel PTQ method that explicitly\nincorporates first-order gradient terms to improve quantization error\ncompensation. FOEM approximates gradients by directly computing the difference\nbetween latent and full-precision weights, avoiding the high cost and limited\ngeneralization of backpropagation-based gradient computation. This approach\nintroduces minimal additional computational overhead. Moreover, FOEM leverages\nprecomputed Cholesky factors to efficiently recover the inverse of Hessian\nsubmatrices in real time. Extensive experiments across a wide range of models\nand benchmarks demonstrate that FOEM consistently outperforms the classical\nGPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of\nLlama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from\n51.7% to 74.9%, approaching the full-precision performance of 78.6%.\nFurthermore, FOEM can be seamlessly integrated with advanced techniques such as\nGPTAQ and SpinQuant, yielding additional improvements under the challenging\nW4A4KV4 setting, and further narrowing the accuracy gap with full-precision\nbaselines beyond what current state-of-the-art methods achieve. The code is\navailable at https://github.com/Xingyu-Zheng/FOEM.\n","authors":["Xingyu Zheng","Haotong Qin","Yuye Li","Jiakai Wang","Jinyang Guo","Michele Magno","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2507.11017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11012v1","updated":"2025-07-15T06:07:14Z","published":"2025-07-15T06:07:14Z","title":"Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from\n  Temperature Observations at an Experimental Prescribed Fire","summary":"  This study explores the potential for predicting turbulent kinetic energy\n(TKE) from more readily acquired temperature data using temperature profiles\nand turbulence data collected concurrently at 10 Hz during a small experimental\nprescribed burn in the New Jersey Pine Barrens. Machine learning models,\nincluding Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and\nGaussian Process Regressor, were employed to assess the potential to predict\nTKE from temperature perturbations and explore temporal and spatial dynamics of\ncorrelations. Data visualization and correlation analyses revealed patterns and\nrelationships between thermocouple temperatures and TKE, providing insight into\nthe underlying dynamics. More accurate predictions of TKE were achieved by\nemploying various machine learning models despite a weak correlation between\nthe predictors and the target variable. The results demonstrate significant\nsuccess, particularly from regression models, in accurately predicting the TKE.\nThe findings of this study demonstrate a novel numerical approach to\nidentifying new relationships between temperature and airflow processes in and\naround the fire environment. These relationships can help refine our\nunderstanding of combustion environment processes and the coupling and\ndecoupling of fire environment processes necessary for improving fire\noperations strategy and fire and smoke model predictions. The findings of this\nstudy additionally highlight the valuable role of machine learning techniques\nin analyzing the complex large datasets of the fire environments, showcasing\ntheir potential to advance fire research and management practices.\n","authors":["Dipak Dulal","Joseph J. Charney","Michael R. Gallagher","Pitambar Acharya","Carmeliza Navasca","Nicholas S. Skowronski"],"pdf_url":"https://arxiv.org/pdf/2507.11012v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2311.05128"},{"id":"http://arxiv.org/abs/2506.19502v2","updated":"2025-07-15T06:04:25Z","published":"2025-06-24T10:40:23Z","title":"MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications","summary":"  Accessibility remains a critical concern in today's society, as many\ntechnologies are not developed to support the full range of user needs.\nExisting multi-agent systems (MAS) often cannot provide comprehensive\nassistance for users in need due to the lack of customization stemming from\nclosed-source designs. Consequently, individuals with disabilities frequently\nencounter significant barriers when attempting to interact with digital\nenvironments. We introduce MATE, a multimodal accessibility MAS, which performs\nthe modality conversions based on the user's needs. The system is useful for\nassisting people with disabilities by ensuring that data will be converted to\nan understandable format. For instance, if the user cannot see well and\nreceives an image, the system converts this image to its audio description.\nMATE can be applied to a wide range of domains, industries, and areas, such as\nhealthcare, and can become a useful assistant for various groups of users. The\nsystem supports multiple types of models, ranging from LLM API calling to using\ncustom machine learning (ML) classifiers. This flexibility ensures that the\nsystem can be adapted to various needs and is compatible with a wide variety of\nhardware. Since the system is expected to run locally, it ensures the privacy\nand security of sensitive information. In addition, the framework can be\neffectively integrated with institutional technologies (e.g., digital\nhealthcare service) for real-time user assistance. Furthermore, we introduce\nModCon-Task-Identifier, a model that is capable of extracting the precise\nmodality conversion task from the user input. Numerous experiments show that\nModCon-Task-Identifier consistently outperforms other LLMs and statistical\nmodels on our custom data. Our code and data are publicly available at\nhttps://github.com/AlgazinovAleksandr/Multi-Agent-MATE.\n","authors":["Aleksandr Algazinov","Matt Laing","Paul Laban"],"pdf_url":"https://arxiv.org/pdf/2506.19502v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.09781v2","updated":"2025-07-15T05:49:44Z","published":"2025-06-11T14:21:05Z","title":"On the Similarities of Embeddings in Contrastive Learning","summary":"  Contrastive learning operates on a simple yet effective principle: Embeddings\nof positive pairs are pulled together, while those of negative pairs are pushed\napart. In this paper, we propose a unified framework for understanding\ncontrastive learning through the lens of cosine similarity, and present two key\ntheoretical insights derived from this framework. First, in full-batch\nsettings, we show that perfect alignment of positive pairs is unattainable when\nnegative-pair similarities fall below a threshold, and this misalignment can be\nmitigated by incorporating within-view negative pairs into the objective.\nSecond, in mini-batch settings, smaller batch sizes induce stronger separation\namong negative pairs in the embedding space, i.e., higher variance in their\nsimilarities, which in turn degrades the quality of learned representations\ncompared to full-batch settings. To address this, we propose an auxiliary loss\nthat reduces the variance of negative-pair similarities in mini-batch settings.\nEmpirical results show that incorporating the proposed loss improves\nperformance in small-batch settings.\n","authors":["Chungpa Lee","Sehee Lim","Kibok Lee","Jy-yong Sohn"],"pdf_url":"https://arxiv.org/pdf/2506.09781v2.pdf","comment":"contrastive learning, representation learning, embedding, similarity,\n  negative pair, positive pair"},{"id":"http://arxiv.org/abs/2507.11005v1","updated":"2025-07-15T05:49:37Z","published":"2025-07-15T05:49:37Z","title":"AdaMuon: Adaptive Muon Optimizer","summary":"  We propose AdaMuon, an adaptive learning-rate framework built upon the\nrecently validated Muon optimizer, which has demonstrated substantial\nefficiency gains over AdamW in large-scale model training. AdaMuon augments\nMuon with two mutually dependent modules: (1) a per-parameter second-moment\nmodulation that captures orthogonal gradient updates to ensure update-level\nadaptivity, and (2) a RMS-aligned rescaling that regulates the overall update\nmagnitude by aligning it with the intrinsic structure of the parameter space.\nEmpirical results on multiple model scales and learning-rate regimes confirm\nthat AdaMuon consistently outperforms the original Muon, delivering higher\nacceleration in convergence while maintaining training stability. Our method\nintroduces no additional tuning burden and can be seamlessly integrated into\nexisting Muon training pipelines.\n","authors":["Chongjie Si","Debing Zhang","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2507.11005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10998v1","updated":"2025-07-15T05:34:44Z","published":"2025-07-15T05:34:44Z","title":"Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data","summary":"  Adversarial attacks on tabular data present fundamental challenges distinct\nfrom image or text domains due to the heterogeneous nature of mixed categorical\nand numerical features. Unlike images where pixel perturbations maintain visual\nsimilarity, tabular data lacks intuitive similarity metrics, making it\ndifficult to define imperceptible modifications. Additionally, traditional\ngradient-based methods prioritise $\\ell_p$-norm constraints, often producing\nadversarial examples that deviate from the original data distributions, making\nthem detectable. We propose a latent space perturbation framework using a\nmixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial\nexamples. The proposed VAE integrates categorical embeddings and numerical\nfeatures into a unified latent manifold, enabling perturbations that preserve\nstatistical consistency. We specify In-Distribution Success Rate (IDSR) to\nmeasure the proportion of adversarial examples that remain statistically\nindistinguishable from the input distribution. Evaluation across six publicly\navailable datasets and three model architectures demonstrates that our method\nachieves substantially lower outlier rates and more consistent performance\ncompared to traditional input-space attacks and other VAE-based methods adapted\nfrom image domain approaches. Our comprehensive analysis includes\nhyperparameter sensitivity, sparsity control mechanisms, and generative\narchitectural comparisons, revealing that VAE-based attacks depend critically\non reconstruction quality but offer superior practical utility when sufficient\ntraining data is available. This work highlights the importance of on-manifold\nperturbations for realistic adversarial attacks on tabular data, offering a\nrobust approach for practical deployment. The source code can be accessed\nthrough https://github.com/ZhipengHe/VAE-TabAttack.\n","authors":["Zhipeng He","Alexander Stevens","Chun Ouyang","Johannes De Smedt","Alistair Barros","Catarina Moreira"],"pdf_url":"https://arxiv.org/pdf/2507.10998v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2503.00877v2","updated":"2025-07-15T05:33:15Z","published":"2025-03-02T12:36:15Z","title":"Patch-wise Structural Loss for Time Series Forecasting","summary":"  Time-series forecasting has gained significant attention in machine learning\ndue to its crucial role in various domains. However, most existing forecasting\nmodels rely heavily on point-wise loss functions like Mean Square Error, which\ntreat each time step independently and neglect the structural dependencies\ninherent in time series data, making it challenging to capture complex temporal\npatterns accurately. To address these challenges, we propose a novel Patch-wise\nStructural (PS) loss, designed to enhance structural alignment by comparing\ntime series at the patch level. Through leveraging local statistical\nproperties, such as correlation, variance, and mean, PS loss captures nuanced\nstructural discrepancies overlooked by traditional point-wise losses.\nFurthermore, it integrates seamlessly with point-wise loss, simultaneously\naddressing local structural inconsistencies and individual time-step errors. PS\nloss establishes a novel benchmark for accurately modeling complex time series\ndata and provides a new perspective on time series loss function design.\nExtensive experiments demonstrate that PS loss significantly improves the\nperformance of state-of-the-art models across diverse real-world datasets.\n","authors":["Dilfira Kudrat","Zongxia Xie","Yanru Sun","Tianyu Jia","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2503.00877v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10995v1","updated":"2025-07-15T05:27:51Z","published":"2025-07-15T05:27:51Z","title":"Misalignment from Treating Means as Ends","summary":"  Reward functions, learned or manually specified, are rarely perfect. Instead\nof accurately expressing human goals, these reward functions are often\ndistorted by human beliefs about how best to achieve those goals. Specifically,\nthese reward functions often express a combination of the human's terminal\ngoals -- those which are ends in themselves -- and the human's instrumental\ngoals -- those which are means to an end. We formulate a simple example in\nwhich even slight conflation of instrumental and terminal goals results in\nsevere misalignment: optimizing the misspecified reward function results in\npoor performance when measured by the true reward function. This example\ndistills the essential properties of environments that make reinforcement\nlearning highly sensitive to conflation of instrumental and terminal goals. We\ndiscuss how this issue can arise with a common approach to reward learning and\nhow it can manifest in real environments.\n","authors":["Henrik Marklund","Alex Infanger","Benjamin Van Roy"],"pdf_url":"https://arxiv.org/pdf/2507.10995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.16790v2","updated":"2025-07-15T05:21:45Z","published":"2025-06-20T07:14:31Z","title":"Exploring and Improving Initialization for Deep Graph Neural Networks: A\n  Signal Propagation Perspective","summary":"  Graph Neural Networks (GNNs) often suffer from performance degradation as the\nnetwork depth increases. This paper addresses this issue by introducing\ninitialization methods that enhance signal propagation (SP) within GNNs. We\npropose three key metrics for effective SP in GNNs: forward propagation,\nbackward propagation, and graph embedding variation (GEV). While the first two\nmetrics derive from classical SP theory, the third is specifically designed for\nGNNs. We theoretically demonstrate that a broad range of commonly used\ninitialization methods for GNNs, which exhibit performance degradation with\nincreasing depth, fail to control these three metrics simultaneously. To deal\nwith this limitation, a direct exploitation of the SP analysis--searching for\nweight initialization variances that optimize the three metrics--is shown to\nsignificantly enhance the SP in deep GCNs. This approach is called Signal\nPropagation on Graph-guided Initialization (SPoGInit). Our experiments\ndemonstrate that SPoGInit outperforms commonly used initialization methods on\nvarious tasks and architectures. Notably, SPoGInit enables performance\nimprovements as GNNs deepen, which represents a significant advancement in\naddressing depth-related challenges and highlights the validity and\neffectiveness of the SP analysis framework.\n","authors":["Senmiao Wang","Yupeng Chen","Yushun Zhang","Ruoyu Sun","Tian Ding"],"pdf_url":"https://arxiv.org/pdf/2506.16790v2.pdf","comment":"Published in TMLR (2025)"},{"id":"http://arxiv.org/abs/2412.19403v3","updated":"2025-07-15T05:16:51Z","published":"2024-12-27T01:53:18Z","title":"Fully Data-driven but Interpretable Human Behavioural Modelling with\n  Differentiable Discrete Choice Model","summary":"  Discrete choice models are essential for modelling various decision-making\nprocesses in human behaviour. However, the specification of these models has\ndepended heavily on domain knowledge from experts, and the fully automated but\ninterpretable modelling of complex human behaviours has been a long-standing\nchallenge. In this paper, we introduce the differentiable discrete choice model\n(Diff-DCM), a fully data-driven method for the interpretable modelling,\nlearning, prediction, and control of complex human behaviours, which is\nrealised by differentiable programming. Solely from input features and choice\noutcomes without any prior knowledge, Diff-DCM can estimate interpretable\nclosed-form utility functions that reproduce observed behaviours. Comprehensive\nexperiments with both synthetic and real-world data demonstrate that Diff-DCM\ncan be applied to various types of data and requires only a small amount of\ncomputational resources for the estimations, which can be completed within tens\nof seconds on a laptop without any accelerators. In these experiments, we also\ndemonstrate that, using its differentiability, Diff-DCM can provide useful\ninsights into human behaviours, such as an optimal intervention path for\neffective behavioural changes. This study provides a strong basis for the fully\nautomated and reliable modelling, prediction, and control of human behaviours.\n","authors":["Fumiyasu Makinoshima","Tatsuya Mitomi","Fumiya Makihara","Eigo Segawa"],"pdf_url":"https://arxiv.org/pdf/2412.19403v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23022v3","updated":"2025-07-15T05:15:18Z","published":"2024-10-30T13:52:43Z","title":"Online Intrinsic Rewards for Decision Making Agents from Large Language\n  Model Feedback","summary":"  Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples, due to requiring LLM annotations for each observation, or\nthey require a diverse offline dataset, which may not exist or be impossible to\ncollect. In this work, we address these limitations through a combination of\nalgorithmic and systems-level contributions. We propose ONI, a distributed\narchitecture that simultaneously learns an RL policy and an intrinsic reward\nfunction using LLM feedback. Our approach annotates the agent's collected\nexperience via an asynchronous LLM server, which is then distilled into an\nintrinsic reward model. We explore a range of algorithmic choices for reward\nmodeling with varying complexity, including hashing, classification, and\nranking models. Our approach achieves state-of-the-art performance across a\nrange of challenging tasks from the NetHack Learning Environment, while\nremoving the need for large offline datasets required by prior work. We make\nour code available at https://github.com/facebookresearch/oni .\n","authors":["Qinqing Zheng","Mikael Henaff","Amy Zhang","Aditya Grover","Brandon Amos"],"pdf_url":"https://arxiv.org/pdf/2410.23022v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05763v2","updated":"2025-07-15T05:12:11Z","published":"2025-05-09T03:53:10Z","title":"BMDetect: A Multimodal Deep Learning Framework for Comprehensive\n  Biomedical Misconduct Detection","summary":"  Academic misconduct detection in biomedical research remains challenging due\nto algorithmic narrowness in existing methods and fragmented analytical\npipelines. We present BMDetect, a multimodal deep learning framework that\nintegrates journal metadata (SJR, institutional data), semantic embeddings\n(PubMedBERT), and GPT-4o-mined textual attributes (methodological statistics,\ndata anomalies) for holistic manuscript evaluation. Key innovations include:\n(1) multimodal fusion of domain-specific features to reduce detection bias; (2)\nquantitative evaluation of feature importance, identifying journal authority\nmetrics (e.g., SJR-index) and textual anomalies (e.g., statistical outliers) as\ndominant predictors; and (3) the BioMCD dataset, a large-scale benchmark with\n13,160 retracted articles and 53,411 controls. BMDetect achieves 74.33% AUC,\noutperforming single-modality baselines by 8.6%, and demonstrates\ntransferability across biomedical subfields. This work advances scalable,\ninterpretable tools for safeguarding research integrity.\n","authors":["Yize Zhou","Jie Zhang","Meijie Wang","Lun Yu"],"pdf_url":"https://arxiv.org/pdf/2505.05763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10990v1","updated":"2025-07-15T05:07:12Z","published":"2025-07-15T05:07:12Z","title":"High-Throughput Distributed Reinforcement Learning via Adaptive Policy\n  Synchronization","summary":"  Scaling reinforcement learning (RL) workloads often requires distributing\nenvironment simulation across compute clusters. Existing frameworks entangle\nsimulation, learning logic, and orchestration into monolithic systems, limiting\nmodularity and reusability. We present ClusterEnv, a lightweight,\nlearner-agnostic interface for distributed environment execution that mirrors\nthe Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples\nsimulation from training by offloading reset() and step() operations to remote\nworkers while keeping learning centralized. To address policy staleness in\ndistributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),\na divergence-triggered update mechanism that reduces synchronization overhead\nwithout sacrificing performance. ClusterEnv integrates cleanly into existing RL\npipelines, supports both on-policy and off-policy methods, and requires minimal\ncode changes. Experiments on discrete control tasks demonstrate that AAPS\nachieves high sample efficiency with significantly fewer weight updates. Source\ncode is available at https://github.com/rodlaf/ClusterEnv.\n","authors":["Rodney Lafuente-Mercado"],"pdf_url":"https://arxiv.org/pdf/2507.10990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10878v4","updated":"2025-07-15T05:05:38Z","published":"2024-08-20T14:08:16Z","title":"Trajectory Imputation in Multi-Agent Sports with Derivative-Accumulating\n  Self-Ensemble","summary":"  Multi-agent trajectory data collected from domains such as team sports often\nsuffer from missing values due to various factors. While many imputation\nmethods have been proposed for spatiotemporal data, they are not well-suited\nfor multi-agent sports scenarios where player movements are highly dynamic and\ninter-agent interactions continuously evolve. To address these challenges, we\npropose MIDAS (Multi-agent Imputer with Derivative-Accumulating Self-ensemble),\na framework that imputes multi-agent trajectories with high accuracy and\nphysical plausibility. It jointly predicts positions, velocities, and\naccelerations through a Set Transformer-based neural network and generates\nalternative estimates by recursively accumulating predicted velocity and\nacceleration values. These predictions are then combined using a learnable\nweighted ensemble to produce final imputed trajectories. Experiments on three\nsports datasets demonstrate that MIDAS significantly outperforms existing\nbaselines in both positional accuracy and physical plausibility. Lastly, we\nshowcase use cases of MIDAS, such as approximating total distance and pass\nsuccess probability, to highlight its applicability to practical downstream\ntasks that require complete tracking data.\n","authors":["Han-Jun Choi","Hyunsung Kim","Minho Lee","Minchul Jeong","Chang-Jo Kim","Jinsung Yoon","Sang-Ki Ko"],"pdf_url":"https://arxiv.org/pdf/2408.10878v4.pdf","comment":"Accepted at ECML/PKDD 2025"},{"id":"http://arxiv.org/abs/2507.10986v1","updated":"2025-07-15T04:59:22Z","published":"2025-07-15T04:59:22Z","title":"StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar\n  Flare Forecasting with Historical & Statistical Data","summary":"  Stellar flare forecasting, a critical research frontier in astronomy, offers\nprofound insights into stellar activity. However, the field is constrained by\nboth the sparsity of recorded flare events and the absence of domain-specific\nlarge-scale predictive models. To address these challenges, this study\nintroduces StellarF (Stellar Flare Forecasting), a novel large model that\nleverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient\nlearning for stellar flare forecasting. At its core, StellarF integrates an\nflare statistical information module with a historical flare record module,\nenabling multi-scale pattern recognition from observational data. Extensive\nexperiments on our self-constructed datasets (derived from Kepler and TESS\nlight curves) demonstrate that StellarF achieves state-of-the-art performance\ncompared to existing methods. The proposed prediction paradigm establishes a\nnovel methodological framework for advancing astrophysical research and\ncross-disciplinary applications.\n","authors":["Tianyu Su","Zhiqiang Zou","Ali Luo","Xiao Kong","Qingyu Lu","Min Li"],"pdf_url":"https://arxiv.org/pdf/2507.10986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10983v1","updated":"2025-07-15T04:56:26Z","published":"2025-07-15T04:56:26Z","title":"Physics-Informed Neural Networks For Semiconductor Film Deposition: A\n  Review","summary":"  Semiconductor manufacturing relies heavily on film deposition processes, such\nas Chemical Vapor Deposition and Physical Vapor Deposition. These complex\nprocesses require precise control to achieve film uniformity, proper adhesion,\nand desired functionality. Recent advancements in Physics-Informed Neural\nNetworks (PINNs), an innovative machine learning (ML) approach, have shown\nsignificant promise in addressing challenges related to process control,\nquality assurance, and predictive modeling within semiconductor film deposition\nand other manufacturing domains. This paper provides a comprehensive review of\nML applications targeted at semiconductor film deposition processes. Through a\nthematic analysis, we identify key trends, existing limitations, and research\ngaps, offering insights into both the advantages and constraints of current\nmethodologies. Our structured analysis aims to highlight the potential\nintegration of these ML techniques to enhance interpretability, accuracy, and\nrobustness in film deposition processes. Additionally, we examine\nstate-of-the-art PINN methods, discussing strategies for embedding physical\nknowledge, governing laws, and partial differential equations into advanced\nneural network architectures tailored for semiconductor manufacturing. Based on\nthis detailed review, we propose novel research directions that integrate the\nstrengths of PINNs to significantly advance film deposition processes. The\ncontributions of this study include establishing a clear pathway for future\nresearch in integrating physics-informed ML frameworks, addressing existing\nmethodological gaps, and ultimately improving precision, scalability, and\noperational efficiency within semiconductor manufacturing.\n","authors":["Tao Han","Zahra Taheri","Hyunwoong Ko"],"pdf_url":"https://arxiv.org/pdf/2507.10983v1.pdf","comment":"11 pages, 1 figure, 3 tables, IDETC-CIE 2025"},{"id":"http://arxiv.org/abs/2404.14442v4","updated":"2025-07-15T04:14:38Z","published":"2024-04-20T01:16:27Z","title":"Unified ODE Analysis of Smooth Q-Learning Algorithms","summary":"  Convergence of Q-learning has been the focus of extensive research over the\npast several decades. Recently, an asymptotic convergence analysis for\nQ-learning was introduced using a switching system framework. This approach\napplies the so-called ordinary differential equation (ODE) approach to prove\nthe convergence of the asynchronous Q-learning modeled as a continuous-time\nswitching system, where notions from switching system theory are used to prove\nits asymptotic stability without using explicit Lyapunov arguments. However, to\nprove stability, restrictive conditions, such as quasi-monotonicity, must be\nsatisfied for the underlying switching systems, which makes it hard to easily\ngeneralize the analysis method to other reinforcement learning algorithms, such\nas the smooth Q-learning variants. In this paper, we present a more general and\nunified convergence analysis that improves upon the switching system approach\nand can analyze Q-learning and its smooth variants. The proposed analysis is\nmotivated by previous work on the convergence of synchronous Q-learning based\non $p$-norm serving as a Lyapunov function. However, the proposed analysis\naddresses more general ODE models that can cover both asynchronous Q-learning\nand its smooth versions with simpler frameworks.\n","authors":["Donghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2404.14442v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.23550v2","updated":"2025-07-15T03:56:08Z","published":"2025-06-30T06:49:31Z","title":"Seeding neural network quantum states with tensor network states","summary":"  We find an efficient approach to approximately convert matrix product states\n(MPSs) into restricted Boltzmann machine wave functions consisting of a\nmultinomial hidden unit through a canonical polyadic (CP) decomposition of the\nMPSs. This method allows us to generate well-behaved initial neural network\nquantum states for quantum many-body ground-state calculations in polynomial\ntime of the number of variational parameters and systematically shorten the\ndistance between the initial states and the ground states with increasing the\nrank of the CP decomposition. We demonstrate the efficiency of our method by\ntaking the transverse-field Ising model as an example and discuss possible\napplications of our method to more general quantum many-body systems in which\nthe ground-state wave functions possess complex nodal structures.\n","authors":["Ryui Kaneko","Shimpei Goto"],"pdf_url":"https://arxiv.org/pdf/2506.23550v2.pdf","comment":"14 pages, 15 figures"},{"id":"http://arxiv.org/abs/2411.15821v4","updated":"2025-07-15T03:41:57Z","published":"2024-11-24T12:51:50Z","title":"Is Training Data Quality or Quantity More Impactful to Small Language\n  Model Performance?","summary":"  This study investigates the relative impact of training data quality versus\nquantity on the performance of small language models (SLMs), utilizing the\nTinyStories dataset for empirical analysis. Analysis of dataset variations with\nrespect to size (25% and 50% of the original size) and duplication (controlled\nrates of 25%, 50%, 75%, and 100%) were performed. Model performance was\nevaluated based on the validation loss, accuracy, and perplexity metrics.\nResults indicate training data quality plays a more significant role in the\noverall performance of SLMs, especially given scale of this experiment. Minimal\nduplication positively impacted model accuracy (+0.87% increase in accuracy at\n25% duplication) without significantly increasing perplexity (+0.52% increase\ngoing from 0% to 25% duplication) but excessive duplication led to pronounced\nperformance degradation (-40% drop in accuracy at 100% duplication). The\nimplications of this exploration extend beyond just model performance; training\nlarge-scale models imposes significant financial and computational burdens,\nwhich can be prohibitive for organizations, individuals, and the public at\nlarge, especially in developing countries. Additionally, the energy consumption\nassociated with large-scale training raises environmental concerns.\nUnderstanding the relative importance of data quality versus quantity could\ndemocratize AI technology, making advanced models more accessible and\nsustainable for all.\n","authors":["Aryan Sajith","Krishna Chaitanya Rao Kathala"],"pdf_url":"https://arxiv.org/pdf/2411.15821v4.pdf","comment":"14 pages, 5 tables, 4 figures | Accepted at International Conference\n  on Neural Computing for Advanced Applications 2025, Conference info:\n  https://aaci.org.hk/ncaa2025"},{"id":"http://arxiv.org/abs/2507.10956v1","updated":"2025-07-15T03:39:07Z","published":"2025-07-15T03:39:07Z","title":"GOLFS: Feature Selection via Combining Both Global and Local Information\n  for High Dimensional Clustering","summary":"  It is important to identify the discriminative features for high dimensional\nclustering. However, due to the lack of cluster labels, the regularization\nmethods developed for supervised feature selection can not be directly applied.\nTo learn the pseudo labels and select the discriminative features\nsimultaneously, we propose a new unsupervised feature selection method, named\nGlObal and Local information combined Feature Selection (GOLFS), for high\ndimensional clustering problems. The GOLFS algorithm combines both local\ngeometric structure via manifold learning and global correlation structure of\nsamples via regularized self-representation to select the discriminative\nfeatures. The combination improves the accuracy of both feature selection and\nclustering by exploiting more comprehensive information. In addition, an\niterative algorithm is proposed to solve the optimization problem and the\nconvergency is proved. Simulations and two real data applications demonstrate\nthe excellent finite-sample performance of GOLFS on both feature selection and\nclustering.\n","authors":["Zhaoyu Xing","Yang Wan","Juan Wen","Wei Zhong"],"pdf_url":"https://arxiv.org/pdf/2507.10956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10955v1","updated":"2025-07-15T03:38:01Z","published":"2025-07-15T03:38:01Z","title":"Diffusion Decoding for Peptide De Novo Sequencing","summary":"  Peptide de novo sequencing is a method used to reconstruct amino acid\nsequences from tandem mass spectrometry data without relying on existing\nprotein sequence databases. Traditional deep learning approaches, such as\nCasanovo, mainly utilize autoregressive decoders and predict amino acids\nsequentially. Subsequently, they encounter cascading errors and fail to\nleverage high-confidence regions effectively. To address these issues, this\npaper investigates using diffusion decoders adapted for the discrete data\ndomain. These decoders provide a different approach, allowing sequence\ngeneration to start from any peptide segment, thereby enhancing prediction\naccuracy. We experiment with three different diffusion decoder designs,\nknapsack beam search, and various loss functions. We find knapsack beam search\ndid not improve performance metrics and simply replacing the transformer\ndecoder with a diffusion decoder lowered performance. Although peptide\nprecision and recall were still 0, the best diffusion decoder design with the\nDINOISER loss function obtained a statistically significant improvement in\namino acid recall by 0.373 compared to the baseline autoregressive\ndecoder-based Casanovo model. These findings highlight the potential of\ndiffusion decoders to not only enhance model sensitivity but also drive\nsignificant advancements in peptide de novo sequencing.\n","authors":["Chi-en Amy Tai","Alexander Wong"],"pdf_url":"https://arxiv.org/pdf/2507.10955v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.10972v1","updated":"2025-07-15T04:31:52Z","published":"2025-07-15T04:31:52Z","title":"Teach Me Sign: Stepwise Prompting LLM for Sign Language Production","summary":"  Large language models, with their strong reasoning ability and rich\nknowledge, have brought revolution to many tasks of AI, but their impact on\nsign language generation remains limited due to its complexity and unique\nrules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign\nlanguage as another natural language. By fine-tuning an LLM, we enable it to\nlearn the correspondence between text and sign language, and facilitate\ngeneration. Considering the differences between sign and spoken language, we\nemploy a stepwise prompting strategy to extract the inherent sign language\nknowledge within the LLM, thereby supporting the learning and generation\nprocess. Experimental results on How2Sign and Phoenix14T datasets demonstrate\nthat our approach effectively leverages both the sign language knowledge and\nreasoning capabilities of LLM to align the different distribution and\ngrammatical rules between sign and spoken language.\n","authors":["Zhaoyi An","Rei Kawakami"],"pdf_url":"https://arxiv.org/pdf/2507.10972v1.pdf","comment":"Accepted by IEEE ICIP 2025"},{"id":"http://arxiv.org/abs/2506.22790v2","updated":"2025-07-15T23:50:11Z","published":"2025-06-28T07:14:23Z","title":"ICME 2025 Generalizable HDR and SDR Video Quality Measurement Grand\n  Challenge","summary":"  This paper reports IEEE International Conference on Multimedia \\& Expo (ICME)\n2025 Grand Challenge on Generalizable HDR and SDR Video Quality Measurement.\nWith the rapid development of video technology, especially High Dynamic Range\n(HDR) and Standard Dynamic Range (SDR) contents, the need for robust and\ngeneralizable Video Quality Assessment (VQA) methods has become increasingly\ndemanded. Existing VQA models often struggle to deliver consistent performance\nacross varying dynamic ranges, distortion types, and diverse content. This\nchallenge was established to benchmark and promote VQA approaches capable of\njointly handling HDR and SDR content. In the final evaluation phase, five teams\nsubmitted seven models along with technical reports to the Full Reference (FR)\nand No Reference (NR) tracks. Among them, four methods outperformed VMAF\nbaseline, while the top-performing model achieved state-of-the-art performance,\nsetting a new benchmark for generalizable video quality assessment.\n","authors":["Yixu Chen","Bowen Chen","Hai Wei","Alan C. Bovik","Baojun Li","Wei Sun","Linhan Cao","Kang Fu","Dandan Zhu","Jun Jia","Menghan Hu","Xiongkuo Min","Guangtao Zhai","Dounia Hammou","Fei Yin","Rafal Mantiuk","Amritha Premkumar","Prajit T Rajendran","Vignesh V Menon"],"pdf_url":"https://arxiv.org/pdf/2506.22790v2.pdf","comment":"ICME 2025 Grand Challenges"},{"id":"http://arxiv.org/abs/2403.05192v4","updated":"2025-07-15T18:32:35Z","published":"2024-03-08T10:14:32Z","title":"An End-to-End Pipeline Perspective on Video Streaming in Best-Effort\n  Networks: A Survey and Tutorial","summary":"  Remaining a dominant force in Internet traffic, video streaming captivates\nend users, service providers, and researchers. This paper takes a pragmatic\napproach to reviewing recent advances in the field by focusing on the prevalent\nstreaming paradigm that involves delivering long-form two-dimensional videos\nover the best-effort Internet with client-side adaptive bitrate (ABR)\nalgorithms and assistance from content delivery networks (CDNs). To enhance\naccessibility, we supplement the survey with tutorial material. Unlike existing\nsurveys that offer fragmented views, our work provides a holistic perspective\non the entire end-to-end streaming pipeline, from video capture by a\ncamera-equipped device to playback by the end user. Our novel perspective\ncovers the ingestion, processing, and distribution stages of the pipeline and\naddresses key challenges such as video compression, upload, transcoding, ABR\nalgorithms, CDN support, and quality of experience. We review over 200 papers\nand classify streaming designs by their problem-solving methodology, whether\nbased on intuition (simple heuristics), theory (formal optimization), or\nmachine learning (generalizable data patterns). The survey further refines\nthese methodology-based categories and characterizes each design by additional\ntraits such as compatible codecs and use of super resolution. We connect the\nreviewed research to real-world applications by discussing the practices of\ncommercial streaming platforms. Finally, the survey highlights prominent\ncurrent trends and outlines future directions in video streaming.\n","authors":["Leonardo Peroni","Sergey Gorinsky"],"pdf_url":"https://arxiv.org/pdf/2403.05192v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15237v2","updated":"2025-07-15T18:09:44Z","published":"2025-03-19T14:14:57Z","title":"QuMATL: Query-based Multi-annotator Tendency Learning","summary":"  Different annotators often assign different labels to the same sample due to\nbackgrounds or preferences, and such labeling patterns are referred to as\ntendency. In multi-annotator scenarios, we introduce a novel task called\nMulti-annotator Tendency Learning (MATL), which aims to capture each annotator\ntendency. Unlike traditional tasks that prioritize consensus-oriented learning,\nwhich averages out annotator differences and leads to tendency information\nloss, MATL emphasizes learning each annotator tendency, better preserves\ntendency information. To this end, we propose an efficient baseline method,\nQuery-based Multi-annotator Tendency Learning (QuMATL), which uses lightweight\nquery to represent each annotator for tendency modeling. It saves the costs of\nbuilding separate conventional models for each annotator, leverages shared\nlearnable queries to capture inter-annotator correlations as an additional\nhidden supervisory signal to enhance modeling performance. Meanwhile, we\nprovide a new metric, Difference of Inter-annotator Consistency (DIC), to\nevaluate how effectively models preserve annotators tendency information.\nAdditionally, we contribute two large-scale datasets, STREET and AMER,\nproviding averages of 4300 and 3118 per-annotator labels, respectively.\nExtensive experiments verified the effectiveness of our QuMATL.\n","authors":["Liyun Zhang","Zheng Lian","Hong Liu","Takanori Takebe","Yuta Nakashima"],"pdf_url":"https://arxiv.org/pdf/2503.15237v2.pdf","comment":"13 pages"}]},"2025-07-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2507.12466v1","updated":"2025-07-16T17:59:45Z","published":"2025-07-16T17:59:45Z","title":"Language Models Improve When Pretraining Data Matches Target Tasks","summary":"  Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.\n","authors":["David Mizrahi","Anders Boesen Lindbo Larsen","Jesse Allardice","Suzie Petryk","Yuri Gorokhov","Jeffrey Li","Alex Fang","Josh Gardner","Tom Gunter","Afshin Dehghan"],"pdf_url":"https://arxiv.org/pdf/2507.12466v1.pdf","comment":"44 pages, 25 figures, 13 tables"},{"id":"http://arxiv.org/abs/2504.19982v2","updated":"2025-07-16T17:52:44Z","published":"2025-04-28T16:57:17Z","title":"TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining\n  Turn-Level Precision with Dialogue-Level Comparisons","summary":"  Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research.\n","authors":["Emre Can Acikgoz","Carl Guo","Suvodip Dey","Akul Datta","Takyoung Kim","Gokhan Tur","Dilek Hakkani-Tür"],"pdf_url":"https://arxiv.org/pdf/2504.19982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12451v1","updated":"2025-07-16T17:47:45Z","published":"2025-07-16T17:47:45Z","title":"S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling","summary":"  Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2507.12451v1.pdf","comment":"Accepted as a long paper for ACL 2025 main conference"},{"id":"http://arxiv.org/abs/2507.12428v1","updated":"2025-07-16T17:16:03Z","published":"2025-07-16T17:16:03Z","title":"Can We Predict Alignment Before Models Finish Thinking? Towards\n  Monitoring Misaligned Reasoning Models","summary":"  Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.\n","authors":["Yik Siu Chan","Zheng-Xin Yong","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2507.12428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12425v1","updated":"2025-07-16T17:13:06Z","published":"2025-07-16T17:13:06Z","title":"Advancing Retrieval-Augmented Generation for Structured Enterprise and\n  Internal Data","summary":"  Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot\n","authors":["Chandana Cheerla"],"pdf_url":"https://arxiv.org/pdf/2507.12425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08218v2","updated":"2025-07-16T16:57:48Z","published":"2025-07-10T23:47:05Z","title":"Simple Mechanistic Explanations for Out-Of-Context Reasoning","summary":"  Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs\nexhibit surprisingly deep out-of-distribution generalization. Rather than\nlearning shallow heuristics, they implicitly internalize and act on the\nconsequences of observations scattered throughout the fine-tuning data. In this\nwork, we investigate this phenomenon mechanistically and find that many\ninstances of OOCR in the literature have a simple explanation: the LoRA\nfine-tuning essentially adds a constant steering vector, steering the model\ntowards a general concept. This improves performance on the fine-tuning task\nand in many other concept-related domains, causing the surprising\ngeneralization. Moreover, we can directly train steering vectors for these\ntasks from scratch, which also induces OOCR. We find that our results hold even\nfor a task that seems like it must involve conditional behavior (model\nbackdoors); it turns out that unconditionally adding a steering vector is\nsufficient. Overall, our work presents one explanation of what gets learned\nduring fine-tuning for OOCR tasks, contributing to the key question of why LLMs\ncan reason out of context, an advanced capability that is highly relevant to\ntheir safe and reliable deployment.\n","authors":["Atticus Wang","Joshua Engels","Oliver Clive-Griffin","Senthooran Rajamanoharan","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2507.08218v2.pdf","comment":"ICML 2025 Workshop R2-FM"},{"id":"http://arxiv.org/abs/2507.12379v1","updated":"2025-07-16T16:27:50Z","published":"2025-07-16T16:27:50Z","title":"Probing for Arithmetic Errors in Language Models","summary":"  We investigate whether internal activations in language models can be used to\ndetect arithmetic errors. Starting with a controlled setting of 3-digit\naddition, we show that simple probes can accurately decode both the model's\npredicted output and the correct answer from hidden states, regardless of\nwhether the model's output is correct. Building on this, we train lightweight\nerror detectors that predict model correctness with over 90% accuracy. We then\nextend our analysis to structured chain-of-thought traces on addition-only\nGSM8K problems and find that probes trained on simple arithmetic generalize\nwell to this more complex setting, revealing consistent internal\nrepresentations. Finally, we demonstrate that these probes can guide selective\nre-prompting of erroneous reasoning steps, improving task accuracy with minimal\ndisruption to correct outputs. Our findings suggest that arithmetic errors can\nbe anticipated from internal activations alone, and that simple probes offer a\nviable path toward lightweight model self-correction.\n","authors":["Yucheng Sun","Alessandro Stolfo","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2507.12379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12378v1","updated":"2025-07-16T16:27:05Z","published":"2025-07-16T16:27:05Z","title":"Developing Visual Augmented Q&A System using Scalable Vision Embedding\n  Retrieval & Late Interaction Re-ranker","summary":"  Traditional information extraction systems face challenges with text only\nlanguage models as it does not consider infographics (visual elements of\ninformation) such as tables, charts, images etc. often used to convey complex\ninformation to readers. Multimodal LLM (MLLM) face challenges of finding needle\nin the haystack problem i.e., either longer context length or substantial\nnumber of documents as search space. Late interaction mechanism over visual\nlanguage models has shown state of the art performance in retrieval-based\nvision augmented Q&A tasks. There are yet few challenges using it for RAG based\nmulti-modal Q&A. Firstly, many popular and widely adopted vector databases do\nnot support native multi-vector retrieval. Secondly, late interaction requires\ncomputation which inflates space footprint and can hinder enterprise adoption.\nLastly, the current state of late interaction mechanism does not leverage the\napproximate neighbor search indexing methods for large speed ups in retrieval\nprocess. This paper explores a pragmatic approach to make vision retrieval\nprocess scalable and efficient without compromising on performance quality. We\npropose multi-step custom implementation utilizing widely adopted hybrid search\n(metadata & embedding) and state of the art late interaction re-ranker to\nretrieve best matching pages. Finally, MLLM are prompted as reader to generate\nanswers from contextualized best matching pages. Through experiments, we\nobserve that the proposed design is scalable (significant speed up) and stable\n(without degrading performance quality), hence can be used as production\nsystems at enterprises.\n","authors":["Rachna Saxena","Abhijeet Kumar","Suresh Shanmugam"],"pdf_url":"https://arxiv.org/pdf/2507.12378v1.pdf","comment":"Presented at NLP@IR workshop at SIGIR conference"},{"id":"http://arxiv.org/abs/2507.12372v1","updated":"2025-07-16T16:21:01Z","published":"2025-07-16T16:21:01Z","title":"Web-Browsing LLMs Can Access Social Media Profiles and Infer User\n  Demographics","summary":"  Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.\n","authors":["Meysam Alizadeh","Fabrizio Gilardi","Zeynab Samei","Mohsen Mosleh"],"pdf_url":"https://arxiv.org/pdf/2507.12372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12370v1","updated":"2025-07-16T16:15:25Z","published":"2025-07-16T16:15:25Z","title":"Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests\n  through Debate","summary":"  Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.\n","authors":["Ana Davila","Jacinto Colan","Yasuhisa Hasegawa"],"pdf_url":"https://arxiv.org/pdf/2507.12370v1.pdf","comment":"Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)"},{"id":"http://arxiv.org/abs/2507.12356v1","updated":"2025-07-16T15:56:09Z","published":"2025-07-16T15:56:09Z","title":"Exploring Gender Bias in Alzheimer's Disease Detection: Insights from\n  Mandarin and Greek Speech Perception","summary":"  Gender bias has been widely observed in speech perception tasks, influenced\nby the fundamental voicing differences between genders. This study reveals a\ngender bias in the perception of Alzheimer's Disease (AD) speech. In a\nperception experiment involving 16 Chinese listeners evaluating both Chinese\nand Greek speech, we identified that male speech was more frequently identified\nas AD, with this bias being particularly pronounced in Chinese speech. Acoustic\nanalysis showed that shimmer values in male speech were significantly\nassociated with AD perception, while speech portion exhibited a significant\nnegative correlation with AD identification. Although language did not have a\nsignificant impact on AD perception, our findings underscore the critical role\nof gender bias in AD speech perception. This work highlights the necessity of\naddressing gender bias when developing AD detection models and calls for\nfurther research to validate model performance across different linguistic\ncontexts.\n","authors":["Liu He","Yuanchao Li","Rui Feng","XinRan Han","Yin-Long Liu","Yuwei Yang","Zude Zhu","Jiahong Yuan"],"pdf_url":"https://arxiv.org/pdf/2507.12356v1.pdf","comment":"12 pages, 5 figures, conference or other essential info"},{"id":"http://arxiv.org/abs/2507.09477v2","updated":"2025-07-16T15:44:18Z","published":"2025-07-13T03:29:41Z","title":"Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs","summary":"  Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.\n","authors":["Yangning Li","Weizhi Zhang","Yuyao Yang","Wei-Chieh Huang","Yaozu Wu","Junyu Luo","Yuanchen Bei","Henry Peng Zou","Xiao Luo","Yusheng Zhao","Chunkit Chan","Yankai Chen","Zhongfen Deng","Yinghui Li","Hai-Tao Zheng","Dongyuan Li","Renhe Jiang","Ming Zhang","Yangqiu Song","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2507.09477v2.pdf","comment":"submitted to ARR May"},{"id":"http://arxiv.org/abs/2410.03103v3","updated":"2025-07-16T15:44:04Z","published":"2024-10-04T02:53:52Z","title":"Planning-Aware Code Infilling via Horizon-Length Prediction","summary":"  Fill-in-the-Middle (FIM), or infilling, has become integral to code language\nmodels, enabling generation of missing code given both left and right contexts.\nHowever, the current FIM training paradigm which performs next-token prediction\n(NTP) over reordered sequence often leads to models struggling to generate\ncontent that aligns well with the surrounding context. We hypothesize that NTP\nalone is insufficient for models to learn effective planning conditioned on the\ndistant right context, a critical factor for successful code infilling. To\novercome this, we propose Horizon-Length Prediction (HLP), a novel training\nobjective that teaches models to predict the number of remaining middle tokens\nat each step. HLP advances FIM with lookahead planning, enabling models to\ninherently learn infilling boundaries for arbitrary left and right contexts\nwithout relying on dataset-specific post-processing. Our evaluation across\ndifferent model families and sizes shows that HLP significantly improves FIM\nperformance by up to 24% relatively on diverse benchmarks, across file-level\nand repository-level. Furthermore, the enhanced planning capability gained\nthrough HLP boosts model performance on code reasoning. Importantly, HLP incurs\nnegligible training overhead and no additional inference cost, ensuring its\npracticality for real-world scenarios.\n","authors":["Yifeng Ding","Hantian Ding","Shiqi Wang","Qing Sun","Varun Kumar","Zijian Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03103v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12341v1","updated":"2025-07-16T15:36:15Z","published":"2025-07-16T15:36:15Z","title":"Nonlinear Concept Erasure: a Density Matching Approach","summary":"  Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.\n","authors":["Antoine Saillenfest","Pirmin Lemberger"],"pdf_url":"https://arxiv.org/pdf/2507.12341v1.pdf","comment":"17 pages, 10 figures, accepted for publication in ECAI 2025 (28th\n  European Conference on Artificial Intelligence)"},{"id":"http://arxiv.org/abs/2507.10644v2","updated":"2025-07-16T15:30:42Z","published":"2025-07-14T16:47:19Z","title":"From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web\n  of Agents","summary":"  The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.\n","authors":["Tatiana Petrova","Boris Bliznioukov","Aleksandr Puzikov","Radu State"],"pdf_url":"https://arxiv.org/pdf/2507.10644v2.pdf","comment":"33 pages, 9 figures, 8 tables"},{"id":"http://arxiv.org/abs/2507.12308v1","updated":"2025-07-16T15:05:30Z","published":"2025-07-16T15:05:30Z","title":"Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and\n  Summarization","summary":"  Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.\n","authors":["Prashanth Vijayaraghavan","Apoorva Nitsure","Charles Mackin","Luyao Shi","Stefano Ambrogio","Arvind Haran","Viresh Paruthi","Ali Elzein","Dan Coops","David Beymer","Tyler Baldwin","Ehsan Degan"],"pdf_url":"https://arxiv.org/pdf/2507.12308v1.pdf","comment":"10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings\n  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.\n  2024 (MLCAD'24)"},{"id":"http://arxiv.org/abs/2507.12295v1","updated":"2025-07-16T14:47:41Z","published":"2025-07-16T14:47:41Z","title":"Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding","summary":"  Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.\n","authors":["Feng Xiao","Jicong Fan"],"pdf_url":"https://arxiv.org/pdf/2507.12295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14335v2","updated":"2025-07-16T14:35:17Z","published":"2024-06-20T14:04:53Z","title":"Linearly-Interpretable Concept Embedding Models for Text Analysis","summary":"  Despite their success, Large-Language Models (LLMs) still face criticism due\nto their lack of interpretability. Traditional post-hoc interpretation methods,\nbased on attention and gradient-based analysis, offer limited insights as they\nonly approximate the model's decision-making processes and have been proved to\nbe unreliable. For this reason, Concept-Bottleneck Models (CBMs) have been\nlately proposed in the textual field to provide interpretable predictions based\non human-understandable concepts. However, CBMs still exhibit several\nlimitations due to their architectural constraints limiting their expressivity,\nto the absence of task-interpretability when employing non-linear task\npredictors and for requiring extensive annotations that are impractical for\nreal-world text data. In this paper, we address these challenges by proposing a\nnovel Linearly Interpretable Concept Embedding Model (LICEM) going beyond the\ncurrent accuracy-interpretability trade-off. LICEMs classification accuracy is\nbetter than existing interpretable models and matches black-box ones. We show\nthat the explanations provided by our models are more interveneable and\ncausally consistent with respect to existing solutions. Finally, we show that\nLICEMs can be trained without requiring any concept supervision, as concepts\ncan be automatically predicted when using an LLM backbone.\n","authors":["Francesco De Santis","Philippe Bich","Gabriele Ciravegna","Pietro Barbiero","Danilo Giordano","Tania Cerquitelli"],"pdf_url":"https://arxiv.org/pdf/2406.14335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12284v1","updated":"2025-07-16T14:31:33Z","published":"2025-07-16T14:31:33Z","title":"MERA Code: A Unified Framework for Evaluating Code Generation Across\n  Tasks","summary":"  Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.\n","authors":["Artem Chervyakov","Alexander Kharitonov","Pavel Zadorozhny","Adamenko Pavel","Rodion Levichev","Dmitrii Vorobev","Dmitrii Salikhov","Aidar Valeev","Alena Pestova","Maria Dziuba","Ilseyar Alimova","Artem Zavgorodnev","Aleksandr Medvedev","Stanislav Moiseev","Elena Bruches","Daniil Grebenkin","Roman Derunets","Vikulov Vladimir","Anton Emelyanov","Dmitrii Babaev","Vladimir V. Ivanov","Valentin Malykh","Alena Fenogenova"],"pdf_url":"https://arxiv.org/pdf/2507.12284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11330v2","updated":"2025-07-16T14:26:34Z","published":"2025-07-15T14:03:55Z","title":"Automated Novelty Evaluation of Academic Paper: A Collaborative Approach\n  Integrating Human and Large Language Model Knowledge","summary":"  Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. One of the most common types of\nnovelty in academic papers is the introduction of new methods. In this paper,\nwe propose leveraging human knowledge and LLM to assist pretrained language\nmodels (PLMs, e.g. BERT etc.) in predicting the method novelty of papers.\nSpecifically, we extract sentences related to the novelty of the academic paper\nfrom peer review reports and use LLM to summarize the methodology section of\nthe academic paper, which are then used to fine-tune PLMs. In addition, we have\ndesigned a text-guided fusion module with novel Sparse-Attention to better\nintegrate human and LLM knowledge. We compared the method we proposed with a\nlarge number of baselines. Extensive experiments demonstrate that our method\nachieves superior performance.\n","authors":["Wenqing Wu","Chengzhi Zhang","Yi Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.11330v2.pdf","comment":"Journal of the Association for Information Science and Technology,\n  2025"},{"id":"http://arxiv.org/abs/2507.10559v2","updated":"2025-07-16T14:25:07Z","published":"2025-07-02T15:50:09Z","title":"NLP Meets the World: Toward Improving Conversations With the Public\n  About Natural Language Processing Research","summary":"  Recent developments in large language models (LLMs) have been accompanied by\nrapidly growing public interest in natural language processing (NLP). This\nattention is reflected by major news venues, which sometimes invite NLP\nresearchers to share their knowledge and views with a wide audience.\nRecognizing the opportunities of the present, for both the research field and\nfor individual researchers, this paper shares recommendations for communicating\nwith a general audience about the capabilities and limitations of NLP. These\nrecommendations cover three themes: vague terminology as an obstacle to public\nunderstanding, unreasonable expectations as obstacles to sustainable growth,\nand ethical failures as obstacles to continued support. Published NLP research\nand popular news coverage are cited to illustrate these themes with examples.\nThe recommendations promote effective, transparent communication with the\ngeneral public about NLP, in order to strengthen public understanding and\nencourage support for research.\n","authors":["Shomir Wilson"],"pdf_url":"https://arxiv.org/pdf/2507.10559v2.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2410.11647v2","updated":"2025-07-16T14:16:47Z","published":"2024-10-15T14:33:23Z","title":"Measuring Spiritual Values and Bias of Large Language Models","summary":"  Large language models (LLMs) have become integral tool for users from various\nbackgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural\nnuances embedded in their pre-training data. However, the values and\nperspectives inherent in this data can influence the behavior of LLMs, leading\nto potential biases. As a result, the use of LLMs in contexts involving\nspiritual or moral values necessitates careful consideration of these\nunderlying biases. Our work starts with verification of our hypothesis by\ntesting the spiritual values of popular LLMs. Experimental results show that\nLLMs' spiritual values are quite diverse, as opposed to the stereotype of\natheists or secularists. We then investigate how different spiritual values\naffect LLMs in social-fairness scenarios e.g., hate speech identification). Our\nfindings reveal that different spiritual values indeed lead to different\nsensitivity to different hate target groups. Furthermore, we propose to\ncontinue pre-training LLMs on spiritual texts, and empirical results\ndemonstrate the effectiveness of this approach in mitigating spiritual bias.\n","authors":["Songyuan Liu","Ziyang Zhang","Runze Yan","Wei Wu","Carl Yang","Jiaying Lu"],"pdf_url":"https://arxiv.org/pdf/2410.11647v2.pdf","comment":"9 pages including appendix; 5 figures; 5 tables"},{"id":"http://arxiv.org/abs/2507.12261v1","updated":"2025-07-16T14:06:51Z","published":"2025-07-16T14:06:51Z","title":"Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form\n  Clinical Notes","summary":"  For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.\n","authors":["Johann Frei","Nils Feldhus","Lisa Raithel","Roland Roller","Alexander Meyer","Frank Kramer"],"pdf_url":"https://arxiv.org/pdf/2507.12261v1.pdf","comment":"Submitted to EMNLP 2025 System Demonstrations | Code:\n  https://github.com/j-frei/Infherno | Video:\n  https://www.youtube.com/watch?v=kyj5C2ivbMw | Demo:\n  https://infherno.misit-augsburg.de | HuggingFace Spaces:\n  https://huggingface.co/spaces/nfel/infherno"},{"id":"http://arxiv.org/abs/2507.12260v1","updated":"2025-07-16T14:06:05Z","published":"2025-07-16T14:06:05Z","title":"Translationese-index: Using Likelihood Ratios for Graded and\n  Generalizable Measurement of Translationese","summary":"  In this paper, we propose the first quantitative measure for translationese\n-- the translationese-index (T-index) for graded and generalizable measurement\nof translationese, computed from the likelihood ratios of two contrastively\nfine-tuned language models (LMs). We use a synthesized dataset and a dataset\nwith translations in the wild to evaluate T-index's generalizability in\ncross-domain settings and its validity against human judgments. Our results\nshow that T-index is both robust and efficient. T-index scored by two 0.5B LMs\nfine-tuned on only 1-5k pairs of synthetic data can well capture translationese\nin the wild. We find that the relative differences in T-indices between\ntranslations can well predict pairwise translationese annotations obtained from\nhuman annotators; and the absolute values of T-indices correlate well with\nhuman ratings of degrees of translationese (Pearson's $r = 0.568$).\nAdditionally, the correlation between T-index and existing machine translation\n(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting\nthat T-index is not covered by these metrics and can serve as a complementary\nmetric in MT QE.\n","authors":["Yikang Liu","Wanyang Zhang","Yiming Wang","Jialong Tang","Pei Zhang","Baosong Yang","Fei Huang","Rui Wang","Hai Hu"],"pdf_url":"https://arxiv.org/pdf/2507.12260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15460v4","updated":"2025-07-16T14:04:55Z","published":"2024-10-20T18:18:23Z","title":"Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model\n  Training","summary":"  As large language models (LLMs) become increasingly prevalent, concerns about\ntheir reliability, particularly due to hallucinations - factually inaccurate or\nirrelevant outputs - have grown. Our research investigates the relationship\nbetween the uncertainty in training dynamics and the emergence of\nhallucinations. Using models from the Pythia suite and several hallucination\ndetection metrics, we analyze hallucination trends and identify significant\nvariance during training. To address this, we propose \\textbf{Sensitivity\nDropout (SenD)}, a novel training protocol designed to reduce hallucination\nvariance during training by deterministically dropping embedding indices with\nsignificant variability. In addition, we develop an unsupervised hallucination\ndetection metric, Efficient EigenScore (EES), which approximates the\ntraditional EigenScore in 2x speed. This metric is integrated into our training\nprotocol, allowing SenD to be both computationally scalable and effective at\nreducing hallucination variance. SenD improves test-time reliability of Pythia\nand Meta's Llama models by up to 17\\% and enhances factual accuracy in\nWikipedia, Medical, Legal, and Coding domains without affecting downstream task\nperformance.\n","authors":["Shahrad Mohammadzadeh","Juan David Guerra","Marco Bonizzato","Reihaneh Rabbany","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2410.15460v4.pdf","comment":"Accepted to ACL 2025, accepted to Safe Generative AI Workshop @\n  NeurIPS 2024. Camera-ready version for ACL 2025 (to appear). Submitted July\n  2025"},{"id":"http://arxiv.org/abs/2507.12252v1","updated":"2025-07-16T13:59:32Z","published":"2025-07-16T13:59:32Z","title":"Improving Contextual ASR via Multi-grained Fusion with Large Language\n  Models","summary":"  While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.\n","authors":["Shilin Zhou","Zhenghua Li"],"pdf_url":"https://arxiv.org/pdf/2507.12252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16994v2","updated":"2025-07-16T13:35:23Z","published":"2025-02-24T09:28:35Z","title":"FADE: Why Bad Descriptions Happen to Good Features","summary":"  Recent advances in mechanistic interpretability have highlighted the\npotential of automating interpretability pipelines in analyzing the latent\nrepresentations within LLMs. While this may enhance our understanding of\ninternal mechanisms, the field lacks standardized evaluation methods for\nassessing the validity of discovered features. We attempt to bridge this gap by\nintroducing FADE: Feature Alignment to Description Evaluation, a scalable\nmodel-agnostic framework for automatically evaluating feature-to-description\nalignment. FADE evaluates alignment across four key metrics - Clarity,\nResponsiveness, Purity, and Faithfulness - and systematically quantifies the\ncauses of the misalignment between features and their descriptions. We apply\nFADE to analyze existing open-source feature descriptions and assess key\ncomponents of automated interpretability pipelines, aiming to enhance the\nquality of descriptions. Our findings highlight fundamental challenges in\ngenerating feature descriptions, particularly for SAEs compared to MLP neurons,\nproviding insights into the limitations and future directions of automated\ninterpretability. We release FADE as an open-source package at:\nhttps://github.com/brunibrun/FADE\n","authors":["Bruno Puri","Aakriti Jain","Elena Golimblevskaia","Patrick Kahardipraja","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2502.16994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00584v2","updated":"2025-07-16T13:33:08Z","published":"2025-04-01T09:39:57Z","title":"Semantic Adapter for Universal Text Embeddings: Diagnosing and\n  Mitigating Negation Blindness to Enhance Universality","summary":"  Negation plays an important role in various natural language processing tasks\nsuch as Natural Language Inference and Sentiment Analysis tasks. Numerous prior\nstudies have found that contextual text embedding models such as BERT, ELMO,\nRoBERTa or XLNet face challenges in accurately understanding negation. Recent\nadvancements in universal text embeddings have demonstrated superior\nperformance over contextual text embeddings in various tasks. However, due to\nthe bias in popular evaluation benchmarks, the negation awareness capacity of\nthese models remains unclear. To bridge the gap in existing literature, an\nin-depth analysis is initiated in this work to study the negation awareness of\ncutting-edge universal text embedding models. Our findings reveal a significant\nlack of negation awareness in these models, often interpreting negated text\npairs as semantically similar. To efficiently deal with the conflict that\ndifferent tasks need different trade-offs between topic and negation\ninformation among other semantic information, a data-efficient and\ncomputational-efficient embedding re-weighting method is proposed without\nmodifying the parameters of text embedding models. The proposed solution is\nable to improve text embedding models' negation awareness significantly on both\nsimple negation understanding task and complex negation understanding task.\nFurthermore, the proposed solution can also significantly improve the negation\nawareness of Large Language Model based task-specific high dimensional\nuniversal text embeddings.\n","authors":["Hongliu Cao"],"pdf_url":"https://arxiv.org/pdf/2504.00584v2.pdf","comment":"Accepted in ECAI 2025 main track"},{"id":"http://arxiv.org/abs/2507.10577v2","updated":"2025-07-16T13:25:34Z","published":"2025-07-11T10:08:05Z","title":"Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos\n  and influence opinions","summary":"  Misinformation poses a significant threat in today's digital world, often\nspreading rapidly through platforms like YouTube. This paper introduces a novel\napproach to combating misinformation by developing an AI-powered system that\nnot only fact-checks claims made in YouTube videos but also actively engages\nusers in the comment section and challenge misleading narratives. Our system\ncomprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented\nGeneration (RAG) approach - drawing on sources like Wikipedia, Google Search,\nGoogle FactCheck - to accurately assess their veracity and generates a nuanced\nand comprehensive report. Through rigorous prompt engineering, Trend Bender\nleverages this report along with a curated corpus of relevant articles to\ngenerate insightful and persuasive comments designed to stimulate a productive\ndebate. With a carefully set up self-evaluation loop, this agent is able to\niteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established\nbenchmark datasets and a real-world deployment on YouTube, showcasing its\npotential to engage users and potentially influence perspectives. Our findings\nhighlight the high accuracy of our fact-checking agent, and confirm the\npotential of AI-driven interventions in combating misinformation and fostering\na more informed online space.\n","authors":["Cécile Logé","Rehan Ghori"],"pdf_url":"https://arxiv.org/pdf/2507.10577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12217v1","updated":"2025-07-16T13:20:32Z","published":"2025-07-16T13:20:32Z","title":"Towards few-shot isolated word reading assessment","summary":"  We explore an ASR-free method for isolated word reading assessment in\nlow-resource settings. Our few-shot approach compares input child speech to a\nsmall set of adult-provided reference templates. Inputs and templates are\nencoded using intermediate layers from large self-supervised learned (SSL)\nmodels. Using an Afrikaans child speech benchmark, we investigate design\noptions such as discretising SSL features and barycentre averaging of the\ntemplates. Idealised experiments show reasonable performance for adults, but a\nsubstantial drop for child speech input, even with child templates. Despite the\nsuccess of employing SSL representations in low-resource speech tasks, our work\nhighlights the limitations of SSL representations for processing child data\nwhen used in a few-shot classification system.\n","authors":["Reuben Smit","Retief Louw","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2507.12217v1.pdf","comment":"Accepted to SLaTE 2025"},{"id":"http://arxiv.org/abs/2507.12208v1","updated":"2025-07-16T13:10:10Z","published":"2025-07-16T13:10:10Z","title":"Toward a Behavioural Translation Style Space: Simulating the Temporal\n  Dynamics of Affect, Behaviour, and Cognition in Human Translation Production","summary":"  The paper introduces a Behavioural Translation Style Space (BTSS) that\ndescribes possible behavioural translation patterns. The suggested BTSS is\norganized as a hierarchical structure that entails various embedded processing\nlayers. We posit that observable translation behaviour - i.e., eye and finger\nmovements - is fundamental when executing the physical act of translation but\nit is caused and shaped by higher-order cognitive processes and affective\ntranslation states. We analyse records of keystrokes and gaze data as\nindicators of the hidden mental processing structure and organize the\nbehavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the\nbasis for a computational translation agent to simulate the temporal dynamics\nof affect, automatized behaviour and cognition during human translation\nproduction.\n","authors":["Michael Carl","Takanori Mizowaki","Aishvarya Ray","Masaru Yamada","Devi Sri Bandaru","Xinyue Ren"],"pdf_url":"https://arxiv.org/pdf/2507.12208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11423v2","updated":"2025-07-16T13:02:26Z","published":"2025-07-15T15:47:47Z","title":"Reasoning Strategies in Large Language Models: Can They Follow, Prefer,\n  and Optimize?","summary":"  Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities.\n","authors":["Yanjian Zhang","Guillaume Wisniewski","Nadi Tomeh","Thierry Charnois"],"pdf_url":"https://arxiv.org/pdf/2507.11423v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07682v4","updated":"2025-07-16T12:27:40Z","published":"2024-12-10T17:13:35Z","title":"TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation","summary":"  The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks.\n","authors":["Alfredo Garrachón Ruiz","Tomás de la Rosa","Daniel Borrajo"],"pdf_url":"https://arxiv.org/pdf/2412.07682v4.pdf","comment":"13 pages, 12 tables, 7 figures"},{"id":"http://arxiv.org/abs/2507.12175v1","updated":"2025-07-16T12:13:13Z","published":"2025-07-16T12:13:13Z","title":"RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance\n  Alignment, Transcription, and Mistake Detection","summary":"  This study introduces RUMAA, a transformer-based framework for music\nperformance analysis that unifies score-to-performance alignment,\nscore-informed transcription, and mistake detection in a near end-to-end\nmanner. Unlike prior methods addressing these tasks separately, RUMAA\nintegrates them using pre-trained score and audio encoders and a novel\ntri-stream decoder capturing task interdependencies through proxy tasks. It\naligns human-readable MusicXML scores with repeat symbols to full-length\nperformance audio, overcoming traditional MIDI-based methods that rely on\nmanually unfolded score-MIDI data with pre-specified repeat structures. RUMAA\nmatches state-of-the-art alignment methods on non-repeated scores and\noutperforms them on scores with repeats in a public piano music dataset, while\nalso delivering promising transcription and mistake detection results.\n","authors":["Sungkyun Chang","Simon Dixon","Emmanouil Benetos"],"pdf_url":"https://arxiv.org/pdf/2507.12175v1.pdf","comment":"Accepted to WASPAA 2025"},{"id":"http://arxiv.org/abs/2403.15740v3","updated":"2025-07-16T11:45:02Z","published":"2024-03-23T06:36:32Z","title":"Protecting Copyrighted Material with Unique Identifiers in Large\n  Language Model Training","summary":"  A primary concern regarding training large language models (LLMs) is whether\nthey abuse copyrighted online text. With the increasing training data scale and\nthe prevalence of LLMs in daily lives, two problems arise: \\textbf{1)} false\npositive membership inference results misled by similar examples; \\textbf{2)}\nmembership inference methods are usually too complex for end users to\nunderstand and use. To address these issues, we propose an alternative\n\\textit{insert-and-detect} methodology, advocating that web users and content\nplatforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{\\textbf{ghost\nsentences}} and a user-friendly last-$k$ words test, allowing end users to chat\nwith LLMs for membership inference. Ghost sentences consist primarily of unique\npassphrases of random natural words, which can come with customized elements to\nbypass possible filter rules. The last-$k$ words test requires a significant\nrepetition time of ghost sentences~($\\ge10$). For cases with fewer repetitions,\nwe designed an extra perplexity test, as LLMs exhibit high perplexity when\nencountering unnatural passphrases. We also conduct a comprehensive study on\nthe memorization and membership inference of ghost sentences, examining factors\nsuch as training data scales, model sizes, repetition times, insertion\npositions, wordlist of passphrases, alignment, \\textit{etc}. Our study shows\nthe possibility of applying ghost sentences in real scenarios and provides\ninstructions for the potential application.\n","authors":["Shuai Zhao","Linchao Zhu","Ruijie Quan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15740v3.pdf","comment":"A technical report, work mainly done in the early of 2024"},{"id":"http://arxiv.org/abs/2504.09037v2","updated":"2025-07-16T11:33:35Z","published":"2025-04-12T01:27:49Z","title":"A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to\n  Reason, and Agentic Systems","summary":"  Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ...\n","authors":["Zixuan Ke","Fangkai Jiao","Yifei Ming","Xuan-Phi Nguyen","Austin Xu","Do Xuan Long","Minzhi Li","Chengwei Qin","Peifeng Wang","Silvio Savarese","Caiming Xiong","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2504.09037v2.pdf","comment":"72 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.23836v3","updated":"2025-07-16T11:25:40Z","published":"2025-05-28T12:03:09Z","title":"Large Language Models Often Know When They Are Being Evaluated","summary":"  If AI models can detect when they are being evaluated, the effectiveness of\nevaluations might be compromised. For example, models could have systematically\ndifferent behavior during evaluations, leading to less reliable benchmarks for\ndeployment and governance decisions. We investigate whether frontier language\nmodels can accurately classify transcripts based on whether they originate from\nevaluations or real-world deployment, a capability we call evaluation\nawareness. To achieve this, we construct a diverse benchmark of 1,000 prompts\nand transcripts from 61 distinct datasets. These span public benchmarks (e.g.,\nMMLU, SWEBench), real-world deployment interactions, and agent trajectories\nfrom scaffolding frameworks (e.g., web-browsing agents). Frontier models\nclearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches\nan AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of\n$0.92$). Furthermore, both AI models and humans are better at identifying\nevaluations in agentic settings compared to chat settings. Additionally, we\ntest whether models can identify the purpose of the evaluation. Under\nmultiple-choice and open-ended questioning, AI models far outperform random\nchance in identifying what an evaluation is testing for. Our results indicate\nthat frontier models already exhibit a substantial, though not yet superhuman,\nlevel of evaluation-awareness. We recommend tracking this capability in future\nmodels.\n","authors":["Joe Needham","Giles Edkins","Govind Pimpale","Henning Bartsch","Marius Hobbhahn"],"pdf_url":"https://arxiv.org/pdf/2505.23836v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12143v1","updated":"2025-07-16T11:19:28Z","published":"2025-07-16T11:19:28Z","title":"Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as\n  Teachers, Students and Evaluators","summary":"  ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable.\n","authors":["Pavel Šindelář","Ondřej Bojar"],"pdf_url":"https://arxiv.org/pdf/2507.12143v1.pdf","comment":"30 pages, 7 figures, CLEF 2025 Conference and Labs of the Evaluation\n  Forum"},{"id":"http://arxiv.org/abs/2507.12142v1","updated":"2025-07-16T11:17:12Z","published":"2025-07-16T11:17:12Z","title":"RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA\n  Optimization","summary":"  Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.\n","authors":["Vladimir Bogachev","Vladimir Aletov","Alexander Molozhavenko","Denis Bobkov","Vera Soboleva","Aibek Alanov","Maxim Rakhuba"],"pdf_url":"https://arxiv.org/pdf/2507.12142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12126v1","updated":"2025-07-16T10:49:30Z","published":"2025-07-16T10:49:30Z","title":"Iterative Augmentation with Summarization Refinement (IASR) Evaluation\n  for Unstructured Survey data Modeling and Analysis","summary":"  Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.\n","authors":["Payal Bhattad","Sai Manoj Pudukotai Dinakarrao","Anju Gupta"],"pdf_url":"https://arxiv.org/pdf/2507.12126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12272v4","updated":"2025-07-16T10:34:26Z","published":"2025-02-17T19:16:37Z","title":"Learning to Reason at the Frontier of Learnability","summary":"  Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.\n","authors":["Thomas Foster","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2502.12272v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12079v1","updated":"2025-07-16T09:39:56Z","published":"2025-07-16T09:39:56Z","title":"Findings of MEGA: Maths Explanation with LLMs using the Socratic Method\n  for Active Learning","summary":"  This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems.\n","authors":["Tosin Adewumi","Foteini Simistira Liwicki","Marcus Liwicki","Viktor Gardelli","Lama Alkhaled","Hamam Mokayed"],"pdf_url":"https://arxiv.org/pdf/2507.12079v1.pdf","comment":"This paper was accepted for the special issue AI for Education by the\n  IEEE Signal Processing Magazine journal"},{"id":"http://arxiv.org/abs/2403.09040v3","updated":"2025-07-16T09:39:02Z","published":"2024-03-14T02:26:31Z","title":"RAGGED: Towards Informed Design of Scalable and Stable RAG Systems","summary":"  Retrieval-augmented generation (RAG) enhances language models by integrating\nexternal knowledge, but its effectiveness is highly dependent on system\nconfiguration. Improper retrieval settings can degrade performance, making RAG\nless reliable than closed-book generation. In this work, we introduce RAGGED, a\nframework for systematically evaluating RAG systems across diverse\nretriever-reader configurations, retrieval depths, and datasets. Our analysis\nreveals that reader robustness to noise is the key determinant of RAG stability\nand scalability. Some readers benefit from increased retrieval depth, while\nothers degrade due to their sensitivity to distracting content. Through\nlarge-scale experiments on open-domain, multi-hop, and specialized-domain\ndatasets, we show that retrievers, rerankers, and prompts influence performance\nbut do not fundamentally alter these reader-driven trends. By providing a\nprincipled framework and new metrics to assess RAG stability and scalability,\nRAGGED enables systematic evaluation of retrieval-augmented generation systems,\nguiding future research on optimizing retrieval depth and model robustness.\n","authors":["Jennifer Hsia","Afreen Shaikh","Zhiruo Wang","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2403.09040v3.pdf","comment":"Project page: https://github.com/neulab/ragged"},{"id":"http://arxiv.org/abs/2507.12075v1","updated":"2025-07-16T09:35:38Z","published":"2025-07-16T09:35:38Z","title":"BOOKCOREF: Coreference Resolution at Book Scale","summary":"  Coreference Resolution systems are typically evaluated on benchmarks\ncontaining small- to medium-scale documents. When it comes to evaluating long\ntexts, however, existing benchmarks, such as LitBank, remain limited in length\nand do not adequately assess system capabilities at the book scale, i.e., when\nco-referring mentions span hundreds of thousands of tokens. To fill this gap,\nwe first put forward a novel automatic pipeline that produces high-quality\nCoreference Resolution annotations on full narrative texts. Then, we adopt this\npipeline to create the first book-scale coreference benchmark, BOOKCOREF, with\nan average document length of more than 200,000 tokens. We carry out a series\nof experiments showing the robustness of our automatic procedure and\ndemonstrating the value of our resource, which enables current long-document\ncoreference systems to gain up to +20 CoNLL-F1 points when evaluated on full\nbooks. Moreover, we report on the new challenges introduced by this\nunprecedented book-scale setting, highlighting that current models fail to\ndeliver the same performance they achieve on smaller documents. We release our\ndata and code to encourage research and development of new book-scale\nCoreference Resolution systems at https://github.com/sapienzanlp/bookcoref.\n","authors":["Giuliano Martinelli","Tommaso Bonomo","Pere-Lluís Huguet Cabot","Roberto Navigli"],"pdf_url":"https://arxiv.org/pdf/2507.12075v1.pdf","comment":"Accepted to ACL 2025 Main Conference. 19 pages"},{"id":"http://arxiv.org/abs/2507.12064v1","updated":"2025-07-16T09:21:20Z","published":"2025-07-16T09:21:20Z","title":"StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric\n  Features","summary":"  This submission to the binary AI detection task is based on a modular\nstylometric pipeline, where: public spaCy models are used for text\npreprocessing (including tokenisation, named entity recognition, dependency\nparsing, part-of-speech tagging, and morphology annotation) and extracting\nseveral thousand features (frequencies of n-grams of the above linguistic\nannotations); light-gradient boosting machines are used as the classifier. We\ncollect a large corpus of more than 500 000 machine-generated texts for the\nclassifier's training. We explore several parameter options to increase the\nclassifier's capacity and take advantage of that training set. Our approach\nfollows the non-neural, computationally inexpensive but explainable approach\nfound effective previously.\n","authors":["Jeremi K. Ochab","Mateusz Matias","Tymoteusz Boba","Tomasz Walkowiak"],"pdf_url":"https://arxiv.org/pdf/2507.12064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12059v1","updated":"2025-07-16T09:16:36Z","published":"2025-07-16T09:16:36Z","title":"Evaluating the Ability of Large Language Models to Reason about Cardinal\n  Directions, Revisited","summary":"  We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24.\n","authors":["Anthony G Cohn","Robert E Blackwell"],"pdf_url":"https://arxiv.org/pdf/2507.12059v1.pdf","comment":"8 pages, 5 figures. Accepted at QR 2025 : 38th International Workshop\n  on Qualitative Reasoning at IJCAI"},{"id":"http://arxiv.org/abs/2507.12039v1","updated":"2025-07-16T08:56:19Z","published":"2025-07-16T08:56:19Z","title":"A Comparative Approach to Assessing Linguistic Creativity of Large\n  Language Models and Humans","summary":"  The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.\n","authors":["Anca Dinu","Andra-Maria Florescu","Alina Resceanu"],"pdf_url":"https://arxiv.org/pdf/2507.12039v1.pdf","comment":"Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)"},{"id":"http://arxiv.org/abs/2503.08506v3","updated":"2025-07-16T08:29:02Z","published":"2025-03-11T14:56:58Z","title":"ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper\n  Reviews","summary":"  Academic paper review is a critical yet time-consuming task within the\nresearch community. With the increasing volume of academic publications,\nautomating the review process has become a significant challenge. The primary\nissue lies in generating comprehensive, accurate, and reasoning-consistent\nreview comments that align with human reviewers' judgments. In this paper, we\naddress this challenge by proposing ReviewAgents, a framework that leverages\nlarge language models (LLMs) to generate academic paper reviews. We first\nintroduce a novel dataset, Review-CoT, consisting of 142k review comments,\ndesigned for training LLM agents. This dataset emulates the structured\nreasoning process of human reviewers-summarizing the paper, referencing\nrelevant works, identifying strengths and weaknesses, and generating a review\nconclusion. Building upon this, we train LLM reviewer agents capable of\nstructured reasoning using a relevant-paper-aware training method. Furthermore,\nwe construct ReviewAgents, a multi-role, multi-LLM agent review framework, to\nenhance the review comment generation process. Additionally, we propose\nReviewBench, a benchmark for evaluating the review comments generated by LLMs.\nOur experimental results on ReviewBench demonstrate that while existing LLMs\nexhibit a certain degree of potential for automating the review process, there\nremains a gap when compared to human-generated reviews. Moreover, our\nReviewAgents framework further narrows this gap, outperforming advanced LLMs in\ngenerating review comments.\n","authors":["Xian Gao","Jiacheng Ruan","Zongyun Zhang","Jingsheng Gao","Ting Liu","Yuzhuo Fu"],"pdf_url":"https://arxiv.org/pdf/2503.08506v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2507.12004v1","updated":"2025-07-16T07:58:20Z","published":"2025-07-16T07:58:20Z","title":"Improving Data and Parameter Efficiency of Neural Language Models Using\n  Representation Analysis","summary":"  This thesis addresses challenges related to data and parameter efficiency in\nneural language models, with a focus on representation analysis and the\nintroduction of new optimization techniques. The first part examines the\nproperties and dynamics of language representations within neural models,\nemphasizing their significance in enhancing robustness and generalization. It\nproposes innovative approaches based on representation smoothness, including\nregularization strategies that utilize Jacobian and Hessian matrices to\nstabilize training and mitigate sensitivity to input perturbations. The second\npart focuses on methods to significantly enhance data and parameter efficiency\nby integrating active learning strategies with parameter-efficient fine-tuning,\nguided by insights from representation smoothness analysis. It presents\nsmoothness-informed early-stopping techniques designed to eliminate the need\nfor labeled validation sets and proposes innovative combinations of active\nlearning and parameter-efficient fine-tuning to reduce labeling efforts and\ncomputational resources. Extensive experimental evaluations across various NLP\ntasks demonstrate that these combined approaches substantially outperform\ntraditional methods in terms of performance, stability, and efficiency. The\nthird part explores weak supervision techniques enhanced by in-context learning\nto effectively utilize unlabeled data, further reducing dependence on extensive\nlabeling. It shows that using in-context learning as a mechanism for weak\nsupervision enables models to better generalize from limited labeled data by\nleveraging unlabeled examples more effectively during training. Comprehensive\nempirical evaluations confirm significant gains in model accuracy,\nadaptability, and robustness, especially in low-resource settings and dynamic\ndata environments.\n","authors":["Josip Jukić"],"pdf_url":"https://arxiv.org/pdf/2507.12004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00691v2","updated":"2025-07-16T07:55:10Z","published":"2025-01-01T01:06:58Z","title":"Labels Generated by Large Language Models Help Measure People's Empathy\n  in Vitro","summary":"  Large language models (LLMs) have revolutionised many fields, with\nLLM-as-a-service (LLMSaaS) offering accessible, general-purpose solutions\nwithout costly task-specific training. In contrast to the widely studied prompt\nengineering for directly solving tasks (in vivo), this paper explores LLMs'\npotential for in-vitro applications: using LLM-generated labels to improve\nsupervised training of mainstream models. We examine two strategies - (1) noisy\nlabel correction and (2) training data augmentation - in empathy computing, an\nemerging task to predict psychology-based questionnaire outcomes from inputs\nlike textual narratives. Crowdsourced datasets in this domain often suffer from\nnoisy labels that misrepresent underlying empathy. We show that replacing or\nsupplementing these crowdsourced labels with LLM-generated labels, developed\nusing psychology-based scale-aware prompts, achieves statistically significant\naccuracy improvements. Notably, the RoBERTa pre-trained language model (PLM)\ntrained with noise-reduced labels yields a state-of-the-art Pearson correlation\ncoefficient of 0.648 on the public NewsEmp benchmarks. This paper further\nanalyses evaluation metric selection and demographic biases to help guide the\nfuture development of more equitable empathy computing models. Code and\nLLM-generated labels are available at\nhttps://github.com/hasan-rakibul/LLMPathy.\n","authors":["Md Rakibul Hasan","Yue Yao","Md Zakir Hossain","Aneesh Krishna","Imre Rudas","Shafin Rahman","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2501.00691v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2502.11078v2","updated":"2025-07-16T07:26:24Z","published":"2025-02-16T11:02:37Z","title":"DEEPER Insight into Your User: Directed Persona Refinement for Dynamic\n  Persona Modeling","summary":"  To advance personalized applications such as recommendation systems and user\nbehavior prediction, recent research increasingly adopts large language models\n(LLMs) for human -readable persona modeling. In dynamic real -world scenarios,\neffective persona modeling necessitates leveraging streaming behavior data to\ncontinually optimize user personas. However, existing methods -whether\nregenerating personas or incrementally extending them with new behaviors -often\nfail to achieve sustained improvements in persona quality or future behavior\nprediction accuracy. To address this, we propose DEEPER, a novel approach for\ndynamic persona modeling that enables continual persona optimization.\nSpecifically, we enhance the model's direction -search capability through an\niterative reinforcement learning framework, allowing it to automatically\nidentify effective update directions and optimize personas using discrepancies\nbetween user behaviors and model predictions. Extensive experiments on dynamic\npersona modeling involving 4800 users across 10 domains highlight the superior\npersona optimization capabilities of DEEPER, delivering an impressive 32.2%\naverage reduction in user behavior prediction error over four update rounds\n-outperforming the best baseline by a remarkable 22.92%.\n","authors":["Aili Chen","Chengyu Du","Jiangjie Chen","Jinghan Xu","Yikai Zhang","Siyu Yuan","Zulong Chen","Liangyue Li","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.11078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11981v1","updated":"2025-07-16T07:25:27Z","published":"2025-07-16T07:25:27Z","title":"Simplifications are Absolutists: How Simplified Language Reduces Word\n  Sense Awareness in LLM-Generated Definitions","summary":"  Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners.\n","authors":["Lukas Ellinger","Miriam Anschütz","Georg Groh"],"pdf_url":"https://arxiv.org/pdf/2507.11981v1.pdf","comment":"Accepted by RANLP 2025"},{"id":"http://arxiv.org/abs/2507.11979v1","updated":"2025-07-16T07:21:59Z","published":"2025-07-16T07:21:59Z","title":"Value-Based Large Language Model Agent Simulation for Mutual Evaluation\n  of Trust and Interpersonal Closeness","summary":"  Large language models (LLMs) have emerged as powerful tools for simulating\ncomplex social phenomena using human-like agents with specific traits. In human\nsocieties, value similarity is important for building trust and close\nrelationships; however, it remains unexplored whether this principle holds true\nin artificial societies comprising LLM agents. Therefore, this study\ninvestigates the influence of value similarity on relationship-building among\nLLM agents through two experiments. First, in a preliminary experiment, we\nevaluated the controllability of values in LLMs to identify the most effective\nmodel and prompt design for controlling the values. Subsequently, in the main\nexperiment, we generated pairs of LLM agents imbued with specific values and\nanalyzed their mutual evaluations of trust and interpersonal closeness\nfollowing a dialogue. The experiments were conducted in English and Japanese to\ninvestigate language dependence. The results confirmed that pairs of agents\nwith higher value similarity exhibited greater mutual trust and interpersonal\ncloseness. Our findings demonstrate that the LLM agent simulation serves as a\nvalid testbed for social science theories, contributes to elucidating the\nmechanisms by which values influence relationship building, and provides a\nfoundation for inspiring new theories and insights into the social sciences.\n","authors":["Yuki Sakamoto","Takahisa Uchida","Hiroshi Ishiguro"],"pdf_url":"https://arxiv.org/pdf/2507.11979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11972v1","updated":"2025-07-16T07:15:59Z","published":"2025-07-16T07:15:59Z","title":"Graph Representations for Reading Comprehension Analysis using Large\n  Language Model and Eye-Tracking Biomarker","summary":"  Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies.\n","authors":["Yuhong Zhang","Jialu Li","Shilai Yang","Yuchen Xu","Gert Cauwenberghs","Tzyy-Ping Jung"],"pdf_url":"https://arxiv.org/pdf/2507.11972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10341v3","updated":"2025-07-16T07:09:29Z","published":"2025-02-14T18:02:37Z","title":"Organize the Web: Constructing Domains Enhances Pre-Training Data\n  Curation","summary":"  Modern language models are trained on large, unstructured datasets consisting\nof trillions of tokens and obtained by crawling the web. The unstructured\nnature makes it difficult to reason about their contents and develop systematic\napproaches to data curation. In this paper, we unpack monolithic web corpora by\ndeveloping taxonomies of their contents and organizing them into domains. We\nintroduce WebOrganizer, a framework for organizing web pages in terms of both\ntheir topic and format. Using these two complementary notions of domains, we\nautomatically annotate pre-training data by distilling annotations from a large\nlanguage model into efficient classifiers. This allows us to study how data\nfrom different domains should be mixed to improve models on downstream tasks,\nand we show that we can combine insights about effective topics and formats to\nfurther boost performance. We demonstrate that our domain mixing also improves\nexisting methods that select data based on quality. Furthermore, we study and\ncompare how quality-based methods will implicitly change the domain mixture.\nOverall, our work demonstrates that constructing and mixing domains provides a\nvaluable complement to quality-based data curation methods, opening new avenues\nfor effective and insightful pre-training data curation.\n","authors":["Alexander Wettig","Kyle Lo","Sewon Min","Hannaneh Hajishirzi","Danqi Chen","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2502.10341v3.pdf","comment":"Accepted at ICML 2025. Project page: https://weborganizer.allen.ai"},{"id":"http://arxiv.org/abs/2507.06210v2","updated":"2025-07-16T07:01:50Z","published":"2025-07-08T17:38:56Z","title":"CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic\n  Images and Contextualized Captions","summary":"  Pretrained vision-language models (VLMs) such as CLIP excel in general\nmultimodal comprehension but often struggle to capture nuanced,\ncontext-dependent visual cues. This makes it difficult to distinguish between\nsimilar-looking concepts with potentially different cultural meanings. Such\ndeficiencies are mainly due to a limited amount of high-quality cultural data,\ncontextual information, and the lack of negative examples that highlight subtle\ndifferences. To mitigate this, we design a data curation pipeline leveraging\nopen-sourced VLMs and text-to-image models to construct CulTwin, a synthetic\ncultural dataset. This dataset consists of paired concept-caption-image\ntriplets, where concepts visually resemble each other but are culturally\ndifferent. Then, we fine-tune CLIP on CulTwin to develop CultureCLIP, which\naligns cultural concepts with contextually enhanced captions and synthetic\nimages through tailored contrastive learning. Experiments on culture-specific\nbenchmarks show that CultureCLIP outperforms the base CLIP, achieving up to a\nnotable 5.49% improvement in fine-grained concept recognition on certain tasks\nwhile preserving CLIP's original generalization ability, validating the\neffectiveness of our data synthesis and VLM backbone training paradigm in\ncapturing subtle cultural distinctions.\n","authors":["Yuchen Huang","Zhiyuan Fan","Zhitao He","Sandeep Polisetty","Wenyan Li","Yi R. Fung"],"pdf_url":"https://arxiv.org/pdf/2507.06210v2.pdf","comment":"25 pages, COLM 2025"},{"id":"http://arxiv.org/abs/2507.06607v2","updated":"2025-07-16T07:00:01Z","published":"2025-07-09T07:27:00Z","title":"Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long\n  Generation","summary":"  Recent advances in language modeling have demonstrated the effectiveness of\nState Space Models (SSMs) for efficient sequence modeling. While hybrid\narchitectures such as Samba and the decoder-decoder architecture, YOCO, have\nshown promising performance gains over Transformers, prior works have not\ninvestigated the efficiency potential of representation sharing between SSM\nlayers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet\neffective mechanism for efficient memory sharing across layers. We apply it to\ncreate SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in\nthe cross-decoder to share memory readout states from a Samba-based\nself-decoder. SambaY significantly enhances decoding efficiency, preserves\nlinear pre-filling time complexity, and boosts long-context performance, all\nwhile eliminating the need for explicit positional encoding. Through extensive\nscaling experiments, we demonstrate that our model exhibits a significantly\nlower irreducible loss compared to a strong YOCO baseline, indicating superior\nperformance scalability under large-scale compute regimes. Our largest model\nenhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves\nsignificantly better performance than Phi4-mini-Reasoning on reasoning tasks\nsuch as Math500, AIME24/25, and GPQA Diamond without any reinforcement\nlearning, while delivering up to 10x higher decoding throughput on 2K-length\nprompts with 32K generation length under the vLLM inference framework. We\nrelease our training codebase on open-source data at\nhttps://github.com/microsoft/ArchScale.\n","authors":["Liliang Ren","Congcong Chen","Haoran Xu","Young Jin Kim","Adam Atkinson","Zheng Zhan","Jiankai Sun","Baolin Peng","Liyuan Liu","Shuohang Wang","Hao Cheng","Jianfeng Gao","Weizhu Chen","Yelong Shen"],"pdf_url":"https://arxiv.org/pdf/2507.06607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11966v1","updated":"2025-07-16T06:58:02Z","published":"2025-07-16T06:58:02Z","title":"Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation","summary":"  As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance.\n","authors":["Ziyu Ge","Gabriel Chua","Leanne Tan","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2507.11966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02445v5","updated":"2025-07-16T06:56:08Z","published":"2025-03-04T09:40:00Z","title":"BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modeling","summary":"  Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by up to 12% on MSE and 6% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data.\n","authors":["Hao Li","Yu-Hao Huang","Chang Xu","Viktor Schlegel","Renhe Jiang","Riza Batista-Navarro","Goran Nenadic","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2503.02445v5.pdf","comment":"ICML 2025 Main Conference"},{"id":"http://arxiv.org/abs/2503.22913v2","updated":"2025-07-16T06:54:34Z","published":"2025-03-28T23:43:33Z","title":"Resona: Improving Context Copying in Linear Recurrence Models with\n  Retrieval","summary":"  Recent shifts in the space of large language model (LLM) research have shown\nan increasing focus on novel architectures to compete with prototypical\nTransformer-based models that have long dominated this space. Linear recurrent\nmodels have proven to be a viable competitor due to their computational\nefficiency. However, such models still demonstrate a sizable gap compared to\nTransformers in terms of in-context learning among other tasks that require\nrecalling information from a context. In this work, we introduce Resona, a\nsimple and scalable framework for augmenting linear recurrent models with\nretrieval. Resona augments models with the ability to integrate retrieved\ninformation from the provided input context, enabling tailored behavior to\ndiverse task requirements. Experiments on a variety of linear recurrent models\ndemonstrate that Resona-augmented models observe significant performance gains\non a variety of synthetic as well as real-world natural language tasks,\nhighlighting its ability to act as a general purpose method to improve the\nin-context learning and language modeling abilities of linear recurrent LLMs.\n","authors":["Xinyu Wang","Linrui Ma","Jerry Huang","Peng Lu","Prasanna Parthasarathi","Xiao-Wen Chang","Boxing Chen","Yufei Cui"],"pdf_url":"https://arxiv.org/pdf/2503.22913v2.pdf","comment":"Comments: Accepted at COLM 2025 (Conference on Learning with\n  Machines)"},{"id":"http://arxiv.org/abs/2507.11959v1","updated":"2025-07-16T06:44:14Z","published":"2025-07-16T06:44:14Z","title":"PoTPTQ: A Two-step Power-of-Two Post-training for LLMs","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.\n","authors":["Xinyu Wang","Vahid Partovi Nia","Peng Lu","Jerry Huang","Xiao-Wen Chang","Boxing Chen","Yufei Cui"],"pdf_url":"https://arxiv.org/pdf/2507.11959v1.pdf","comment":"Accepted at ECAI 2025 (European Conference on Artificial\n  Intelligence)"},{"id":"http://arxiv.org/abs/2507.11954v1","updated":"2025-07-16T06:41:03Z","published":"2025-07-16T06:41:03Z","title":"The benefits of query-based KGQA systems for complex and temporal\n  questions in LLM era","summary":"  Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System\n","authors":["Artem Alekseev","Mikhail Chaichuk","Miron Butko","Alexander Panchenko","Elena Tutubalina","Oleg Somov"],"pdf_url":"https://arxiv.org/pdf/2507.11954v1.pdf","comment":"15 pages, 3 figures, 7 tables"},{"id":"http://arxiv.org/abs/2507.11953v1","updated":"2025-07-16T06:39:11Z","published":"2025-07-16T06:39:11Z","title":"IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs","summary":"  LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.\n","authors":["Yi Zhao","Zuchao Li","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.11953v1.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2507.11942v1","updated":"2025-07-16T06:16:06Z","published":"2025-07-16T06:16:06Z","title":"DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt\n  Compression","summary":"  Task-agnostic prompt compression leverages the redundancy in natural language\nto reduce computational overhead and enhance information density within\nprompts, especially in long-context scenarios. Existing methods predominantly\nrely on information entropy as the metric to compress lexical units, aiming to\nachieve minimal information loss. However, these approaches overlook two\ncritical aspects: (i) the importance of attention-critical tokens at the\nalgorithmic level, and (ii) shifts in information entropy during the\ncompression process. Motivated by these challenges, we propose a dynamic\nattention-aware approach for task-agnostic prompt compression (DAC). This\napproach effectively integrates entropy and attention information, dynamically\nsensing entropy shifts during compression to achieve fine-grained prompt\ncompression. Extensive experiments across various domains, including LongBench,\nGSM8K, and BBH, show that DAC consistently yields robust and substantial\nimprovements across a diverse range of tasks and LLMs, offering compelling\nevidence of its efficacy.\n","authors":["Yi Zhao","Zuchao Li","Hai Zhao","Baoyuan Qi","Guoming Liu"],"pdf_url":"https://arxiv.org/pdf/2507.11942v1.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2507.11941v1","updated":"2025-07-16T06:12:41Z","published":"2025-07-16T06:12:41Z","title":"BlockBPE: Parallel BPE Tokenization","summary":"  Tokenization is a critical preprocessing step in large language model\npipelines, yet widely-used implementations remain CPU-bound and suboptimal for\nbatch inference workflows on GPU. We present BlockBPE, a parallel GPU\nimplementation of byte-pair encoding (BPE) that achieves near linear-time\ncomplexity under realistic assumptions and is optimized for high-throughput,\nbatch inference. Unlike existing Rust-based tokenizers such as HuggingFace\nTokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex\npre-tokenization and exhibit $O(n \\log n)$ runtime-BlockBPE eliminates the\nRegex pre-tokenization which leads to small loss in generation quality, but\nenables highly parallelized token merges within thread blocks, reducing overall\ncomplexity to $O(nd)$ where $d \\ll n$. On high-batch inference workloads,\nBlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over\nHuggingFace Tokenizers.\n","authors":["Amos You"],"pdf_url":"https://arxiv.org/pdf/2507.11941v1.pdf","comment":"ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models\n  (ICML 2025)"},{"id":"http://arxiv.org/abs/2507.11939v1","updated":"2025-07-16T06:09:02Z","published":"2025-07-16T06:09:02Z","title":"POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual\n  Chart Question Answering","summary":"  Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.\n","authors":["Yichen Xu","Liangyu Chen","Liang Zhang","Wenxuan Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2507.11939v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2507.11936v1","updated":"2025-07-16T06:03:08Z","published":"2025-07-16T06:03:08Z","title":"A Survey of Deep Learning for Geometry Problem Solving","summary":"  Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.\n","authors":["Jianzhe Ma","Wenxuan Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2507.11936v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2501.00226v2","updated":"2025-07-16T04:59:12Z","published":"2024-12-31T02:23:10Z","title":"Generative Emergent Communication: Large Language Model is a Collective\n  World Model","summary":"  Large Language Models (LLMs) have demonstrated a remarkable ability to\ncapture extensive world knowledge, yet how this is achieved without direct\nsensorimotor experience remains a fundamental puzzle. This study proposes a\nnovel theoretical solution by introducing the Collective World Model\nhypothesis. We argue that an LLM does not learn a world model from scratch;\ninstead, it learns a statistical approximation of a collective world model that\nis already implicitly encoded in human language through a society-wide process\nof embodied, interactive sense-making. To formalize this process, we introduce\ngenerative emergent communication (Generative EmCom), a framework built on the\nCollective Predictive Coding (CPC). This framework models the emergence of\nlanguage as a process of decentralized Bayesian inference over the internal\nstates of multiple agents. We argue that this process effectively creates an\nencoder-decoder structure at a societal scale: human society collectively\nencodes its grounded, internal representations into language, and an LLM\nsubsequently decodes these symbols to reconstruct a latent space that mirrors\nthe structure of the original collective representations. This perspective\nprovides a principled, mathematical explanation for how LLMs acquire their\ncapabilities. The main contributions of this paper are: 1) the formalization of\nthe Generative EmCom framework, clarifying its connection to world models and\nmulti-agent reinforcement learning, and 2) its application to interpret LLMs,\nexplaining phenomena such as distributional semantics as a natural consequence\nof representation reconstruction. This work provides a unified theory that\nbridges individual cognitive development, collective language evolution, and\nthe foundations of large-scale AI.\n","authors":["Tadahiro Taniguchi","Ryo Ueda","Tomoaki Nakamura","Masahiro Suzuki","Akira Taniguchi"],"pdf_url":"https://arxiv.org/pdf/2501.00226v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13959v3","updated":"2025-07-16T04:56:27Z","published":"2025-01-21T06:32:25Z","title":"Learning an Effective Premise Retrieval Model for Efficient Mathematical\n  Formalization","summary":"  Formalized mathematics has recently garnered significant attention for its\nability to assist mathematicians across various fields. Premise retrieval, as a\ncommon step in mathematical formalization, has been a challenge, particularly\nfor inexperienced users. Existing retrieval methods that facilitate natural\nlanguage queries require a certain level of mathematical expertise from users,\nwhile approaches based on formal languages (e.g., Lean) typically struggle with\nthe scarcity of training data, hindering the training of effective and\ngeneralizable retrieval models. In this work, we introduce a novel method that\nleverages data extracted from Mathlib to train a lightweight and effective\npremise retrieval model. In particular, the proposed model embeds queries\n(i.e., proof state provided by Lean) and premises in a latent space, featuring\na tokenizer specifically trained on formal corpora. The model is learned in a\ncontrastive learning framework, in which a fine-grained similarity calculation\nmethod and a re-ranking module are applied to enhance the retrieval\nperformance. Experimental results demonstrate that our model outperforms\nexisting baselines, achieving higher accuracy while maintaining a lower\ncomputational load. We have released an open-source search engine based on our\nretrieval model at https://premise-search.com/. The source code and the trained\nmodel can be found at https://github.com/ruc-ai4math/Premise-Retrieval.\n","authors":["Yicheng Tao","Haotian Liu","Shanwen Wang","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2501.13959v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11049v2","updated":"2025-07-16T03:58:24Z","published":"2025-07-15T07:22:04Z","title":"Journalism-Guided Agentic In-Context Learning for News Stance Detection","summary":"  As online news consumption grows, personalized recommendation systems have\nbecome integral to digital journalism. However, these systems risk reinforcing\nfilter bubbles and political polarization by failing to incorporate diverse\nperspectives. Stance detection -- identifying a text's position on a target --\ncan help mitigate this by enabling viewpoint-aware recommendations and\ndata-driven analyses of media bias. Yet, existing stance detection research\nremains largely limited to short texts and high-resource languages. To address\nthese gaps, we introduce \\textsc{K-News-Stance}, the first Korean dataset for\narticle-level stance detection, comprising 2,000 news articles with\narticle-level and 19,650 segment-level stance annotations across 47 societal\nissues. We also propose \\textsc{JoA-ICL}, a \\textbf{Jo}urnalism-guided\n\\textbf{A}gentic \\textbf{I}n-\\textbf{C}ontext \\textbf{L}earning framework that\nemploys a language model agent to predict the stances of key structural\nsegments (e.g., leads, quotes), which are then aggregated to infer the overall\narticle stance. Experiments show that \\textsc{JoA-ICL} outperforms existing\nstance detection methods, highlighting the benefits of segment-level agency in\ncapturing the overall position of long-form news articles. Two case studies\nfurther demonstrate its broader utility in promoting viewpoint diversity in\nnews recommendations and uncovering patterns of media bias.\n","authors":["Dahyun Lee","Jonghyeon Choi","Jiyoung Han","Kunwoo Park"],"pdf_url":"https://arxiv.org/pdf/2507.11049v2.pdf","comment":"Preprint. 24 pages"},{"id":"http://arxiv.org/abs/2507.11882v1","updated":"2025-07-16T03:49:41Z","published":"2025-07-16T03:49:41Z","title":"Marco-Bench-MIF: On Multilingual Instruction-Following Capability of\n  Large Language Models","summary":"  Instruction-following capability has become a major ability to be evaluated\nfor Large Language Models (LLMs). However, existing datasets, such as IFEval,\nare either predominantly monolingual and centered on English or simply machine\ntranslated to other languages, limiting their applicability in multilingual\ncontexts. In this paper, we present an carefully-curated extension of IFEval to\na localized multilingual version named Marco-Bench-MIF, covering 30 languages\nwith varying levels of localization. Our benchmark addresses linguistic\nconstraints (e.g., modifying capitalization requirements for Chinese) and\ncultural references (e.g., substituting region-specific company names in\nprompts) via a hybrid pipeline combining translation with verification. Through\ncomprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)\n25-35% accuracy gap between high/low-resource languages, (2) model scales\nlargely impact performance by 45-60% yet persists script-specific challenges,\nand (3) machine-translated data underestimates accuracy by7-22% versus\nlocalized data. Our analysis identifies challenges in multilingual instruction\nfollowing, including keyword consistency preservation and compositional\nconstraint adherence across languages. Our Marco-Bench-MIF is available at\nhttps://github.com/AIDC-AI/Marco-Bench-MIF.\n","authors":["Bo Zeng","Chenyang Lyu","Sinuo Liu","Mingyan Zeng","Minghao Wu","Xuanfan Ni","Tianqi Shi","Yu Zhao","Yefeng Liu","Chenyu Zhu","Ruizhe Li","Jiahui Geng","Qing Li","Yu Tong","Longyue Wang","Weihua Luo","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.11882v1.pdf","comment":"ACL 2025 Main Conference paper"},{"id":"http://arxiv.org/abs/2507.11878v1","updated":"2025-07-16T03:48:03Z","published":"2025-07-16T03:48:03Z","title":"LLMs Encode Harmfulness and Refusal Separately","summary":"  LLMs are trained to refuse harmful instructions, but do they truly understand\nharmfulness beyond just refusing? Prior work has shown that LLMs' refusal\nbehaviors can be mediated by a one-dimensional subspace, i.e., a refusal\ndirection. In this work, we identify a new dimension to analyze safety\nmechanisms in LLMs, i.e., harmfulness, which is encoded internally as a\nseparate concept from refusal. There exists a harmfulness direction that is\ndistinct from the refusal direction. As causal evidence, steering along the\nharmfulness direction can lead LLMs to interpret harmless instructions as\nharmful, but steering along the refusal direction tends to elicit refusal\nresponses directly without reversing the model's judgment on harmfulness.\nFurthermore, using our identified harmfulness concept, we find that certain\njailbreak methods work by reducing the refusal signals without reversing the\nmodel's internal belief of harmfulness. We also find that adversarially\nfinetuning models to accept harmful instructions has minimal impact on the\nmodel's internal belief of harmfulness. These insights lead to a practical\nsafety application: The model's latent harmfulness representation can serve as\nan intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing\nover-refusals that is robust to finetuning attacks. For instance, our Latent\nGuard achieves performance comparable to or better than Llama Guard 3 8B, a\ndedicated finetuned safeguard model, across different jailbreak methods. Our\nfindings suggest that LLMs' internal understanding of harmfulness is more\nrobust than their refusal decision to diverse input instructions, offering a\nnew perspective to study AI safety\n","authors":["Jiachen Zhao","Jing Huang","Zhengxuan Wu","David Bau","Weiyan Shi"],"pdf_url":"https://arxiv.org/pdf/2507.11878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11875v1","updated":"2025-07-16T03:39:36Z","published":"2025-07-16T03:39:36Z","title":"DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests\n  Distractor Generation","summary":"  This paper introduces DualReward, a novel reinforcement learning framework\nfor automatic distractor generation in cloze tests. Unlike conventional\napproaches that rely primarily on supervised learning or static generative\nmodels, our method employs a dual reward structure with adaptive scaling that\ndifferentiates between human-created gold standard distractors and\nmodel-generated candidates. The framework dynamically adjusts reward signal\nintensity based on model performance and confidence. We evaluate our approach\non both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,\ndemonstrating consistent improvements over state-of-the-art baselines.\nExperimental results show that our adaptive reward scaling mechanism provides\nmodest but consistent benefits on homogeneous datasets (CLOTH-F) and more\nsubstantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data\n(MCQ), suggesting its particular effectiveness for handling varied question\ntypes and domains. Our work offers a flexible framework that effectively\nbalances learning from reliable human examples while exploring novel,\nhigh-quality distractors for automated test generation.\n","authors":["Tianyou Huang","Xinglu Chen","Jingshen Zhang","Xinying Qiu","Ruiying Niu"],"pdf_url":"https://arxiv.org/pdf/2507.11875v1.pdf","comment":"Accepted to CCL 2025"},{"id":"http://arxiv.org/abs/2507.11867v1","updated":"2025-07-16T03:29:05Z","published":"2025-07-16T03:29:05Z","title":"COLA-GEC: A Bidirectional Framework for Enhancing Grammatical\n  Acceptability and Error Correction","summary":"  Grammatical Error Correction (GEC) and grammatical acceptability judgment\n(COLA) are core tasks in natural language processing, sharing foundational\ngrammatical knowledge yet typically evolving independently. This paper\nintroduces COLA-GEC, a novel bidirectional framework that enhances both tasks\nthrough mutual knowledge transfer. First, we augment grammatical acceptability\nmodels using GEC datasets, significantly improving their performance across\nmultiple languages. Second, we integrate grammatical acceptability signals into\nGEC model training via a dynamic loss function, effectively guiding corrections\ntoward grammatically acceptable outputs. Our approach achieves state-of-the-art\nresults on several multilingual benchmarks. Comprehensive error analysis\nhighlights remaining challenges, particularly in punctuation error correction,\nproviding insights for future improvements in grammatical modeling.\n","authors":["Xiangyu Yang","Xinying Qiu"],"pdf_url":"https://arxiv.org/pdf/2507.11867v1.pdf","comment":"Accepted to CLNLP 2025"},{"id":"http://arxiv.org/abs/2507.11862v1","updated":"2025-07-16T03:14:36Z","published":"2025-07-16T03:14:36Z","title":"Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable\n  Information Recognition","summary":"  Accurate recognition of personally identifiable information (PII) is central\nto automated text anonymization. This paper investigates the effectiveness of\ncross-domain model transfer, multi-domain data fusion, and sample-efficient\nlearning for PII recognition. Using annotated corpora from healthcare (I2B2),\nlegal (TAB), and biography (Wikipedia), we evaluate models across four\ndimensions: in-domain performance, cross-domain transferability, fusion, and\nfew-shot learning. Results show legal-domain data transfers well to\nbiographical texts, while medical domains resist incoming transfer. Fusion\nbenefits are domain-specific, and high-quality recognition is achievable with\nonly 10% of training data in low-specialization domains.\n","authors":["Junhong Ye","Xu Yuan","Xinying Qiu"],"pdf_url":"https://arxiv.org/pdf/2507.11862v1.pdf","comment":"Accepted to CLNLP 2025"},{"id":"http://arxiv.org/abs/2412.10543v2","updated":"2025-07-16T03:02:57Z","published":"2024-12-13T20:39:30Z","title":"METIS: Fast Quality-Aware RAG Systems with Configuration Adaptation","summary":"  RAG (Retrieval Augmented Generation) allows LLMs (large language models) to\ngenerate better responses with external knowledge, but using more external\nknowledge often improves generation quality at the expense of response delay.\nPrior work either reduces the response delay (through better scheduling of RAG\nqueries) or strives to maximize quality (which involves tuning the RAG\nworkflow), but they fall short in optimizing the tradeoff between the delay and\nquality of RAG responses. This paper presents METIS, the first RAG system that\njointly schedules queries and adapts the key RAG configurations of each query,\nsuch as the number of retrieved text chunks and synthesis methods, in order to\nbalance quality optimization and response delay reduction. Using 4 popular\nRAG-QA datasets, we show that compared with the state-of-the-art RAG\noptimization schemes, METIS reduces the generation latency by $1.64-2.54\\times$\nwithout sacrificing generation quality.\n","authors":["Siddhant Ray","Rui Pan","Zhuohan Gu","Kuntai Du","Shaoting Feng","Ganesh Ananthanarayanan","Ravi Netravali","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.10543v2.pdf","comment":"17 pages, 18 figures"},{"id":"http://arxiv.org/abs/2507.11851v1","updated":"2025-07-16T02:31:40Z","published":"2025-07-16T02:31:40Z","title":"Your LLM Knows the Future: Uncovering Its Multi-Token Prediction\n  Potential","summary":"  Autoregressive language models are constrained by their inherently sequential\nnature, generating one token at a time. This paradigm limits inference speed\nand parallelism, especially during later stages of generation when the\ndirection and semantics of text are relatively certain. In this work, we\npropose a novel framework that leverages the inherent knowledge of vanilla\nautoregressive language models about future tokens, combining techniques to\nrealize this potential and enable simultaneous prediction of multiple\nsubsequent tokens. Our approach introduces several key innovations: (1) a\nmasked-input formulation where multiple future tokens are jointly predicted\nfrom a common prefix; (2) a gated LoRA formulation that preserves the original\nLLM's functionality, while equipping it for multi-token prediction; (3) a\nlightweight, learnable sampler module that generates coherent sequences from\nthe predicted future tokens; (4) a set of auxiliary training losses, including\na consistency loss, to enhance the coherence and accuracy of jointly generated\ntokens; and (5) a speculative generation strategy that expands tokens\nquadratically in the future while maintaining high fidelity. Our method\nachieves significant speedups through supervised fine-tuning on pretrained\nmodels. For example, it generates code and math nearly 5x faster, and improves\ngeneral chat and knowledge tasks by almost 2.5x. These gains come without any\nloss in quality.\n","authors":["Mohammad Samragh","Arnav Kundu","David Harrison","Kumari Nishu","Devang Naik","Minsik Cho","Mehrdad Farajtabar"],"pdf_url":"https://arxiv.org/pdf/2507.11851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11832v1","updated":"2025-07-16T01:39:32Z","published":"2025-07-16T01:39:32Z","title":"ILID: Native Script Language Identification for Indian Languages","summary":"  The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script making the task even more challenging.\nIn this paper, we release a dataset of 230K sentences consisting of English and\nall 22 official Indian languages labeled with their language identifiers where\ndata in most languages are newly created. We also develop and release robust\nbaseline models using state-of-the-art approaches in machine learning and deep\nlearning that can aid the research in this field. Our baseline models are\ncomparable to the state-of-the-art models for the language identification task.\n","authors":["Yash Ingle","Pruthwik Mishra"],"pdf_url":"https://arxiv.org/pdf/2507.11832v1.pdf","comment":"8 pages, 1 figure, 7 tables, Paper accepted in RANLP 2025"},{"id":"http://arxiv.org/abs/2502.13497v4","updated":"2025-07-16T01:13:21Z","published":"2025-02-19T07:29:58Z","title":"Towards Geo-Culturally Grounded LLM Generations","summary":"  Generative large language models (LLMs) have demonstrated gaps in diverse\ncultural awareness across the globe. We investigate the effect of retrieval\naugmented generation and search-grounding techniques on LLMs' ability to\ndisplay familiarity with various national cultures. Specifically, we compare\nthe performance of standard LLMs, LLMs augmented with retrievals from a bespoke\nknowledge base (i.e., KB grounding), and LLMs augmented with retrievals from a\nweb search (i.e., search grounding) on multiple cultural awareness benchmarks.\nWe find that search grounding significantly improves the LLM performance on\nmultiple-choice benchmarks that test propositional knowledge (e.g., cultural\nnorms, artifacts, and institutions), while KB grounding's effectiveness is\nlimited by inadequate knowledge base coverage and a suboptimal retriever.\nHowever, search grounding also increases the risk of stereotypical judgments by\nlanguage models and fails to improve evaluators' judgments of cultural\nfamiliarity in a human evaluation with adequate statistical power. These\nresults highlight the distinction between propositional cultural knowledge and\nopen-ended cultural fluency when it comes to evaluating LLMs' cultural\nawareness.\n","authors":["Piyawat Lertvittayakumjorn","David Kinney","Vinodkumar Prabhakaran","Donald Martin Jr.","Sunipa Dev"],"pdf_url":"https://arxiv.org/pdf/2502.13497v4.pdf","comment":"ACL 2025 (main conference)"},{"id":"http://arxiv.org/abs/2505.04457v3","updated":"2025-07-16T01:08:39Z","published":"2025-05-07T14:27:46Z","title":"Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale\n  Data Restoration","summary":"  Training data cleaning is a new application for generative model-based speech\nrestoration (SR). This paper introduces Miipher-2, an SR model designed for\nmillion-hour scale data, for training data cleaning for large-scale generative\nmodels like large language models. Key challenges addressed include\ngeneralization to unseen languages, operation without explicit conditioning\n(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a\nfrozen, pre-trained Universal Speech Model (USM), supporting over 300\nlanguages, as a robust, conditioning-free feature extractor. To optimize\nefficiency and minimize memory, Miipher-2 incorporates parallel adapters for\npredicting clean USM features from noisy inputs and employs the WaveFit neural\nvocoder for waveform synthesis. These components were trained on 3,000 hours of\nmulti-lingual, studio-quality recordings with augmented degradations, while USM\nparameters remained fixed. Experimental results demonstrate Miipher-2's\nsuperior or comparable performance to conventional SR models in\nword-error-rate, speaker similarity, and both objective and subjective sound\nquality scores across all tested languages. Miipher-2 operates efficiently on\nconsumer-grade accelerators, achieving a real-time factor of 0.0078, enabling\nthe processing of a million-hour speech dataset in approximately three days\nusing only 100 such accelerators.\n","authors":["Shigeki Karita","Yuma Koizumi","Heiga Zen","Haruko Ishikawa","Robin Scheibler","Michiel Bacchiani"],"pdf_url":"https://arxiv.org/pdf/2505.04457v3.pdf","comment":"Accepted to IEEE WASPAA2025"},{"id":"http://arxiv.org/abs/2507.11809v1","updated":"2025-07-16T00:08:48Z","published":"2025-07-16T00:08:48Z","title":"Tracing Facts or just Copies? A critical investigation of the\n  Competitions of Mechanisms in Large Language Models","summary":"  This paper presents a reproducibility study examining how Large Language\nModels (LLMs) manage competing factual and counterfactual information, focusing\non the role of attention heads in this process. We attempt to reproduce and\nreconcile findings from three recent studies by Ortu et al., Yu, Merullo, and\nPavlick and McDougall et al. that investigate the competition between\nmodel-learned facts and contradictory context information through Mechanistic\nInterpretability tools. Our study specifically examines the relationship\nbetween attention head strength and factual output ratios, evaluates competing\nhypotheses about attention heads' suppression mechanisms, and investigates the\ndomain specificity of these attention patterns. Our findings suggest that\nattention heads promoting factual output do so via general copy suppression\nrather than selective counterfactual suppression, as strengthening them can\nalso inhibit correct facts. Additionally, we show that attention head behavior\nis domain-dependent, with larger models exhibiting more specialized and\ncategory-sensitive patterns.\n","authors":["Dante Campregher","Yanxu Chen","Sander Hoffman","Maria Heuss"],"pdf_url":"https://arxiv.org/pdf/2507.11809v1.pdf","comment":"18 Pages, 13 figures"},{"id":"http://arxiv.org/abs/2507.12679v1","updated":"2025-07-16T23:29:19Z","published":"2025-07-16T23:29:19Z","title":"Improving Drug Identification in Overdose Death Surveillance using Large\n  Language Models","summary":"  The rising rate of drug-related deaths in the United States, largely driven\nby fentanyl, requires timely and accurate surveillance. However, critical\noverdose data are often buried in free-text coroner reports, leading to delays\nand information loss when coded into ICD (International Classification of\nDisease)-10 classifications. Natural language processing (NLP) models may\nautomate and enhance overdose surveillance, but prior applications have been\nlimited. A dataset of 35,433 death records from multiple U.S. jurisdictions in\n2020 was used for model training and internal testing. External validation was\nconducted using a novel separate dataset of 3,335 records from 2023-2024.\nMultiple NLP approaches were evaluated for classifying specific drug\ninvolvement from unstructured death certificate text. These included\ntraditional single- and multi-label classifiers, as well as fine-tuned\nencoder-only language models such as Bidirectional Encoder Representations from\nTransformers (BERT) and BioClinicalBERT, and contemporary decoder-only large\nlanguage models such as Qwen 3 and Llama 3. Model performance was assessed\nusing macro-averaged F1 scores, and 95% confidence intervals were calculated to\nquantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect\nperformance, with macro F1 scores >=0.998 on the internal test set. External\nvalidation confirmed robustness (macro F1=0.966), outperforming conventional\nmachine learning, general-domain BERT models, and various decoder-only large\nlanguage models. NLP models, particularly fine-tuned clinical variants like\nBioClinicalBERT, offer a highly accurate and scalable solution for overdose\ndeath classification from free-text reports. These methods can significantly\naccelerate surveillance workflows, overcoming the limitations of manual ICD-10\ncoding and supporting near real-time detection of emerging substance use\ntrends.\n","authors":["Arthur J. Funnell","Panayiotis Petousis","Fabrice Harel-Canada","Ruby Romero","Alex A. T. Bui","Adam Koncsol","Hritika Chaturvedi","Chelsea Shover","David Goodman-Meza"],"pdf_url":"https://arxiv.org/pdf/2507.12679v1.pdf","comment":"30 pages, 1 figure, 4 tables, 2 supplemental figures, 4 supplemental\n  tables, submitted to Journal of Forensic Sciences (JFS)"},{"id":"http://arxiv.org/abs/2507.12672v1","updated":"2025-07-16T23:07:07Z","published":"2025-07-16T23:07:07Z","title":"The first open machine translation system for the Chechen language","summary":"  We introduce the first open-source model for translation between the\nvulnerable Chechen language and Russian, and the dataset collected to train and\nevaluate it. We explore fine-tuning capabilities for including a new language\ninto a large language model system for multilingual translation NLLB-200. The\nBLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for\ntranslation from Russian to Chechen and reverse direction, respectively. The\nrelease of the translation models is accompanied by the distribution of\nparallel words, phrases and sentences corpora and multilingual sentence encoder\nadapted to the Chechen language.\n","authors":["Abu-Viskhan A. Umishov","Vladislav A. Grigorian"],"pdf_url":"https://arxiv.org/pdf/2507.12672v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2502.15082v2","updated":"2025-07-16T22:34:30Z","published":"2025-02-20T22:51:10Z","title":"UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning","summary":"  User specifications or legal frameworks often require information to be\nremoved from pretrained models, including large language models (LLMs). This\nrequires deleting or \"forgetting\" a set of data points from an already-trained\nmodel, which typically degrades its performance on other data points. Thus, a\nbalance must be struck between removing information and keeping the model's\nother abilities intact, with a failure to balance this trade-off leading to\npoor deletion or an unusable model. To this end, we propose UPCORE\n(Utility-Preserving Coreset Selection), a method-agnostic data selection\nframework for mitigating collateral damage during unlearning. Finding that the\nmodel damage is correlated with the variance of the model's representations on\nthe forget set, we selectively prune the forget set to remove outliers, thereby\nminimizing model degradation after unlearning. Across three standard unlearning\nmethods, UPCORE consistently achieves a superior balance between the competing\nobjectives of deletion efficacy and model preservation. To better evaluate this\ntrade-off, we introduce a new metric, measuring the area-under-the-curve (AUC)\nacross standard metrics. Our results show that UPCORE improves both standard\nmetrics and AUC, benefiting from positive transfer between the coreset and\npruned points while reducing negative transfer from the forget set to points\noutside of it.\n","authors":["Vaidehi Patil","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2502.15082v2.pdf","comment":"Code: https://github.com/Vaidehi99/UPCORE"},{"id":"http://arxiv.org/abs/2507.12653v1","updated":"2025-07-16T22:05:13Z","published":"2025-07-16T22:05:13Z","title":"A Fuzzy Approach to Project Success: Measuring What Matters","summary":"  This paper introduces a novel approach to project success evaluation by\nintegrating fuzzy logic into an existing construct. Traditional Likert-scale\nmeasures often overlook the context-dependent and multifaceted nature of\nproject success. The proposed hierarchical Type-1 Mamdani fuzzy system\nprioritizes sustained positive impact for end-users, reducing emphasis on\nsecondary outcomes like stakeholder satisfaction and internal project success.\nThis dynamic approach may provide a more accurate measure of project success\nand could be adaptable to complex evaluations. Future research will focus on\nempirical testing and broader applications of fuzzy logic in social science.\n","authors":["João Granja-Correia","Remedios Hernández-Linares","Luca Ferranti","Arménio Rego"],"pdf_url":"https://arxiv.org/pdf/2507.12653v1.pdf","comment":"3 pages, 1 figure, presented at FUZZ-IEEE 2025"},{"id":"http://arxiv.org/abs/2503.12989v2","updated":"2025-07-16T21:47:50Z","published":"2025-03-17T09:44:50Z","title":"A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation\n  Classification Using Large Language Models","summary":"  Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations, especially for smaller models. To address these challenges, we\npropose a multi-stage framework consisting of inference, retrieval, and\nreranking stages, which integrates taxonomy-guided reasoning examples to\nenhance performance by aligning outputs with taxonomic knowledge. Evaluations\non a large-scale dataset show that our framework not only enhances occupation\nand skill classification tasks, but also provides a cost-effective alternative\nto frontier models like GPT-4o, significantly reducing computational costs\nwhile maintaining strong performance. This makes it a practical and scalable\nsolution for occupation classification and related tasks across LLMs.\n","authors":["Palakorn Achananuparp","Ee-Peng Lim","Yao Lu"],"pdf_url":"https://arxiv.org/pdf/2503.12989v2.pdf","comment":"Accepted to ICWSM'26"},{"id":"http://arxiv.org/abs/2505.24189v2","updated":"2025-07-16T21:38:06Z","published":"2025-05-30T03:59:35Z","title":"Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code\n  Workflows","summary":"  Large Language Models (LLMs) such as GPT-4o can handle a wide range of\ncomplex tasks with the right prompt. As per token costs are reduced, the\nadvantages of fine-tuning Small Language Models (SLMs) for real-world\napplications -- faster inference, lower costs -- may no longer be clear. In\nthis work, we present evidence that, for domain-specific tasks that require\nstructured outputs, SLMs still have a quality advantage. We compare fine-tuning\nan SLM against prompting LLMs on the task of generating low-code workflows in\nJSON form. We observe that while a good prompt can yield reasonable results,\nfine-tuning improves quality by 10% on average. We also perform systematic\nerror analysis to reveal model limitations.\n","authors":["Orlando Marquez Ayala","Patrice Bechard","Emily Chen","Maggie Baird","Jingfei Chen"],"pdf_url":"https://arxiv.org/pdf/2505.24189v2.pdf","comment":"8 pages, 7 figures. Accepted to Workshop on Structured Knowledge for\n  Large Language Models (SKnowLLM) at KDD 2025"},{"id":"http://arxiv.org/abs/2506.20040v2","updated":"2025-07-16T21:35:12Z","published":"2025-06-24T22:43:36Z","title":"Cross-Layer Discrete Concept Discovery for Interpreting Language Models","summary":"  Uncovering emergent concepts across transformer layers remains a significant\nchallenge because the residual stream linearly mixes and duplicates\ninformation, obscuring how features evolve within large language models.\nCurrent research efforts primarily inspect neural representations at single\nlayers, thereby overlooking this cross-layer superposition and the redundancy\nit introduces. These representations are typically either analyzed directly for\nactivation patterns or passed to probing classifiers that map them to a limited\nset of predefined concepts. To address these limitations, we propose\ncross-layer VQ-VAE (CLVQ-VAE), a framework that uses vector quantization to map\nrepresentations across layers and in the process collapse duplicated\nresidual-stream features into compact, interpretable concept vectors. Our\napproach uniquely combines top-k temperature-based sampling during quantization\nwith EMA codebook updates, providing controlled exploration of the discrete\nlatent space while maintaining code-book diversity. We further enhance the\nframework with scaled-spherical k-means++ for codebook initialization, which\nclusters by directional similarity rather than magnitude, better aligning with\nsemantic structure in word embedding space.\n","authors":["Ankur Garg","Xuemin Yu","Hassan Sajjad","Samira Ebrahimi Kahou"],"pdf_url":"https://arxiv.org/pdf/2506.20040v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04652v2","updated":"2025-07-16T21:23:37Z","published":"2025-01-08T18:05:30Z","title":"Multi-task retriever fine-tuning for domain-specific and efficient RAG","summary":"  Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying\nLarge Language Models (LLMs), as it can address typical limitations such as\ngenerating hallucinated or outdated information. However, when building\nreal-world RAG applications, practical issues arise. First, the retrieved\ninformation is generally domain-specific. Since it is computationally expensive\nto fine-tune LLMs, it is more feasible to fine-tune the retriever to improve\nthe quality of the data included in the LLM input. Second, as more applications\nare deployed in the same real-world system, one cannot afford to deploy\nseparate retrievers. Moreover, these RAG applications normally retrieve\ndifferent kinds of data. Our solution is to instruction fine-tune a small\nretriever encoder on a variety of domain-specific tasks to allow us to deploy\none encoder that can serve many use cases, thereby achieving low-cost,\nscalability, and speed. We show how this encoder generalizes to out-of-domain\nsettings as well as to an unseen retrieval task on real-world enterprise use\ncases.\n","authors":["Patrice Béchard","Orlando Marquez Ayala"],"pdf_url":"https://arxiv.org/pdf/2501.04652v2.pdf","comment":"7 pages, 2 figures. Accepted at Workshop on Structured Knowledge for\n  Large Language Models (SKnowLLM) at KDD 2025"},{"id":"http://arxiv.org/abs/2410.20625v2","updated":"2025-07-16T21:18:50Z","published":"2024-10-27T22:57:12Z","title":"LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA\n  Optimization","summary":"  Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning\nmethod for LLM that reduces memory requirements. However, current LoRA\noptimizers lack transformation invariance, meaning the actual updates to the\nweights depends on how the two LoRA factors are scaled or rotated. This\ndeficiency leads to inefficient learning and sub-optimal solutions in practice.\nThis paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method\nfor LoRA optimization, which can achieve transformation invariance and remain\ncomputationally efficient. We provide theoretical analysis to demonstrate the\nbenefit of our method and conduct experiments on various LLM tasks with\ndifferent models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate\nconsistent improvements against existing optimizers. For example, replacing\nAdam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6\\% accuracy\ngain on Super-Natural Instructions and 3.5\\% accuracy gain across other four\nLLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA).\n","authors":["Jui-Nan Yen","Si Si","Zhao Meng","Felix Yu","Sai Surya Duvvuri","Inderjit S. Dhillon","Cho-Jui Hsieh","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.20625v2.pdf","comment":"Published as an oral paper at ICLR 2025. The code for our project is\n  available at https://github.com/gkevinyen5418/LoRA-RITE"},{"id":"http://arxiv.org/abs/2410.20788v3","updated":"2025-07-16T19:32:23Z","published":"2024-10-28T07:10:10Z","title":"SCULPT: Systematic Tuning of Long Prompts","summary":"  Prompt optimization is essential for effective utilization of large language\nmodels (LLMs) across diverse tasks. While existing optimization methods are\neffective in optimizing short prompts, they struggle with longer, more complex\nones, often risking information loss and being sensitive to small\nperturbations. To address these challenges, we propose SCULPT (Systematic\nTuning of Long Prompts), a framework that treats prompt optimization as a\nhierarchical tree refinement problem. SCULPT represents prompts as tree\nstructures, enabling targeted modifications while preserving contextual\nintegrity. It employs a Critic-Actor framework that generates reflections and\napplies actions to refine the prompt. Evaluations demonstrate SCULPT's\neffectiveness on long prompts, its robustness to adversarial perturbations, and\nits ability to generate high-performing prompts even without any initial\nhuman-written prompt. Compared to existing state of the art methods, SCULPT\nconsistently improves LLM performance by preserving essential task information\nwhile applying structured refinements. Both qualitative and quantitative\nanalyses show that SCULPT produces more stable and interpretable prompt\nmodifications, ensuring better generalization across tasks.\n","authors":["Shanu Kumar","Akhila Yesantarao Venkata","Shubhanshu Khandelwal","Bishal Santra","Parag Agrawal","Manish Gupta"],"pdf_url":"https://arxiv.org/pdf/2410.20788v3.pdf","comment":"Accepted at ACL Main 2025"},{"id":"http://arxiv.org/abs/2502.01491v2","updated":"2025-07-16T18:39:35Z","published":"2025-02-03T16:26:06Z","title":"Memorization Inheritance in Sequence-Level Knowledge Distillation for\n  Neural Machine Translation","summary":"  In this work, we explore how instance-level memorization in the teacher\nNeural Machine Translation (NMT) model gets inherited by the student model in\nsequence-level knowledge distillation (SeqKD). We find that despite not\ndirectly seeing the original training data, students memorize more than\nbaseline models (models of the same size, trained on the original data) -- 3.4%\nfor exact matches and 57% for extractive memorization -- and show increased\nhallucination rates. Further, under this SeqKD setting, we also characterize\nhow students behave on specific training data subgroups, such as subgroups with\nlow quality and specific counterfactual memorization (CM) scores, and find that\nstudents exhibit amplified denoising on low-quality subgroups. Finally, we\npropose a modification to SeqKD named Adaptive-SeqKD, which intervenes in SeqKD\nto reduce memorization and hallucinations. Overall, we recommend caution when\napplying SeqKD: students inherit both their teachers' superior performance and\ntheir fault modes, thereby requiring active monitoring.\n","authors":["Verna Dankers","Vikas Raunak"],"pdf_url":"https://arxiv.org/pdf/2502.01491v2.pdf","comment":"To appear at ACL 2025; 15 pages total (5 in the main paper, 3 pages\n  of limitations and references and 7 pages with appendices)"},{"id":"http://arxiv.org/abs/2507.12566v1","updated":"2025-07-16T18:31:23Z","published":"2025-07-16T18:31:23Z","title":"Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal\n  Large Language Models","summary":"  This paper focuses on monolithic Multimodal Large Language Models (MLLMs),\nwhich integrate visual encoding and language decoding into a single model.\nExisting structures and pre-training strategies for monolithic MLLMs often\nsuffer from unstable optimization and catastrophic forgetting. To address these\nchallenges, our key idea is to embed a new visual parameter space into a\npre-trained LLM, enabling stable learning of visual knowledge from noisy data\nvia delta tuning. Based on this principle, we first introduce Mono-InternVL, an\nadvanced monolithic MLLM that incorporates a set of visual experts through a\nmultimodal mixture-of-experts architecture. In addition, we design an\ninnovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize\nits visual capabilities via progressive learning. Mono-InternVL achieves\ncompetitive performance against existing MLLMs but also leads to relatively\nexpensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper\nand stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++\nintroduces additional visual attention experts to Mono-InternVL-1.5 and\nre-organizes the pre-training process in an efficient manner. During inference,\nit includes a fused CUDA kernel to speed up its MoE operations. With these\ndesigns, Mono-InternVL-1.5 significantly reduces training and inference costs,\nwhile still maintaining competitive performance with Mono-InternVL. To evaluate\nour approach, we conduct extensive experiments across 15 benchmarks. Results\ndemonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out\nof 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared\nto its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves\nsimilar multimodal performance while reducing first-token latency by up to 69%.\nCode and models are released at https://github.com/OpenGVLab/Mono-InternVL.\n","authors":["Gen Luo","Wenhan Dou","Wenhao Li","Zhaokai Wang","Xue Yang","Changyao Tian","Hao Li","Weiyun Wang","Wenhai Wang","Xizhou Zhu","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2507.12566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08339v2","updated":"2025-07-16T18:06:32Z","published":"2025-07-11T06:37:44Z","title":"What Factors Affect LLMs and RLLMs in Financial Question Answering?","summary":"  Recently, the development of large language models (LLMs) and reasoning large\nlanguage models (RLLMs) have gained considerable attention from many\nresearchers. RLLMs enhance the reasoning capabilities of LLMs through Long\nChain-of-Thought (Long CoT) processes, significantly improving the performance\nof LLMs in addressing complex problems. However, there are few works that\nsystematically explore what methods can fully unlock the performance of LLMs\nand RLLMs within the financial domain. To investigate the impact of various\nmethods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the\neffects of prompting methods, agentic frameworks, and multilingual alignment\nmethods on financial question-answering tasks. Our research findings indicate:\n(1) Current prompting methods and agent frameworks enhance the performance of\nLLMs in financial question answering by simulating Long CoT; (2) RLLMs possess\ninherent Long CoT capabilities, which limits the effectiveness of conventional\nmethods in further enhancing their performance; (3) Current advanced\nmultilingual alignment methods primarily improve the multilingual performance\nof LLMs by extending the reasoning length, which yields minimal benefits for\nRLLMs. We hope that this study can serve as an important reference for LLMs and\nRLLMs in the field of financial question answering.\n","authors":["Peng Wang","Xuesi Hu","Jiageng Wu","Yuntao Zou","Qiancheng Zhang","Dagang Li"],"pdf_url":"https://arxiv.org/pdf/2507.08339v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2507.12553v1","updated":"2025-07-16T18:04:26Z","published":"2025-07-16T18:04:26Z","title":"Is This Just Fantasy? Language Model Representations Reflect Human\n  Judgments of Event Plausibility","summary":"  Language models (LMs) are used for a diverse range of tasks, from question\nanswering to writing fantastical stories. In order to reliably accomplish these\ntasks, LMs must be able to discern the modal category of a sentence (i.e.,\nwhether it describes something that is possible, impossible, completely\nnonsensical, etc.). However, recent studies have called into question the\nability of LMs to categorize sentences according to modality (Michaelov et al.,\n2025; Kauf et al., 2023). In this work, we identify linear representations that\ndiscriminate between modal categories within a variety of LMs, or modal\ndifference vectors. Analysis of modal difference vectors reveals that LMs have\naccess to more reliable modal categorization judgments than previously\nreported. Furthermore, we find that modal difference vectors emerge in a\nconsistent order as models become more competent (i.e., through training steps,\nlayers, and parameter count). Notably, we find that modal difference vectors\nidentified within LM activations can be used to model fine-grained human\ncategorization behavior. This potentially provides a novel view into how human\nparticipants distinguish between modal categories, which we explore by\ncorrelating projections along modal difference vectors with human participants'\nratings of interpretable features. In summary, we derive new insights into LM\nmodal categorization using techniques from mechanistic interpretability, with\nthe potential to inform our understanding of modal categorization in humans.\n","authors":["Michael A. Lepori","Jennifer Hu","Ishita Dasgupta","Roma Patel","Thomas Serre","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2507.12553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07188v2","updated":"2025-07-16T18:02:56Z","published":"2025-07-09T18:01:50Z","title":"Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses","summary":"  Large Language Models (LLMs) are increasingly used as proxies for human\nsubjects in social science surveys, but their reliability and susceptibility to\nknown response biases are poorly understood. This paper investigates the\nresponse robustness of LLMs in normative survey contexts - we test nine diverse\nLLMs on questions from the World Values Survey (WVS), applying a comprehensive\nset of 11 perturbations to both question phrasing and answer option structure,\nresulting in over 167,000 simulated interviews. In doing so, we not only reveal\nLLMs' vulnerabilities to perturbations but also show that all tested models\nexhibit a consistent recency bias varying in intensity, disproportionately\nfavoring the last-presented answer option. While larger models are generally\nmore robust, all models remain sensitive to semantic variations like\nparaphrasing and to combined perturbations. By applying a set of perturbations,\nwe reveal that LLMs partially align with survey response biases identified in\nhumans. This underscores the critical importance of prompt design and\nrobustness testing when using LLMs to generate synthetic survey data.\n","authors":["Jens Rupprecht","Georg Ahnert","Markus Strohmaier"],"pdf_url":"https://arxiv.org/pdf/2507.07188v2.pdf","comment":"18 pages, 17 figures"},{"id":"http://arxiv.org/abs/2507.12547v1","updated":"2025-07-16T18:01:03Z","published":"2025-07-16T18:01:03Z","title":"Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic\n  Models","summary":"  When faced with novel situations, people are able to marshal relevant\nconsiderations from a wide range of background knowledge and put these to use\nin inferences and predictions. What permits us to draw in globally relevant\ninformation and reason over it coherently? Here, we explore the hypothesis that\npeople use a combination of distributed and symbolic representations to\nconstruct bespoke mental models tailored to novel situations. We propose a\ncomputational implementation of this idea -- a ``Model Synthesis Architecture''\n(MSA) -- using language models to implement global relevance-based retrieval\nand model synthesis and probabilistic programs to implement bespoke, coherent\nworld models. We evaluate our MSA as a model of human judgments on a novel\nreasoning dataset. The dataset -- built around a `Model Olympics` domain of\nsports vignettes -- tests models' capacity for human-like, open-ended reasoning\nby requiring (i) judgments about novel causal structures described in language;\n(ii) drawing on large bodies of background knowledge; and (iii) doing both in\nlight of observations that introduce arbitrary novel variables. Our MSA\napproach captures human judgments better than language model-only baselines,\nunder both direct and chain-of-thought generations from the LM that supports\nmodel synthesis. These results suggest that MSAs can be implemented in a way\nthat mirrors people's ability to deliver locally coherent reasoning over\nglobally relevant variables, offering a path to understanding and replicating\nhuman reasoning in open-ended domains.\n","authors":["Lionel Wong","Katherine M. Collins","Lance Ying","Cedegao E. Zhang","Adrian Weller","Tobias Gersternberg","Timothy O'Donnell","Alexander K. Lew","Jacob D. Andreas","Joshua B. Tenenbaum","Tyler Brooke-Wilson"],"pdf_url":"https://arxiv.org/pdf/2507.12547v1.pdf","comment":"Presented at CogSci 2025"},{"id":"http://arxiv.org/abs/2507.12507v1","updated":"2025-07-16T17:59:24Z","published":"2025-07-16T17:59:24Z","title":"Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged\n  Training","summary":"  Recent advancements in reasoning-focused language models such as OpenAI's O1\nand DeepSeek-R1 have shown that scaling test-time computation-through\nchain-of-thought reasoning and iterative exploration-can yield substantial\nimprovements on complex tasks like mathematics and code generation. These\nbreakthroughs have been driven by large-scale reinforcement learning (RL),\nparticularly when combined with verifiable reward signals that provide\nobjective and grounded supervision. In this report, we investigate the effects\nof prolonged reinforcement learning on a small language model across a diverse\nset of reasoning domains. Our work identifies several key ingredients for\neffective training, including the use of verifiable reward tasks, enhancements\nto Group Relative Policy Optimization (GRPO), and practical techniques to\nimprove training stability and generalization. We introduce controlled KL\nregularization, clipping ratio, and periodic reference policy resets as\ncritical components for unlocking long-term performance gains. Our model\nachieves significant improvements over strong baselines, including +14.7% on\nmath, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate\ncontinued research, we release our model publicly.\n","authors":["Mingjie Liu","Shizhe Diao","Jian Hu","Ximing Lu","Xin Dong","Hao Zhang","Alexander Bukharin","Shaokun Zhang","Jiaqi Zeng","Makesh Narsimhan Sreedhar","Gerald Shen","David Mosallanezhad","Di Zhang","Jonas Yang","June Yang","Oleksii Kuchaiev","Guilin Liu","Zhiding Yu","Pavlo Molchanov","Yejin Choi","Jan Kautz","Yi Dong"],"pdf_url":"https://arxiv.org/pdf/2507.12507v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2507.12059v1","updated":"2025-07-16T09:16:36Z","published":"2025-07-16T09:16:36Z","title":"Evaluating the Ability of Large Language Models to Reason about Cardinal\n  Directions, Revisited","summary":"  We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24.\n","authors":["Anthony G Cohn","Robert E Blackwell"],"pdf_url":"https://arxiv.org/pdf/2507.12059v1.pdf","comment":"8 pages, 5 figures. Accepted at QR 2025 : 38th International Workshop\n  on Qualitative Reasoning at IJCAI. arXiv admin note: substantial text overlap\n  with arXiv:2406.16528"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2507.12465v1","updated":"2025-07-16T17:59:35Z","published":"2025-07-16T17:59:35Z","title":"PhysX: Physical-Grounded 3D Asset Generation","summary":"  3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose \\textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose \\textbf{PhysXGen}, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.\n","authors":["Ziang Cao","Zhaoxi Chen","Linag Pan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2507.12465v1.pdf","comment":"Project page: https://physx-3d.github.io/"},{"id":"http://arxiv.org/abs/2507.12464v1","updated":"2025-07-16T17:59:32Z","published":"2025-07-16T17:59:32Z","title":"CytoSAE: Interpretable Cell Embeddings for Hematology","summary":"  Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic\ninterpretability of transformer-based foundation models. Very recently, SAEs\nwere also adopted for the visual domain, enabling the discovery of visual\nconcepts and their patch-wise attribution to tokens in the transformer model.\nWhile a growing number of foundation models emerged for medical imaging, tools\nfor explaining their inferences are still lacking. In this work, we show the\napplicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder\nwhich is trained on over 40,000 peripheral blood single-cell images. CytoSAE\ngeneralizes to diverse and out-of-domain datasets, including bone marrow\ncytology, where it identifies morphologically relevant concepts which we\nvalidated with medical experts. Furthermore, we demonstrate scenarios in which\nCytoSAE can generate patient-specific and disease-specific concepts, enabling\nthe detection of pathognomonic cells and localized cellular abnormalities at\nthe patch level. We quantified the effect of concepts on a patient-level AML\nsubtype classification task and show that CytoSAE concepts reach performance\ncomparable to the state-of-the-art, while offering explainability on the\nsub-cellular level. Source code and model weights are available at\nhttps://github.com/dynamical-inference/cytosae.\n","authors":["Muhammed Furkan Dasdelen","Hyesu Lim","Michele Buck","Katharina S. Götze","Carsten Marr","Steffen Schneider"],"pdf_url":"https://arxiv.org/pdf/2507.12464v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.12463v1","updated":"2025-07-16T17:59:30Z","published":"2025-07-16T17:59:30Z","title":"MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding","summary":"  Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behavior$\\unicode{x2014}$such as motion, trajectories, and\nintention$\\unicode{x2014}$a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose $\\textbf{MMHU}$, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasks$\\unicode{x2014}$ranging from motion prediction to motion\ngeneration and human behavior question answering$\\unicode{x2014}$thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.\n","authors":["Renjie Li","Ruijie Ye","Mingyang Wu","Hao Frank Yang","Zhiwen Fan","Hezhen Hu","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2507.12463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12462v1","updated":"2025-07-16T17:59:03Z","published":"2025-07-16T17:59:03Z","title":"SpatialTrackerV2: 3D Point Tracking Made Easy","summary":"  We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50$\\times$ faster.\n","authors":["Yuxi Xiao","Jianyuan Wang","Nan Xue","Nikita Karaev","Yuri Makarov","Bingyi Kang","Xing Zhu","Hujun Bao","Yujun Shen","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.12462v1.pdf","comment":"International Conference on Computer Vision, ICCV 2025. Huggingface\n  Demo: https://huggingface.co/spaces/Yuxihenry/SpatialTrackerV2, Code:\n  https://github.com/henry123-boy/SpaTrackerV2"},{"id":"http://arxiv.org/abs/2507.12461v1","updated":"2025-07-16T17:58:35Z","published":"2025-07-16T17:58:35Z","title":"Interpreting Radiologist's Intention from Eye Movements in Chest X-ray\n  Diagnosis","summary":"  Radiologists rely on eye movements to navigate and interpret medical images.\nA trained radiologist possesses knowledge about the potential diseases that may\nbe present in the images and, when searching, follows a mental checklist to\nlocate them using their gaze. This is a key observation, yet existing models\nfail to capture the underlying intent behind each fixation. In this paper, we\nintroduce a deep learning-based approach, RadGazeIntent, designed to model this\nbehavior: having an intention to find something and actively searching for it.\nOur transformer-based architecture processes both the temporal and spatial\ndimensions of gaze data, transforming fine-grained fixation features into\ncoarse, meaningful representations of diagnostic intent to interpret\nradiologists' goals. To capture the nuances of radiologists' varied\nintention-driven behaviors, we process existing medical eye-tracking datasets\nto create three intention-labeled subsets: RadSeq (Systematic Sequential\nSearch), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid\nPattern). Experimental results demonstrate RadGazeIntent's ability to predict\nwhich findings radiologists are examining at specific moments, outperforming\nbaseline methods across all intention-labeled datasets.\n","authors":["Trong-Thang Pham","Anh Nguyen","Zhigang Deng","Carol C. Wu","Hien Van Nguyen","Ngan Le"],"pdf_url":"https://arxiv.org/pdf/2507.12461v1.pdf","comment":"ACM MM 2025"},{"id":"http://arxiv.org/abs/2507.12455v1","updated":"2025-07-16T17:55:43Z","published":"2025-07-16T17:55:43Z","title":"Mitigating Object Hallucinations via Sentence-Level Early Intervention","summary":"  Multimodal large language models (MLLMs) have revolutionized cross-modal\nunderstanding but continue to struggle with hallucinations - fabricated content\ncontradicting visual inputs. Existing hallucination mitigation methods either\nincur prohibitive computational costs or introduce distribution mismatches\nbetween training data and model outputs. We identify a critical insight:\nhallucinations predominantly emerge at the early stages of text generation and\npropagate through subsequent outputs. To address this, we propose **SENTINEL**\n(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain\npr**E**ference **L**earning), a framework that eliminates dependency on human\nannotations. Specifically, we first bootstrap high-quality in-domain preference\npairs by iteratively sampling model outputs, validating object existence\nthrough cross-checking with two open-vocabulary detectors, and classifying\nsentences into hallucinated/non-hallucinated categories. Subsequently, we use\ncontext-coherent positive samples and hallucinated negative samples to build\ncontext-aware preference data iteratively. Finally, we train models using a\ncontext-aware preference loss (C-DPO) that emphasizes discriminative learning\nat the sentence level where hallucinations initially manifest. Experimental\nresults show that SENTINEL can reduce hallucinations by over 90\\% compared to\nthe original model and outperforms the previous state-of-the-art method on both\nhallucination benchmarks and general capabilities benchmarks, demonstrating its\nsuperiority and generalization ability. The models, datasets, and code are\navailable at https://github.com/pspdada/SENTINEL.\n","authors":["Shangpin Peng","Senqiao Yang","Li Jiang","Zhuotao Tian"],"pdf_url":"https://arxiv.org/pdf/2507.12455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.08677v2","updated":"2025-07-16T17:46:33Z","published":"2025-06-10T10:37:21Z","title":"MAMBO: High-Resolution Generative Approach for Mammography Images","summary":"  Mammography is the gold standard for the detection and diagnosis of breast\ncancer. This procedure can be significantly enhanced with Artificial\nIntelligence (AI)-based software, which assists radiologists in identifying\nabnormalities. However, training AI systems requires large and diverse\ndatasets, which are often difficult to obtain due to privacy and ethical\nconstraints. To address this issue, the paper introduces MAMmography ensemBle\nmOdel (MAMBO), a novel patch-based diffusion approach designed to generate\nfull-resolution mammograms. Diffusion models have shown breakthrough results in\nrealistic image generation, yet few studies have focused on mammograms, and\nnone have successfully generated high-resolution outputs required to capture\nfine-grained features of small lesions. To achieve this, MAMBO integrates\nseparate diffusion models to capture both local and global (image-level)\ncontexts. The contextual information is then fed into the final model,\nsignificantly aiding the noise removal process. This design enables MAMBO to\ngenerate highly realistic mammograms of up to 3840x3840 pixels. Importantly,\nthis approach can be used to enhance the training of classification models and\nextended to anomaly segmentation. Experiments, both numerical and radiologist\nvalidation, assess MAMBO's capabilities in image generation, super-resolution,\nand anomaly segmentation, highlighting its potential to enhance mammography\nanalysis for more accurate diagnoses and earlier lesion detection. The source\ncode used in this study is publicly available at:\nhttps://github.com/iai-rs/mambo.\n","authors":["Milica Škipina","Nikola Jovišić","Nicola Dall'Asen","Vanja Švenda","Anil Osman Tur","Slobodan Ilić","Elisa Ricci","Dubravko Ćulibrk"],"pdf_url":"https://arxiv.org/pdf/2506.08677v2.pdf","comment":"21 pages, 14 figures, 7 tables This work has been submitted to the\n  IEEE for possible publication"},{"id":"http://arxiv.org/abs/2507.12449v1","updated":"2025-07-16T17:41:14Z","published":"2025-07-16T17:41:14Z","title":"Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance\n  Scenarios","summary":"  Obstacle avoidance is essential for ensuring the safety of autonomous\nvehicles. Accurate perception and motion planning are crucial to enabling\nvehicles to navigate complex environments while avoiding collisions. In this\npaper, we propose an efficient obstacle avoidance pipeline that leverages a\ncamera-only perception module and a Frenet-Pure Pursuit-based planning\nstrategy. By integrating advancements in computer vision, the system utilizes\nYOLOv11 for object detection and state-of-the-art monocular depth estimation\nmodels, such as Depth Anything V2, to estimate object distances. A comparative\nanalysis of these models provides valuable insights into their accuracy,\nefficiency, and robustness in real-world conditions. The system is evaluated in\ndiverse scenarios on a university campus, demonstrating its effectiveness in\nhandling various obstacles and enhancing autonomous navigation. The video\npresenting the results of the obstacle avoidance experiments is available at:\nhttps://www.youtube.com/watch?v=FoXiO5S_tA8\n","authors":["Van-Hoang-Anh Phan","Chi-Tam Nguyen","Doan-Trung Au","Thanh-Danh Phan","Minh-Thien Duong","My-Ha Le"],"pdf_url":"https://arxiv.org/pdf/2507.12449v1.pdf","comment":"7 pages, 6 figures, 4 tables, HSI 2025"},{"id":"http://arxiv.org/abs/2505.05470v4","updated":"2025-07-16T17:37:02Z","published":"2025-05-08T17:58:45Z","title":"Flow-GRPO: Training Flow Matching Models via Online RL","summary":"  We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation. Flow-GRPO\nalso achieves substantial gains in human preference alignment. Notably, very\nlittle reward hacking occurred, meaning rewards did not increase at the cost of\nappreciable image quality or diversity degradation.\n","authors":["Jie Liu","Gongye Liu","Jiajun Liang","Yangguang Li","Jiaheng Liu","Xintao Wang","Pengfei Wan","Di Zhang","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2505.05470v4.pdf","comment":"Code: https://github.com/yifan123/flow_grpo"},{"id":"http://arxiv.org/abs/2507.09953v2","updated":"2025-07-16T17:34:35Z","published":"2025-07-14T06:02:05Z","title":"4D-MISR: A unified model for low-dose super-resolution imaging via\n  feature fusion","summary":"  While electron microscopy offers crucial atomic-resolution insights into\nstructure-property relationships, radiation damage severely limits its use on\nbeam-sensitive materials like proteins and 2D materials. To overcome this\nchallenge, we push beyond the electron dose limits of conventional electron\nmicroscopy by adapting principles from multi-image super-resolution (MISR) that\nhave been widely used in remote sensing. Our method fuses multiple\nlow-resolution, sub-pixel-shifted views and enhances the reconstruction with a\nconvolutional neural network (CNN) that integrates features from synthetic,\nmulti-angle observations. We developed a dual-path, attention-guided network\nfor 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose\ndata. This provides robust atomic-scale visualization across amorphous,\nsemi-crystalline, and crystalline beam-sensitive specimens. Systematic\nevaluations on representative materials demonstrate comparable spatial\nresolution to conventional ptychography under ultra-low-dose conditions. Our\nwork expands the capabilities of 4D-STEM, offering a new and generalizable\nmethod for the structural analysis of radiation-vulnerable materials.\n","authors":["Zifei Wang","Zian Mao","Xiaoya He","Xi Huang","Haoran Zhang","Chun Cheng","Shufen Chu","Tingzheng Hou","Xiaoqin Zeng","Yujun Xie"],"pdf_url":"https://arxiv.org/pdf/2507.09953v2.pdf","comment":"The authors have decided to withdraw this submission due to incorrect\n  affiliation information and certain logical inconsistencies in the manuscript\n  that require revision"},{"id":"http://arxiv.org/abs/2507.12441v1","updated":"2025-07-16T17:28:19Z","published":"2025-07-16T17:28:19Z","title":"Describe Anything Model for Visual Question Answering on Text-rich\n  Images","summary":"  Recent progress has been made in region-aware vision-language modeling,\nparticularly with the emergence of the Describe Anything Model (DAM). DAM is\ncapable of generating detailed descriptions of any specific image areas or\nobjects without the need for additional localized image-text alignment\nsupervision. We hypothesize that such region-level descriptive capability is\nbeneficial for the task of Visual Question Answering (VQA), especially in\nchallenging scenarios involving images with dense text. In such settings, the\nfine-grained extraction of textual information is crucial to producing correct\nanswers. Motivated by this, we introduce DAM-QA, a framework with a tailored\nevaluation protocol, developed to investigate and harness the region-aware\ncapabilities from DAM for the text-rich VQA problem that requires reasoning\nover text-based information within images. DAM-QA incorporates a mechanism that\naggregates answers from multiple regional views of image content, enabling more\neffective identification of evidence that may be tied to text-related elements.\nExperiments on six VQA benchmarks show that our approach consistently\noutperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA\nalso achieves the best overall performance among region-aware models with fewer\nparameters, significantly narrowing the gap with strong generalist VLMs. These\nresults highlight the potential of DAM-like models for text-rich and broader\nVQA tasks when paired with efficient usage and integration strategies. Our code\nis publicly available at https://github.com/Linvyl/DAM-QA.git.\n","authors":["Yen-Linh Vu","Dinh-Thang Duong","Truong-Binh Duong","Anh-Khoi Nguyen","Thanh-Huy Nguyen","Le Thien Phuc Nguyen","Jianhua Xing","Xingjian Li","Tianyang Wang","Ulas Bagci","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2507.12441v1.pdf","comment":"11 pages, 5 figures. Accepted to VisionDocs @ ICCV 2025"},{"id":"http://arxiv.org/abs/2507.12440v1","updated":"2025-07-16T17:27:44Z","published":"2025-07-16T17:27:44Z","title":"EgoVLA: Learning Vision-Language-Action Models from Egocentric Human\n  Videos","summary":"  Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Isaac\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA\n","authors":["Ruihan Yang","Qinxi Yu","Yecheng Wu","Rui Yan","Borui Li","An-Chieh Cheng","Xueyan Zou","Yunhao Fang","Hongxu Yin","Sifei Liu","Song Han","Yao Lu","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2507.12440v1.pdf","comment":"More videos can be found on our website:\n  https://rchalyang.github.io/EgoVLA"},{"id":"http://arxiv.org/abs/2507.12433v1","updated":"2025-07-16T17:20:36Z","published":"2025-07-16T17:20:36Z","title":"Traffic-Aware Pedestrian Intention Prediction","summary":"  Accurate pedestrian intention estimation is crucial for the safe navigation\nof autonomous vehicles (AVs) and hence attracts a lot of research attention.\nHowever, current models often fail to adequately consider dynamic traffic\nsignals and contextual scene information, which are critical for real-world\napplications. This paper presents a Traffic-Aware Spatio-Temporal Graph\nConvolutional Network (TA-STGCN) that integrates traffic signs and their states\n(Red, Yellow, Green) into pedestrian intention prediction. Our approach\nintroduces the integration of dynamic traffic signal states and bounding box\nsize as key features, allowing the model to capture both spatial and temporal\ndependencies in complex urban environments. The model surpasses existing\nmethods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy\ncompared to the baseline model on the PIE dataset, demonstrating its\neffectiveness in improving pedestrian intention prediction.\n","authors":["Fahimeh Orvati Nia","Hai Lin"],"pdf_url":"https://arxiv.org/pdf/2507.12433v1.pdf","comment":"6 pages, 4 figures. Accepted to the American Control Conference (ACC)\n  2025"},{"id":"http://arxiv.org/abs/2502.17066v2","updated":"2025-07-16T17:16:15Z","published":"2025-02-24T11:28:00Z","title":"DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth\n  Observation Applications","summary":"  Significant efforts have been directed towards adapting self-supervised\nmultimodal learning for Earth observation applications. However, most current\nmethods produce coarse patch-sized embeddings, limiting their effectiveness and\nintegration with other modalities like LiDAR. To close this gap, we present\nDUNIA, an approach to learn pixel-sized embeddings through cross-modal\nalignment between images and full-waveform LiDAR data. As the model is trained\nin a contrastive manner, the embeddings can be directly leveraged in the\ncontext of a variety of environmental monitoring tasks in a zero-shot setting.\nIn our experiments, we demonstrate the effectiveness of the embeddings for\nseven such tasks: canopy height mapping, fractional canopy cover, land cover\nmapping, tree species identification, plant area index, crop type\nclassification, and per-pixel waveform-based vertical structure mapping. The\nresults show that the embeddings, along with zero-shot classifiers, often\noutperform specialized supervised models, even in low-data regimes. In the\nfine-tuning setting, we show strong performances near or better than the\nstate-of-the-art on five out of six tasks.\n","authors":["Ibrahim Fayad","Max Zimmer","Martin Schwartz","Fabian Gieseke","Philippe Ciais","Gabriel Belouze","Sarah Brood","Aurelien De Truchis","Alexandre d'Aspremont"],"pdf_url":"https://arxiv.org/pdf/2502.17066v2.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.12427v1","updated":"2025-07-16T17:15:18Z","published":"2025-07-16T17:15:18Z","title":"Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature\n  Representation","summary":"  We propose UTS, a unit-based tissue segmentation framework for histopathology\nthat classifies each fixed-size 32 * 32 tile, rather than each pixel, as the\nsegmentation unit. This approach reduces annotation effort and improves\ncomputational efficiency without compromising accuracy. To implement this\napproach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits\nthe multi-level feature representation to capture both fine-grained morphology\nand global tissue context. Trained to segment breast tissue into three\ncategories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports\nclinically relevant tasks such as tumor-stroma quantification and surgical\nmargin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it\noutperforms U-Net variants and transformer-based baselines. Code and Dataset\nwill be available at GitHub.\n","authors":["Ashkan Shakarami","Azade Farshad","Yousef Yeganeh","Lorenzo Nicole","Peter Schuffler","Stefano Ghidoni","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2507.12427v1.pdf","comment":"12 pages, 6 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.12425v1","updated":"2025-07-16T17:13:06Z","published":"2025-07-16T17:13:06Z","title":"Advancing Retrieval-Augmented Generation for Structured Enterprise and\n  Internal Data","summary":"  Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot\n","authors":["Chandana Cheerla"],"pdf_url":"https://arxiv.org/pdf/2507.12425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12378v1","updated":"2025-07-16T16:27:05Z","published":"2025-07-16T16:27:05Z","title":"Developing Visual Augmented Q&A System using Scalable Vision Embedding\n  Retrieval & Late Interaction Re-ranker","summary":"  Traditional information extraction systems face challenges with text only\nlanguage models as it does not consider infographics (visual elements of\ninformation) such as tables, charts, images etc. often used to convey complex\ninformation to readers. Multimodal LLM (MLLM) face challenges of finding needle\nin the haystack problem i.e., either longer context length or substantial\nnumber of documents as search space. Late interaction mechanism over visual\nlanguage models has shown state of the art performance in retrieval-based\nvision augmented Q&A tasks. There are yet few challenges using it for RAG based\nmulti-modal Q&A. Firstly, many popular and widely adopted vector databases do\nnot support native multi-vector retrieval. Secondly, late interaction requires\ncomputation which inflates space footprint and can hinder enterprise adoption.\nLastly, the current state of late interaction mechanism does not leverage the\napproximate neighbor search indexing methods for large speed ups in retrieval\nprocess. This paper explores a pragmatic approach to make vision retrieval\nprocess scalable and efficient without compromising on performance quality. We\npropose multi-step custom implementation utilizing widely adopted hybrid search\n(metadata & embedding) and state of the art late interaction re-ranker to\nretrieve best matching pages. Finally, MLLM are prompted as reader to generate\nanswers from contextualized best matching pages. Through experiments, we\nobserve that the proposed design is scalable (significant speed up) and stable\n(without degrading performance quality), hence can be used as production\nsystems at enterprises.\n","authors":["Rachna Saxena","Abhijeet Kumar","Suresh Shanmugam"],"pdf_url":"https://arxiv.org/pdf/2507.12378v1.pdf","comment":"Presented at NLP@IR workshop at SIGIR conference"},{"id":"http://arxiv.org/abs/2507.12311v1","updated":"2025-07-16T15:06:29Z","published":"2025-07-16T15:06:29Z","title":"An Ecosystem for Ontology Interoperability","summary":"  Ontology interoperability is one of the complicated issues that restricts the\nuse of ontologies in knowledge graphs (KGs). Different ontologies with\nconflicting and overlapping concepts make it difficult to design, develop, and\ndeploy an interoperable ontology for downstream tasks. We propose an ecosystem\nfor ontology interoperability. The ecosystem employs three state-of-the-art\nsemantic techniques in different phases of the ontology engineering life cycle:\nontology design patterns (ODPs) in the design phase, ontology matching and\nversioning (OM\\&OV) in the develop phase, and ontology-compliant knowledge\ngraphs (OCKGs) in the deploy phase, to achieve better ontology interoperability\nin real-world applications. A case study in the building domain validates the\nusefulness of the proposed ecosystem.\n","authors":["Zhangcheng Qiang"],"pdf_url":"https://arxiv.org/pdf/2507.12311v1.pdf","comment":"4 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.12242v1","updated":"2025-07-16T13:53:02Z","published":"2025-07-16T13:53:02Z","title":"Looking for Fairness in Recommender Systems","summary":"  Recommender systems can be found everywhere today, shaping our everyday\nexperience whenever we're consuming content, ordering food, buying groceries\nonline, or even just reading the news. Let's imagine we're in the process of\nbuilding a recommender system to make content suggestions to users on social\nmedia. When thinking about fairness, it becomes clear there are several\nperspectives to consider: the users asking for tailored suggestions, the\ncontent creators hoping for some limelight, and society at large, navigating\nthe repercussions of algorithmic recommendations. A shared fairness concern\nacross all three is the emergence of filter bubbles, a side-effect that takes\nplace when recommender systems are almost \"too good\", making recommendations so\ntailored that users become inadvertently confined to a narrow set of\nopinions/themes and isolated from alternative ideas. From the user's\nperspective, this is akin to manipulation. From the small content creator's\nperspective, this is an obstacle preventing them access to a whole range of\npotential fans. From society's perspective, the potential consequences are\nfar-reaching, influencing collective opinions, social behavior and political\ndecisions. How can our recommender system be fine-tuned to avoid the creation\nof filter bubbles, and ensure a more inclusive and diverse content landscape?\nApproaching this problem involves defining one (or more) performance metric to\nrepresent diversity, and tweaking our recommender system's performance through\nthe lens of fairness. By incorporating this metric into our evaluation\nframework, we aim to strike a balance between personalized recommendations and\nthe broader societal goal of fostering rich and varied cultures and points of\nview.\n","authors":["Cécile Logé"],"pdf_url":"https://arxiv.org/pdf/2507.12242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12202v1","updated":"2025-07-16T12:57:43Z","published":"2025-07-16T12:57:43Z","title":"Sparse Autoencoders for Sequential Recommendation Models: Interpretation\n  and Flexible Control","summary":"  Many current state-of-the-art models for sequential recommendations are based\non transformer architectures. Interpretation and explanation of such black box\nmodels is an important research question, as a better understanding of their\ninternals can help understand, influence, and control their behavior, which is\nvery important in a variety of real-world applications. Recently sparse\nautoencoders (SAE) have been shown to be a promising unsupervised approach for\nextracting interpretable features from language models. These autoencoders\nlearn to reconstruct hidden states of the transformer's internal layers from\nsparse linear combinations of directions in their activation space.\n  This paper is focused on the application of SAE to the sequential\nrecommendation domain. We show that this approach can be successfully applied\nto the transformer trained on a sequential recommendation task: learned\ndirections turn out to be more interpretable and monosemantic than the original\nhidden state dimensions. Moreover, we demonstrate that the features learned by\nSAE can be used to effectively and flexibly control the model's behavior,\nproviding end-users with a straightforward method to adjust their\nrecommendations to different custom scenarios and contexts.\n","authors":["Anton Klenitskiy","Konstantin Polev","Daria Denisova","Alexey Vasilev","Dmitry Simakov","Gleb Gusev"],"pdf_url":"https://arxiv.org/pdf/2507.12202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15740v3","updated":"2025-07-16T11:45:02Z","published":"2024-03-23T06:36:32Z","title":"Protecting Copyrighted Material with Unique Identifiers in Large\n  Language Model Training","summary":"  A primary concern regarding training large language models (LLMs) is whether\nthey abuse copyrighted online text. With the increasing training data scale and\nthe prevalence of LLMs in daily lives, two problems arise: \\textbf{1)} false\npositive membership inference results misled by similar examples; \\textbf{2)}\nmembership inference methods are usually too complex for end users to\nunderstand and use. To address these issues, we propose an alternative\n\\textit{insert-and-detect} methodology, advocating that web users and content\nplatforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{\\textbf{ghost\nsentences}} and a user-friendly last-$k$ words test, allowing end users to chat\nwith LLMs for membership inference. Ghost sentences consist primarily of unique\npassphrases of random natural words, which can come with customized elements to\nbypass possible filter rules. The last-$k$ words test requires a significant\nrepetition time of ghost sentences~($\\ge10$). For cases with fewer repetitions,\nwe designed an extra perplexity test, as LLMs exhibit high perplexity when\nencountering unnatural passphrases. We also conduct a comprehensive study on\nthe memorization and membership inference of ghost sentences, examining factors\nsuch as training data scales, model sizes, repetition times, insertion\npositions, wordlist of passphrases, alignment, \\textit{etc}. Our study shows\nthe possibility of applying ghost sentences in real scenarios and provides\ninstructions for the potential application.\n","authors":["Shuai Zhao","Linchao Zhu","Ruijie Quan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15740v3.pdf","comment":"A technical report, work mainly done in the early of 2024"},{"id":"http://arxiv.org/abs/2507.03556v2","updated":"2025-07-16T07:47:37Z","published":"2025-07-04T13:09:08Z","title":"A Multistakeholder Approach to Value-Driven Co-Design of Recommender\n  System Evaluation Metrics in Digital Archives","summary":"  This paper presents the first multistakeholder approach for translating\ndiverse stakeholder values into an evaluation metric setup for Recommender\nSystems (RecSys) in digital archives. While commercial platforms mainly rely on\nengagement metrics, cultural heritage domains require frameworks that balance\ncompeting priorities among archivists, platform owners, researchers, and other\nstakeholders. To address this challenge, we conducted high-profile focus groups\n(5 groups x 5 persons) with upstream, provider, system, consumer, and\ndownstream stakeholders, identifying value priorities across critical\ndimensions: visibility/representation, expertise adaptation, and\ntransparency/trust. Our analysis shows that stakeholder concerns naturally\nalign with four sequential research funnel stages: discovery, interaction,\nintegration, and impact. The resulting evaluation setup addresses\ndomain-specific challenges including collection representation imbalances,\nnon-linear research patterns, and tensions between specialized expertise and\nbroader accessibility. We propose directions for tailored metrics in each stage\nof this research journey, such as research path quality for discovery,\ncontextual appropriateness for interaction, metadata-weighted relevance for\nintegration, and cross-stakeholder value alignment for impact assessment. Our\ncontributions extend beyond digital archives to the broader RecSys community,\noffering transferable evaluation approaches for domains where value emerges\nthrough sustained engagement rather than immediate consumption.\n","authors":["Florian Atzenhofer-Baumgartner","Georg Vogeler","Dominik Kowald"],"pdf_url":"https://arxiv.org/pdf/2507.03556v2.pdf","comment":"Accepted at RecSys 2025"},{"id":"http://arxiv.org/abs/2411.13789v3","updated":"2025-07-16T07:09:05Z","published":"2024-11-21T02:22:35Z","title":"LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display\n  Advertisement Recommender System","summary":"  Display advertising provides significant value to advertisers, publishers,\nand users. Traditional display advertising systems utilize a multi-stage\narchitecture consisting of retrieval, coarse ranking, and final ranking.\nHowever, conventional retrieval methods rely on ID-based learning to rank\nmechanisms and fail to adequately utilize the content information of ads, which\nhampers their ability to provide diverse recommendation lists.\n  To address this limitation, we propose leveraging the extensive world\nknowledge of LLMs. However, three key challenges arise when attempting to\nmaximize the effectiveness of LLMs: \"How to capture user interests\", \"How to\nbridge the knowledge gap between LLMs and advertising system\", and \"How to\nefficiently deploy LLMs\". To overcome these challenges, we introduce a novel\nLLM-based framework called LLM Empowered Display ADvertisement REcommender\nsystem (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware\nPrompt Engineering introduces multi-faceted knowledge and designs intent-aware\n<Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users'\npersonal interests. (2) The Advertising-Specific Knowledge Alignment\nincorporates auxiliary fine-tuning tasks and Direct Preference Optimization\n(DPO) to align LLMs with ad semantic and business value. (3) The Efficient\nSystem Deployment deploys LEADRE in an online environment by integrating both\nlatency-tolerant and latency-sensitive service. Extensive offline experiments\ndemonstrate the effectiveness of LEADRE and validate the contributions of\nindividual modules. Online A/B test shows that LEADRE leads to a 1.57% and\n1.17% GMV lift for serviced users on WeChat Channels and Moments separately.\nLEADRE has been deployed on both platforms, serving tens of billions of\nrequests each day.\n","authors":["Fengxin Li","Yi Li","Yue Liu","Chao Zhou","Yuan Wang","Xiaoxiang Deng","Wei Xue","Dapeng Liu","Lei Xiao","Haijie Gu","Jie Jiang","Hongyan Liu","Biao Qin","Jun He"],"pdf_url":"https://arxiv.org/pdf/2411.13789v3.pdf","comment":"Accepted by VLDB 2025 Industrial Track"},{"id":"http://arxiv.org/abs/2501.13959v3","updated":"2025-07-16T04:56:27Z","published":"2025-01-21T06:32:25Z","title":"Learning an Effective Premise Retrieval Model for Efficient Mathematical\n  Formalization","summary":"  Formalized mathematics has recently garnered significant attention for its\nability to assist mathematicians across various fields. Premise retrieval, as a\ncommon step in mathematical formalization, has been a challenge, particularly\nfor inexperienced users. Existing retrieval methods that facilitate natural\nlanguage queries require a certain level of mathematical expertise from users,\nwhile approaches based on formal languages (e.g., Lean) typically struggle with\nthe scarcity of training data, hindering the training of effective and\ngeneralizable retrieval models. In this work, we introduce a novel method that\nleverages data extracted from Mathlib to train a lightweight and effective\npremise retrieval model. In particular, the proposed model embeds queries\n(i.e., proof state provided by Lean) and premises in a latent space, featuring\na tokenizer specifically trained on formal corpora. The model is learned in a\ncontrastive learning framework, in which a fine-grained similarity calculation\nmethod and a re-ranking module are applied to enhance the retrieval\nperformance. Experimental results demonstrate that our model outperforms\nexisting baselines, achieving higher accuracy while maintaining a lower\ncomputational load. We have released an open-source search engine based on our\nretrieval model at https://premise-search.com/. The source code and the trained\nmodel can be found at https://github.com/ruc-ai4math/Premise-Retrieval.\n","authors":["Yicheng Tao","Haotian Liu","Shanwen Wang","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2501.13959v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11911v1","updated":"2025-07-16T04:55:09Z","published":"2025-07-16T04:55:09Z","title":"AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG\n  Decoding","summary":"  Electroencephalogram (EEG) decoding models for brain-computer interfaces\n(BCIs) struggle with cross-dataset learning and generalization due to channel\nlayout inconsistencies, non-stationary signal distributions, and limited\nneurophysiological prior integration. To address these issues, we propose a\nplug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has\ntwo main components: 1) Spatial Alignment, which selects task-relevant channels\nbased on brain-region priors, aligns EEG distributions across domains, and\nremaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,\nwhich models multi-dataset signals into unified spatiotemporal patches for EEG\ndecoding. Compared to 17 state-of-the-art approaches that need dataset-specific\ntuning, the proposed calibration-free AFPM achieves performance gains of up to\n4.40% on motor imagery and 3.58% on event-related potential tasks. To our\nknowledge, this is the first calibration-free cross-dataset EEG decoding\nframework, substantially enhancing the practicalness of BCIs in real-world\napplications.\n","authors":["Xiaoqing Chen","Siyang Li","Dongrui Wu"],"pdf_url":"https://arxiv.org/pdf/2507.11911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11907v1","updated":"2025-07-16T04:46:28Z","published":"2025-07-16T04:46:28Z","title":"SIEVE: Effective Filtered Vector Search with Collection of Indexes","summary":"  Many real-world tasks such as recommending videos with the kids tag can be\nreduced to finding most similar vectors associated with hard predicates. This\ntask, filtered vector search, is challenging as prior state-of-the-art\ngraph-based (unfiltered) similarity search techniques quickly degenerate when\nhard constraints are considered. That is, effective graph-based filtered\nsimilarity search relies on sufficient connectivity for reaching the most\nsimilar items within just a few hops. To consider predicates, recent works\npropose modifying graph traversal to visit only the items that may satisfy\npredicates. However, they fail to offer the just-a-few-hops property for a wide\nrange of predicates: they must restrict predicates significantly or lose\nefficiency if only a small fraction of items satisfy predicates.\n  We propose an opposite approach: instead of constraining traversal, we build\nmany indexes each serving different predicate forms. For effective\nconstruction, we devise a three-dimensional analytical model capturing\nrelationships among index size, search time, and recall, with which we follow a\nworkload-aware approach to pack as many useful indexes as possible into a\ncollection. At query time, the analytical model is employed yet again to\ndiscern the one that offers the fastest search at a given recall. We show\nsuperior performance and support on datasets with varying selectivities and\nforms: our approach achieves up to 8.06x speedup while having as low as 1%\nbuild time versus other indexes, with less than 2.15x memory of a standard HNSW\ngraph and modest knowledge of past workloads.\n","authors":["Zhaoheng Li","Silu Huang","Wei Ding","Yongjoo Park","Jianjun Chen"],"pdf_url":"https://arxiv.org/pdf/2507.11907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11894v1","updated":"2025-07-16T04:21:46Z","published":"2025-07-16T04:21:46Z","title":"Context-Aware Search and Retrieval Over Erasure Channels","summary":"  This paper introduces and analyzes a search and retrieval model that adopts\nkey semantic communication principles from retrieval-augmented generation. We\nspecifically present an information-theoretic analysis of a remote document\nretrieval system operating over a symbol erasure channel. The proposed model\nencodes the feature vector of a query, derived from term-frequency weights of a\nlanguage corpus by using a repetition code with an adaptive rate dependent on\nthe contextual importance of the terms. At the decoder, we select between two\ndocuments based on the contextual closeness of the recovered query. By\nleveraging a jointly Gaussian approximation for both the true and reconstructed\nsimilarity scores, we derive an explicit expression for the retrieval error\nprobability, i.e., the probability under which the less similar document is\nselected. Numerical simulations on synthetic and real-world data (Google NQ)\nconfirm the validity of the analysis. They further demonstrate that assigning\ngreater redundancy to critical features effectively reduces the error rate,\nhighlighting the effectiveness of semantic-aware feature encoding in\nerror-prone communication settings.\n","authors":["Sara Ghasvarianjahromi","Yauhen Yakimenka","Jörg Kliewer"],"pdf_url":"https://arxiv.org/pdf/2507.11894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11866v1","updated":"2025-07-16T03:26:24Z","published":"2025-07-16T03:26:24Z","title":"Similarity-Guided Diffusion for Contrastive Sequential Recommendation","summary":"  In sequential recommendation systems, data augmentation and contrastive\nlearning techniques have recently been introduced using diffusion models to\nachieve robust representation learning. However, most of the existing\napproaches use random augmentation, which risk damaging the contextual\ninformation of the original sequence. Accordingly, we propose a\nSimilarity-Guided Diffusion for Contrastive Sequential Recommendation. Our\nmethod leverages the similarity between item embedding vectors to generate\nsemantically consistent noise. Moreover, we utilize high confidence score in\nthe denoising process to select our augmentation positions. This approach more\neffectively reflects contextual and structural information compared to\naugmentation at random positions. From a contrastive learning perspective, the\nproposed augmentation technique provides more discriminative positive and\nnegative samples, simultaneously improving training efficiency and\nrecommendation performance. Experimental results on five benchmark datasets\nshow that SimDiffRec outperforms the existing baseline models.\n","authors":["Jinkyeong Choi","Yejin Noh","Donghyeon Park"],"pdf_url":"https://arxiv.org/pdf/2507.11866v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.10543v2","updated":"2025-07-16T03:02:57Z","published":"2024-12-13T20:39:30Z","title":"METIS: Fast Quality-Aware RAG Systems with Configuration Adaptation","summary":"  RAG (Retrieval Augmented Generation) allows LLMs (large language models) to\ngenerate better responses with external knowledge, but using more external\nknowledge often improves generation quality at the expense of response delay.\nPrior work either reduces the response delay (through better scheduling of RAG\nqueries) or strives to maximize quality (which involves tuning the RAG\nworkflow), but they fall short in optimizing the tradeoff between the delay and\nquality of RAG responses. This paper presents METIS, the first RAG system that\njointly schedules queries and adapts the key RAG configurations of each query,\nsuch as the number of retrieved text chunks and synthesis methods, in order to\nbalance quality optimization and response delay reduction. Using 4 popular\nRAG-QA datasets, we show that compared with the state-of-the-art RAG\noptimization schemes, METIS reduces the generation latency by $1.64-2.54\\times$\nwithout sacrificing generation quality.\n","authors":["Siddhant Ray","Rui Pan","Zhuohan Gu","Kuntai Du","Shaoting Feng","Ganesh Ananthanarayanan","Ravi Netravali","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.10543v2.pdf","comment":"17 pages, 18 figures"},{"id":"http://arxiv.org/abs/2501.04652v2","updated":"2025-07-16T21:23:37Z","published":"2025-01-08T18:05:30Z","title":"Multi-task retriever fine-tuning for domain-specific and efficient RAG","summary":"  Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying\nLarge Language Models (LLMs), as it can address typical limitations such as\ngenerating hallucinated or outdated information. However, when building\nreal-world RAG applications, practical issues arise. First, the retrieved\ninformation is generally domain-specific. Since it is computationally expensive\nto fine-tune LLMs, it is more feasible to fine-tune the retriever to improve\nthe quality of the data included in the LLM input. Second, as more applications\nare deployed in the same real-world system, one cannot afford to deploy\nseparate retrievers. Moreover, these RAG applications normally retrieve\ndifferent kinds of data. Our solution is to instruction fine-tune a small\nretriever encoder on a variety of domain-specific tasks to allow us to deploy\none encoder that can serve many use cases, thereby achieving low-cost,\nscalability, and speed. We show how this encoder generalizes to out-of-domain\nsettings as well as to an unseen retrieval task on real-world enterprise use\ncases.\n","authors":["Patrice Béchard","Orlando Marquez Ayala"],"pdf_url":"https://arxiv.org/pdf/2501.04652v2.pdf","comment":"7 pages, 2 figures. Accepted at Workshop on Structured Knowledge for\n  Large Language Models (SKnowLLM) at KDD 2025"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2507.12466v1","updated":"2025-07-16T17:59:45Z","published":"2025-07-16T17:59:45Z","title":"Language Models Improve When Pretraining Data Matches Target Tasks","summary":"  Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.\n","authors":["David Mizrahi","Anders Boesen Lindbo Larsen","Jesse Allardice","Suzie Petryk","Yuri Gorokhov","Jeffrey Li","Alex Fang","Josh Gardner","Tom Gunter","Afshin Dehghan"],"pdf_url":"https://arxiv.org/pdf/2507.12466v1.pdf","comment":"44 pages, 25 figures, 13 tables"},{"id":"http://arxiv.org/abs/2507.12464v1","updated":"2025-07-16T17:59:32Z","published":"2025-07-16T17:59:32Z","title":"CytoSAE: Interpretable Cell Embeddings for Hematology","summary":"  Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic\ninterpretability of transformer-based foundation models. Very recently, SAEs\nwere also adopted for the visual domain, enabling the discovery of visual\nconcepts and their patch-wise attribution to tokens in the transformer model.\nWhile a growing number of foundation models emerged for medical imaging, tools\nfor explaining their inferences are still lacking. In this work, we show the\napplicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder\nwhich is trained on over 40,000 peripheral blood single-cell images. CytoSAE\ngeneralizes to diverse and out-of-domain datasets, including bone marrow\ncytology, where it identifies morphologically relevant concepts which we\nvalidated with medical experts. Furthermore, we demonstrate scenarios in which\nCytoSAE can generate patient-specific and disease-specific concepts, enabling\nthe detection of pathognomonic cells and localized cellular abnormalities at\nthe patch level. We quantified the effect of concepts on a patient-level AML\nsubtype classification task and show that CytoSAE concepts reach performance\ncomparable to the state-of-the-art, while offering explainability on the\nsub-cellular level. Source code and model weights are available at\nhttps://github.com/dynamical-inference/cytosae.\n","authors":["Muhammed Furkan Dasdelen","Hyesu Lim","Michele Buck","Katharina S. Götze","Carsten Marr","Steffen Schneider"],"pdf_url":"https://arxiv.org/pdf/2507.12464v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.07615v2","updated":"2025-07-16T17:59:28Z","published":"2025-05-12T14:36:47Z","title":"Diffused Responsibility: Analyzing the Energy Consumption of Generative\n  Text-to-Audio Diffusion Models","summary":"  Text-to-audio models have recently emerged as a powerful technology for\ngenerating sound from textual descriptions. However, their high computational\ndemands raise concerns about energy consumption and environmental impact. In\nthis paper, we conduct an analysis of the energy usage of 7 state-of-the-art\ntext-to-audio diffusion-based generative models, evaluating to what extent\nvariations in generation parameters affect energy consumption at inference\ntime. We also aim to identify an optimal balance between audio quality and\nenergy consumption by considering Pareto-optimal solutions across all selected\nmodels. Our findings provide insights into the trade-offs between performance\nand environmental impact, contributing to the development of more efficient\ngenerative audio models.\n","authors":["Riccardo Passoni","Francesca Ronchini","Luca Comanducci","Romain Serizel","Fabio Antonacci"],"pdf_url":"https://arxiv.org/pdf/2505.07615v2.pdf","comment":"Accepted at WASPAA 2025"},{"id":"http://arxiv.org/abs/2507.12453v1","updated":"2025-07-16T17:54:14Z","published":"2025-07-16T17:54:14Z","title":"Cost-aware Stopping for Bayesian Optimization","summary":"  In automated machine learning, scientific discovery, and other applications\nof Bayesian optimization, deciding when to stop evaluating expensive black-box\nfunctions is an important practical consideration. While several adaptive\nstopping rules have been proposed, in the cost-aware setting they lack\nguarantees ensuring they stop before incurring excessive function evaluation\ncosts. We propose a cost-aware stopping rule for Bayesian optimization that\nadapts to varying evaluation costs and is free of heuristic tuning. Our rule is\ngrounded in a theoretical connection to state-of-the-art cost-aware acquisition\nfunctions, namely the Pandora's Box Gittins Index (PBGI) and log expected\nimprovement per cost. We prove a theoretical guarantee bounding the expected\ncumulative evaluation cost incurred by our stopping rule when paired with these\ntwo acquisition functions. In experiments on synthetic and empirical tasks,\nincluding hyperparameter optimization and neural architecture size search, we\nshow that combining our stopping rule with the PBGI acquisition function\nconsistently matches or outperforms other acquisition-function--stopping-rule\npairs in terms of cost-adjusted simple regret, a metric capturing trade-offs\nbetween solution quality and cumulative evaluation cost.\n","authors":["Qian Xie","Linda Cai","Alexander Terenin","Peter I. Frazier","Ziv Scully"],"pdf_url":"https://arxiv.org/pdf/2507.12453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10438v3","updated":"2025-07-16T17:48:35Z","published":"2024-11-15T18:57:39Z","title":"MARS: Unleashing the Power of Variance Reduction for Training Large\n  Models","summary":"  Training deep neural networks--and more recently, large models demands\nefficient and scalable optimizers. Adaptive gradient algorithms like Adam,\nAdamW, and their variants have been central to this task. Despite the\ndevelopment of numerous variance reduction algorithms in the past decade aimed\nat accelerating stochastic optimization in both convex and nonconvex settings,\nvariance reduction has not found widespread success in training deep neural\nnetworks or large language models. Consequently, it has remained a less favored\napproach in modern AI. In this paper, to unleash the power of variance\nreduction for efficient training of large models, we propose a unified\noptimization framework, MARS (Make vAriance Reduction Shine), which reconciles\npreconditioned gradient methods with variance reduction via a scaled stochastic\nrecursive momentum technique. Within our framework, we introduce three\ninstances of MARS that leverage preconditioned gradient updates based on AdamW,\nLion, and Shampoo, respectively. We also draw a connection between our\nalgorithms and existing optimizers. Experimental results on training GPT-2\nmodels indicate that MARS consistently outperforms AdamW by a large margin. The\nimplementation of MARS is available at https://github.com/AGI-Arena/MARS.\n","authors":["Huizhuo Yuan","Yifeng Liu","Shuang Wu","Xun Zhou","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2411.10438v3.pdf","comment":"35 pages, 19 figures, 12 tables"},{"id":"http://arxiv.org/abs/2507.12451v1","updated":"2025-07-16T17:47:45Z","published":"2025-07-16T17:47:45Z","title":"S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling","summary":"  Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.\n","authors":["Suman Adhya","Debarshi Kumar Sanyal"],"pdf_url":"https://arxiv.org/pdf/2507.12451v1.pdf","comment":"Accepted as a long paper for ACL 2025 main conference"},{"id":"http://arxiv.org/abs/2502.09724v2","updated":"2025-07-16T17:43:06Z","published":"2025-02-13T19:13:55Z","title":"Navigating the Social Welfare Frontier: Portfolios for Multi-objective\n  Reinforcement Learning","summary":"  In many real-world applications of reinforcement learning (RL), deployed\npolicies have varied impacts on different stakeholders, creating challenges in\nreaching consensus on how to effectively aggregate their preferences.\nGeneralized $p$-means form a widely used class of social welfare functions for\nthis purpose, with broad applications in fair resource allocation, AI\nalignment, and decision-making. This class includes well-known welfare\nfunctions such as Egalitarian, Nash, and Utilitarian welfare. However,\nselecting the appropriate social welfare function is challenging for\ndecision-makers, as the structure and outcomes of optimal policies can be\nhighly sensitive to the choice of $p$. To address this challenge, we study the\nconcept of an $\\alpha$-approximate portfolio in RL, a set of policies that are\napproximately optimal across the family of generalized $p$-means for all $p \\in\n[-\\infty, 1]$. We propose algorithms to compute such portfolios and provide\ntheoretical guarantees on the trade-offs among approximation factor, portfolio\nsize, and computational efficiency. Experimental results on synthetic and\nreal-world datasets demonstrate the effectiveness of our approach in\nsummarizing the policy space induced by varying $p$ values, empowering\ndecision-makers to navigate this landscape more effectively.\n","authors":["Cheol Woo Kim","Jai Moondra","Shresth Verma","Madeleine Pollack","Lingkai Kong","Milind Tambe","Swati Gupta"],"pdf_url":"https://arxiv.org/pdf/2502.09724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09730v5","updated":"2025-07-16T17:29:38Z","published":"2023-08-17T19:12:32Z","title":"The Utility of the Virtual Imaging Trials Methodology for Objective\n  Characterization of AI Systems and Training Data","summary":"  Purpose: The credibility of Artificial Intelligence (AI) models for medical\nimaging continues to be a challenge, affected by the diversity of models, the\ndata used to train the models, and applicability of their combination to\nproduce reproducible results for new data. Approach: In this work we aimed to\nexplore if the emerging Virtual Imaging Trials (VIT) methodologies can provide\nan objective resource to approach this challenge. The study was conducted for\nthe case example of COVID-19 diagnosis using clinical and virtual computed\ntomography (CT) and chest radiography (CXR) processed with convolutional neural\nnetworks. Multiple AI models were developed and tested using 3D ResNet-like and\n2D EfficientNetv2 architectures across diverse datasets. Results: The\nperformance differences were evaluated in terms of the area under the curve\n(AUC) and the DeLong method for AUC confidence intervals. The models trained on\nthe most diverse datasets showed the highest external testing performance, with\nAUC values ranging from 0.73-0.76 for CT and 0.70-0.73 for CXR. Internal\ntesting yielded higher AUC values (0.77 -0.85 for CT and 0.77-1.0 for CXR),\nhighlighting a substantial drop in performance during external validation,\nwhich underscores the importance of diverse and comprehensive training and\ntesting data. Most notably, VIT approach provided objective assessment of the\nutility of diverse models and datasets while further providing insight into the\ninfluence of dataset characteristics, patient factors, and imaging physics on\nAI efficacy. Conclusions: The VIT approach can be used to enhance model\ntransparency and reliability, offering nuanced insights into the factors\ndriving AI performance and bridging the gap between experimental and clinical\nsettings.\n","authors":["Fakrul Islam Tushar","Lavsen Dahal","Saman Sotoudeh-Paima","Ehsan Abadi","W. Paul Segars","Ehsan Samei","Joseph Y. Lo"],"pdf_url":"https://arxiv.org/pdf/2308.09730v5.pdf","comment":"8 figures, 4 Tables"},{"id":"http://arxiv.org/abs/2507.12442v1","updated":"2025-07-16T17:28:40Z","published":"2025-07-16T17:28:40Z","title":"Characterizing State Space Model (SSM) and SSM-Transformer Hybrid\n  Language Model Performance with Long Context Length","summary":"  The demand for machine intelligence capable of processing continuous,\nlong-context inputs on local devices is growing rapidly. However, the quadratic\ncomplexity and memory requirements of traditional Transformer architectures\nmake them inefficient and often unusable for these tasks. This has spurred a\nparadigm shift towards new architectures like State Space Models (SSMs) and\nhybrids, which promise near-linear scaling. While most current research focuses\non the accuracy and theoretical throughput of these models, a systematic\nperformance characterization on practical consumer hardware is critically\nneeded to guide system-level optimization and unlock new applications.\n  To address this gap, we present a comprehensive, comparative benchmarking of\ncarefully selected Transformer, SSM, and hybrid models specifically for\nlong-context inference on consumer and embedded GPUs. Our analysis reveals that\nSSMs are not only viable but superior for this domain, capable of processing\nsequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than\ncomparable Transformers. While Transformers may be up to 1.8x faster at short\nsequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x\nfaster at very long contexts (~57K tokens). Our operator-level analysis reveals\nthat custom, hardware-aware SSM kernels dominate the inference runtime,\naccounting for over 55% of latency on edge platforms, identifying them as a\nprimary target for future hardware acceleration. We also provide detailed,\ndevice-specific characterization results to guide system co-design for the\nedge. To foster further research, we will open-source our characterization\nframework.\n","authors":["Saptarshi Mitra","Rachid Karami","Haocheng Xu","Sitao Huang","Hyoukjun Kwon"],"pdf_url":"https://arxiv.org/pdf/2507.12442v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2507.12441v1","updated":"2025-07-16T17:28:19Z","published":"2025-07-16T17:28:19Z","title":"Describe Anything Model for Visual Question Answering on Text-rich\n  Images","summary":"  Recent progress has been made in region-aware vision-language modeling,\nparticularly with the emergence of the Describe Anything Model (DAM). DAM is\ncapable of generating detailed descriptions of any specific image areas or\nobjects without the need for additional localized image-text alignment\nsupervision. We hypothesize that such region-level descriptive capability is\nbeneficial for the task of Visual Question Answering (VQA), especially in\nchallenging scenarios involving images with dense text. In such settings, the\nfine-grained extraction of textual information is crucial to producing correct\nanswers. Motivated by this, we introduce DAM-QA, a framework with a tailored\nevaluation protocol, developed to investigate and harness the region-aware\ncapabilities from DAM for the text-rich VQA problem that requires reasoning\nover text-based information within images. DAM-QA incorporates a mechanism that\naggregates answers from multiple regional views of image content, enabling more\neffective identification of evidence that may be tied to text-related elements.\nExperiments on six VQA benchmarks show that our approach consistently\noutperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA\nalso achieves the best overall performance among region-aware models with fewer\nparameters, significantly narrowing the gap with strong generalist VLMs. These\nresults highlight the potential of DAM-like models for text-rich and broader\nVQA tasks when paired with efficient usage and integration strategies. Our code\nis publicly available at https://github.com/Linvyl/DAM-QA.git.\n","authors":["Yen-Linh Vu","Dinh-Thang Duong","Truong-Binh Duong","Anh-Khoi Nguyen","Thanh-Huy Nguyen","Le Thien Phuc Nguyen","Jianhua Xing","Xingjian Li","Tianyang Wang","Ulas Bagci","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2507.12441v1.pdf","comment":"11 pages, 5 figures. Accepted to VisionDocs @ ICCV 2025"},{"id":"http://arxiv.org/abs/2501.13916v3","updated":"2025-07-16T17:27:54Z","published":"2025-01-23T18:53:43Z","title":"PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy","summary":"  We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),\na communication-efficient Vertical Federated Learning algorithm with\nDifferential Privacy guarantees. PBM-VFL combines Secure Multi-Party\nComputation with the recently introduced Poisson Binomial Mechanism to protect\nparties' private datasets during model training. We define the novel concept of\nfeature privacy and analyze end-to-end feature and sample privacy of our\nalgorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We\nalso provide the first theoretical characterization of the relationship between\nprivacy budget, convergence error, and communication cost in\ndifferentially-private VFL. Finally, we empirically show that our model\nperforms well with high levels of privacy.\n","authors":["Linh Tran","Timothy Castiglia","Stacy Patterson","Ana Milanova"],"pdf_url":"https://arxiv.org/pdf/2501.13916v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12440v1","updated":"2025-07-16T17:27:44Z","published":"2025-07-16T17:27:44Z","title":"EgoVLA: Learning Vision-Language-Action Models from Egocentric Human\n  Videos","summary":"  Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Isaac\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA\n","authors":["Ruihan Yang","Qinxi Yu","Yecheng Wu","Rui Yan","Borui Li","An-Chieh Cheng","Xueyan Zou","Yunhao Fang","Hongxu Yin","Sifei Liu","Song Han","Yao Lu","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2507.12440v1.pdf","comment":"More videos can be found on our website:\n  https://rchalyang.github.io/EgoVLA"},{"id":"http://arxiv.org/abs/2507.12439v1","updated":"2025-07-16T17:27:25Z","published":"2025-07-16T17:27:25Z","title":"A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning","summary":"  Federated learning (FL) enables collaborative model training across\ndecentralized clients while preserving data privacy. However, its\nopen-participation nature exposes it to data-poisoning attacks, in which\nmalicious actors submit corrupted model updates to degrade the global model.\nExisting defenses are often reactive, relying on statistical aggregation rules\nthat can be computationally expensive and that typically assume an honest\nmajority. This paper introduces a proactive, economic defense: a lightweight\nBayesian incentive mechanism that makes malicious behavior economically\nirrational. Each training round is modeled as a Bayesian game of incomplete\ninformation in which the server, acting as the principal, uses a small, private\nvalidation dataset to verify update quality before issuing payments. The design\nsatisfies Individual Rationality (IR) for benevolent clients, ensuring their\nparticipation is profitable, and Incentive Compatibility (IC), making poisoning\nan economically dominated strategy. Extensive experiments on non-IID partitions\nof MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping\nadversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3\npercentage points lower than in a scenario with 30% label-flipping adversaries.\nThis outcome is 51.7 percentage points better than standard FedAvg, which\ncollapses under the same 50% attack. The mechanism is computationally light,\nbudget-bounded, and readily integrates into existing FL frameworks, offering a\npractical route to economically robust and sustainable FL ecosystems.\n","authors":["Daniel Commey","Rebecca A. Sarpong","Griffith S. Klogo","Winful Bagyl-Bac","Garth V. Crosby"],"pdf_url":"https://arxiv.org/pdf/2507.12439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12435v1","updated":"2025-07-16T17:24:06Z","published":"2025-07-16T17:24:06Z","title":"Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal\n  Inference in Neural Networks","summary":"  Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets.\n","authors":["Yi Li","David Mccoy","Nolan Gunter","Kaitlyn Lee","Alejandro Schuler","Mark van der Laan"],"pdf_url":"https://arxiv.org/pdf/2507.12435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17066v2","updated":"2025-07-16T17:16:15Z","published":"2025-02-24T11:28:00Z","title":"DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth\n  Observation Applications","summary":"  Significant efforts have been directed towards adapting self-supervised\nmultimodal learning for Earth observation applications. However, most current\nmethods produce coarse patch-sized embeddings, limiting their effectiveness and\nintegration with other modalities like LiDAR. To close this gap, we present\nDUNIA, an approach to learn pixel-sized embeddings through cross-modal\nalignment between images and full-waveform LiDAR data. As the model is trained\nin a contrastive manner, the embeddings can be directly leveraged in the\ncontext of a variety of environmental monitoring tasks in a zero-shot setting.\nIn our experiments, we demonstrate the effectiveness of the embeddings for\nseven such tasks: canopy height mapping, fractional canopy cover, land cover\nmapping, tree species identification, plant area index, crop type\nclassification, and per-pixel waveform-based vertical structure mapping. The\nresults show that the embeddings, along with zero-shot classifiers, often\noutperform specialized supervised models, even in low-data regimes. In the\nfine-tuning setting, we show strong performances near or better than the\nstate-of-the-art on five out of six tasks.\n","authors":["Ibrahim Fayad","Max Zimmer","Martin Schwartz","Fabian Gieseke","Philippe Ciais","Gabriel Belouze","Sarah Brood","Aurelien De Truchis","Alexandre d'Aspremont"],"pdf_url":"https://arxiv.org/pdf/2502.17066v2.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.12428v1","updated":"2025-07-16T17:16:03Z","published":"2025-07-16T17:16:03Z","title":"Can We Predict Alignment Before Models Finish Thinking? Towards\n  Monitoring Misaligned Reasoning Models","summary":"  Open-weights reasoning language models generate long chains-of-thought (CoTs)\nbefore producing a final response, which improves performance but introduces\nadditional alignment risks, with harmful content often appearing in both the\nCoTs and the final outputs. In this work, we investigate if we can use CoTs to\npredict final response misalignment. We evaluate a range of monitoring\napproaches, including humans, highly-capable large language models, and text\nclassifiers, using either CoT text or activations. First, we find that a simple\nlinear probe trained on CoT activations can significantly outperform all\ntext-based methods in predicting whether a final response will be safe or\nunsafe. CoT texts are often unfaithful and can mislead humans and classifiers,\nwhile model latents (i.e., CoT activations) offer a more reliable predictive\nsignal. Second, the probe makes accurate predictions before reasoning\ncompletes, achieving strong performance even when applied to early CoT\nsegments. These findings generalize across model sizes, families, and safety\nbenchmarks, suggesting that lightweight probes could enable real-time safety\nmonitoring and early intervention during generation.\n","authors":["Yik Siu Chan","Zheng-Xin Yong","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2507.12428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12427v1","updated":"2025-07-16T17:15:18Z","published":"2025-07-16T17:15:18Z","title":"Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature\n  Representation","summary":"  We propose UTS, a unit-based tissue segmentation framework for histopathology\nthat classifies each fixed-size 32 * 32 tile, rather than each pixel, as the\nsegmentation unit. This approach reduces annotation effort and improves\ncomputational efficiency without compromising accuracy. To implement this\napproach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits\nthe multi-level feature representation to capture both fine-grained morphology\nand global tissue context. Trained to segment breast tissue into three\ncategories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports\nclinically relevant tasks such as tumor-stroma quantification and surgical\nmargin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it\noutperforms U-Net variants and transformer-based baselines. Code and Dataset\nwill be available at GitHub.\n","authors":["Ashkan Shakarami","Azade Farshad","Yousef Yeganeh","Lorenzo Nicole","Peter Schuffler","Stefano Ghidoni","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2507.12427v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.12419v1","updated":"2025-07-16T17:08:46Z","published":"2025-07-16T17:08:46Z","title":"Mixture of Raytraced Experts","summary":"  We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts\n(MoE) architecture which can dynamically select sequences of experts, producing\ncomputational graphs of variable width and depth. Existing MoE architectures\ngenerally require a fixed amount of computation for a given sample. Our\napproach, in contrast, yields predictions with increasing accuracy as the\ncomputation cycles through the experts' sequence. We train our model by\niteratively sampling from a set of candidate experts, unfolding the sequence\nakin to how Recurrent Neural Networks are trained. Our method does not require\nload-balancing mechanisms, and preliminary experiments show a reduction in\ntraining epochs of 10\\% to 40\\% with a comparable/higher accuracy. These\nresults point to new research directions in the field of MoEs, allowing the\ndesign of potentially faster and more expressive models. The code is available\nat https://github.com/nutig/RayTracing\n","authors":["Andrea Perin","Giacomo Lagomarsini","Claudio Gallicchio","Giuseppe Nuti"],"pdf_url":"https://arxiv.org/pdf/2507.12419v1.pdf","comment":"Preliminary version (pre-submission)"},{"id":"http://arxiv.org/abs/2507.12414v1","updated":"2025-07-16T17:04:49Z","published":"2025-07-16T17:04:49Z","title":"AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models","summary":"  Training of autonomous driving systems requires extensive datasets with\nprecise annotations to attain robust performance. Human annotations suffer from\nimperfections, and multiple iterations are often needed to produce high-quality\ndatasets. However, manually reviewing large datasets is laborious and\nexpensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)\nframework and investigate the utilization of Vision-Language Models (VLMs) to\nautomatically identify erroneous annotations in vision datasets, thereby\nenabling users to eliminate these errors and enhance data quality. We validate\nour approach using the KITTI and nuImages datasets, which contain object\ndetection benchmarks for autonomous driving. To test the effectiveness of\nAutoVDC, we create dataset variants with intentionally injected erroneous\nannotations and observe the error detection rate of our approach. Additionally,\nwe compare the detection rates using different VLMs and explore the impact of\nVLM fine-tuning on our pipeline. The results demonstrate our method's high\nperformance in error detection and data cleaning experiments, indicating its\npotential to significantly improve the reliability and accuracy of large-scale\nproduction datasets in autonomous driving.\n","authors":["Santosh Vasa","Aditi Ramadwar","Jnana Rama Krishna Darabattula","Md Zafar Anwar","Stanislaw Antol","Andrei Vatavu","Thomas Monninger","Sihao Ding"],"pdf_url":"https://arxiv.org/pdf/2507.12414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12189v2","updated":"2025-07-16T17:04:43Z","published":"2025-01-21T14:50:19Z","title":"MirrorCBO: A consensus-based optimization method in the spirit of mirror\n  descent","summary":"  In this work we propose MirrorCBO, a consensus-based optimization (CBO)\nmethod which generalizes standard CBO in the same way that mirror descent\ngeneralizes gradient descent. For this we apply the CBO methodology to a swarm\nof dual particles and retain the primal particle positions by applying the\ninverse of the mirror map, which we parametrize as the subdifferential of a\nstrongly convex function $\\phi$. In this way, we combine the advantages of a\nderivative-free non-convex optimization algorithm with those of mirror descent.\nAs a special case, the method extends CBO to optimization problems with convex\nconstraints. Assuming bounds on the Bregman distance associated to $\\phi$, we\nprovide asymptotic convergence results for MirrorCBO with explicit exponential\nrate. Another key contribution is an exploratory numerical study of this new\nalgorithm across different application settings, focusing on (i)\nsparsity-inducing optimization, and (ii) constrained optimization,\ndemonstrating the competitive performance of MirrorCBO. We observe empirically\nthat the method can also be used for optimization on (non-convex) submanifolds\nof Euclidean space, can be adapted to mirrored versions of other recent CBO\nvariants, and that it inherits from mirror descent the capability to select\ndesirable minimizers, like sparse ones. We also include an overview of recent\nCBO approaches for constrained optimization and compare their performance to\nMirrorCBO.\n","authors":["Leon Bungert","Franca Hoffmann","Dohyeon Kim","Tim Roith"],"pdf_url":"https://arxiv.org/pdf/2501.12189v2.pdf","comment":"66 pages, 18 figures, 19 tables"},{"id":"http://arxiv.org/abs/2507.12412v1","updated":"2025-07-16T17:00:41Z","published":"2025-07-16T17:00:41Z","title":"NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal\n  Data","summary":"  In many critical applications, resource constraints limit the amount of\ninformation that can be gathered to make predictions. For example, in\nhealthcare, patient data often spans diverse features ranging from lab tests to\nimaging studies. Each feature may carry different information and must be\nacquired at a respective cost of time, money, or risk to the patient. Moreover,\ntemporal prediction tasks, where both instance features and labels evolve over\ntime, introduce additional complexity in deciding when or what information is\nimportant. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff\nAcquisition method that sequentially acquires the most informative features at\ninference time while accounting for both temporal dynamics and acquisition\ncost. We first introduce a cohesive estimation target for our NOCTA setting,\nand then develop two complementary estimators: 1) a non-parametric method based\non nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric\nmethod that directly predicts the utility of potential acquisitions (NOCTA-P).\nExperiments on synthetic and real-world medical datasets demonstrate that both\nNOCTA variants outperform existing baselines.\n","authors":["Dzung Dinh","Boqi Chen","Marc Niethammer","Junier Oliva"],"pdf_url":"https://arxiv.org/pdf/2507.12412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08218v2","updated":"2025-07-16T16:57:48Z","published":"2025-07-10T23:47:05Z","title":"Simple Mechanistic Explanations for Out-Of-Context Reasoning","summary":"  Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs\nexhibit surprisingly deep out-of-distribution generalization. Rather than\nlearning shallow heuristics, they implicitly internalize and act on the\nconsequences of observations scattered throughout the fine-tuning data. In this\nwork, we investigate this phenomenon mechanistically and find that many\ninstances of OOCR in the literature have a simple explanation: the LoRA\nfine-tuning essentially adds a constant steering vector, steering the model\ntowards a general concept. This improves performance on the fine-tuning task\nand in many other concept-related domains, causing the surprising\ngeneralization. Moreover, we can directly train steering vectors for these\ntasks from scratch, which also induces OOCR. We find that our results hold even\nfor a task that seems like it must involve conditional behavior (model\nbackdoors); it turns out that unconditionally adding a steering vector is\nsufficient. Overall, our work presents one explanation of what gets learned\nduring fine-tuning for OOCR tasks, contributing to the key question of why LLMs\ncan reason out of context, an advanced capability that is highly relevant to\ntheir safe and reliable deployment.\n","authors":["Atticus Wang","Joshua Engels","Oliver Clive-Griffin","Senthooran Rajamanoharan","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2507.08218v2.pdf","comment":"ICML 2025 Workshop R2-FM"},{"id":"http://arxiv.org/abs/2503.23175v2","updated":"2025-07-16T16:49:21Z","published":"2025-03-29T18:09:36Z","title":"Large Language Models are Unreliable for Cyber Threat Intelligence","summary":"  Several recent works have argued that Large Language Models (LLMs) can be\nused to tame the data deluge in the cybersecurity field, by improving the\nautomation of Cyber Threat Intelligence (CTI) tasks. This work presents an\nevaluation methodology that other than allowing to test LLMs on CTI tasks when\nusing zero-shot learning, few-shot learning and fine-tuning, also allows to\nquantify their consistency and their confidence level. We run experiments with\nthree state-of-the-art LLMs and a dataset of 350 threat intelligence reports\nand present new evidence of potential security risks in relying on LLMs for\nCTI. We show how LLMs cannot guarantee sufficient performance on real-size\nreports while also being inconsistent and overconfident. Few-shot learning and\nfine-tuning only partially improve the results, thus posing doubts about the\npossibility of using LLMs for CTI scenarios, where labelled datasets are\nlacking and where confidence is a fundamental factor.\n","authors":["Emanuele Mezzi","Fabio Massacci","Katja Tuma"],"pdf_url":"https://arxiv.org/pdf/2503.23175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12404v1","updated":"2025-07-16T16:47:38Z","published":"2025-07-16T16:47:38Z","title":"Neural Network-Guided Symbolic Regression for Interpretable Descriptor\n  Discovery in Perovskite Catalysts","summary":"  Understanding and predicting the activity of oxide perovskite catalysts for\nthe oxygen evolution reaction (OER) requires descriptors that are both accurate\nand physically interpretable. While symbolic regression (SR) offers a path to\ndiscover such formulas, its performance degrades with high-dimensional inputs\nand small datasets. We present a two-phase framework that combines neural\nnetworks (NN), feature importance analysis, and symbolic regression (SR) to\ndiscover interpretable descriptors for OER activity in oxide perovskites. In\nPhase I, using a small dataset and seven structural features, we reproduce and\nimprove the known {\\mu}/t descriptor by engineering composite features and\napplying symbolic regression, achieving training and validation MAEs of 22.8\nand 20.8 meV, respectively. In Phase II, we expand to 164 features, reduce\ndimensionality, and identify LUMO energy as a key electronic descriptor. A\nfinal formula using {\\mu}/t, {\\mu}/RA, and LUMO energy achieves improved\naccuracy (training and validation MAEs of 22.1 and 20.6 meV) with strong\nphysical interpretability. Our results demonstrate that NN-guided symbolic\nregression enables accurate, interpretable, and physically meaningful\ndescriptor discovery in data-scarce regimes, indicating interpretability need\nnot sacrifice accuracy for materials informatics.\n","authors":["Yeming Xian","Xiaoming Wang","Yanfa Yan"],"pdf_url":"https://arxiv.org/pdf/2507.12404v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2507.12399v1","updated":"2025-07-16T16:44:29Z","published":"2025-07-16T16:44:29Z","title":"ROC-n-reroll: How verifier imperfection affects test-time scaling","summary":"  Test-time scaling aims to improve language model performance by leveraging\nadditional compute during inference. While many works have empirically studied\ntechniques like Best-of-N (BoN) and rejection sampling that make use of a\nverifier to enable test-time scaling, there is little theoretical understanding\nof how verifier imperfection affects performance. In this work, we address this\ngap. Specifically, we prove how instance-level accuracy of these methods is\nprecisely characterized by the geometry of the verifier's ROC curve.\nInterestingly, while scaling is determined by the local geometry of the ROC\ncurve for rejection sampling, it depends on global properties of the ROC curve\nfor BoN. As a consequence when the ROC curve is unknown, it is impossible to\nextrapolate the performance of rejection sampling based on the low-compute\nregime. Furthermore, while rejection sampling outperforms BoN for fixed\ncompute, in the infinite-compute limit both methods converge to the same level\nof accuracy, determined by the slope of the ROC curve near the origin. Our\ntheoretical results are confirmed by experiments on GSM8K using different\nversions of Llama and Qwen to generate and verify solutions.\n","authors":["Florian E. Dorner","Yatong Chen","André F. Cruz","Fanny Yang"],"pdf_url":"https://arxiv.org/pdf/2507.12399v1.pdf","comment":"35 pages, 9 Figures"},{"id":"http://arxiv.org/abs/2504.03205v2","updated":"2025-07-16T16:36:00Z","published":"2025-04-04T06:29:29Z","title":"BondMatcher: H-Bond Stability Analysis in Molecular Systems","summary":"  This application paper investigates the stability of hydrogen bonds\n(H-bonds), as characterized by the Quantum Theory of Atoms in Molecules\n(QTAIM). First, we contribute a database of 4544 electron densities associated\nto four isomers of water hexamers (the so-called Ring, Book, Cage and Prism),\ngenerated by distorting their equilibrium geometry under various structural\nperturbations, modeling the natural dynamic behavior of molecular systems.\nSecond, we present a new stability measure, called bond occurrence rate,\nassociating each bond path present at equilibrium with its rate of occurrence\nwithin the input ensemble. We also provide an algorithm, called BondMatcher,\nfor its automatic computation, based on a tailored, geometry-aware partial\nisomorphism estimation between the extremum graphs of the considered electron\ndensities. Our new stability measure allows for the automatic identification of\ndensities lacking H-bond paths, enabling further visual inspections.\nSpecifically, the topological analysis enabled by our framework corroborates\nexperimental observations and provides refined geometrical criteria for\ncharacterizing the disappearance of H-bond paths. Our electron density database\nand our C++ implementation are available at this address:\nhttps://github.com/thom-dani/BondMatcher.\n","authors":["Thomas Daniel","Malgorzata Olejniczak","Julien Tierny"],"pdf_url":"https://arxiv.org/pdf/2504.03205v2.pdf","comment":"To appear in IEEE Transactions on Visualization and Computer Graphics\n  (Proc. of IEEE VIS 2025)"},{"id":"http://arxiv.org/abs/2507.12384v1","updated":"2025-07-16T16:31:20Z","published":"2025-07-16T16:31:20Z","title":"Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog\n  CAM with Inherent Soft Boundaries","summary":"  The rapid advancement of artificial intelligence has raised concerns\nregarding its trustworthiness, especially in terms of interpretability and\nrobustness. Tree-based models like Random Forest and XGBoost excel in\ninterpretability and accuracy for tabular data, but scaling them remains\ncomputationally expensive due to poor data locality and high data dependence.\nPrevious efforts to accelerate these models with analog content addressable\nmemory (CAM) have struggled, due to the fact that the difficult-to-implement\nsharp decision boundaries are highly susceptible to device variations, which\nleads to poor hardware performance and vulnerability to adversarial attacks.\nThis work presents a novel hardware-software co-design approach using $MoS_2$\nFlash-based analog CAM with inherent soft boundaries, enabling efficient\ninference with soft tree-based models. Our soft tree model inference\nexperiments on $MoS_2$ analog CAM arrays show this method achieves exceptional\nrobustness against device variation and adversarial attacks while achieving\nstate-of-the-art accuracy. Specifically, our fabricated analog CAM arrays\nachieve $96\\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,\nwhile maintaining decision explainability. Our experimentally calibrated model\nvalidated only a $0.6\\%$ accuracy drop on the MNIST dataset under $10\\%$ device\nthreshold variation, compared to a $45.3\\%$ drop for traditional decision\ntrees. This work paves the way for specialized hardware that enhances AI's\ntrustworthiness and efficiency.\n","authors":["Bo Wen","Guoyun Gao","Zhicheng Xu","Ruibin Mao","Xiaojuan Qi","X. Sharon Hu","Xunzhao Yin","Can Li"],"pdf_url":"https://arxiv.org/pdf/2507.12384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12383v1","updated":"2025-07-16T16:31:17Z","published":"2025-07-16T16:31:17Z","title":"Improving Reinforcement Learning Sample-Efficiency using Local\n  Approximation","summary":"  In this study, we derive Probably Approximately Correct (PAC) bounds on the\nasymptotic sample-complexity for RL within the infinite-horizon Markov Decision\nProcess (MDP) setting that are sharper than those in existing literature. The\npremise of our study is twofold: firstly, the further two states are from each\nother, transition-wise, the less relevant the value of the first state is when\nlearning the $\\epsilon$-optimal value of the second; secondly, the amount of\n'effort', sample-complexity-wise, expended in learning the $\\epsilon$-optimal\nvalue of a state is independent of the number of samples required to learn the\n$\\epsilon$-optimal value of a second state that is a sufficient number of\ntransitions away from the first. Inversely, states within each other's vicinity\nhave values that are dependent on each other and will require a similar number\nof samples to learn. By approximating the original MDP using smaller MDPs\nconstructed using subsets of the original's state-space, we are able to reduce\nthe sample-complexity by a logarithmic factor to $O(SA \\log A)$ timesteps,\nwhere $S$ and $A$ are the state and action space sizes. We are able to extend\nthese results to an infinite-horizon, model-free setting by constructing a\nPAC-MDP algorithm with the aforementioned sample-complexity. We conclude with\nshowing how significant the improvement is by comparing our algorithm against\nprior work in an experimental setting.\n","authors":["Mohit Prashant","Arvind Easwaran"],"pdf_url":"https://arxiv.org/pdf/2507.12383v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2507.12380v1","updated":"2025-07-16T16:28:10Z","published":"2025-07-16T16:28:10Z","title":"Heat Kernel Goes Topological","summary":"  Topological neural networks have emerged as powerful successors of graph\nneural networks. However, they typically involve higher-order message passing,\nwhich incurs significant computational expense. We circumvent this issue with a\nnovel topological framework that introduces a Laplacian operator on\ncombinatorial complexes (CCs), enabling efficient computation of heat kernels\nthat serve as node descriptors. Our approach captures multiscale information\nand enables permutation-equivariant representations, allowing easy integration\ninto modern transformer-based architectures.\n  Theoretically, the proposed method is maximally expressive because it can\ndistinguish arbitrary non-isomorphic CCs. Empirically, it significantly\noutperforms existing topological methods in terms of computational efficiency.\nBesides demonstrating competitive performance with the state-of-the-art\ndescriptors on standard molecular datasets, it exhibits superior capability in\ndistinguishing complex topological structures and avoiding blind spots on\ntopological benchmarks. Overall, this work advances topological deep learning\nby providing expressive yet scalable representations, thereby opening up\nexciting avenues for molecular classification and property prediction tasks.\n","authors":["Maximilian Krahn","Vikas Garg"],"pdf_url":"https://arxiv.org/pdf/2507.12380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08788v3","updated":"2025-07-16T16:14:48Z","published":"2024-06-13T03:47:12Z","title":"Towards Understanding Link Predictor Generalizability Under Distribution\n  Shifts","summary":"  State-of-the-art link prediction (LP) models demonstrate impressive benchmark\nresults. However, popular benchmark datasets often assume that training,\nvalidation, and testing samples are representative of the overall dataset\ndistribution. In real-world situations, this assumption is often incorrect;\nuncontrolled factors lead new dataset samples to come from a different\ndistribution than training samples. Additionally, the majority of recent work\nwith graph dataset shift focuses on node- and graph-level tasks, largely\nignoring link-level tasks. To bridge this gap, we introduce a novel splitting\nstrategy, known as LPShift, which utilizes structural properties to induce a\ncontrolled distribution shift. We verify LPShift's effect through empirical\nevaluation of SOTA LP models on 16 LPShift variants of original dataset splits,\nwith results indicating drastic changes to model performance. Additional\nexperiments demonstrate graph structure has a strong influence on the success\nof current generalization methods. Source Code Available Here:\nhttps://github.com/revolins/LPShift\n","authors":["Jay Revolinsky","Harry Shomer","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2406.08788v3.pdf","comment":"KDD 25' Datasets & Benchmarks Track, 14 pages, 8 figures, 18 tables"},{"id":"http://arxiv.org/abs/2311.05128v2","updated":"2025-07-16T16:04:34Z","published":"2023-11-09T03:47:49Z","title":"Exploring and Analyzing Wildland Fire Data Via Machine Learning\n  Techniques","summary":"  This research project investigated the correlation between a 10 Hz time\nseries of thermocouple temperatures and turbulent kinetic energy (TKE) computed\nfrom wind speeds collected from a small experimental prescribed burn at the\nSilas Little Experimental Forest in New Jersey, USA. The primary objective of\nthis project was to explore the potential for using thermocouple temperatures\nas predictors for estimating the TKE produced by a wildland fire. Machine\nlearning models, including Deep Neural Networks, Random Forest Regressor,\nGradient Boosting, and Gaussian Process Regressor, are employed to assess the\npotential for thermocouple temperature perturbations to predict TKE values.\nData visualization and correlation analyses reveal patterns and relationships\nbetween thermocouple temperatures and TKE, providing insight into the\nunderlying dynamics. The project achieves high accuracy in predicting TKE by\nemploying various machine learning models despite a weak correlation between\nthe predictors and the target variable. The results demonstrate significant\nsuccess, particularly from regression models, in accurately estimating the TKE.\nThe research findings contribute to fire behavior and smoke modeling science,\nemphasizing the importance of incorporating machine learning approaches and\nidentifying complex relationships between fine-scale fire behavior and\nturbulence. Accurate TKE estimation using thermocouple temperatures allows for\nthe refinement of models that can inform decision-making in fire management\nstrategies, facilitate effective risk mitigation, and optimize fire management\nefforts. This project highlights the valuable role of machine learning\ntechniques in analyzing wildland fire data, showcasing their potential to\nadvance fire research and management practices.\n","authors":["Dipak Dulal","Joseph J. Charney","Michael Gallagher","Carmeliza Navasca","Nicholas Skowronski"],"pdf_url":"https://arxiv.org/pdf/2311.05128v2.pdf","comment":"This version has been significantly updated and superseded by a more\n  complete and revised version under the new title \"Leveraging Advanced Machine\n  Learning to Predict Turbulence Dynamics from Temperature Observations at an\n  Experimental Prescribed Fire\", available at arXiv ID: arXiv:2507.11012"},{"id":"http://arxiv.org/abs/2410.09474v4","updated":"2025-07-16T16:01:02Z","published":"2024-10-12T10:27:23Z","title":"Distilling Invariant Representations with Dual Augmentation","summary":"  Knowledge distillation (KD) has been widely used to transfer knowledge from\nlarge, accurate models (teachers) to smaller, efficient ones (students). Recent\nmethods have explored enforcing consistency by incorporating causal\ninterpretations to distill invariant representations. In this work, we extend\nthis line of research by introducing a dual augmentation strategy to promote\ninvariant feature learning in both teacher and student models. Our approach\nleverages different augmentations applied to both models during distillation,\npushing the student to capture robust, transferable features. This dual\naugmentation strategy complements invariant causal distillation by ensuring\nthat the learned representations remain stable across a wider range of data\nvariations and transformations. Extensive experiments on CIFAR-100 demonstrate\nthe effectiveness of this approach, achieving competitive results in\nsame-architecture KD.\n","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2410.09474v4.pdf","comment":"After further review, we determined that the submission does not meet\n  the quality standards we intended"},{"id":"http://arxiv.org/abs/2505.14635v2","updated":"2025-07-16T16:00:09Z","published":"2025-05-20T17:25:16Z","title":"Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep\n  Learning","summary":"  We present the first theoretical framework that connects predictive coding\n(PC), a biologically inspired local learning rule, with the minimum description\nlength (MDL) principle in deep networks. We prove that layerwise PC performs\nblock-coordinate descent on the MDL two-part code objective, thereby jointly\nminimizing empirical risk and model complexity. Using Hoeffding's inequality\nand a prefix-code prior, we derive a novel generalization bound of the form\n$R(\\theta) \\le \\hat{R}(\\theta) + \\frac{L(\\theta)}{N}$, capturing the tradeoff\nbetween fit and compression. We further prove that each PC sweep monotonically\ndecreases the empirical two-part codelength, yielding tighter high-probability\nrisk bounds than unconstrained gradient descent. Finally, we show that repeated\nPC updates converge to a block-coordinate stationary point, providing an\napproximate MDL-optimal solution. To our knowledge, this is the first result\noffering formal generalization and convergence guarantees for PC-trained deep\nmodels, positioning PC as a theoretically grounded and biologically plausible\nalternative to backpropagation.\n","authors":["Benjamin Prada","Shion Matsumoto","Abdul Malik Zekri","Ankur Mali"],"pdf_url":"https://arxiv.org/pdf/2505.14635v2.pdf","comment":"24 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.03103v3","updated":"2025-07-16T15:44:04Z","published":"2024-10-04T02:53:52Z","title":"Planning-Aware Code Infilling via Horizon-Length Prediction","summary":"  Fill-in-the-Middle (FIM), or infilling, has become integral to code language\nmodels, enabling generation of missing code given both left and right contexts.\nHowever, the current FIM training paradigm which performs next-token prediction\n(NTP) over reordered sequence often leads to models struggling to generate\ncontent that aligns well with the surrounding context. We hypothesize that NTP\nalone is insufficient for models to learn effective planning conditioned on the\ndistant right context, a critical factor for successful code infilling. To\novercome this, we propose Horizon-Length Prediction (HLP), a novel training\nobjective that teaches models to predict the number of remaining middle tokens\nat each step. HLP advances FIM with lookahead planning, enabling models to\ninherently learn infilling boundaries for arbitrary left and right contexts\nwithout relying on dataset-specific post-processing. Our evaluation across\ndifferent model families and sizes shows that HLP significantly improves FIM\nperformance by up to 24% relatively on diverse benchmarks, across file-level\nand repository-level. Furthermore, the enhanced planning capability gained\nthrough HLP boosts model performance on code reasoning. Importantly, HLP incurs\nnegligible training overhead and no additional inference cost, ensuring its\npracticality for real-world scenarios.\n","authors":["Yifeng Ding","Hantian Ding","Shiqi Wang","Qing Sun","Varun Kumar","Zijian Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03103v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02813v2","updated":"2025-07-16T15:39:50Z","published":"2024-11-05T05:19:09Z","title":"Sparse Orthogonal Parameters Tuning for Continual Learning","summary":"  Continual learning methods based on pre-trained models (PTM) have recently\ngained attention which adapt to successive downstream tasks without\ncatastrophic forgetting. These methods typically refrain from updating the\npre-trained parameters and instead employ additional adapters, prompts, and\nclassifiers. In this paper, we from a novel perspective investigate the benefit\nof sparse orthogonal parameters for continual learning. We found that merging\nsparse orthogonality of models learned from multiple streaming tasks has great\npotential in addressing catastrophic forgetting. Leveraging this insight, we\npropose a novel yet effective method called SoTU (Sparse Orthogonal Parameters\nTUning). We hypothesize that the effectiveness of SoTU lies in the\ntransformation of knowledge learned from multiple domains into the fusion of\northogonal delta parameters. Experimental evaluations on diverse CL benchmarks\ndemonstrate the effectiveness of the proposed approach. Notably, SoTU achieves\noptimal feature representation for streaming data without necessitating complex\nclassifier designs, making it a Plug-and-Play solution.\n","authors":["Hai-Jian Ke","Kun-Peng Ning","Yu-Yang Liu","Jia-Yu Yao","Yong-Hong Tian","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2411.02813v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01234v2","updated":"2025-07-16T15:38:41Z","published":"2024-03-02T15:34:31Z","title":"Active Deep Kernel Learning of Molecular Properties: Realizing Dynamic\n  Structural Embeddings","summary":"  As vast databases of chemical identities become increasingly available, the\nchallenge shifts to how we effectively explore and leverage these resources to\nstudy molecular properties. This paper presents an active learning approach for\nmolecular discovery using Deep Kernel Learning (DKL), demonstrated on the QM9\ndataset. DKL links structural embeddings directly to properties, creating\norganized latent spaces that prioritize relevant property information. By\niteratively recalculating embedding vectors in alignment with target\nproperties, DKL uncovers concentrated maxima representing key molecular\nproperties and reveals unexplored regions with potential for innovation. This\napproach underscores DKL's potential in advancing molecular research and\ndiscovery.\n","authors":["Ayana Ghosh","Maxim Ziatdinov","Sergei V. Kalinin"],"pdf_url":"https://arxiv.org/pdf/2403.01234v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12341v1","updated":"2025-07-16T15:36:15Z","published":"2025-07-16T15:36:15Z","title":"Nonlinear Concept Erasure: a Density Matching Approach","summary":"  Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.\n","authors":["Antoine Saillenfest","Pirmin Lemberger"],"pdf_url":"https://arxiv.org/pdf/2507.12341v1.pdf","comment":"17 pages, 10 figures, accepted for publication in ECAI 2025 (28th\n  European Conference on Artificial Intelligence)"},{"id":"http://arxiv.org/abs/2507.10628v2","updated":"2025-07-16T15:30:11Z","published":"2025-07-14T08:10:00Z","title":"GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement\n  Learning","summary":"  Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for facilitating the self-improvement of large language\nmodels (LLMs), particularly in the domain of complex reasoning tasks. However,\nprevailing on-policy RL methods often contend with significant training\ninstability and inefficiency. This is primarily due to a capacity-difficulty\nmismatch, where the complexity of training data frequently outpaces the model's\ncurrent capabilities, leading to critically sparse reward signals and stalled\nlearning progress. This challenge is particularly acute for smaller, more\nresource-efficient LLMs. To overcome this, we introduce the Guided Hybrid\nPolicy Optimization (GHPO), a novel difficulty-aware reinforcement learning\nframework. GHPO dynamically calibrates task difficulty by employing adaptive\nprompt refinement to provide targeted guidance. This unique approach adaptively\nbalances direct imitation learning for problems currently beyond the model's\nreach with exploration-based reinforcement learning for more manageable tasks,\neffectively creating a smooth and optimized learning curriculum. Extensive\nexperiments demonstrate that GHPO achieves an average performance gain of\napproximately 5% across six challenging mathematics benchmarks, consistently\noutperforming strong on-policy reinforcement learning and curriculum learning\nbaselines. Further analysis confirms that our framework significantly enhances\nboth training stability and final reasoning performance, thus offering a\nscalable and efficient solution for developing powerful and robust reasoning\nmodels.\n","authors":["Ziru Liu","Cheng Gong","Xinyu Fu","Yaofang Liu","Ran Chen","Shoubo Hu","Suiyun Zhang","Rui Liu","Qingfu Zhang","Dandan Tu"],"pdf_url":"https://arxiv.org/pdf/2507.10628v2.pdf","comment":"Code avaiable at https://github.com/hkgc-1/GHPO"},{"id":"http://arxiv.org/abs/2507.12329v1","updated":"2025-07-16T15:22:34Z","published":"2025-07-16T15:22:34Z","title":"Neural Polar Decoders for Deletion Channels","summary":"  This paper introduces a neural polar decoder (NPD) for deletion channels with\na constant deletion rate. Existing polar decoders for deletion channels exhibit\nhigh computational complexity of $O(N^4)$, where $N$ is the block length. This\nlimits the application of polar codes for deletion channels to\nshort-to-moderate block lengths. In this work, we demonstrate that employing\nNPDs for deletion channels can reduce the computational complexity. First, we\nextend the architecture of the NPD to support deletion channels. Specifically,\nthe NPD architecture consists of four neural networks (NNs), each replicating\nfundamental successive cancellation (SC) decoder operations. To support\ndeletion channels, we change the architecture of only one. The computational\ncomplexity of the NPD is $O(AN\\log N)$, where the parameter $A$ represents a\ncomputational budget determined by the user and is independent of the channel.\nWe evaluate the new extended NPD for deletion channels with deletion rates\n$\\delta\\in\\{0.01, 0.1\\}$ and we verify the NPD with the ground truth given by\nthe trellis decoder by Tal et al. We further show that due to the reduced\ncomplexity of the NPD, we are able to incorporate list decoding and further\nimprove performance. We believe that the extended NPD presented here could have\napplications in future technologies like DNA storage.\n","authors":["Ziv Aharoni","Henry D. Pfister"],"pdf_url":"https://arxiv.org/pdf/2507.12329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00265v2","updated":"2025-07-16T15:16:27Z","published":"2024-10-31T23:54:21Z","title":"Quantifying calibration error in modern neural networks through evidence\n  based theory","summary":"  Trustworthiness in neural networks is crucial for their deployment in\ncritical applications, where reliability, confidence, and uncertainty play\npivotal roles in decision-making. Traditional performance metrics such as\naccuracy and precision fail to capture these aspects, particularly in cases\nwhere models exhibit overconfidence. To address these limitations, this paper\nintroduces a novel framework for quantifying the trustworthiness of neural\nnetworks by incorporating subjective logic into the evaluation of Expected\nCalibration Error (ECE). This method provides a comprehensive measure of trust,\ndisbelief, and uncertainty by clustering predicted probabilities and fusing\nopinions using appropriate fusion operators. We demonstrate the effectiveness\nof this approach through experiments on MNIST and CIFAR-10 datasets, where\npost-calibration results indicate improved trustworthiness. The proposed\nframework offers a more interpretable and nuanced assessment of AI models, with\npotential applications in sensitive domains such as healthcare and autonomous\nsystems.\n","authors":["Koffi Ismael Ouattara"],"pdf_url":"https://arxiv.org/pdf/2411.00265v2.pdf","comment":"Accepted at FUSION 2025 Conference"},{"id":"http://arxiv.org/abs/2506.04602v2","updated":"2025-07-16T15:12:42Z","published":"2025-06-05T03:40:22Z","title":"MVP-Shapley: Feature-based Modeling for Evaluating the Most Valuable\n  Player in Basketball","summary":"  The burgeoning growth of the esports and multiplayer online gaming community\nhas highlighted the critical importance of evaluating the Most Valuable Player\n(MVP). The establishment of an explainable and practical MVP evaluation method\nis very challenging. In our study, we specifically focus on play-by-play data,\nwhich records related events during the game, such as assists and points. We\naim to address the challenges by introducing a new MVP evaluation framework,\ndenoted as \\oursys, which leverages Shapley values. This approach encompasses\nfeature processing, win-loss model training, Shapley value allocation, and MVP\nranking determination based on players' contributions. Additionally, we\noptimize our algorithm to align with expert voting results from the perspective\nof causality. Finally, we substantiated the efficacy of our method through\nvalidation using the NBA dataset and the Dunk City Dynasty dataset and\nimplemented online deployment in the industry.\n","authors":["Haifeng Sun","Yu Xiong","Runze Wu","Kai Wang","Lan Zhang","Changjie Fan","Shaojie Tang","Xiang-Yang Li"],"pdf_url":"https://arxiv.org/pdf/2506.04602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12318v1","updated":"2025-07-16T15:12:17Z","published":"2025-07-16T15:12:17Z","title":"Compositional Discrete Latent Code for High Fidelity, Productive\n  Diffusion Models","summary":"  We argue that diffusion models' success in modeling complex distributions is,\nfor the most part, coming from their input conditioning. This paper\ninvestigates the representation used to condition diffusion models from the\nperspective that ideal representations should improve sample fidelity, be easy\nto generate, and be compositional to allow out-of-training samples generation.\nWe introduce Discrete Latent Code (DLC), an image representation derived from\nSimplicial Embeddings trained with a self-supervised learning objective. DLCs\nare sequences of discrete tokens, as opposed to the standard continuous image\nembeddings. They are easy to generate and their compositionality enables\nsampling of novel images beyond the training distribution. Diffusion models\ntrained with DLCs have improved generation fidelity, establishing a new\nstate-of-the-art for unconditional image generation on ImageNet. Additionally,\nwe show that composing DLCs allows the image generator to produce\nout-of-distribution samples that coherently combine the semantics of images in\ndiverse ways. Finally, we showcase how DLCs can enable text-to-image generation\nby leveraging large-scale pretrained language models. We efficiently finetune a\ntext diffusion language model to generate DLCs that produce novel samples\noutside of the image generator training distribution.\n","authors":["Samuel Lavoie","Michael Noukhovitch","Aaron Courville"],"pdf_url":"https://arxiv.org/pdf/2507.12318v1.pdf","comment":"In submission, 22 pages, 7 tables, 12 figures"},{"id":"http://arxiv.org/abs/2507.12314v1","updated":"2025-07-16T15:09:13Z","published":"2025-07-16T15:09:13Z","title":"Thought Purity: Defense Paradigm For Chain-of-Thought Attack","summary":"  While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures.\n","authors":["Zihao Xue","Zhen Bi","Long Ma","Zhenlin Hu","Yan Wang","Zhenfang Liu","Qing Sheng","Jie Xiao","Jungang Lou"],"pdf_url":"https://arxiv.org/pdf/2507.12314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12305v1","updated":"2025-07-16T15:04:46Z","published":"2025-07-16T15:04:46Z","title":"PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt\n  Online Learning","summary":"  The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL.\n","authors":["M. Anwar Ma'sum","Mahardhika Pratama","Savitha Ramasamy","Lin Liu","Habibullah Habibullah","Ryszard Kowalczyk"],"pdf_url":"https://arxiv.org/pdf/2507.12305v1.pdf","comment":"ICCV 2025"},{"id":"http://arxiv.org/abs/2503.22526v3","updated":"2025-07-16T14:55:25Z","published":"2025-03-28T15:30:42Z","title":"AnnoPage Dataset: Dataset of Non-Textual Elements in Documents with\n  Fine-Grained Categorization","summary":"  We introduce the AnnoPage Dataset, a novel collection of 7,550 pages from\nhistorical documents, primarily in Czech and German, spanning from 1485 to the\npresent, focusing on the late 19th and early 20th centuries. The dataset is\ndesigned to support research in document layout analysis and object detection.\nEach page is annotated with axis-aligned bounding boxes (AABB) representing\nelements of 25 categories of non-textual elements, such as images, maps,\ndecorative elements, or charts, following the Czech Methodology of image\ndocument processing. The annotations were created by expert librarians to\nensure accuracy and consistency. The dataset also incorporates pages from\nmultiple, mainly historical, document datasets to enhance variability and\nmaintain continuity. The dataset is divided into development and test subsets,\nwith the test set carefully selected to maintain the category distribution. We\nprovide baseline results using YOLO and DETR object detectors, offering a\nreference point for future research. The AnnoPage Dataset is publicly available\non Zenodo (https://doi.org/10.5281/zenodo.12788419), along with ground-truth\nannotations in YOLO format.\n","authors":["Martin Kišš","Michal Hradiš","Martina Dvořáková","Václav Jiroušek","Filip Kersch"],"pdf_url":"https://arxiv.org/pdf/2503.22526v3.pdf","comment":"17 pages, 2 tables, 7 figures; Accepted to GREC Workshop at ICDAR2025"},{"id":"http://arxiv.org/abs/2411.02572v2","updated":"2025-07-16T14:52:58Z","published":"2024-11-04T20:09:51Z","title":"ViTally Consistent: Scaling Biological Representation Learning for Cell\n  Microscopy","summary":"  Large-scale cell microscopy screens are used in drug discovery and molecular\nbiology research to study the effects of millions of chemical and genetic\nperturbations on cells. To use these images in downstream analysis, we need\nmodels that can map each image into a feature space that represents diverse\nbiological phenotypes consistently, in the sense that perturbations with\nsimilar biological effects have similar representations. In this work, we\npresent the largest foundation model for cell microscopy data to date, a new\n1.9 billion-parameter ViT-G/8 MAE trained on over 8 billion microscopy image\ncrops. Compared to a previous published ViT-L/8 MAE, our new model achieves a\n60% improvement in linear separability of genetic perturbations and obtains the\nbest overall performance on whole-genome biological relationship recall and\nreplicate consistency benchmarks. Beyond scaling, we developed two key methods\nthat improve performance: (1) training on a curated and diverse dataset; and,\n(2) using biologically motivated linear probing tasks to search across each\ntransformer block for the best candidate representation of whole-genome\nscreens. We find that many self-supervised vision transformers, pretrained on\neither natural or microscopy images, yield significantly more biologically\nmeaningful representations of microscopy images in their intermediate blocks\nthan in their typically used final blocks. More broadly, our approach and\nresults provide insights toward a general strategy for successfully building\nfoundation models for large-scale biological data.\n","authors":["Kian Kenyon-Dean","Zitong Jerry Wang","John Urbanik","Konstantin Donhauser","Jason Hartford","Saber Saberian","Nil Sahin","Ihab Bendidi","Safiye Celik","Marta Fay","Juan Sebastian Rodriguez Vera","Imran S Haque","Oren Kraus"],"pdf_url":"https://arxiv.org/pdf/2411.02572v2.pdf","comment":"ICML 2025 main-track paper (42nd International Conference on Machine\n  Learning). Formerly appeared as best paper runner-up at NeurIPS 2024\n  Foundation Models for Science Workshop (38th Conference on Neural Information\n  Processing Systems). 18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2507.12297v1","updated":"2025-07-16T14:51:37Z","published":"2025-07-16T14:51:37Z","title":"RegCL: Continual Adaptation of Segment Anything Model via Model Merging","summary":"  To address the performance limitations of the Segment Anything Model (SAM) in\nspecific domains, existing works primarily adopt adapter-based one-step\nadaptation paradigms. However, some of these methods are specific developed for\nspecific domains. If used on other domains may lead to performance degradation.\nThis issue of catastrophic forgetting severely limits the model's scalability.\nTo address this issue, this paper proposes RegCL, a novel non-replay continual\nlearning (CL) framework designed for efficient multi-domain knowledge\nintegration through model merging. Specifically, RegCL incorporates the model\nmerging algorithm into the continual learning paradigm by merging the\nparameters of SAM's adaptation modules (e.g., LoRA modules) trained on\ndifferent domains. The merging process is guided by weight optimization, which\nminimizes prediction discrepancies between the merged model and each of the\ndomain-specific models. RegCL effectively consolidates multi-domain knowledge\nwhile maintaining parameter efficiency, i.e., the model size remains constant\nregardless of the number of tasks, and no historical data storage is required.\nExperimental results demonstrate that RegCL achieves favorable continual\nlearning performance across multiple downstream datasets, validating its\neffectiveness in dynamic scenarios.\n","authors":["Yuan-Chen Shu","Zhiwei Lin","Yongtao Wang"],"pdf_url":"https://arxiv.org/pdf/2507.12297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12295v1","updated":"2025-07-16T14:47:41Z","published":"2025-07-16T14:47:41Z","title":"Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding","summary":"  Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.\n","authors":["Feng Xiao","Jicong Fan"],"pdf_url":"https://arxiv.org/pdf/2507.12295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15801v2","updated":"2025-07-16T14:47:13Z","published":"2024-01-28T23:18:10Z","title":"On the Statistical Properties of Generative Adversarial Models for Low\n  Intrinsic Data Dimension","summary":"  Despite the remarkable empirical successes of Generative Adversarial Networks\n(GANs), the theoretical guarantees for their statistical accuracy remain rather\npessimistic. In particular, the data distributions on which GANs are applied,\nsuch as natural images, are often hypothesized to have an intrinsic\nlow-dimensional structure in a typically high-dimensional feature space, but\nthis is often not reflected in the derived rates in the state-of-the-art\nanalyses. In this paper, we attempt to bridge the gap between the theory and\npractice of GANs and their bidirectional variant, Bi-directional GANs (BiGANs),\nby deriving statistical guarantees on the estimated densities in terms of the\nintrinsic dimension of the data and the latent space. We analytically show that\nif one has access to $n$ samples from the unknown target distribution and the\nnetwork architectures are properly chosen, the expected Wasserstein-1 distance\nof the estimates from the target scales as $O\\left( n^{-1/d_\\mu } \\right)$ for\nGANs and $\\tilde{O}\\left( n^{-1/(d_\\mu+\\ell)} \\right)$ for BiGANs, where\n$d_\\mu$ and $\\ell$ are the upper Wasserstein-1 dimension of the\ndata-distribution and latent-space dimension, respectively. The theoretical\nanalyses not only suggest that these methods successfully avoid the curse of\ndimensionality, in the sense that the exponent of $n$ in the error rates does\nnot depend on the data dimension but also serve to bridge the gap between the\ntheoretical analyses of GANs and the known sharp rates from optimal transport\nliterature. Additionally, we demonstrate that GANs can effectively achieve the\nminimax optimal rate even for non-smooth underlying distributions, with the use\nof interpolating generator networks.\n","authors":["Saptarshi Chakraborty","Peter L. Bartlett"],"pdf_url":"https://arxiv.org/pdf/2401.15801v2.pdf","comment":"Journal of Machine Learning Research (2025), volume 26"},{"id":"http://arxiv.org/abs/2312.07003v2","updated":"2025-07-16T14:47:03Z","published":"2023-12-12T06:21:30Z","title":"RACER: Rational Artificial Intelligence Car-following-model Enhanced by\n  Reality","summary":"  This paper introduces RACER, the Rational Artificial Intelligence\nCar-following model Enhanced by Reality, a cutting-edge deep learning\ncar-following model, that satisfies partial derivative constraints, designed to\npredict Adaptive Cruise Control (ACC) driving behavior while staying\ntheoretically feasible. Unlike conventional models, RACER effectively\nintegrates Rational Driving Constraints (RDCs), crucial tenets of actual\ndriving, resulting in strikingly accurate and realistic predictions. Against\nestablished models like the Optimal Velocity Relative Velocity (OVRV), a\ncar-following Neural Network (NN), and a car-following Physics-Informed Neural\nNetwork (PINN), RACER excels across key metrics, such as acceleration,\nvelocity, and spacing. Notably, it displays a perfect adherence to the RDCs,\nregistering zero violations, in stark contrast to other models. This study\nhighlights the immense value of incorporating physical constraints within AI\nmodels, especially for augmenting safety measures in transportation. It also\npaves the way for future research to test these models against human driving\ndata, with the potential to guide safer and more rational driving behavior. The\nversatility of the proposed model, including its potential to incorporate\nadditional derivative constraints and broader architectural applications,\nenhances its appeal and broadens its impact within the scientific community.\n","authors":["Tianyi Li","Alexander Halatsis","Raphael Stern"],"pdf_url":"https://arxiv.org/pdf/2312.07003v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.07511v2","updated":"2025-07-16T14:27:01Z","published":"2025-07-10T07:57:50Z","title":"Uncertainty Quantification for Motor Imagery BCI -- Machine Learning vs.\n  Deep Learning","summary":"  Brain-computer interfaces (BCIs) turn brain signals into functionally useful\noutput, but they are not always accurate. A good Machine Learning classifier\nshould be able to indicate how confident it is about a given classification, by\ngiving a probability for its classification. Standard classifiers for Motor\nImagery BCIs do give such probabilities, but research on uncertainty\nquantification has been limited to Deep Learning. We compare the uncertainty\nquantification ability of established BCI classifiers using Common Spatial\nPatterns (CSP-LDA) and Riemannian Geometry (MDRM) to specialized methods in\nDeep Learning (Deep Ensembles and Direct Uncertainty Quantification) as well as\nstandard Convolutional Neural Networks (CNNs).\n  We found that the overconfidence typically seen in Deep Learning is not a\nproblem in CSP-LDA and MDRM. We found that MDRM is underconfident, which we\nsolved by adding Temperature Scaling (MDRM-T). CSP-LDA and MDRM-T give the best\nuncertainty estimates, but Deep Ensembles and standard CNNs give the best\nclassifications. We show that all models are able to separate between easy and\ndifficult estimates, so that we can increase the accuracy of a Motor Imagery\nBCI by rejecting samples that are ambiguous.\n","authors":["Joris Suurmeijer","Ivo Pascal de Jong","Matias Valdenegro-Toro","Andreea Ioana Sburlea"],"pdf_url":"https://arxiv.org/pdf/2507.07511v2.pdf","comment":"6 pages, 3 figures; fixed typos"},{"id":"http://arxiv.org/abs/2507.12269v1","updated":"2025-07-16T14:19:44Z","published":"2025-07-16T14:19:44Z","title":"Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust\n  Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in\n  Extremely Preterm Infants","summary":"  Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.\n","authors":["Sybelle Goedicke-Fritz","Michelle Bous","Annika Engel","Matthias Flotho","Pascal Hirsch","Hannah Wittig","Dino Milanovic","Dominik Mohr","Mathias Kaspar","Sogand Nemat","Dorothea Kerner","Arno Bücker","Andreas Keller","Sascha Meyer","Michael Zemlin","Philipp Flotho"],"pdf_url":"https://arxiv.org/pdf/2507.12269v1.pdf","comment":"S.G.-F., M.B., and A.E. contributed equally to this work and share\n  first authorship. M.Z. and P.F. contributed equally to this work and share\n  senior authorship"},{"id":"http://arxiv.org/abs/2504.21042v3","updated":"2025-07-16T14:13:44Z","published":"2025-04-28T13:30:48Z","title":"What's Pulling the Strings? Evaluating Integrity and Attribution in AI\n  Training and Inference through Concept Shift","summary":"  The growing adoption of artificial intelligence (AI) has amplified concerns\nabout trustworthiness, including integrity, privacy, robustness, and bias. To\nassess and attribute these threats, we propose ConceptLens, a generic framework\nthat leverages pre-trained multimodal models to identify the root causes of\nintegrity threats by analyzing Concept Shift in probing samples. ConceptLens\ndemonstrates strong detection performance for vanilla data poisoning attacks\nand uncovers vulnerabilities to bias injection, such as the generation of\ncovert advertisements through malicious concept shifts. It identifies privacy\nrisks in unaltered but high-risk samples, filters them before training, and\nprovides insights into model weaknesses arising from incomplete or imbalanced\ntraining data. Additionally, at the model level, it attributes concepts that\nthe target model is overly dependent on, identifies misleading concepts, and\nexplains how disrupting key concepts negatively impacts the model. Furthermore,\nit uncovers sociological biases in generative content, revealing disparities\nacross sociological contexts. Strikingly, ConceptLens reveals how safe training\nand inference data can be unintentionally and easily exploited, potentially\nundermining safety alignment. Our study informs actionable insights to breed\ntrust in AI systems, thereby speeding adoption and driving greater innovation.\n","authors":["Jiamin Chang","Haoyang Li","Hammond Pearce","Ruoxi Sun","Bo Li","Minhui Xue"],"pdf_url":"https://arxiv.org/pdf/2504.21042v3.pdf","comment":"Accepted to The ACM Conference on Computer and Communications\n  Security (CCS) 2025"},{"id":"http://arxiv.org/abs/2407.00765v3","updated":"2025-07-16T14:12:38Z","published":"2024-06-30T17:00:42Z","title":"Structured and Balanced Multi-Component and Multi-Layer Neural Networks","summary":"  In this work, we propose a balanced multi-component and multi-layer neural\nnetwork (MMNN) structure to accurately and efficiently approximate functions\nwith complex features, in terms of both degrees of freedom and computational\ncost. The main idea is inspired by a multi-component approach, in which each\ncomponent can be effectively approximated by a single-layer network, combined\nwith a multi-layer decomposition strategy to capture the complexity of the\ntarget function. Although MMNNs can be viewed as a simple modification of fully\nconnected neural networks (FCNNs) or multi-layer perceptrons (MLPs) by\nintroducing balanced multi-component structures, they achieve a significant\nreduction in training parameters, a much more efficient training process, and\nimproved accuracy compared to FCNNs or MLPs. Extensive numerical experiments\ndemonstrate the effectiveness of MMNNs in approximating highly oscillatory\nfunctions and their ability to automatically adapt to localized features.\n","authors":["Shijun Zhang","Hongkai Zhao","Yimin Zhong","Haomin Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.00765v3.pdf","comment":"Our codes and implementation details are available at\n  https://github.com/ShijunZhangMath/MMNN"},{"id":"http://arxiv.org/abs/2507.12262v1","updated":"2025-07-16T14:09:49Z","published":"2025-07-16T14:09:49Z","title":"A Framework for Nonstationary Gaussian Processes with Neural Network\n  Parameters","summary":"  Gaussian processes have become a popular tool for nonparametric regression\nbecause of their flexibility and uncertainty quantification. However, they\noften use stationary kernels, which limit the expressiveness of the model and\nmay be unsuitable for many datasets. We propose a framework that uses\nnonstationary kernels whose parameters vary across the feature space, modeling\nthese parameters as the output of a neural network that takes the features as\ninput. The neural network and Gaussian process are trained jointly using the\nchain rule to calculate derivatives. Our method clearly describes the behavior\nof the nonstationary parameters and is compatible with approximation methods\nfor scaling to large datasets. It is flexible and easily adapts to different\nnonstationary kernels without needing to redesign the optimization procedure.\nOur methods are implemented with the GPyTorch library and can be readily\nmodified. We test a nonstationary variance and noise variant of our method on\nseveral machine learning datasets and find that it achieves better accuracy and\nlog-score than both a stationary model and a hierarchical model approximated\nwith variational inference. Similar results are observed for a model with only\nnonstationary variance. We also demonstrate our approach's ability to recover\nthe nonstationary parameters of a spatial dataset.\n","authors":["Zachary James","Joseph Guinness"],"pdf_url":"https://arxiv.org/pdf/2507.12262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06771v2","updated":"2025-07-16T14:08:22Z","published":"2024-12-09T18:56:32Z","title":"Proactive Agents for Multi-Turn Text-to-Image Generation Under\n  Uncertainty","summary":"  User prompts for generative AI models are often underspecified, leading to a\nmisalignment between the user intent and models' understanding. As a result,\nusers commonly have to painstakingly refine their prompts. We study this\nalignment problem in text-to-image (T2I) generation and propose a prototype for\nproactive T2I agents equipped with an interface to (1) actively ask\nclarification questions when uncertain, and (2) present their uncertainty about\nuser intent as an understandable and editable belief graph. We build simple\nprototypes for such agents and propose a new scalable and automated evaluation\napproach using two agents, one with a ground truth intent (an image) while the\nother tries to ask as few questions as possible to align with the ground truth.\nWe experiment over three image-text datasets: ImageInWords (Garg et al., 2024),\nCOCO (Lin et al., 2014) and DesignBench, a benchmark we curated with strong\nartistic and design elements. Experiments over the three datasets demonstrate\nthe proposed T2I agents' ability to ask informative questions and elicit\ncrucial information to achieve successful alignment with at least 2 times\nhigher VQAScore (Lin et al., 2024) than the standard T2I generation. Moreover,\nwe conducted human studies and observed that at least 90% of human subjects\nfound these agents and their belief graphs helpful for their T2I workflow,\nhighlighting the effectiveness of our approach. Code and DesignBench can be\nfound at https://github.com/google-deepmind/proactive_t2i_agents.\n","authors":["Meera Hahn","Wenjun Zeng","Nithish Kannen","Rich Galt","Kartikeya Badola","Been Kim","Zi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.06771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17070v2","updated":"2025-07-16T14:02:29Z","published":"2025-03-21T11:53:36Z","title":"A Thorough Assessment of the Non-IID Data Impact in Federated Learning","summary":"  Federated learning (FL) allows collaborative machine learning (ML) model\ntraining among decentralized clients' information, ensuring data privacy. The\ndecentralized nature of FL deals with non-independent and identically\ndistributed (non-IID) data. This open problem has notable consequences, such as\ndecreased model performance and more significant convergence times. Despite its\nimportance, experimental studies systematically addressing all types of data\nheterogeneity (a.k.a. non-IIDness) remain scarce. We aim to fill this gap by\nassessing and quantifying the non-IID effect through a thorough empirical\nanalysis. We use the Hellinger Distance (HD) to measure differences in\ndistribution among clients. Our study benchmarks four state-of-the-art\nstrategies for handling non-IID data, including label, feature, quantity, and\nspatiotemporal skewness, under realistic and controlled conditions. This is the\nfirst comprehensive analysis of the spatiotemporal skew effect in FL. Our\nfindings highlight the significant impact of label and spatiotemporal skew\nnon-IID types on FL model performance, with notable performance drops occurring\nat specific HD thresholds. Additionally, the FL performance is heavily affected\nmainly when the non-IIDness is extreme. Thus, we provide recommendations for FL\nresearch to tackle data heterogeneity effectively. Our work represents the most\nextensive examination of non-IIDness in FL, offering a robust foundation for\nfuture research.\n","authors":["Daniel M. Jimenez-Gutierrez","Mehrdad Hassanzadeh","Aris Anagnostopoulos","Ioannis Chatzigiannakis","Andrea Vitaletti"],"pdf_url":"https://arxiv.org/pdf/2503.17070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12257v1","updated":"2025-07-16T14:02:21Z","published":"2025-07-16T14:02:21Z","title":"Robust Causal Discovery in Real-World Time Series with Power-Laws","summary":"  Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance.\n","authors":["Matteo Tusoni","Giuseppe Masi","Andrea Coletta","Aldo Glielmo","Viviana Arrigoni","Novella Bartolini"],"pdf_url":"https://arxiv.org/pdf/2507.12257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12256v1","updated":"2025-07-16T14:02:01Z","published":"2025-07-16T14:02:01Z","title":"Surrogate Quantum Circuit Design for the Lattice Boltzmann Collision\n  Operator","summary":"  Direct numerical simulation of turbulent flows at high Reynolds numbers\nremains a major challenge for traditional computational fluid dynamics (CFD)\ntools running on classical computer hardware. This has motivated growing\ninterest in quantum algorithms for CFD to enable flow simulations on quantum\ncomputers. The reason being that these computers are expected to deliver\npotential speed-ups for certain problems. One promising quantum CFD approach is\na fully quantum implementation of the lattice Boltzmann method called QLBM.\nAlthough efficient quantum routines are now available for the streaming step,\nimplementing the nonlinear, irreversible collision step with a low depth\ncircuit that avoids additional ancilla qubits, probabilistic post-selection and\nrepeated executions remains a significant challenge. In this study, we address\nthis challenge by introducing a framework for learning a surrogate quantum\ncircuit (SQC) that approximates the full Bhatnagar Gross Krook (BGK) collision\noperator for the D2Q9 lattice. The four qubit circuit is trained to respect the\nphysical properties of the BGK collision operator, including mass and momentum\nconservation, D8 equivariance and scale equivariance. When compiled to the gate\nset used by IBM Heron processor under the assumption of full qubit\nconnectivity, the 15 block SQC requires only 2,430 native gates and uses\nneither ancilla qubits nor post-selection or repeated executions. Moreover, its\ndepth is independent of the grid resolution, as collision is a local operation\nthat can exploit quantum parallelism to its full extent. We validate the SQC on\ntwo benchmark flows, the Taylor Green vortex decay and the lid driven cavity,\ndemonstrating that it accurately captures vortex dissipation and flow\nrecirculation.\n","authors":["Monica Lăcătuş","Matthias Möller"],"pdf_url":"https://arxiv.org/pdf/2507.12256v1.pdf","comment":"31 pages, 14 figures"},{"id":"http://arxiv.org/abs/2507.12248v1","updated":"2025-07-16T13:57:50Z","published":"2025-07-16T13:57:50Z","title":"Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on\n  PathMNIST","summary":"  Deep learning has significantly advanced the field of medical image\nclassification, particularly with the adoption of Convolutional Neural Networks\n(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer\nunique advantages in model development and deployment. However, their\ncomparative performance in medical imaging tasks remains underexplored. This\nstudy presents a comprehensive analysis of CNN implementations across these\nframeworks, using the PathMNIST dataset as a benchmark. We evaluate training\nefficiency, classification accuracy and inference speed to assess their\nsuitability for real-world applications. Our findings highlight the trade-offs\nbetween computational speed and model accuracy, offering valuable insights for\nresearchers and practitioners in medical image analysis.\n","authors":["Anida Nezović","Jalal Romano","Nada Marić","Medina Kapo","Amila Akagić"],"pdf_url":"https://arxiv.org/pdf/2507.12248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12233v1","updated":"2025-07-16T13:47:20Z","published":"2025-07-16T13:47:20Z","title":"Universal Fourier Neural Operators for Micromechanics","summary":"  \\noindent Solving cell problems in homogenization is hard, and available\ndeep-learning frameworks fail to match the speed and generality of traditional\ncomputational frameworks. More to the point, it is generally unclear what to\nexpect of machine-learning approaches, let alone single out which approaches\nare promising. In the work at hand, we advocate Fourier Neural Operators (FNOs)\nfor micromechanics, empowering them by insights from computational\nmicromechanics methods based on the fast Fourier transform (FFT). We construct\nan FNO surrogate mimicking the basic scheme foundational for FFT-based methods\nand show that the resulting operator predicts solutions to cell problems with\n\\emph{arbitrary} stiffness distribution only subject to a material-contrast\nconstraint up to a desired accuracy. In particular, there are no restrictions\non the material symmetry like isotropy, on the number of phases and on the\ngeometry of the interfaces between materials. Also, the provided fidelity is\nsharp and uniform, providing explicit guarantees leveraging our physical\nempowerment of FNOs. To show the desired universal approximation property, we\nconstruct an FNO explicitly that requires no training to begin with. Still, the\nobtained neural operator complies with the same memory requirements as the\nbasic scheme and comes with runtimes proportional to classical FFT solvers. In\nparticular, large-scale problems with more than 100 million voxels are readily\nhandled. The goal of this work is to underline the potential of FNOs for\nsolving micromechanical problems, linking FFT-based methods to FNOs. This\nconnection is expected to provide a fruitful exchange between both worlds.\n","authors":["Binh Huy Nguyen","Matti Schneider"],"pdf_url":"https://arxiv.org/pdf/2507.12233v1.pdf","comment":"48 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.16994v2","updated":"2025-07-16T13:35:23Z","published":"2025-02-24T09:28:35Z","title":"FADE: Why Bad Descriptions Happen to Good Features","summary":"  Recent advances in mechanistic interpretability have highlighted the\npotential of automating interpretability pipelines in analyzing the latent\nrepresentations within LLMs. While this may enhance our understanding of\ninternal mechanisms, the field lacks standardized evaluation methods for\nassessing the validity of discovered features. We attempt to bridge this gap by\nintroducing FADE: Feature Alignment to Description Evaluation, a scalable\nmodel-agnostic framework for automatically evaluating feature-to-description\nalignment. FADE evaluates alignment across four key metrics - Clarity,\nResponsiveness, Purity, and Faithfulness - and systematically quantifies the\ncauses of the misalignment between features and their descriptions. We apply\nFADE to analyze existing open-source feature descriptions and assess key\ncomponents of automated interpretability pipelines, aiming to enhance the\nquality of descriptions. Our findings highlight fundamental challenges in\ngenerating feature descriptions, particularly for SAEs compared to MLP neurons,\nproviding insights into the limitations and future directions of automated\ninterpretability. We release FADE as an open-source package at:\nhttps://github.com/brunibrun/FADE\n","authors":["Bruno Puri","Aakriti Jain","Elena Golimblevskaia","Patrick Kahardipraja","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2502.16994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14628v3","updated":"2025-07-16T13:35:12Z","published":"2023-12-22T11:58:53Z","title":"Holistic analysis on the sustainability of Federated Learning across AI\n  product lifecycle","summary":"  In light of emerging legal requirements and policies focused on privacy\nprotection, there is a growing trend of companies across various industries\nadopting Federated Learning (FL). This decentralized approach involves multiple\nclients or silos, collaboratively training a global model under the\ncoordination of a central server while utilizing their private local data.\nUnlike traditional methods that necessitate data sharing and transmission,\nCross-Silo FL allows clients to share model updates rather than raw data,\nthereby enhancing privacy. Despite its growing adoption, the carbon impact\nassociated with Cross-Silo FL remains poorly understood due to the limited\nresearch in this area. This study seeks to bridge this gap by evaluating the\nsustainability of Cross-Silo FL throughout the entire AI product lifecycle,\nextending the analysis beyond the model training phase alone. We systematically\ncompare this decentralized method with traditional centralized approaches and\npresent a robust quantitative framework for assessing the costs and CO2\nemissions in real-world Cross-Silo FL environments. Our findings indicate that\nthe energy consumption and costs of model training are comparable between\nCross-Silo Federated Learning and Centralized Learning. However, the additional\ndata transfer and storage requirements inherent in Centralized Learning can\nresult in significant, often overlooked CO2 emissions. Moreover, we introduce\nan innovative data and application management system that integrates Cross-Silo\nFL and analytics, aiming at improving the sustainability and economic\nefficiency of IT enterprises.\n","authors":["Hongliu Cao"],"pdf_url":"https://arxiv.org/pdf/2312.14628v3.pdf","comment":"Accepted in ECAI 2025 (PAIS track)"},{"id":"http://arxiv.org/abs/2507.12224v1","updated":"2025-07-16T13:33:31Z","published":"2025-07-16T13:33:31Z","title":"Optimizers Qualitatively Alter Solutions And We Should Leverage This","summary":"  Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not\nguarantee convergence to a unique global minimum of the loss when using\noptimizers relying only on local information, such as SGD. Indeed, this was a\nprimary source of skepticism regarding the feasibility of DNNs in the early\ndays of the field. The past decades of progress in deep learning have revealed\nthis skepticism to be misplaced, and a large body of empirical evidence shows\nthat sufficiently large DNNs following standard training protocols exhibit\nwell-behaved optimization dynamics that converge to performant solutions. This\nsuccess has biased the community to use convex optimization as a mental model\nfor learning, leading to a focus on training efficiency, either in terms of\nrequired iteration, FLOPs or wall-clock time, when improving optimizers. We\nargue that, while this perspective has proven extremely fruitful, another\nperspective specific to DNNs has received considerably less attention: the\noptimizer not only influences the rate of convergence, but also the qualitative\nproperties of the learned solutions. Restated, the optimizer can and will\nencode inductive biases and change the effective expressivity of a given class\nof models. Furthermore, we believe the optimizer can be an effective way of\nencoding desiderata in the learning process. We contend that the community\nshould aim at understanding the biases of already existing methods, as well as\naim to build new optimizers with the explicit intent of inducing certain\nproperties of the solution, rather than solely judging them based on their\nconvergence rates. We hope our arguments will inspire research to improve our\nunderstanding of how the learning process can impact the type of solution we\nconverge to, and lead to a greater recognition of optimizers design as a\ncritical lever that complements the roles of architecture and data in shaping\nmodel outcomes.\n","authors":["Razvan Pascanu","Clare Lyle","Ionut-Vlad Modoranu","Naima Elosegui Borras","Dan Alistarh","Petar Velickovic","Sarath Chandar","Soham De","James Martens"],"pdf_url":"https://arxiv.org/pdf/2507.12224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02004v3","updated":"2025-07-16T13:30:45Z","published":"2024-03-04T12:57:26Z","title":"Error bounds for particle gradient descent, and extensions of the\n  log-Sobolev and Talagrand inequalities","summary":"  We prove non-asymptotic error bounds for particle gradient descent (PGD,\nKuntz et al., 2023), a recently introduced algorithm for maximum likelihood\nestimation of large latent variable models obtained by discretizing a gradient\nflow of the free energy. We begin by showing that the flow converges\nexponentially fast to the free energy's minimizers for models satisfying a\ncondition that generalizes both the log-Sobolev and the Polyak--{\\L}ojasiewicz\ninequalities (LSI and P{\\L}I, respectively). We achieve this by extending a\nresult well-known in the optimal transport literature (that the LSI implies the\nTalagrand inequality) and its counterpart in the optimization literature (that\nthe P{\\L}I implies the so-called quadratic growth condition), and applying the\nextension to our new setting. We also generalize the Bakry--\\'Emery Theorem and\nshow that the LSI/P{\\L}I extension holds for models with strongly concave\nlog-likelihoods. For such models, we further control PGD's discretization error\nand obtain the non-asymptotic error bounds. While we are motivated by the study\nof PGD, we believe that the inequalities and results we extend may be of\nindependent interest.\n","authors":["Rocco Caprio","Juan Kuntz","Samuel Power","Adam M. Johansen"],"pdf_url":"https://arxiv.org/pdf/2403.02004v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12218v1","updated":"2025-07-16T13:23:39Z","published":"2025-07-16T13:23:39Z","title":"Physics-Informed Linear Model (PILM): Analytical Representations and\n  Application to Crustal Strain Rate Estimation","summary":"  Many physical systems are described by partial differential equations (PDEs),\nand solving these equations and estimating their coefficients or boundary\nconditions (BCs) from observational data play a crucial role in understanding\nthe associated phenomena. Recently, a machine learning approach known as\nphysics-informed neural network, which solves PDEs using neural networks by\nminimizing the sum of residuals from the PDEs, BCs, and data, has gained\nsignificant attention in the scientific community. In this study, we\ninvestigate a physics-informed linear model (PILM) that uses linear\ncombinations of basis functions to represent solutions, thereby enabling an\nanalytical representation of optimal solutions. The PILM was formulated and\nverified for illustrative forward and inverse problems including cases with\nuncertain BCs. Furthermore, the PILM was applied to estimate crustal strain\nrates using geodetic data. Specifically, physical regularization that enforces\nelastic equilibrium on the velocity fields was compared with mathematical\nregularization that imposes smoothness constraints. From a Bayesian\nperspective, mathematical regularization exhibited superior performance. The\nPILM provides an analytically solvable framework applicable to linear forward\nand inverse problems, underdetermined systems, and physical regularization.\n","authors":["Tomohisa Okazaki"],"pdf_url":"https://arxiv.org/pdf/2507.12218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12202v1","updated":"2025-07-16T12:57:43Z","published":"2025-07-16T12:57:43Z","title":"Sparse Autoencoders for Sequential Recommendation Models: Interpretation\n  and Flexible Control","summary":"  Many current state-of-the-art models for sequential recommendations are based\non transformer architectures. Interpretation and explanation of such black box\nmodels is an important research question, as a better understanding of their\ninternals can help understand, influence, and control their behavior, which is\nvery important in a variety of real-world applications. Recently sparse\nautoencoders (SAE) have been shown to be a promising unsupervised approach for\nextracting interpretable features from language models. These autoencoders\nlearn to reconstruct hidden states of the transformer's internal layers from\nsparse linear combinations of directions in their activation space.\n  This paper is focused on the application of SAE to the sequential\nrecommendation domain. We show that this approach can be successfully applied\nto the transformer trained on a sequential recommendation task: learned\ndirections turn out to be more interpretable and monosemantic than the original\nhidden state dimensions. Moreover, we demonstrate that the features learned by\nSAE can be used to effectively and flexibly control the model's behavior,\nproviding end-users with a straightforward method to adjust their\nrecommendations to different custom scenarios and contexts.\n","authors":["Anton Klenitskiy","Konstantin Polev","Daria Denisova","Alexey Vasilev","Dmitry Simakov","Gleb Gusev"],"pdf_url":"https://arxiv.org/pdf/2507.12202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10301v3","updated":"2025-07-16T12:50:28Z","published":"2023-09-19T04:04:59Z","title":"Prominent Roles of Conditionally Invariant Components in Domain\n  Adaptation: Theory and Algorithms","summary":"  Domain adaptation (DA) is a statistical learning problem that arises when the\ndistribution of the source data used to train a model differs from that of the\ntarget data used to evaluate the model. While many DA algorithms have\ndemonstrated considerable empirical success, blindly applying these algorithms\ncan often lead to worse performance on new datasets. To address this, it is\ncrucial to clarify the assumptions under which a DA algorithm has good target\nperformance. In this work, we focus on the assumption of the presence of\nconditionally invariant components (CICs), which are relevant for prediction\nand remain conditionally invariant across the source and target data. We\ndemonstrate that CICs, which can be estimated through conditional invariant\npenalty (CIP), play three prominent roles in providing target risk guarantees\nin DA. First, we propose a new algorithm based on CICs, importance-weighted\nconditional invariant penalty (IW-CIP), which has target risk guarantees beyond\nsimple settings such as covariate shift and label shift. Second, we show that\nCICs help identify large discrepancies between source and target risks of other\nDA algorithms. Finally, we demonstrate that incorporating CICs into the domain\ninvariant projection (DIP) algorithm can address its failure scenario caused by\nlabel-flipping features. We support our new algorithms and theoretical findings\nvia numerical experiments on synthetic data, MNIST, CelebA, Camelyon17, and\nDomainNet datasets.\n","authors":["Keru Wu","Yuansi Chen","Wooseok Ha","Bin Yu"],"pdf_url":"https://arxiv.org/pdf/2309.10301v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12196v1","updated":"2025-07-16T12:46:04Z","published":"2025-07-16T12:46:04Z","title":"Selective Quantization Tuning for ONNX Models","summary":"  Quantization is a process that reduces the precision of deep neural network\nmodels to lower model size and computational demands, often at the cost of\naccuracy. However, fully quantized models may exhibit sub-optimal performance\nbelow acceptable levels and face deployment challenges on low-end hardware\naccelerators due to practical constraints. To address these issues,\nquantization can be selectively applied to only a subset of layers, but\nselecting which layers to exclude is non-trivial. To this direction, we propose\nTuneQn, a suite enabling selective quantization, deployment and execution of\nONNX models across various CPU and GPU devices, combined with profiling and\nmulti-objective optimization. TuneQn generates selectively quantized ONNX\nmodels, deploys them on different hardware, measures performance on metrics\nlike accuracy and size, performs Pareto Front minimization to identify the best\nmodel candidate and visualizes the results. To demonstrate the effectiveness of\nTuneQn, we evaluated TuneQn on four ONNX models with two quantization settings\nacross CPU and GPU devices. As a result, we demonstrated that our utility\neffectively performs selective quantization and tuning, selecting ONNX model\ncandidates with up to a $54.14$% reduction in accuracy loss compared to the\nfully quantized model, and up to a $72.9$% model size reduction compared to the\noriginal model.\n","authors":["Nikolaos Louloudakis","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2507.12196v1.pdf","comment":"5 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2507.12192v1","updated":"2025-07-16T12:44:25Z","published":"2025-07-16T12:44:25Z","title":"Explainable Evidential Clustering","summary":"  Unsupervised classification is a fundamental machine learning problem.\nReal-world data often contain imperfections, characterized by uncertainty and\nimprecision, which are not well handled by traditional methods. Evidential\nclustering, based on Dempster-Shafer theory, addresses these challenges. This\npaper explores the underexplored problem of explaining evidential clustering\nresults, which is crucial for high-stakes domains such as healthcare. Our\nanalysis shows that, in the general case, representativity is a necessary and\nsufficient condition for decision trees to serve as abductive explainers.\nBuilding on the concept of representativity, we generalize this idea to\naccommodate partial labeling through utility functions. These functions enable\nthe representation of \"tolerable\" mistakes, leading to the definition of\nevidential mistakeness as explanation cost and the construction of explainers\ntailored to evidential classifiers. Finally, we propose the Iterative\nEvidential Mistake Minimization (IEMM) algorithm, which provides interpretable\nand cautious decision tree explanations for evidential clustering functions. We\nvalidate the proposed algorithm on synthetic and real-world data. Taking into\naccount the decision-maker's preferences, we were able to provide an\nexplanation that was satisfactory up to 93% of the time.\n","authors":["Victor F. Lopes de Souza","Karima Bakhti","Sofiane Ramdani","Denis Mottet","Abdelhak Imoussaten"],"pdf_url":"https://arxiv.org/pdf/2507.12192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12189v1","updated":"2025-07-16T12:43:25Z","published":"2025-07-16T12:43:25Z","title":"BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum\n  architecture search","summary":"  We introduce BenchRL-QAS, a unified benchmarking framework for systematically\nevaluating reinforcement learning (RL) algorithms in quantum architecture\nsearch (QAS) across diverse variational quantum algorithm tasks and system\nsizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including\nboth value-based and policy-gradient methods on representative quantum problems\nsuch as variational quantum eigensolver, variational quantum state\ndiagonalization, quantum classification, and state preparation, spanning both\nnoiseless and realistic noisy regimes. We propose a weighted ranking metric\nthat balances accuracy, circuit depth, gate count, and computational\nefficiency, enabling fair and comprehensive comparison. Our results first\nreveal that RL-based quantum classifier outperforms baseline variational\nclassifiers. Then we conclude that no single RL algorithm is universally\noptimal when considering a set of QAS tasks; algorithmic performance is highly\ncontext-dependent, varying with task structure, qubit count, and noise. This\nempirical finding provides strong evidence for the \"no free lunch\" principle in\nRL-based quantum circuit design and highlights the necessity of tailored\nalgorithm selection and systematic benchmarking for advancing quantum circuit\nsynthesis. This work represents the most comprehensive RL-QAS benchmarking\neffort to date, and BenchRL-QAS along with all experimental data are made\npublicly available to support reproducibility and future research\nhttps://github.com/azhar-ikhtiarudin/bench-rlqas.\n","authors":["Azhar Ikhtiarudin","Aditi Das","Param Thakkar","Akash Kundu"],"pdf_url":"https://arxiv.org/pdf/2507.12189v1.pdf","comment":"Comprehensive RL agent benchmark for QAS. Contributions are welcomed\n  here: https://github.com/azhar-ikhtiarudin/bench-rlqas"},{"id":"http://arxiv.org/abs/2507.10158v2","updated":"2025-07-16T12:39:22Z","published":"2025-07-14T11:17:28Z","title":"MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping","summary":"  Federated Learning (FL) is a promising machine learning paradigm that enables\nparticipating devices to train privacy-preserved and collaborative models. FL\nhas proven its benefits for robotic manipulation tasks. However, grasping tasks\nlack exploration in such settings where robots train a global model without\nmoving data and ensuring data privacy. The main challenge is that each robot\nlearns from data that is nonindependent and identically distributed (non-IID)\nand of low quantity. This exhibits performance degradation, particularly in\nrobotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL\napproach for robotic grasping, acknowledging the unique challenges posed by the\nnon-IID data distribution across robots, including quantitative skewness.\nMTF-Grasp harnesses data quality and quantity across robots to select a set of\n\"top-level\" robots with better data distribution and higher sample count. It\nthen utilizes top-level robots to train initial seed models and distribute them\nto the remaining \"low-level\" robots, reducing the risk of model performance\ndegradation in low-level robots. Our approach outperforms the conventional FL\nsetup by up to 8% on the quantity-skewed Cornell and Jacquard grasping\ndatasets.\n","authors":["Obaidullah Zaland","Erik Elmroth","Monowar Bhuyan"],"pdf_url":"https://arxiv.org/pdf/2507.10158v2.pdf","comment":"The work is accepted for presentation at IEEE SMC 2025"},{"id":"http://arxiv.org/abs/2507.03564v2","updated":"2025-07-16T12:36:59Z","published":"2025-07-04T13:16:59Z","title":"2.5D Object Detection for Intelligent Roadside Infrastructure","summary":"  On-board sensors of autonomous vehicles can be obstructed, occluded, or\nlimited by restricted fields of view, complicating downstream driving\ndecisions. Intelligent roadside infrastructure perception systems, installed at\nelevated vantage points, can provide wide, unobstructed intersection coverage,\nsupplying a complementary information stream to autonomous vehicles via\nvehicle-to-everything (V2X) communication. However, conventional 3D\nobject-detection algorithms struggle to generalize under the domain shift\nintroduced by top-down perspectives and steep camera angles. We introduce a\n2.5D object detection framework, tailored specifically for infrastructure\nroadside-mounted cameras. Unlike conventional 2D or 3D object detection, we\nemploy a prediction approach to detect ground planes of vehicles as\nparallelograms in the image frame. The parallelogram preserves the planar\nposition, size, and orientation of objects while omitting their height, which\nis unnecessary for most downstream applications. For training, a mix of\nreal-world and synthetically generated scenes is leveraged. We evaluate\ngeneralizability on a held-out camera viewpoint and in adverse-weather\nscenarios absent from the training set. Our results show high detection\naccuracy, strong cross-viewpoint generalization, and robustness to diverse\nlighting and weather conditions. Model weights and inference code are provided\nat: https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection\n","authors":["Nikolai Polley","Yacin Boualili","Ferdinand Mütsch","Maximilian Zipfl","Tobias Fleck","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2507.03564v2.pdf","comment":"Accepted at 2025 IEEE 28th International Conference on Intelligent\n  Transportation Systems (ITSC)"},{"id":"http://arxiv.org/abs/2404.05102v3","updated":"2025-07-16T12:28:25Z","published":"2024-04-07T22:58:18Z","title":"LHU-Net: a Lean Hybrid U-Net for Cost-efficient, High-performance\n  Volumetric Segmentation","summary":"  The rise of Transformer architectures has advanced medical image\nsegmentation, leading to hybrid models that combine Convolutional Neural\nNetworks (CNNs) and Transformers. However, these models often suffer from\nexcessive complexity and fail to effectively integrate spatial and channel\nfeatures, crucial for precise segmentation. To address this, we propose\nLHU-Net, a Lean Hybrid U-Net for volumetric medical image segmentation. LHU-Net\nprioritizes spatial feature extraction before refining channel features,\noptimizing both efficiency and accuracy. Evaluated on four benchmark datasets\n(Synapse, Left Atrial, BraTS-Decathlon, and Lung-Decathlon), LHU-Net\nconsistently outperforms existing models across diverse modalities (CT/MRI) and\noutput configurations. It achieves state-of-the-art Dice scores while using\nfour times fewer parameters and 20% fewer FLOPs than competing models, without\nthe need for pre-training, additional data, or model ensembles. With an average\nof 11 million parameters, LHU-Net sets a new benchmark for computational\nefficiency and segmentation accuracy. Our implementation is available on\nGitHub: https://github.com/xmindflow/LHUNet\n","authors":["Yousef Sadegheih","Afshin Bozorgpour","Pratibha Kumari","Reza Azad","Dorit Merhof"],"pdf_url":"https://arxiv.org/pdf/2404.05102v3.pdf","comment":"Accepted at MICCAI-2025"},{"id":"http://arxiv.org/abs/2507.09888v2","updated":"2025-07-16T12:16:13Z","published":"2025-07-14T03:48:48Z","title":"NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting","summary":"  Time series forecasting is a fundamental task with broad applications, yet\nconventional methods often treat data as discrete sequences, overlooking their\norigin as noisy samples of continuous processes. Crucially, discrete noisy\nobservations cannot uniquely determine a continuous function; instead, they\ncorrespond to a family of plausible functions. Mathematically, time series can\nbe viewed as noisy observations of a continuous function family governed by a\nshared probability measure. Thus, the forecasting task can be framed as\nlearning the transition from the historical function family to the future\nfunction family. This reframing introduces two key challenges: (1) How can we\nleverage discrete historical and future observations to learn the relationships\nbetween their underlying continuous functions? (2) How can we model the\ntransition path in function space from the historical function family to the\nfuture function family? To address these challenges, we propose NeuTSFlow, a\nnovel framework that leverages Neural Operators to facilitate flow matching for\nlearning path of measure between historical and future function families. By\nparameterizing the velocity field of the flow in infinite-dimensional function\nspaces, NeuTSFlow moves beyond traditional methods that focus on dependencies\nat discrete points, directly modeling function-level features instead.\nExperiments on diverse forecasting tasks demonstrate NeuTSFlow's superior\naccuracy and robustness, validating the effectiveness of the function-family\nperspective.\n","authors":["Huibo Xu","Likang Wu","Xianquan Wang","Haoning Dang","Chun-Wun Cheng","Angelica I Aviles-Rivero","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2507.09888v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12175v1","updated":"2025-07-16T12:13:13Z","published":"2025-07-16T12:13:13Z","title":"RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance\n  Alignment, Transcription, and Mistake Detection","summary":"  This study introduces RUMAA, a transformer-based framework for music\nperformance analysis that unifies score-to-performance alignment,\nscore-informed transcription, and mistake detection in a near end-to-end\nmanner. Unlike prior methods addressing these tasks separately, RUMAA\nintegrates them using pre-trained score and audio encoders and a novel\ntri-stream decoder capturing task interdependencies through proxy tasks. It\naligns human-readable MusicXML scores with repeat symbols to full-length\nperformance audio, overcoming traditional MIDI-based methods that rely on\nmanually unfolded score-MIDI data with pre-specified repeat structures. RUMAA\nmatches state-of-the-art alignment methods on non-repeated scores and\noutperforms them on scores with repeats in a public piano music dataset, while\nalso delivering promising transcription and mistake detection results.\n","authors":["Sungkyun Chang","Simon Dixon","Emmanouil Benetos"],"pdf_url":"https://arxiv.org/pdf/2507.12175v1.pdf","comment":"Accepted to WASPAA 2025"},{"id":"http://arxiv.org/abs/2312.12216v2","updated":"2025-07-16T12:10:49Z","published":"2023-12-19T15:05:52Z","title":"Sharing is CAIRing: Characterizing Principles and Assessing Properties\n  of Universal Privacy Evaluation for Synthetic Tabular Data","summary":"  Data sharing is a necessity for innovative progress in many domains,\nespecially in healthcare. However, the ability to share data is hindered by\nregulations protecting the privacy of natural persons. Synthetic tabular data\nprovide a promising solution to address data sharing difficulties but does not\ninherently guarantee privacy. Still, there is a lack of agreement on\nappropriate methods for assessing the privacy-preserving capabilities of\nsynthetic data, making it difficult to compare results across studies. To the\nbest of our knowledge, this is the first work to identify properties that\nconstitute good universal privacy evaluation metrics for synthetic tabular\ndata. The goal of universally applicable metrics is to enable comparability\nacross studies and to allow non-technical stakeholders to understand how\nprivacy is protected. We identify four principles for the assessment of\nmetrics: Comparability, Applicability, Interpretability, and Representativeness\n(CAIR). To quantify and rank the degree to which evaluation metrics conform to\nthe CAIR principles, we design a rubric using a scale of 1-4. Each of the four\nproperties is scored on four parameters, yielding 16 total dimensions. We study\nthe applicability and usefulness of the CAIR principles and rubric by assessing\na selection of metrics popular in other studies. The results provide granular\ninsights into the strengths and weaknesses of existing metrics that not only\nrank the metrics but highlight areas of potential improvements. We expect that\nthe CAIR principles will foster agreement among researchers and organizations\non which universal privacy evaluation metrics are appropriate for synthetic\ntabular data.\n","authors":["Tobias Hyrup","Anton Danholt Lautrup","Arthur Zimek","Peter Schneider-Kamp"],"pdf_url":"https://arxiv.org/pdf/2312.12216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08802v4","updated":"2025-07-16T12:09:34Z","published":"2024-02-05T14:20:19Z","title":"Governance of Generative Artificial Intelligence for Companies","summary":"  Generative Artificial Intelligence (GenAI), specifically large language\nmodels(LLMs) like ChatGPT, has swiftly entered organizations without adequate\ngovernance, posing both opportunities and risks. Despite extensive debates on\nGenAI's transformative nature and regulatory measures, limited research\naddresses organizational governance, encompassing technical and business\nperspectives. Although numerous frameworks for governance of AI exist, it is\nnot clear to what extent they apply to GenAI. Our review paper fills this gap\nby surveying recent works with the purpose of better understanding fundamental\ncharacteristics of GenAI and adjusting prior frameworks specifically towards\nGenAI governance within companies. To do so, it extends Nickerson's framework\ndevelopment processes to include prior conceptualizations. Our framework\noutlines the scope, objectives, and governance mechanisms tailored to harness\nbusiness opportunities as well as mitigate risks associated with GenAI\nintegration. Our research contributes a focused approach to GenAI governance,\noffering practical insights for companies navigating the challenges of GenAI\nadoption and highlighting research gaps.\n","authors":["Johannes Schneider","Pauline Kuss","Rene Abraham","Christian Meske"],"pdf_url":"https://arxiv.org/pdf/2403.08802v4.pdf","comment":"This paper is under submission"},{"id":"http://arxiv.org/abs/2507.12166v1","updated":"2025-07-16T11:54:08Z","published":"2025-07-16T11:54:08Z","title":"RadioDiff-3D: A 3D$\\times$3D Radio Map Dataset and Generative Diffusion\n  Based Benchmark for 6G Environment-Aware Communication","summary":"  Radio maps (RMs) serve as a critical foundation for enabling\nenvironment-aware wireless communication, as they provide the spatial\ndistribution of wireless channel characteristics. Despite recent progress in RM\nconstruction using data-driven approaches, most existing methods focus solely\non pathloss prediction in a fixed 2D plane, neglecting key parameters such as\ndirection of arrival (DoA), time of arrival (ToA), and vertical spatial\nvariations. Such a limitation is primarily due to the reliance on static\nlearning paradigms, which hinder generalization beyond the training data\ndistribution. To address these challenges, we propose UrbanRadio3D, a\nlarge-scale, high-resolution 3D RM dataset constructed via ray tracing in\nrealistic urban environments. UrbanRadio3D is over 37$\\times$3 larger than\nprevious datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,\nforming a novel 3D$\\times$33D dataset with 7$\\times$3 more height layers than\nprior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet\nwith 3D convolutional operators is proposed. Moreover, we further introduce\nRadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D\nconvolutional architecture. RadioDiff-3D supports both radiation-aware\nscenarios with known transmitter locations and radiation-unaware settings based\non sparse spatial observations. Extensive evaluations on UrbanRadio3D validate\nthat RadioDiff-3D achieves superior performance in constructing rich,\nhigh-dimensional radio maps under diverse environmental dynamics. This work\nprovides a foundational dataset and benchmark for future research in 3D\nenvironment-aware communication. The dataset is available at\nhttps://github.com/UNIC-Lab/UrbanRadio3D.\n","authors":["Xiucheng Wang","Qiming Zhang","Nan Cheng","Junting Chen","Zezhong Zhang","Zan Li","Shuguang Cui","Xuemin Shen"],"pdf_url":"https://arxiv.org/pdf/2507.12166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12165v1","updated":"2025-07-16T11:53:08Z","published":"2025-07-16T11:53:08Z","title":"Multi-Component VAE with Gaussian Markov Random Field","summary":"  Multi-component datasets with intricate dependencies, like industrial\nassemblies or multi-modal imaging, challenge current generative modeling\ntechniques. Existing Multi-component Variational AutoEncoders typically rely on\nsimplified aggregation strategies, neglecting critical nuances and consequently\ncompromising structural coherence across generated components. To explicitly\naddress this gap, we introduce the Gaussian Markov Random Field Multi-Component\nVariational AutoEncoder , a novel generative framework embedding Gaussian\nMarkov Random Fields into both prior and posterior distributions. This design\nchoice explicitly models cross-component relationships, enabling richer\nrepresentation and faithful reproduction of complex interactions. Empirically,\nour GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula\ndataset specifically constructed to evaluate intricate component relationships,\ndemonstrates competitive results on the PolyMNIST benchmark, and significantly\nenhances structural coherence on the real-world BIKED dataset. Our results\nindicate that the GMRF MCVAE is especially suited for practical applications\ndemanding robust and realistic modeling of multi-component coherence\n","authors":["Fouad Oubari","Mohamed El-Baha","Raphael Meunier","Rodrigue Décatoire","Mathilde Mougeot"],"pdf_url":"https://arxiv.org/pdf/2507.12165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16425v2","updated":"2025-07-16T11:52:13Z","published":"2024-12-21T01:23:58Z","title":"Patherea: Cell Detection and Classification for the 2020s","summary":"  We present Patherea, a unified framework for point-based cell detection and\nclassification that enables the development and fair evaluation of\nstate-of-the-art methods. To support this, we introduce a large-scale dataset\nthat replicates the clinical workflow for Ki-67 proliferation index estimation.\nOur method directly predicts cell locations and classes without relying on\nintermediate representations. It incorporates a hybrid Hungarian matching\nstrategy for accurate point assignment and supports flexible backbones and\ntraining regimes, including recent pathology foundation models. Patherea\nachieves state-of-the-art performance on public datasets - Lizard, BRCA-M2C,\nand BCData - while highlighting performance saturation on these benchmarks. In\ncontrast, our newly proposed Patherea dataset presents a significantly more\nchallenging benchmark. Additionally, we identify and correct common errors in\ncurrent evaluation protocols and provide an updated benchmarking utility for\nstandardized assessment. The Patherea dataset and code are publicly available\nto facilitate further research and fair comparisons.\n","authors":["Dejan Štepec","Maja Jerše","Snežana Đokić","Jera Jeruc","Nina Zidar","Danijel Skočaj"],"pdf_url":"https://arxiv.org/pdf/2412.16425v2.pdf","comment":"Submitted to Medical Image Analysis"},{"id":"http://arxiv.org/abs/2403.15740v3","updated":"2025-07-16T11:45:02Z","published":"2024-03-23T06:36:32Z","title":"Protecting Copyrighted Material with Unique Identifiers in Large\n  Language Model Training","summary":"  A primary concern regarding training large language models (LLMs) is whether\nthey abuse copyrighted online text. With the increasing training data scale and\nthe prevalence of LLMs in daily lives, two problems arise: \\textbf{1)} false\npositive membership inference results misled by similar examples; \\textbf{2)}\nmembership inference methods are usually too complex for end users to\nunderstand and use. To address these issues, we propose an alternative\n\\textit{insert-and-detect} methodology, advocating that web users and content\nplatforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{\\textbf{ghost\nsentences}} and a user-friendly last-$k$ words test, allowing end users to chat\nwith LLMs for membership inference. Ghost sentences consist primarily of unique\npassphrases of random natural words, which can come with customized elements to\nbypass possible filter rules. The last-$k$ words test requires a significant\nrepetition time of ghost sentences~($\\ge10$). For cases with fewer repetitions,\nwe designed an extra perplexity test, as LLMs exhibit high perplexity when\nencountering unnatural passphrases. We also conduct a comprehensive study on\nthe memorization and membership inference of ghost sentences, examining factors\nsuch as training data scales, model sizes, repetition times, insertion\npositions, wordlist of passphrases, alignment, \\textit{etc}. Our study shows\nthe possibility of applying ghost sentences in real scenarios and provides\ninstructions for the potential application.\n","authors":["Shuai Zhao","Linchao Zhu","Ruijie Quan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15740v3.pdf","comment":"A technical report, work mainly done in the early of 2024"},{"id":"http://arxiv.org/abs/2507.11439v2","updated":"2025-07-16T11:40:43Z","published":"2025-07-15T16:01:58Z","title":"Data Augmentation in Time Series Forecasting through Inverted Framework","summary":"  Currently, iTransformer is one of the most popular and effective models for\nmultivariate time series (MTS) forecasting. Thanks to its inverted framework,\niTransformer effectively captures multivariate correlation. However, the\ninverted framework still has some limitations. It diminishes temporal\ninterdependency information, and introduces noise in cases of nonsignificant\nvariable correlation. To address these limitations, we introduce a novel data\naugmentation method on inverted framework, called DAIF. Unlike previous data\naugmentation methods, DAIF stands out as the first real-time augmentation\nspecifically designed for the inverted framework in MTS forecasting. We first\ndefine the structure of the inverted sequence-to-sequence framework, then\npropose two different DAIF strategies, Frequency Filtering and Cross-variation\nPatching to address the existing challenges of the inverted framework.\nExperiments across multiple datasets and inverted models have demonstrated the\neffectiveness of our DAIF.\n","authors":["Hongming Tan","Ting Chen","Ruochong Jin","Wai Kin Chan"],"pdf_url":"https://arxiv.org/pdf/2507.11439v2.pdf","comment":"The paper is under consideration at Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2411.09127v2","updated":"2025-07-16T11:39:10Z","published":"2024-11-14T02:00:22Z","title":"Complexity-Aware Training of Deep Neural Networks for Optimal Structure\n  Discovery","summary":"  We propose a novel algorithm for combined unit and layer pruning of deep\nneural networks that functions during training and without requiring a\npre-trained network to apply. Our algorithm optimally trades-off learning\naccuracy and pruning levels while balancing layer vs. unit pruning and\ncomputational vs. parameter complexity using only three user-defined\nparameters, which are easy to interpret and tune. We formulate a stochastic\noptimization problem over the network weights and the parameters of variational\nBernoulli distributions for binary Random Variables taking values either 0 or 1\nand scaling the units and layers of the network. Optimal network structures are\nfound as the solution to this optimization problem. Pruning occurs when a\nvariational parameter converges to 0 rendering the corresponding structure\npermanently inactive, thus saving computations both during training and\nprediction. A key contribution of our approach is to define a cost function\nthat combines the objectives of prediction accuracy and network pruning in a\ncomputational/parameter complexity-aware manner and the automatic selection of\nthe many regularization parameters. We show that the proposed algorithm\nconverges to solutions of the optimization problem corresponding to\ndeterministic networks. We analyze the ODE system that underlies our stochastic\noptimization algorithm and establish domains of attraction for the dynamics of\nthe network parameters. These theoretical results lead to practical pruning\nconditions avoiding the premature pruning of units and layers during training.\nWe evaluate our method on the CIFAR-10/100 and ImageNet datasets using ResNet\narchitectures and demonstrate that it gives improved results with respect to\npruning ratios and test accuracy over layer-only or unit-only pruning and\nfavorably competes with combined unit and layer pruning algorithms requiring\npre-trained networks.\n","authors":["Valentin Frank Ingmar Guenter","Athanasios Sideris"],"pdf_url":"https://arxiv.org/pdf/2411.09127v2.pdf","comment":"31 pages, 6 figures, 6 tables. Restructured Sections 1 and 3, added\n  simulation data in Section 5 and added appendices"},{"id":"http://arxiv.org/abs/2507.12145v1","updated":"2025-07-16T11:25:03Z","published":"2025-07-16T11:25:03Z","title":"PRISM: Distributed Inference for Foundation Models at Edge","summary":"  Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.\n","authors":["Muhammad Azlan Qazi","Alexandros Iosifidis","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.12145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12144v1","updated":"2025-07-16T11:22:18Z","published":"2025-07-16T11:22:18Z","title":"FourCastNet 3: A geometric approach to probabilistic machine-learning\n  weather forecasting at scale","summary":"  FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 90-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 20 seconds. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions.\n","authors":["Boris Bonev","Thorsten Kurth","Ankur Mahesh","Mauro Bisson","Jean Kossaifi","Karthik Kashinath","Anima Anandkumar","William D. Collins","Michael S. Pritchard","Alexander Keller"],"pdf_url":"https://arxiv.org/pdf/2507.12144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12142v1","updated":"2025-07-16T11:17:12Z","published":"2025-07-16T11:17:12Z","title":"RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA\n  Optimization","summary":"  Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.\n","authors":["Vladimir Bogachev","Vladimir Aletov","Alexander Molozhavenko","Denis Bobkov","Vera Soboleva","Aibek Alanov","Maxim Rakhuba"],"pdf_url":"https://arxiv.org/pdf/2507.12142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12138v1","updated":"2025-07-16T11:11:18Z","published":"2025-07-16T11:11:18Z","title":"Neural Human Pose Prior","summary":"  We introduce a principled, data-driven approach for modeling a neural prior\nover human body poses using normalizing flows. Unlike heuristic or\nlow-expressivity alternatives, our method leverages RealNVP to learn a flexible\ndensity over poses represented in the 6D rotation format. We address the\nchallenge of modeling distributions on the manifold of valid 6D rotations by\ninverting the Gram-Schmidt process during training, enabling stable learning\nwhile preserving downstream compatibility with rotation-based frameworks. Our\narchitecture and training pipeline are framework-agnostic and easily\nreproducible. We demonstrate the effectiveness of the learned prior through\nboth qualitative and quantitative evaluations, and we analyze its impact via\nablation studies. This work provides a sound probabilistic foundation for\nintegrating pose priors into human motion capture and reconstruction pipelines.\n","authors":["Michal Heker","Sefy Kararlitsky","David Tolpin"],"pdf_url":"https://arxiv.org/pdf/2507.12138v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2506.23210v2","updated":"2025-07-16T11:06:41Z","published":"2025-06-29T12:41:11Z","title":"FedRef: Communication-Efficient Bayesian Fine Tuning with Reference\n  Model","summary":"  Federated learning(FL) is used for distributed scenarios to train artificial\nintelligence(AI) models while ensuring users' privacy. In federated learning\nscenario, the server generally never knows about users' data. This type of\nconcept makes the AI training process efficient in terms of data privacy.\nHowever, regarding model performance, federated AI models may not sufficiently\nsatisfy AI users' expectations. Furthermore, AI users have a wide range of\ndifferent needs. It is not easy to satisfy the whole users needs. These types\nof issues can be addressed through AI model optimization, fine-tuning, or\npersonalization to achieve optimal model performance. To address model\noptimization challenges, we propose reference model-based federated learning\nfor optimal fine-tuning, which overcomes catastrophic forgetting in each round.\nThis method is derived from Bayesian parameter-efficient transfer learning,\nwhich includes an optimal proximal term and utilizes a reference model that\nincorporates previous model parameters. As a result, this method achieves both\nhigh model performance and clients' low computing cost.\n","authors":["Taehwan Yoon","Bongjun Choi"],"pdf_url":"https://arxiv.org/pdf/2506.23210v2.pdf","comment":"6 pages,14 equation, 4 figure, 1table"},{"id":"http://arxiv.org/abs/2507.12133v1","updated":"2025-07-16T11:02:11Z","published":"2025-07-16T11:02:11Z","title":"HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with\n  Optimized VMD","summary":"  Device recognition is vital for security in wireless communication systems,\nparticularly for applications like access control. Radio Frequency Fingerprint\nIdentification (RFFI) offers a non-cryptographic solution by exploiting\nhardware-induced signal distortions. This paper proposes HyDRA, a Hybrid\nDual-mode RF Architecture that integrates an optimized Variational Mode\nDecomposition (VMD) with a novel architecture based on the fusion of\nConvolutional Neural Networks (CNNs), Transformers, and Mamba components,\ndesigned to support both closed-set and open-set classification tasks. The\noptimized VMD enhances preprocessing efficiency and classification accuracy by\nfixing center frequencies and using closed-form solutions. HyDRA employs the\nTransformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and\nthe Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting\nto varying conditions. Evaluation on public datasets demonstrates\nstate-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance\nin our proposed open-set classification method, effectively identifying\nunauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves\nmillisecond-level inference speed with low power consumption, providing a\npractical solution for real-time wireless authentication in real-world\nenvironments.\n","authors":["Hanwen Liu","Yuhe Huang","Yifeng Gong","Yanjie Zhai","Jiaxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2507.12133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12127v1","updated":"2025-07-16T10:53:19Z","published":"2025-07-16T10:53:19Z","title":"Self-Adaptive and Robust Federated Spectrum Sensing without Benign\n  Majority for Cellular Networks","summary":"  Advancements in wireless and mobile technologies, including 5G advanced and\nthe envisioned 6G, are driving exponential growth in wireless devices. However,\nthis rapid expansion exacerbates spectrum scarcity, posing a critical\nchallenge. Dynamic spectrum allocation (DSA)--which relies on sensing and\ndynamically sharing spectrum--has emerged as an essential solution to address\nthis issue. While machine learning (ML) models hold significant potential for\nimproving spectrum sensing, their adoption in centralized ML-based DSA systems\nis limited by privacy concerns, bandwidth constraints, and regulatory\nchallenges. To overcome these limitations, distributed ML-based approaches such\nas Federated Learning (FL) offer promising alternatives. This work addresses\ntwo key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of\nlabeled data for training FL models in practical spectrum sensing scenarios is\ntackled with a semi-supervised FL approach, combined with energy detection,\nenabling model training on unlabeled datasets. Second, we examine the security\nvulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our\nanalysis highlights the shortcomings of existing majority-based defenses in\ncountering such attacks. To address these vulnerabilities, we propose a novel\ndefense mechanism inspired by vaccination, which effectively mitigates data\npoisoning attacks without relying on majority-based assumptions. Extensive\nexperiments on both synthetic and real-world datasets validate our solutions,\ndemonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets\nand maintain Byzantine robustness against both targeted and untargeted data\npoisoning attacks, even when a significant proportion of participants are\nmalicious.\n","authors":["Ngoc Duy Pham","Thusitha Dayaratne","Viet Vo","Shangqi Lai","Sharif Abuadbba","Hajime Suzuki","Xingliang Yuan","Carsten Rudolph"],"pdf_url":"https://arxiv.org/pdf/2507.12127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12126v1","updated":"2025-07-16T10:49:30Z","published":"2025-07-16T10:49:30Z","title":"Iterative Augmentation with Summarization Refinement (IASR) Evaluation\n  for Unstructured Survey data Modeling and Analysis","summary":"  Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.\n","authors":["Payal Bhattad","Sai Manoj Pudukotai Dinakarrao","Anju Gupta"],"pdf_url":"https://arxiv.org/pdf/2507.12126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11381v2","updated":"2025-07-16T10:38:29Z","published":"2025-07-15T14:50:41Z","title":"From Observational Data to Clinical Recommendations: A Causal Framework\n  for Estimating Patient-level Treatment Effects and Learning Policies","summary":"  We propose a framework for building patient-specific treatment recommendation\nmodels, building on the large recent literature on learning patient-level\ncausal models and inspired by the target trial paradigm of Hernan and Robins.\nWe focus on safety and validity, including the crucial issue of causal\nidentification when using observational data. We do not provide a specific\nmodel, but rather a way to integrate existing methods and know-how into a\npractical pipeline. We further provide a real world use-case of treatment\noptimization for patients with heart failure who develop acute kidney injury\nduring hospitalization. The results suggest our pipeline can improve patient\noutcomes over the current treatment regime.\n","authors":["Rom Gutman","Shimon Sheiba","Omer Noy Klein","Naama Dekel Bird","Amit Gruber","Doron Aronson","Oren Caspi","Uri Shalit"],"pdf_url":"https://arxiv.org/pdf/2507.11381v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12272v4","updated":"2025-07-16T10:34:26Z","published":"2025-02-17T19:16:37Z","title":"Learning to Reason at the Frontier of Learnability","summary":"  Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.\n","authors":["Thomas Foster","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2502.12272v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12108v1","updated":"2025-07-16T10:25:45Z","published":"2025-07-16T10:25:45Z","title":"Multimodal Coordinated Online Behavior: Trade-offs and Strategies","summary":"  Coordinated online behavior, which spans from beneficial collective actions\nto harmful manipulation such as disinformation campaigns, has become a key\nfocus in digital ecosystem analysis. Traditional methods often rely on\nmonomodal approaches, focusing on single types of interactions like co-retweets\nor co-hashtags, or consider multiple modalities independently of each other.\nHowever, these approaches may overlook the complex dynamics inherent in\nmultimodal coordination. This study compares different ways of operationalizing\nthe detection of multimodal coordinated behavior. It examines the trade-off\nbetween weakly and strongly integrated multimodal models, highlighting the\nbalance between capturing broader coordination patterns and identifying tightly\ncoordinated behavior. By comparing monomodal and multimodal approaches, we\nassess the unique contributions of different data modalities and explore how\nvarying implementations of multimodality impact detection outcomes. Our\nfindings reveal that not all the modalities provide distinct insights, but that\nwith a multimodal approach we can get a more comprehensive understanding of\ncoordination dynamics. This work enhances the ability to detect and analyze\ncoordinated online behavior, offering new perspectives for safeguarding the\nintegrity of digital platforms.\n","authors":["Lorenzo Mannocci","Stefano Cresci","Matteo Magnani","Anna Monreale","Maurizio Tesconi"],"pdf_url":"https://arxiv.org/pdf/2507.12108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12098v1","updated":"2025-07-16T10:07:19Z","published":"2025-07-16T10:07:19Z","title":"A Privacy-Preserving Framework for Advertising Personalization\n  Incorporating Federated Learning and Differential Privacy","summary":"  To mitigate privacy leakage and performance issues in personalized\nadvertising, this paper proposes a framework that integrates federated learning\nand differential privacy. The system combines distributed feature extraction,\ndynamic privacy budget allocation, and robust model aggregation to balance\nmodel accuracy, communication overhead, and privacy protection. Multi-party\nsecure computing and anomaly detection mechanisms further enhance system\nresilience against malicious attacks. Experimental results demonstrate that the\nframework achieves dual optimization of recommendation accuracy and system\nefficiency while ensuring privacy, providing both a practical solution and a\ntheoretical foundation for applying privacy protection technologies in\nadvertisement recommendation.\n","authors":["Xiang Li","Yifan Lin","Yuanzhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.12098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05668v2","updated":"2025-07-16T10:04:45Z","published":"2025-02-08T19:09:16Z","title":"The late-stage training dynamics of (stochastic) subgradient descent on\n  homogeneous neural networks","summary":"  We analyze the implicit bias of constant step stochastic subgradient descent\n(SGD). We consider the setting of binary classification with homogeneous neural\nnetworks - a large class of deep neural networks with ReLU-type activation\nfunctions such as MLPs and CNNs without biases. We interpret the dynamics of\nnormalized SGD iterates as an Euler-like discretization of a conservative field\nflow that is naturally associated to the normalized classification margin.\nOwing to this interpretation, we show that normalized SGD iterates converge to\nthe set of critical points of the normalized margin at late-stage training\n(i.e., assuming that the data is correctly classified with positive normalized\nmargin). Up to our knowledge, this is the first extension of the analysis of\nLyu and Li (2020) on the discrete dynamics of gradient descent to the nonsmooth\nand stochastic setting. Our main result applies to binary classification with\nexponential or logistic losses. We additionally discuss extensions to more\ngeneral settings.\n","authors":["Sholom Schechtman","Nicolas Schreuder"],"pdf_url":"https://arxiv.org/pdf/2502.05668v2.pdf","comment":"Accepted/presented at the 38th Annual Conference on Learning Theory\n  (COLT 2025)"},{"id":"http://arxiv.org/abs/2507.12094v1","updated":"2025-07-16T10:01:22Z","published":"2025-07-16T10:01:22Z","title":"Measuring Informativeness Gap of (Mis)Calibrated Predictors","summary":"  In many applications, decision-makers must choose between multiple predictive\nmodels that may all be miscalibrated. Which model (i.e., predictor) is more\n\"useful\" in downstream decision tasks? To answer this, our first contribution\nintroduces the notion of the informativeness gap between any two predictors,\ndefined as the maximum normalized payoff advantage one predictor offers over\nthe other across all decision-making tasks. Our framework strictly generalizes\nseveral existing notions: it subsumes U-Calibration [KLST-23] and Calibration\nDecision Loss [HW-24], which compare a miscalibrated predictor to its\ncalibrated counterpart, and it recovers Blackwell informativeness [Bla-51,\nBla-53] as a special case when both predictors are perfectly calibrated. Our\nsecond contribution is a dual characterization of the informativeness gap,\nwhich gives rise to a natural informativeness measure that can be viewed as a\nrelaxed variant of the earth mover's distance (EMD) between two prediction\ndistributions. We show that this measure satisfies natural desiderata: it is\ncomplete and sound, and it can be estimated sample-efficiently in the\nprediction-only access setting. Along the way, we also obtain novel\ncombinatorial structural results when applying this measure to perfectly\ncalibrated predictors.\n","authors":["Yiding Feng","Wei Tang"],"pdf_url":"https://arxiv.org/pdf/2507.12094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12091v1","updated":"2025-07-16T09:54:08Z","published":"2025-07-16T09:54:08Z","title":"Improved Analysis for Sign-based Methods with Momentum Updates","summary":"  In this paper, we present enhanced analysis for sign-based optimization\nalgorithms with momentum updates. Traditional sign-based methods, under the\nseparable smoothness assumption, guarantee a convergence rate of\n$\\mathcal{O}(T^{-1/4})$, but they either require large batch sizes or assume\nunimodal symmetric stochastic noise. To address these limitations, we\ndemonstrate that signSGD with momentum can achieve the same convergence rate\nusing constant batch sizes without additional assumptions. Our analysis, under\nthe standard $l_2$-smoothness condition, improves upon the result of the prior\nmomentum-based signSGD method by a factor of $\\mathcal{O}(d^{1/2})$, where $d$\nis the problem dimension. Furthermore, we explore sign-based methods with\nmajority vote in distributed settings and show that the proposed momentum-based\nmethod yields convergence rates of $\\mathcal{O}\\left( d^{1/2}T^{-1/2} +\ndn^{-1/2} \\right)$ and $\\mathcal{O}\\left( \\max \\{ d^{1/4}T^{-1/4},\nd^{1/10}T^{-1/5} \\} \\right)$, which outperform the previous results of\n$\\mathcal{O}\\left( dT^{-1/4} + dn^{-1/2} \\right)$ and $\\mathcal{O}\\left(\nd^{3/8}T^{-1/8} \\right)$, respectively. Numerical experiments further validate\nthe effectiveness of the proposed methods.\n","authors":["Wei Jiang","Dingzhi Yu","Sifan Yang","Wenhao Yang","Lijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.12091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12070v1","updated":"2025-07-16T09:27:54Z","published":"2025-07-16T09:27:54Z","title":"Emergence of Quantised Representations Isolated to Anisotropic Functions","summary":"  This paper describes a novel methodology for determining representational\nalignment, developed upon the existing Spotlight Resonance method. Using this,\nit is found that algebraic symmetries of network primitives are a strong\npredictor for task-agnostic structure in representations. Particularly, this\nnew tool is used to gain insight into how discrete representations can form and\narrange in autoencoder models, through an ablation study where only the\nactivation function is altered. Representations are found to tend to discretise\nwhen the activation functions are defined through a discrete algebraic\npermutation-equivariant symmetry. In contrast, they remain continuous under a\ncontinuous algebraic orthogonal-equivariant definition. These findings\ncorroborate the hypothesis that functional form choices can carry unintended\ninductive biases which produce task-independent artefactual structures in\nrepresentations, particularly that contemporary forms induce discretisation of\notherwise continuous structure -- a quantisation effect. Moreover, this\nsupports a general causal model for one mode in which discrete representations\nmay form, and could constitute a prerequisite for downstream interpretability\nphenomena, including grandmother neurons, discrete coding schemes, general\nlinear features and possibly Superposition. Hence, this tool and proposed\nmechanism for the influence of functional form on representations may provide\nseveral insights into emergent interpretability research. Finally, preliminary\nresults indicate that quantisation of representations appears to correlate with\na measurable increase in reconstruction error, reinforcing previous conjectures\nthat this collapse can be detrimental.\n","authors":["George Bird"],"pdf_url":"https://arxiv.org/pdf/2507.12070v1.pdf","comment":"36 pages, 31 figures"},{"id":"http://arxiv.org/abs/2507.09016v2","updated":"2025-07-16T09:24:11Z","published":"2025-07-11T20:49:04Z","title":"Enhancing RLHF with Human Gaze Modeling","summary":"  Reinforcement Learning from Human Feedback (RLHF) aligns language models with\nhuman preferences but is computationally expensive. We explore two approaches\nthat leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models\nand (2) gaze-based distribution of sparse rewards at token level. Our\nexperiments demonstate that gaze-informed RLHF achieves faster convergence\nwhile maintaining or slightly improving performance, thus, reducing\ncomputational costs during policy optimization. These results show that human\ngaze provides a valuable and underused signal for policy optimization, pointing\nto a promising direction for improving RLHF efficiency.\n","authors":["Karim Galliamov","Ivan Titov","Ilya Pershin"],"pdf_url":"https://arxiv.org/pdf/2507.09016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12064v1","updated":"2025-07-16T09:21:20Z","published":"2025-07-16T09:21:20Z","title":"StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric\n  Features","summary":"  This submission to the binary AI detection task is based on a modular\nstylometric pipeline, where: public spaCy models are used for text\npreprocessing (including tokenisation, named entity recognition, dependency\nparsing, part-of-speech tagging, and morphology annotation) and extracting\nseveral thousand features (frequencies of n-grams of the above linguistic\nannotations); light-gradient boosting machines are used as the classifier. We\ncollect a large corpus of more than 500 000 machine-generated texts for the\nclassifier's training. We explore several parameter options to increase the\nclassifier's capacity and take advantage of that training set. Our approach\nfollows the non-neural, computationally inexpensive but explainable approach\nfound effective previously.\n","authors":["Jeremi K. Ochab","Mateusz Matias","Tymoteusz Boba","Tomasz Walkowiak"],"pdf_url":"https://arxiv.org/pdf/2507.12064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12053v1","updated":"2025-07-16T09:12:38Z","published":"2025-07-16T09:12:38Z","title":"FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional\n  GANs and Dynamic Region Decoupling","summary":"  The mobility patterns of people in cities evolve alongside changes in land\nuse and population. This makes it crucial for urban planners to simulate and\nanalyze human mobility patterns for purposes such as transportation\noptimization and sustainable urban development. Existing generative models\nborrowed from machine learning rely heavily on historical trajectories and\noften overlook evolving factors like changes in population density and land\nuse. Mechanistic approaches incorporate population density and facility\ndistribution but assume static scenarios, limiting their utility for future\nprojections where historical data for calibration is unavailable. This study\nintroduces a novel, data-driven approach for generating origin-destination\nmobility flows tailored to simulated urban scenarios. Our method leverages\nadaptive factors such as dynamic region sizes and land use archetypes, and it\nutilizes conditional generative adversarial networks (cGANs) to blend\nhistorical data with these adaptive parameters. The approach facilitates rapid\nmobility flow generation with adjustable spatial granularity based on regions\nof interest, without requiring extensive calibration data or complex behavior\nmodeling. The promising performance of our approach is demonstrated by its\napplication to mobile phone data from Singapore, and by its comparison with\nexisting methods.\n","authors":["Seanglidet Yean","Jiazu Zhou","Bu-Sung Lee","Markus Schläpfer"],"pdf_url":"https://arxiv.org/pdf/2507.12053v1.pdf","comment":"International Conference on Intelligent Digitization of Systems and\n  Services, Valencia, Spain, 2025 (IDSS 2025)"},{"id":"http://arxiv.org/abs/2507.12043v1","updated":"2025-07-16T09:00:57Z","published":"2025-07-16T09:00:57Z","title":"Information-Theoretic Generalization Bounds of Replay-based Continual\n  Learning","summary":"  Continual learning (CL) has emerged as a dominant paradigm for acquiring\nknowledge from sequential tasks while avoiding catastrophic forgetting.\nAlthough many CL methods have been proposed to show impressive empirical\nperformance, the theoretical understanding of their generalization behavior\nremains limited, particularly for replay-based approaches. In this paper, we\nestablish a unified theoretical framework for replay-based CL, deriving a\nseries of information-theoretic bounds that explicitly characterize how the\nmemory buffer interacts with the current task to affect generalization.\nSpecifically, our hypothesis-based bounds reveal that utilizing the limited\nexemplars of previous tasks alongside the current task data, rather than\nexhaustive replay, facilitates improved generalization while effectively\nmitigating catastrophic forgetting. Furthermore, our prediction-based bounds\nyield tighter and computationally tractable upper bounds of the generalization\ngap through the use of low-dimensional variables. Our analysis is general and\nbroadly applicable to a wide range of learning algorithms, exemplified by\nstochastic gradient Langevin dynamics (SGLD) as a representative method.\nComprehensive experimental evaluations demonstrate the effectiveness of our\nderived bounds in capturing the generalization dynamics in replay-based CL\nsettings.\n","authors":["Wen Wen","Tieliang Gong","Yunjiao Zhang","Zeyu Gao","Weizhan Zhang","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2507.12043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12041v1","updated":"2025-07-16T08:58:27Z","published":"2025-07-16T08:58:27Z","title":"Granular feedback merits sophisticated aggregation","summary":"  Human feedback is increasingly used across diverse applications like training\nAI models, developing recommender systems, and measuring public opinion -- with\ngranular feedback often being preferred over binary feedback for its greater\ninformativeness. While it is easy to accurately estimate a population's\ndistribution of feedback given feedback from a large number of individuals,\ncost constraints typically necessitate using smaller groups. A simple method to\napproximate the population distribution is regularized averaging: compute the\nempirical distribution and regularize it toward a prior. Can we do better? As\nwe will discuss, the answer to this question depends on feedback granularity.\n  Suppose one wants to predict a population's distribution of feedback using\nfeedback from a limited number of individuals. We show that, as feedback\ngranularity increases, one can substantially improve upon predictions of\nregularized averaging by combining individuals' feedback in ways more\nsophisticated than regularized averaging.\n  Our empirical analysis using questions on social attitudes confirms this\npattern. In particular, with binary feedback, sophistication barely reduces the\nnumber of individuals required to attain a fixed level of performance. By\ncontrast, with five-point feedback, sophisticated methods match the performance\nof regularized averaging with about half as many individuals.\n","authors":["Anmol Kagrecha","Henrik Marklund","Potsawee Manakul","Richard Zeckhauser","Benjamin Van Roy"],"pdf_url":"https://arxiv.org/pdf/2507.12041v1.pdf","comment":"31 pages, 8 figures"},{"id":"http://arxiv.org/abs/2306.06974v2","updated":"2025-07-16T08:53:15Z","published":"2023-06-12T09:15:58Z","title":"A Computational Theory and Semi-Supervised Algorithm for Clustering","summary":"  A computational theory for clustering and a semi-supervised clustering\nalgorithm is presented. Clustering is defined to be the obtainment of groupings\nof data such that each group contains no anomalies with respect to a chosen\ngrouping principle and measure; all other examples are considered to be fringe\npoints, isolated anomalies, anomalous clusters or unknown clusters. More\nprecisely, after appropriate modelling under the assumption of uniform random\ndistribution, any example whose expectation of occurrence is <1 with respect to\na group is considered an anomaly; otherwise it is assigned a membership of that\ngroup. Thus, clustering is conceived as the dual of anomaly detection. The\nrepresentation of data is taken to be the Euclidean distance of a point to a\ncluster median. This is due to the robustness properties of the median to\noutliers, its approximate location of centrality and so that decision\nboundaries are general purpose. The kernel of the clustering method is the\nperception anomaly detection algorithm, resulting in a parameter-free, fast,\nand efficient clustering algorithm. Acknowledging that clustering is an\ninteractive and iterative process, the algorithm relies on a small fraction of\nknown relationships between examples. These relationships serve as seeds to\ndefine the user's objectives and guide the clustering process. The method then\nexpands the clusters accordingly, leaving the remaining examples for\nexploration and subsequent iterations. Results are presented on synthetic and\nrealworld data sets, demonstrating the advantages over the most popular\nunsupervised and semi-supervised clustering methods.\n","authors":["Nassir Mohammad"],"pdf_url":"https://arxiv.org/pdf/2306.06974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12023v1","updated":"2025-07-16T08:30:41Z","published":"2025-07-16T08:30:41Z","title":"MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model","summary":"  Air pollutants pose a significant threat to the environment and human health,\nthus forecasting accurate pollutant concentrations is essential for pollution\nwarnings and policy-making. Existing studies predominantly focus on\nsingle-pollutant forecasting, neglecting the interactions among different\npollutants and their diverse spatial responses. To address the practical needs\nof forecasting multivariate air pollutants, we propose MultiVariate\nAutoRegressive air pollutants forecasting model (MVAR), which reduces the\ndependency on long-time-window inputs and boosts the data utilization\nefficiency. We also design the Multivariate Autoregressive Training Paradigm,\nenabling MVAR to achieve 120-hour long-term sequential forecasting.\nAdditionally, MVAR develops Meteorological Coupled Spatial Transformer block,\nenabling the flexible coupling of AI-based meteorological forecasts while\nlearning the interactions among pollutants and their diverse spatial responses.\nAs for the lack of standardized datasets in air pollutants forecasting, we\nconstruct a comprehensive dataset covering 6 major pollutants across 75 cities\nin North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0\nforecast data. Experimental results demonstrate that the proposed model\noutperforms state-of-the-art methods and validate the effectiveness of the\nproposed architecture.\n","authors":["Xu Fan","Zhihao Wang","Yuetan Lin","Yan Zhang","Yang Xiang","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2507.12023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12021v1","updated":"2025-07-16T08:28:01Z","published":"2025-07-16T08:28:01Z","title":"Incorporating Fairness Constraints into Archetypal Analysis","summary":"  Archetypal Analysis (AA) is an unsupervised learning method that represents\ndata as convex combinations of extreme patterns called archetypes. While AA\nprovides interpretable and low-dimensional representations, it can\ninadvertently encode sensitive attributes, leading to fairness concerns. In\nthis work, we propose Fair Archetypal Analysis (FairAA), a modified formulation\nthat explicitly reduces the influence of sensitive group information in the\nlearned projections. We also introduce FairKernelAA, a nonlinear extension that\naddresses fairness in more complex data distributions. Our approach\nincorporates a fairness regularization term while preserving the structure and\ninterpretability of the archetypes. We evaluate FairAA and FairKernelAA on\nsynthetic datasets, including linear, nonlinear, and multi-group scenarios,\ndemonstrating their ability to reduce group separability -- as measured by mean\nmaximum discrepancy and linear separability -- without substantially\ncompromising explained variance. We further validate our methods on the\nreal-world ANSUR I dataset, confirming their robustness and practical utility.\nThe results show that FairAA achieves a favorable trade-off between utility and\nfairness, making it a promising tool for responsible representation learning in\nsensitive applications.\n","authors":["Aleix Alcacer","Irene Epifanio"],"pdf_url":"https://arxiv.org/pdf/2507.12021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12011v1","updated":"2025-07-16T08:09:41Z","published":"2025-07-16T08:09:41Z","title":"DUSE: A Data Expansion Framework for Low-resource Automatic Modulation\n  Recognition based on Active Learning","summary":"  Although deep neural networks have made remarkable achievements in the field\nof automatic modulation recognition (AMR), these models often require a large\namount of labeled data for training. However, in many practical scenarios, the\navailable target domain data is scarce and difficult to meet the needs of model\ntraining. The most direct way is to collect data manually and perform expert\nannotation, but the high time and labor costs are unbearable. Another common\nmethod is data augmentation. Although it can enrich training samples to a\ncertain extent, it does not introduce new data and therefore cannot\nfundamentally solve the problem of data scarcity. To address these challenges,\nwe introduce a data expansion framework called Dynamic Uncertainty-driven\nSample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring\nfunction to filter out useful samples from relevant AMR datasets and employs an\nactive learning strategy to continuously refine the scorer. Extensive\nexperiments demonstrate that DUSE consistently outperforms 8 coreset selection\nbaselines in both class-balance and class-imbalance settings. Besides, DUSE\nexhibits strong cross-architecture generalization for unseen models.\n","authors":["Yao Lu","Hongyu Gao","Zhuangzhi Chen","Dongwei Xu","Yun Lin","Qi Xuan","Guan Gui"],"pdf_url":"https://arxiv.org/pdf/2507.12011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01048v2","updated":"2025-07-16T08:00:57Z","published":"2025-04-01T05:57:01Z","title":"How does Watermarking Affect Visual Language Models in Document\n  Understanding?","summary":"  Visual Language Models (VLMs) have become foundational models for document\nunderstanding tasks, widely used in the processing of complex multimodal\ndocuments across domains such as finance, law, and academia. However, documents\noften contain noise-like information, such as watermarks, which inevitably\nleads us to inquire: \\emph{Do watermarks degrade the performance of VLMs in\ndocument understanding?} To address this, we propose a novel evaluation\nframework to investigate the effect of visible watermarks on VLMs performance.\nWe takes into account various factors, including different types of document\ndata, the positions of watermarks within documents and variations in watermark\ncontent. Our experimental results reveal that VLMs performance can be\nsignificantly compromised by watermarks, with performance drop rates reaching\nup to 36\\%. We discover that \\emph{scattered} watermarks cause stronger\ninterference than centralized ones, and that \\emph{semantic contents} in\nwatermarks creates greater disruption than simple visual occlusion. Through\nattention mechanism analysis and embedding similarity examination, we find that\nthe performance drops are mainly attributed to that watermarks 1) force\nwidespread attention redistribution, and 2) alter semantic representation in\nthe embedding space. Our research not only highlights significant challenges in\ndeploying VLMs for document understanding, but also provides insights towards\ndeveloping robust inference mechanisms on watermarked documents.\n","authors":["Chunxue Xu","Yiwei Wang","Bryan Hooi","Yujun Cai","Songze Li"],"pdf_url":"https://arxiv.org/pdf/2504.01048v2.pdf","comment":"Accepted to COLM 2025"},{"id":"http://arxiv.org/abs/2507.12003v1","updated":"2025-07-16T07:57:57Z","published":"2025-07-16T07:57:57Z","title":"Expanding ML-Documentation Standards For Better Security","summary":"  This article presents the current state of ML-security and of the\ndocumentation of ML-based systems, models and datasets in research and practice\nbased on an extensive review of the existing literature. It shows a generally\nlow awareness of security aspects among ML-practitioners and organizations and\nan often unstandardized approach to documentation, leading to overall low\nquality of ML-documentation. Existing standards are not regularly adopted in\npractice and IT-security aspects are often not included in documentation. Due\nto these factors, there is a clear need for improved security documentation in\nML, as one step towards addressing the existing gaps in ML-security. To achieve\nthis, we propose expanding existing documentation standards for\nML-documentation to include a security section with specific security relevant\ninformation. Implementing this, a novel expanded method of documenting security\nrequirements in ML-documentation is presented, based on the existing Model\nCards and Datasheets for Datasets standards, but with the recommendation to\nadopt these findings in all ML-documentation.\n","authors":["Cara Ellen Appel"],"pdf_url":"https://arxiv.org/pdf/2507.12003v1.pdf","comment":"Accepted for publication at the 33rd IEEE International Requirements\n  Engineering Workshop (REW 2025)"},{"id":"http://arxiv.org/abs/2507.12002v1","updated":"2025-07-16T07:57:15Z","published":"2025-07-16T07:57:15Z","title":"Detecting In-Person Conversations in Noisy Real-World Environments with\n  Smartwatch Audio and Motion Sensing","summary":"  Social interactions play a crucial role in shaping human behavior,\nrelationships, and societies. It encompasses various forms of communication,\nsuch as verbal conversation, non-verbal gestures, facial expressions, and body\nlanguage. In this work, we develop a novel computational approach to detect a\nfoundational aspect of human social interactions, in-person verbal\nconversations, by leveraging audio and inertial data captured with a commodity\nsmartwatch in acoustically-challenging scenarios. To evaluate our approach, we\nconducted a lab study with 11 participants and a semi-naturalistic study with\n24 participants. We analyzed machine learning and deep learning models with 3\ndifferent fusion methods, showing the advantages of fusing audio and inertial\ndata to consider not only verbal cues but also non-verbal gestures in\nconversations. Furthermore, we perform a comprehensive set of evaluations\nacross activities and sampling rates to demonstrate the benefits of multimodal\nsensing in specific contexts. Overall, our framework achieved 82.0$\\pm$3.0%\nmacro F1-score when detecting conversations in the lab and 77.2$\\pm$1.8% in the\nsemi-naturalistic setting.\n","authors":["Alice Zhang","Callihan Bertley","Dawei Liang","Edison Thomaz"],"pdf_url":"https://arxiv.org/pdf/2507.12002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00691v2","updated":"2025-07-16T07:55:10Z","published":"2025-01-01T01:06:58Z","title":"Labels Generated by Large Language Models Help Measure People's Empathy\n  in Vitro","summary":"  Large language models (LLMs) have revolutionised many fields, with\nLLM-as-a-service (LLMSaaS) offering accessible, general-purpose solutions\nwithout costly task-specific training. In contrast to the widely studied prompt\nengineering for directly solving tasks (in vivo), this paper explores LLMs'\npotential for in-vitro applications: using LLM-generated labels to improve\nsupervised training of mainstream models. We examine two strategies - (1) noisy\nlabel correction and (2) training data augmentation - in empathy computing, an\nemerging task to predict psychology-based questionnaire outcomes from inputs\nlike textual narratives. Crowdsourced datasets in this domain often suffer from\nnoisy labels that misrepresent underlying empathy. We show that replacing or\nsupplementing these crowdsourced labels with LLM-generated labels, developed\nusing psychology-based scale-aware prompts, achieves statistically significant\naccuracy improvements. Notably, the RoBERTa pre-trained language model (PLM)\ntrained with noise-reduced labels yields a state-of-the-art Pearson correlation\ncoefficient of 0.648 on the public NewsEmp benchmarks. This paper further\nanalyses evaluation metric selection and demographic biases to help guide the\nfuture development of more equitable empathy computing models. Code and\nLLM-generated labels are available at\nhttps://github.com/hasan-rakibul/LLMPathy.\n","authors":["Md Rakibul Hasan","Yue Yao","Md Zakir Hossain","Aneesh Krishna","Imre Rudas","Shafin Rahman","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2501.00691v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2507.11997v1","updated":"2025-07-16T07:50:43Z","published":"2025-07-16T07:50:43Z","title":"Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection","summary":"  Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods.\n","authors":["Tairan Huang","Yili Wang"],"pdf_url":"https://arxiv.org/pdf/2507.11997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04715v6","updated":"2025-07-16T07:46:43Z","published":"2025-03-06T18:58:29Z","title":"Predictable Scale: Part I, Step Law -- Optimal Hyperparameter Scaling\n  Law in Large Language Model Pretraining","summary":"  The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well\\text{-}established, yet their effective deployment\nnecessitates careful hyperparameter optimization. Although existing methods\nhave explored the influence of hyperparameters on model performance, a\nprincipled and generalizable framework across model architectures and data\nrecipes remains absent. In this study, we conduct an unprecedented empirical\ninvestigation\\text{-} training over 3,700 LLMs from scratch across 100 trillion\ntokens, consuming nearly one million NVIDIA H800 GPU hours to establish a\nuniversal Scaling Law for hyperparameter optimization in LLM Pre-training,\ncalled \\textbf{Step Law}. We empirically observe that, under fixed model size\n($N$) and dataset size ($D$), the hyperparameter landscape exhibits convexity\nwith a broad optimum, substantially reducing the complexity of hyperparameter\nsearch. Building on this insight, we formally define and empirically validate\nthe Step Law: The optimal learning rate follows a power-law relationship with\n$N$ and $D$, while the optimal batch size is primarily influenced by $D$ and\nremains largely invariant to $N$.Notably, our estimated optima deviate from the\nglobal best performance found via exhaustive search by merely \\textbf{0.094\\%}\non the test set. To our best known, Step Law is the \\textbf{first} that unifies\ndifferent model shapes and structures, such as Mixture-of-Experts models and\ndense transformers, as well as establishes optimal hyperparameter scaling laws\nacross diverse data recipes. We contribute a universal, plug-and-play optimal\nhyperparameter tool for the community, which is expected to advance efficient\nLLM training at scale. All experimental code, data and checkpoints are publicly\navailable at\n\\href{https://github.com/step-law/steplaw}{https://github.com/step-law/steplaw}.\n","authors":["Houyi Li","Wenzhen Zheng","Qiufeng Wang","Hanshan Zhang","Zili Wang","Shijie Xuyang","Yuantao Fan","Zhenyu Ding","Haoying Wang","Ning Ding","Shuigeng Zhou","Xiangyu Zhang","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.04715v6.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2507.11984v1","updated":"2025-07-16T07:32:08Z","published":"2025-07-16T07:32:08Z","title":"Dataset-Adaptive Dimensionality Reduction","summary":"  Selecting the appropriate dimensionality reduction (DR) technique and\ndetermining its optimal hyperparameter settings that maximize the accuracy of\nthe output projections typically involves extensive trial and error, often\nresulting in unnecessary computational overhead. To address this challenge, we\npropose a dataset-adaptive approach to DR optimization guided by structural\ncomplexity metrics. These metrics quantify the intrinsic complexity of a\ndataset, predicting whether higher-dimensional spaces are necessary to\nrepresent it accurately. Since complex datasets are often inaccurately\nrepresented in two-dimensional projections, leveraging these metrics enables us\nto predict the maximum achievable accuracy of DR techniques for a given\ndataset, eliminating redundant trials in optimizing DR. We introduce the design\nand theoretical foundations of these structural complexity metrics. We\nquantitatively verify that our metrics effectively approximate the ground truth\ncomplexity of datasets and confirm their suitability for guiding\ndataset-adaptive DR workflow. Finally, we empirically show that our\ndataset-adaptive workflow significantly enhances the efficiency of DR\noptimization without compromising accuracy.\n","authors":["Hyeon Jeon","Jeongin Park","Soohyun Lee","Dae Hyun Kim","Sungbok Shin","Jinwook Seo"],"pdf_url":"https://arxiv.org/pdf/2507.11984v1.pdf","comment":"IEEE VIS 2025 & IEEE Transactions on Visualization and Computer\n  Graphics (TVCG)"},{"id":"http://arxiv.org/abs/2507.11977v1","updated":"2025-07-16T07:18:51Z","published":"2025-07-16T07:18:51Z","title":"Recent results on searches with boosted Higgs bosons at CMS","summary":"  The study of boosted Higgs bosons at the LHC provides a unique window to\nprobe Higgs boson couplings at high energy scales and search for signs of\nphysics beyond the standard model. In these proceedings, we present recent\nresults on boosted Higgs boson searches at the CMS experiment, highlighting\ninnovative reconstruction and tagging techniques that enhance sensitivity in\nthis challenging regime.\n","authors":["Farouk Mokhtar"],"pdf_url":"https://arxiv.org/pdf/2507.11977v1.pdf","comment":"6 pages, 3 figures, The Thirteenth Annual Large Hadron Collider\n  Physics (LHCP2025)"},{"id":"http://arxiv.org/abs/2507.11975v1","updated":"2025-07-16T07:17:41Z","published":"2025-07-16T07:17:41Z","title":"Online Training and Pruning of Deep Reinforcement Learning Networks","summary":"  Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms\nhas been shown to enhance performance when feature extraction networks are used\nbut the gained performance comes at the significant expense of increased\ncomputational and memory complexity. Neural network pruning methods have\nsuccessfully addressed this challenge in supervised learning. However, their\napplication to RL is underexplored. We propose an approach to integrate\nsimultaneous training and pruning within advanced RL methods, in particular to\nRL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our\nnetworks (XiNet) are trained to solve stochastic optimization problems over the\nRL networks' weights and the parameters of variational Bernoulli distributions\nfor 0/1 Random Variables $\\xi$ scaling each unit in the networks. The\nstochastic problem formulation induces regularization terms that promote\nconvergence of the variational parameters to 0 when a unit contributes little\nto the performance. In this case, the corresponding structure is rendered\npermanently inactive and pruned from its network. We propose a cost-aware,\nsparsity-promoting regularization scheme, tailored to the DenseNet architecture\nof OFENets expressing the parameter complexity of involved networks in terms of\nthe parameters of the RVs in these networks. Then, when matching this cost with\nthe regularization terms, the many hyperparameters associated with them are\nautomatically selected, effectively combining the RL objectives and network\ncompression. We evaluate our method on continuous control benchmarks (MuJoCo)\nand the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned\nconsiderably with minimal loss in performance. Furthermore, our results confirm\nthat pruning large networks during training produces more efficient and higher\nperforming RL agents rather than training smaller networks from scratch.\n","authors":["Valentin Frank Ingmar Guenter","Athanasios Sideris"],"pdf_url":"https://arxiv.org/pdf/2507.11975v1.pdf","comment":"25 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2506.10972v3","updated":"2025-07-16T07:09:02Z","published":"2025-06-12T17:59:23Z","title":"Predictable Scale: Part II, Farseer: A Refined Scaling Law in Large\n  Language Models","summary":"  Training Large Language Models (LLMs) is prohibitively expensive, creating a\ncritical scaling gap where insights from small-scale experiments often fail to\ntransfer to resource-intensive production systems, thereby hindering efficient\ninnovation. To bridge this, we introduce Farseer, a novel and refined scaling\nlaw offering enhanced predictive accuracy across scales. By systematically\nconstructing a model loss surface $L(N,D)$, Farseer achieves a significantly\nbetter fit to empirical data than prior laws (e.g., Chinchilla's law). Our\nmethodology yields accurate, robust, and highly generalizable predictions,\ndemonstrating excellent extrapolation capabilities, improving upon Chinchilla's\nlaw by reducing extrapolation error by 433\\%. This allows for the reliable\nevaluation of competing training strategies across all $(N,D)$ settings,\nenabling conclusions from small-scale ablation studies to be confidently\nextrapolated to predict large-scale performance. Furthermore, Farseer provides\nnew insights into optimal compute allocation, better reflecting the nuanced\ndemands of modern LLM training. To validate our approach, we trained an\nextensive suite of approximately 1,000 LLMs across diverse scales and\nconfigurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are\ncomprehensively open-sourcing all models, data, results, and logs at\nhttps://github.com/Farseer-Scaling-Law/Farseer to foster further research.\n","authors":["Houyi Li","Wenzhen Zheng","Qiufeng Wang","Zhenyu Ding","Haoying Wang","Zili Wang","Shijie Xuyang","Ning Ding","Shuigeng Zhou","Xiangyu Zhang","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2506.10972v3.pdf","comment":"34"},{"id":"http://arxiv.org/abs/2507.09828v2","updated":"2025-07-16T07:03:01Z","published":"2025-07-13T23:37:31Z","title":"Regret Analysis of Posterior Sampling-Based Expected Improvement for\n  Bayesian Optimization","summary":"  Bayesian optimization is a powerful tool for optimizing an\nexpensive-to-evaluate black-box function. In particular, the effectiveness of\nexpected improvement (EI) has been demonstrated in a wide range of\napplications. However, theoretical analyses of EI are limited compared with\nother theoretically established algorithms. This paper analyzes a randomized\nvariant of EI, which evaluates the EI from the maximum of the posterior sample\npath. We show that this posterior sampling-based random EI achieves the\nsublinear Bayesian cumulative regret bounds under the assumption that the\nblack-box function follows a Gaussian process. Finally, we demonstrate the\neffectiveness of the proposed method through numerical experiments.\n","authors":["Shion Takeno","Yu Inatsu","Masayuki Karasuyama","Ichiro Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2507.09828v2.pdf","comment":"35pages, 5 figures, fix trivial errors"},{"id":"http://arxiv.org/abs/2507.03560v2","updated":"2025-07-16T07:00:43Z","published":"2025-07-04T13:12:09Z","title":"Simplifying Graph Kernels for Efficient","summary":"  While kernel methods and Graph Neural Networks offer complementary strengths,\nintegrating the two has posed challenges in efficiency and scalability. The\nGraph Neural Tangent Kernel provides a theoretical bridge by interpreting GNNs\nthrough the lens of neural tangent kernels. However, its reliance on deep,\nstacked layers introduces repeated computations that hinder performance. In\nthis work, we introduce a new perspective by designing the simplified graph\nkernel, which replaces deep layer stacking with a streamlined $K$-step message\naggregation process. This formulation avoids iterative layer-wise propagation\naltogether, leading to a more concise and computationally efficient framework\nwithout sacrificing the expressive power needed for graph tasks. Beyond this\nsimplification, we propose another Simplified Graph Kernel, which draws from\nGaussian Process theory to model infinite-width GNNs. Rather than simulating\nnetwork depth, this kernel analytically computes kernel values based on the\nstatistical behavior of nonlinear activations in the infinite limit. This\neliminates the need for explicit architecture simulation, further reducing\ncomplexity. Our experiments on standard graph and node classification\nbenchmarks show that our methods achieve competitive accuracy while reducing\nruntime. This makes them practical alternatives for learning on graphs at\nscale. Full implementation and reproducibility materials are provided at:\nhttps://anonymous.4open.science/r/SGNK-1CE4/.\n","authors":["Lin Wang","Shijie Wang","Sirui Huang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2507.03560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06607v2","updated":"2025-07-16T07:00:01Z","published":"2025-07-09T07:27:00Z","title":"Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long\n  Generation","summary":"  Recent advances in language modeling have demonstrated the effectiveness of\nState Space Models (SSMs) for efficient sequence modeling. While hybrid\narchitectures such as Samba and the decoder-decoder architecture, YOCO, have\nshown promising performance gains over Transformers, prior works have not\ninvestigated the efficiency potential of representation sharing between SSM\nlayers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet\neffective mechanism for efficient memory sharing across layers. We apply it to\ncreate SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in\nthe cross-decoder to share memory readout states from a Samba-based\nself-decoder. SambaY significantly enhances decoding efficiency, preserves\nlinear pre-filling time complexity, and boosts long-context performance, all\nwhile eliminating the need for explicit positional encoding. Through extensive\nscaling experiments, we demonstrate that our model exhibits a significantly\nlower irreducible loss compared to a strong YOCO baseline, indicating superior\nperformance scalability under large-scale compute regimes. Our largest model\nenhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves\nsignificantly better performance than Phi4-mini-Reasoning on reasoning tasks\nsuch as Math500, AIME24/25, and GPQA Diamond without any reinforcement\nlearning, while delivering up to 10x higher decoding throughput on 2K-length\nprompts with 32K generation length under the vLLM inference framework. We\nrelease our training codebase on open-source data at\nhttps://github.com/microsoft/ArchScale.\n","authors":["Liliang Ren","Congcong Chen","Haoran Xu","Young Jin Kim","Adam Atkinson","Zheng Zhan","Jiankai Sun","Baolin Peng","Liyuan Liu","Shuohang Wang","Hao Cheng","Jianfeng Gao","Weizhu Chen","Yelong Shen"],"pdf_url":"https://arxiv.org/pdf/2507.06607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01912v3","updated":"2025-07-16T06:59:50Z","published":"2025-02-04T01:05:12Z","title":"PATCH: a deep learning method to assess heterogeneity of artistic\n  practice in historical paintings","summary":"  The history of art has seen significant shifts in the manner in which\nartworks are created, making understanding of creative processes a central\nquestion in technical art history. In the Renaissance and Early Modern period,\npaintings were largely produced by master painters directing workshops of\napprentices who often contributed to projects. The masters varied significantly\nin artistic and managerial styles, meaning different combinations of artists\nand implements might be seen both between masters and within workshops or even\nindividual canvases. Information on how different workshops were managed and\nthe processes by which artworks were created remains elusive. Machine learning\nmethods have potential to unearth new information about artists' creative\nprocesses by extending the analysis of brushwork to a microscopic scale.\nAnalysis of workshop paintings, however, presents a challenge in that\ndocumentation of the artists and materials involved is sparse, meaning external\nexamples are not available to train networks to recognize their contributions.\nHere we present a novel machine learning approach we call pairwise assignment\ntraining for classifying heterogeneity (PATCH) that is capable of identifying\nindividual artistic practice regimes with no external training data, or \"ground\ntruth.\" The method achieves unsupervised results by supervised means, and\noutperforms both simple statistical procedures and unsupervised machine\nlearning methods. We apply this method to two historical paintings by the\nSpanish Renaissance master, El Greco: The Baptism of Christ and Christ on the\nCross with Landscape, and our findings regarding the former potentially\nchallenge previous work that has assigned the painting to workshop members.\nFurther, the results of our analyses create a measure of heterogeneity of\nartistic practice that can be used to characterize artworks across time and\nspace.\n","authors":["Andrew Van Horn","Lauryn Smith","Mahamad Mahmoud","Michael McMaster","Clara Pinchbeck","Ina Martin","Andrew Lininger","Anthony Ingrisano","Adam Lowe","Carlos Bayod","Elizabeth Bolman","Kenneth Singer","Michael Hinczewski"],"pdf_url":"https://arxiv.org/pdf/2502.01912v3.pdf","comment":"main text: 15 pages, 5 figures; SI: 10 pages, 4 figures; v2: minor\n  typo corrections, higher resolution figures; v3: additional comparisons with\n  alternative methods"},{"id":"http://arxiv.org/abs/2503.02445v5","updated":"2025-07-16T06:56:08Z","published":"2025-03-04T09:40:00Z","title":"BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modeling","summary":"  Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by up to 12% on MSE and 6% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data.\n","authors":["Hao Li","Yu-Hao Huang","Chang Xu","Viktor Schlegel","Renhe Jiang","Riza Batista-Navarro","Goran Nenadic","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2503.02445v5.pdf","comment":"ICML 2025 Main Conference"},{"id":"http://arxiv.org/abs/2507.03034v2","updated":"2025-07-16T06:52:55Z","published":"2025-07-03T02:45:51Z","title":"Rethinking Data Protection in the (Generative) Artificial Intelligence\n  Era","summary":"  The (generative) artificial intelligence (AI) era has profoundly reshaped the\nmeaning and value of data. No longer confined to static content, data now\npermeates every stage of the AI lifecycle from the training samples that shape\nmodel parameters to the prompts and outputs that drive real-world model\ndeployment. This shift renders traditional notions of data protection\ninsufficient, while the boundaries of what needs safeguarding remain poorly\ndefined. Failing to safeguard data in AI systems can inflict societal and\nindividual, underscoring the urgent need to clearly delineate the scope of and\nrigorously enforce data protection. In this perspective, we propose a\nfour-level taxonomy, including non-usability, privacy preservation,\ntraceability, and deletability, that captures the diverse protection needs\narising in modern (generative) AI models and systems. Our framework offers a\nstructured understanding of the trade-offs between data utility and control,\nspanning the entire AI pipeline, including training datasets, model weights,\nsystem prompts, and AI-generated content. We analyze representative technical\napproaches at each level and reveal regulatory blind spots that leave critical\nassets exposed. By offering a structured lens to align future AI technologies\nand governance with trustworthy data practices, we underscore the urgency of\nrethinking data protection for modern AI techniques and provide timely guidance\nfor developers, researchers, and regulators alike.\n","authors":["Yiming Li","Shuo Shao","Yu He","Junfeng Guo","Tianwei Zhang","Zhan Qin","Pin-Yu Chen","Michael Backes","Philip Torr","Dacheng Tao","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2507.03034v2.pdf","comment":"Perspective paper for a broader scientific audience. The first two\n  authors contributed equally to this paper. 13 pages"},{"id":"http://arxiv.org/abs/2507.11960v1","updated":"2025-07-16T06:45:08Z","published":"2025-07-16T06:45:08Z","title":"d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality\n  Improvement","summary":"  Approaches to enhancing data quality (DQ) are classified into two main\ncategories: data- and process-driven. However, prior research has predominantly\nutilized batch data preprocessing within the data-driven framework, which often\nproves insufficient for optimizing machine learning (ML) model performance and\nfrequently leads to distortions in data characteristics. Existing studies have\nprimarily focused on data preprocessing rather than genuine data quality\nimprovement (DQI). In this paper, we introduce d-DQIVAR, a novel visual\nanalytics system designed to facilitate DQI strategies aimed at improving ML\nmodel performance. Our system integrates visual analytics techniques that\nleverage both data-driven and process-driven approaches. Data-driven techniques\ntackle DQ issues such as imputation, outlier detection, deletion, format\nstandardization, removal of duplicate records, and feature selection.\nProcess-driven strategies encompass evaluating DQ and DQI procedures by\nconsidering DQ dimensions and ML model performance and applying the\nKolmogorov-Smirnov test. We illustrate how our system empowers users to harness\nexpert and domain knowledge effectively within a practical workflow through\ncase studies, evaluations, and user studies.\n","authors":["Hyein Hong","Sangbong Yoo","SeokHwan Choi","Jisue Kim","Seongbum Seo","Haneol Cho","Chansoo Kim","Yun Jang"],"pdf_url":"https://arxiv.org/pdf/2507.11960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11959v1","updated":"2025-07-16T06:44:14Z","published":"2025-07-16T06:44:14Z","title":"PoTPTQ: A Two-step Power-of-Two Post-training for LLMs","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.\n","authors":["Xinyu Wang","Vahid Partovi Nia","Peng Lu","Jerry Huang","Xiao-Wen Chang","Boxing Chen","Yufei Cui"],"pdf_url":"https://arxiv.org/pdf/2507.11959v1.pdf","comment":"Accepted at ECAI 2025 (European Conference on Artificial\n  Intelligence)"},{"id":"http://arxiv.org/abs/2502.12937v2","updated":"2025-07-16T06:42:07Z","published":"2025-02-18T15:16:23Z","title":"Tuning Algorithmic and Architectural Hyperparameters in Graph-Based\n  Semi-Supervised Learning with Provable Guarantees","summary":"  Graph-based semi-supervised learning is a powerful paradigm in machine\nlearning for modeling and exploiting the underlying graph structure that\ncaptures the relationship between labeled and unlabeled data. A large number of\nclassical as well as modern deep learning based algorithms have been proposed\nfor this problem, often having tunable hyperparameters. We initiate a formal\nstudy of tuning algorithm hyperparameters from parameterized algorithm families\nfor this problem. We obtain novel $O(\\log n)$ pseudo-dimension upper bounds for\nhyperparameter selection in three classical label propagation-based algorithm\nfamilies, where $n$ is the number of nodes, implying bounds on the amount of\ndata needed for learning provably good parameters. We further provide matching\n$\\Omega(\\log n)$ pseudo-dimension lower bounds, thus asymptotically\ncharacterizing the learning-theoretic complexity of the parameter tuning\nproblem. We extend our study to selecting architectural hyperparameters in\nmodern graph neural networks. We bound the Rademacher complexity for tuning the\nself-loop weighting in recently proposed Simplified Graph Convolution (SGC)\nnetworks. We further propose a tunable architecture that interpolates graph\nconvolutional neural networks (GCN) and graph attention networks (GAT) in every\nlayer, and provide Rademacher complexity bounds for tuning the interpolation\ncoefficient.\n","authors":["Ally Yalei Du","Eric Huang","Dravyansh Sharma"],"pdf_url":"https://arxiv.org/pdf/2502.12937v2.pdf","comment":"31 pages (12 pages main body), 2 figures. UAI 2025"},{"id":"http://arxiv.org/abs/2507.11954v1","updated":"2025-07-16T06:41:03Z","published":"2025-07-16T06:41:03Z","title":"The benefits of query-based KGQA systems for complex and temporal\n  questions in LLM era","summary":"  Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System\n","authors":["Artem Alekseev","Mikhail Chaichuk","Miron Butko","Alexander Panchenko","Elena Tutubalina","Oleg Somov"],"pdf_url":"https://arxiv.org/pdf/2507.11954v1.pdf","comment":"15 pages, 3 figures, 7 tables"},{"id":"http://arxiv.org/abs/2507.11953v1","updated":"2025-07-16T06:39:11Z","published":"2025-07-16T06:39:11Z","title":"IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs","summary":"  LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.\n","authors":["Yi Zhao","Zuchao Li","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.11953v1.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2507.11950v1","updated":"2025-07-16T06:33:50Z","published":"2025-07-16T06:33:50Z","title":"RNAMunin: A Deep Machine Learning Model for Non-coding RNA Discovery","summary":"  Functional annotation of microbial genomes is often biased toward\nprotein-coding genes, leaving a vast, unexplored landscape of non-coding RNAs\n(ncRNAs) that are critical for regulating bacterial and archaeal physiology,\nstress response and metabolism. Identifying ncRNAs directly from genomic\nsequence is a paramount challenge in bioinformatics and biology, essential for\nunderstanding the complete regulatory potential of an organism. This paper\npresents RNAMunin, a machine learning (ML) model that is capable of finding\nncRNAs using genomic sequence alone. It is also computationally viable for\nlarge sequence datasets such as long read metagenomic assemblies with contigs\ntotaling multiple Gbp. RNAMunin is trained on Rfam sequences extracted from\napproximately 60 Gbp of long read metagenomes from 16 San Francisco Estuary\nsamples. We know of no other model that can detect ncRNAs based solely on\ngenomic sequence at this scale. Since RNAMunin only requires genomic sequence\nas input, we do not need for an ncRNA to be transcribed to find it, i.e., we do\nnot need transcriptomics data. We wrote this manuscript in a narrative style in\norder to best convey how RNAMunin was developed and how it works in detail.\nUnlike almost all current ML models, at approximately 1M parameters, RNAMunin\nis very small and very fast.\n","authors":["Lauren Lui","Torben Nielsen"],"pdf_url":"https://arxiv.org/pdf/2507.11950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11948v1","updated":"2025-07-16T06:33:07Z","published":"2025-07-16T06:33:07Z","title":"Kevin: Multi-Turn RL for Generating CUDA Kernels","summary":"  Writing GPU kernels is a challenging task and critical for AI systems'\nefficiency. It is also highly iterative: domain experts write code and improve\nperformance through execution feedback. Moreover, it presents verifiable\nrewards like correctness and speedup, making it a natural environment to apply\nReinforcement Learning (RL). To explicitly incorporate the iterative nature of\nthis process into training, we develop a flexible multi-turn RL recipe that\naddresses unique challenges encountered in real-world settings, such as\nlearning from long trajectories and effective reward attribution across turns.\nWe present Kevin - K(ernel D)evin, the first model trained with multi-turn RL\nfor CUDA kernel generation and optimization. In our evaluation setup, Kevin\nshows significant gains over its base model (QwQ-32B), improving correctness of\ngenerated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to\n1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini\n(0.78x). Finally, we study its behavior across test-time scaling axes: we found\nscaling serial refinement more beneficial than parallel sampling. In\nparticular, when given more refinement turns, Kevin shows a higher rate of\nimprovement.\n","authors":["Carlo Baronio","Pietro Marsella","Ben Pan","Simon Guo","Silas Alberti"],"pdf_url":"https://arxiv.org/pdf/2507.11948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11936v1","updated":"2025-07-16T06:03:08Z","published":"2025-07-16T06:03:08Z","title":"A Survey of Deep Learning for Geometry Problem Solving","summary":"  Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.\n","authors":["Jianzhe Ma","Wenxuan Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2507.11936v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.01570v6","updated":"2025-07-16T05:59:25Z","published":"2024-10-02T14:09:51Z","title":"Truncated Kernel Stochastic Gradient Descent on Spheres","summary":"  Inspired by the structure of spherical harmonics, we propose the truncated\nkernel stochastic gradient descent (T-kernel SGD) algorithm with a least-square\nloss function for spherical data fitting. T-kernel SGD introduces a novel\nregularization strategy by implementing stochastic gradient descent through a\nclosed-form solution of the projection of the stochastic gradient in a\nlow-dimensional subspace. In contrast to traditional kernel SGD, the\nregularization strategy implemented by T-kernel SGD is more effective in\nbalancing bias and variance by dynamically adjusting the hypothesis space\nduring iterations. The most significant advantage of the proposed algorithm is\nthat it can achieve theoretically optimal convergence rates using a constant\nstep size (independent of the sample size) while overcoming the inherent\nsaturation problem of kernel SGD. Additionally, we leverage the structure of\nspherical polynomials to derive an equivalent T-kernel SGD, significantly\nreducing storage and computational costs compared to kernel SGD. Typically,\nT-kernel SGD requires only $\\mathcal{O}(n^{1+\\frac{d}{d-1}\\epsilon})$\ncomputational complexity and $\\mathcal{O}(n^{\\frac{d}{d-1}\\epsilon})$ storage\nto achieve optimal rates for the d-dimensional sphere, where\n$0<\\epsilon<\\frac{1}{2}$ can be arbitrarily small if the optimal fitting or the\nunderlying space possesses sufficient regularity. This regularity is determined\nby the smoothness parameter of the objective function and the decaying rate of\nthe eigenvalues of the integral operator associated with the kernel function,\nboth of which reflect the difficulty of the estimation problem. Our main\nresults quantitatively characterize how this prior information influences the\nconvergence of T-kernel SGD. The numerical experiments further validate the\ntheoretical findings presented in this paper.\n","authors":["Jinhui Bai","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2410.01570v6.pdf","comment":"74 pages, 22 figures"},{"id":"http://arxiv.org/abs/2507.11928v1","updated":"2025-07-16T05:52:24Z","published":"2025-07-16T05:52:24Z","title":"Accelerating RF Power Amplifier Design via Intelligent Sampling and\n  ML-Based Parameter Tuning","summary":"  This paper presents a machine learning-accelerated optimization framework for\nRF power amplifier design that reduces simulation requirements by 65% while\nmaintaining $\\pm0.3$ to $\\pm0.4$ dBm accuracy. The proposed method combines\nMaxMin Latin Hypercube Sampling with CatBoost gradient boosting to\nintelligently explore multidimensional parameter spaces. Instead of\nexhaustively simulating all parameter combinations to achieve target P2dB\ncompression specifications, our approach strategically selects approximately\n35% of critical simulation points. The framework processes ADS netlists,\nexecutes harmonic balance simulations on the reduced dataset, and trains a\nCatBoost model to predict P2dB performance across the entire design space.\nValidation across 15 PA operating modes yields an average $R^2$ of 0.901, with\nthe system ranking parameter combinations by their likelihood of meeting target\nspecifications. The integrated solution delivers 58.24% to 77.78% reduction in\nsimulation time through automated GUI-based workflows, enabling rapid design\niterations without compromising accuracy standards required for production RF\ncircuits.\n","authors":["Abhishek Sriram","Neal Tuffy"],"pdf_url":"https://arxiv.org/pdf/2507.11928v1.pdf","comment":"This paper is a pre-print version and has been submitted to the IEEE\n  International Conference on Future Machine Learning and Data Science (FMLDS\n  2025)"},{"id":"http://arxiv.org/abs/2507.11926v1","updated":"2025-07-16T05:43:46Z","published":"2025-07-16T05:43:46Z","title":"From Generative to Episodic: Sample-Efficient Replicable Reinforcement\n  Learning","summary":"  The epidemic failure of replicability across empirical science and machine\nlearning has recently motivated the formal study of replicable learning\nalgorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from\na fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the\ndesign of data-efficient replicable algorithms is now more or less understood.\nIn contrast, there remain significant gaps in our knowledge for control\nsettings like reinforcement learning where an agent must interact directly with\na shifting environment. Karbasi et. al show that with access to a generative\nmodel of an environment with $S$ states and $A$ actions (the RL 'batch\nsetting'), replicably learning a near-optimal policy costs only\n$\\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a\ngenerative model jumps to $\\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the\nsubstantial difficulty of environment exploration. This gap raises a key\nquestion in the broader theory of replicability: Is replicable exploration\ninherently more expensive than batch learning? Is sample-efficient replicable\nRL even possible?\n  In this work, we (nearly) resolve this problem (for low-horizon tabular\nMDPs): exploration is not a significant barrier to replicable learning! Our\nmain result is a replicable RL algorithm on $\\tilde{O}(S^2A)$ samples, bridging\nthe gap between the generative and episodic settings. We complement this with a\nmatching $\\tilde{\\Omega}(S^2A)$ lower bound in the generative setting (under\nthe common parallel sampling assumption) and an unconditional lower bound in\nthe episodic setting of $\\tilde{\\Omega}(S^2)$ showcasing the near-optimality of\nour algorithm with respect to the state space $S$.\n","authors":["Max Hopkins","Sihan Liu","Christopher Ye","Yuichi Yoshida"],"pdf_url":"https://arxiv.org/pdf/2507.11926v1.pdf","comment":"67 pages"},{"id":"http://arxiv.org/abs/2411.00355v3","updated":"2025-07-16T05:32:21Z","published":"2024-11-01T04:41:00Z","title":"TextDestroyer: A Training- and Annotation-Free Diffusion Method for\n  Destroying Anomal Text from Images","summary":"  In this paper, we propose TextDestroyer, the first training- and\nannotation-free method for scene text destruction using a pre-trained diffusion\nmodel. Existing scene text removal models require complex annotation and\nretraining, and may leave faint yet recognizable text information, compromising\nprivacy protection and content concealment. TextDestroyer addresses these\nissues by employing a three-stage hierarchical process to obtain accurate text\nmasks. Our method scrambles text areas in the latent start code using a\nGaussian distribution before reconstruction. During the diffusion denoising\nprocess, self-attention key and value are referenced from the original latent\nto restore the compromised background. Latent codes saved at each inversion\nstep are used for replacement during reconstruction, ensuring perfect\nbackground restoration. The advantages of TextDestroyer include: (1) it\neliminates labor-intensive data annotation and resource-intensive training; (2)\nit achieves more thorough text destruction, preventing recognizable traces; and\n(3) it demonstrates better generalization capabilities, performing well on both\nreal-world scenes and generated images.\n","authors":["Mengcheng Li","Fei Chao"],"pdf_url":"https://arxiv.org/pdf/2411.00355v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.04632v2","updated":"2025-07-16T05:06:13Z","published":"2025-07-07T03:20:52Z","title":"Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning\n  of Reasoning Models?","summary":"  Recent advances have witnessed the effectiveness of reinforcement learning\n(RL) finetuning in enhancing the reasoning capabilities of large language\nmodels (LLMs). The optimization process often requires numerous iterations to\nachieve satisfactory performance, resulting in high computational costs due to\nthe need for frequent prompt evaluations under intensive LLM interactions and\nrepeated policy updates. Appropriate online prompt selection methods reduce\niteration steps by prioritizing informative prompts during training, while the\npipeline's reliance on exhaustive prompt evaluation and subset selection for\noptimization still incurs substantial computational overhead due to frequent\nLLM inference calls. Distinguished from these direct evaluate-then-select\nschemes, this work investigates iterative approximate evaluation for arbitrary\nprompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian\nrisk-predictive framework that online estimates prompt difficulty without\nrequiring costly LLM interactions. Technically, MoPPS models each prompt's\nsuccess rate as a latent variable, performs streaming Bayesian inference, and\nemploys posterior sampling in a constructed multi-armed bandit machine,\nenabling sample efficient and adaptive prompt selection. Extensive experiments\nacross mathematics, planning, and vision-based geometry tasks show that MoPPS\nreliably predicts prompt difficulty and accelerates training with significantly\nreduced LLM rollouts.\n","authors":["Yun Qu","Qi Cheems Wang","Yixiu Mao","Vincent Tao Hu","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2507.04632v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11911v1","updated":"2025-07-16T04:55:09Z","published":"2025-07-16T04:55:09Z","title":"AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG\n  Decoding","summary":"  Electroencephalogram (EEG) decoding models for brain-computer interfaces\n(BCIs) struggle with cross-dataset learning and generalization due to channel\nlayout inconsistencies, non-stationary signal distributions, and limited\nneurophysiological prior integration. To address these issues, we propose a\nplug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has\ntwo main components: 1) Spatial Alignment, which selects task-relevant channels\nbased on brain-region priors, aligns EEG distributions across domains, and\nremaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,\nwhich models multi-dataset signals into unified spatiotemporal patches for EEG\ndecoding. Compared to 17 state-of-the-art approaches that need dataset-specific\ntuning, the proposed calibration-free AFPM achieves performance gains of up to\n4.40% on motor imagery and 3.58% on event-related potential tasks. To our\nknowledge, this is the first calibration-free cross-dataset EEG decoding\nframework, substantially enhancing the practicalness of BCIs in real-world\napplications.\n","authors":["Xiaoqing Chen","Siyang Li","Dongrui Wu"],"pdf_url":"https://arxiv.org/pdf/2507.11911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00397v6","updated":"2025-07-16T04:40:32Z","published":"2024-06-29T10:50:23Z","title":"Learning Time-Varying Multi-Region Brain Communications via Scalable\n  Markovian Gaussian Processes","summary":"  Understanding and constructing brain communications that capture dynamic\ncommunications across multiple regions is fundamental to modern system\nneuroscience, yet current methods struggle to find time-varying region-level\ncommunications or scale to large neural datasets with long recording durations.\nWe present a novel framework using Markovian Gaussian Processes to learn brain\ncommunications with time-varying temporal delays from multi-region neural\nrecordings, named Adaptive Delay Model (ADM). Our method combines Gaussian\nProcesses with State Space Models and employs parallel scan inference\nalgorithms, enabling efficient scaling to large datasets while identifying\nconcurrent communication patterns that evolve over time. This time-varying\napproach captures how brain region interactions shift dynamically during\ncognitive processes. Validated on synthetic and multi-region neural recordings\ndatasets, our approach discovers both the directionality and temporal dynamics\nof neural communication. This work advances our understanding of distributed\nneural computation and provides a scalable tool for analyzing dynamic brain\nnetworks.\n","authors":["Weihan Li","Yule Wang","Chengrui Li","Anqi Wu"],"pdf_url":"https://arxiv.org/pdf/2407.00397v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.00646v3","updated":"2025-07-16T04:37:59Z","published":"2023-02-01T18:19:37Z","title":"Epic-Sounds: A Large-scale Dataset of Actions That Sound","summary":"  We introduce EPIC-SOUNDS, a large-scale dataset of audio annotations\ncapturing temporal extents and class labels within the audio stream of the\negocentric videos. We propose an annotation pipeline where annotators\ntemporally label distinguishable audio segments and describe the action that\ncould have caused this sound. We identify actions that can be discriminated\npurely from audio, through grouping these free-form descriptions of audio into\nclasses. For actions that involve objects colliding, we collect human\nannotations of the materials of these objects (e.g. a glass object being placed\non a wooden surface), which we verify from video, discarding ambiguities.\nOverall, EPIC-SOUNDS includes 78.4k categorised segments of audible events and\nactions, distributed across 44 classes as well as 39.2k non-categorised\nsegments. We train and evaluate state-of-the-art audio recognition and\ndetection models on our dataset, for both audio-only and audio-visual methods.\nWe also conduct analysis on: the temporal overlap between audio events, the\ntemporal and label correlations between audio and visual modalities, the\nambiguities in annotating materials from audio-only input, the importance of\naudio-only labels and the limitations of current models to understand actions\nthat sound.\n","authors":["Jaesung Huh","Jacob Chalk","Evangelos Kazakos","Dima Damen","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2302.00646v3.pdf","comment":"Accepted at TPAMI"},{"id":"http://arxiv.org/abs/2507.11902v1","updated":"2025-07-16T04:34:42Z","published":"2025-07-16T04:34:42Z","title":"Resampling strategies for imbalanced regression: a survey and empirical\n  analysis","summary":"  Imbalanced problems can arise in different real-world situations, and to\naddress this, certain strategies in the form of resampling or balancing\nalgorithms are proposed. This issue has largely been studied in the context of\nclassification, and yet, the same problem features in regression tasks, where\ntarget values are continuous. This work presents an extensive experimental\nstudy comprising various balancing and predictive models, and wich uses metrics\nto capture important elements for the user and to evaluate the predictive model\nin an imbalanced regression data context. It also proposes a taxonomy for\nimbalanced regression approaches based on three crucial criteria: regression\nmodel, learning process, and evaluation metrics. The study offers new insights\ninto the use of such strategies, highlighting the advantages they bring to each\nmodel's learning process, and indicating directions for further studies. The\ncode, data and further information related to the experiments performed herein\ncan be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.\n","authors":["Juscimara G. Avelino","George D. C. Cavalcanti","Rafael M. O. Cruz"],"pdf_url":"https://arxiv.org/pdf/2507.11902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11901v1","updated":"2025-07-16T04:34:02Z","published":"2025-07-16T04:34:02Z","title":"Imbalanced Regression Pipeline Recommendation","summary":"  Imbalanced problems are prevalent in various real-world scenarios and are\nextensively explored in classification tasks. However, they also present\nchallenges for regression tasks due to the rarity of certain target values. A\ncommon alternative is to employ balancing algorithms in preprocessing to\naddress dataset imbalance. However, due to the variety of resampling methods\nand learning models, determining the optimal solution requires testing many\ncombinations. Furthermore, the learning model, dataset, and evaluation metric\naffect the best strategies. This work proposes the Meta-learning for Imbalanced\nRegression (Meta-IR) framework, which diverges from existing literature by\ntraining meta-classifiers to recommend the best pipeline composed of the\nresampling strategy and learning model per task in a zero-shot fashion. The\nmeta-classifiers are trained using a set of meta-features to learn how to map\nthe meta-features to the classes indicating the best pipeline. We propose two\nformulations: Independent and Chained. Independent trains the meta-classifiers\nto separately indicate the best learning algorithm and resampling strategy.\nChained involves a sequential procedure where the output of one meta-classifier\nis used as input for another to model intrinsic relationship factors. The\nChained scenario showed superior performance, suggesting a relationship between\nthe learning algorithm and the resampling strategy per task. Compared with\nAutoML frameworks, Meta-IR obtained better results. Moreover, compared with\nbaselines of six learning algorithms and six resampling algorithms plus no\nresampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of\nthem. The code, data, and further information of the experiments can be found\non GitHub: https://github.com/JusciAvelino/Meta-IR.\n","authors":["Juscimara G. Avelino","George D. C. Cavalcanti","Rafael M. O. Cruz"],"pdf_url":"https://arxiv.org/pdf/2507.11901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11895v1","updated":"2025-07-16T04:22:16Z","published":"2025-07-16T04:22:16Z","title":"Newfluence: Boosting Model interpretability and Understanding in High\n  Dimensions","summary":"  The increasing complexity of machine learning (ML) and artificial\nintelligence (AI) models has created a pressing need for tools that help\nscientists, engineers, and policymakers interpret and refine model decisions\nand predictions. Influence functions, originating from robust statistics, have\nemerged as a popular approach for this purpose.\n  However, the heuristic foundations of influence functions rely on\nlow-dimensional assumptions where the number of parameters $p$ is much smaller\nthan the number of observations $n$. In contrast, modern AI models often\noperate in high-dimensional regimes with large $p$, challenging these\nassumptions.\n  In this paper, we examine the accuracy of influence functions in\nhigh-dimensional settings. Our theoretical and empirical analyses reveal that\ninfluence functions cannot reliably fulfill their intended purpose. We then\nintroduce an alternative approximation, called Newfluence, that maintains\nsimilar computational efficiency while offering significantly improved\naccuracy.\n  Newfluence is expected to provide more accurate insights than many existing\nmethods for interpreting complex AI models and diagnosing their issues.\nMoreover, the high-dimensional framework we develop in this paper can also be\napplied to analyze other popular techniques, such as Shapley values.\n","authors":["Haolin Zou","Arnab Auddy","Yongchan Kwon","Kamiar Rahnama Rad","Arian Maleki"],"pdf_url":"https://arxiv.org/pdf/2507.11895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11891v1","updated":"2025-07-16T04:14:52Z","published":"2025-07-16T04:14:52Z","title":"Choosing the Better Bandit Algorithm under Data Sharing: When Do A/B\n  Experiments Work?","summary":"  We study A/B experiments that are designed to compare the performance of two\nrecommendation algorithms. Prior work has shown that the standard\ndifference-in-means estimator is biased in estimating the global treatment\neffect (GTE) due to a particular form of interference between experimental\nunits. Specifically, units under the treatment and control algorithms\ncontribute to a shared pool of data that subsequently train both algorithms,\nresulting in interference between the two groups. The bias arising from this\ntype of data sharing is known as \"symbiosis bias\". In this paper, we highlight\nthat, for decision-making purposes, the sign of the GTE often matters more than\nits precise magnitude when selecting the better algorithm. We formalize this\ninsight under a multi-armed bandit framework and theoretically characterize\nwhen the sign of the expected GTE estimate under data sharing aligns with or\ncontradicts the sign of the true GTE. Our analysis identifies the level of\nexploration versus exploitation as a key determinant of how symbiosis bias\nimpacts algorithm selection.\n","authors":["Shuangning Li","Chonghuan Wang","Jingyan Wang"],"pdf_url":"https://arxiv.org/pdf/2507.11891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03194v2","updated":"2025-07-16T03:42:22Z","published":"2025-05-31T22:59:48Z","title":"HueManity: Probing Fine-Grained Visual Perception in MLLMs","summary":"  Multimodal Large Language Models (MLLMs) excel at high-level visual\nreasoning, but their performance on nuanced perceptual tasks remains\nsurprisingly limited. We present HueManity, a benchmark designed to assess\nvisual perception in MLLMs. The dataset comprises 83,850 images featuring\ntwo-character alphanumeric strings embedded in Ishihara test style dot\npatterns, challenging models on precise pattern recognition. Our evaluation of\nnine state-of-the-art MLLMs on HueManity demonstrates a significant performance\ndeficit compared to human and traditional computer vision baselines. The\nbest-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a\nstriking 3% on the alphanumeric `hard' task. In contrast, human participants\nachieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model\nreached accuracies of 96.5% and 94.5%. These results highlight a critical gap\nin the visual capabilities of current MLLMs. Our analysis further explores\npotential architectural and training-paradigm factors contributing to this\nperceptual gap in MLLMs. We open-source HueManity dataset and code to foster\nfurther research in improving perceptual robustness of MLLMs.\n","authors":["Rynaa Grover","Jayant Sravan Tamarapalli","Sahiti Yerramilli","Nilay Pande"],"pdf_url":"https://arxiv.org/pdf/2506.03194v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.00785v2","updated":"2025-07-16T03:41:55Z","published":"2025-06-01T02:24:46Z","title":"GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning","summary":"  This paper introduces GeoChain, a large-scale benchmark for evaluating\nstep-by-step geographic reasoning in multimodal large language models (MLLMs).\nLeveraging 1.46 million Mapillary street-level images, GeoChain pairs each\nimage with a 21-step chain-of-thought (CoT) question sequence (over 30 million\nQ&A pairs). These sequences guide models from coarse attributes to fine-grained\nlocalization across four reasoning categories - visual, spatial, cultural, and\nprecise geolocation - annotated by difficulty. Images are also enriched with\nsemantic segmentation (150 classes) and a visual locatability score. Our\nbenchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5\nvariants) on a diverse 2,088-image subset reveals consistent challenges: models\nfrequently exhibit weaknesses in visual grounding, display erratic reasoning,\nand struggle to achieve accurate localization, especially as the reasoning\ncomplexity escalates. GeoChain offers a robust diagnostic methodology, critical\nfor fostering significant advancements in complex geographic reasoning within\nMLLMs.\n","authors":["Sahiti Yerramilli","Nilay Pande","Rynaa Grover","Jayant Sravan Tamarapalli"],"pdf_url":"https://arxiv.org/pdf/2506.00785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11865v1","updated":"2025-07-16T03:24:54Z","published":"2025-07-16T03:24:54Z","title":"A Policy-Improved Deep Deterministic Policy Gradient Framework for the\n  Discount Order Acceptance Strategy of Ride-hailing Drivers","summary":"  The rapid expansion of platform integration has emerged as an effective\nsolution to mitigate market fragmentation by consolidating multiple\nride-hailing platforms into a single application. To address heterogeneous\npassenger preferences, third-party integrators provide Discount Express service\ndelivered by express drivers at lower trip fares. For the individual platform,\nencouraging broader participation of drivers in Discount Express services has\nthe potential to expand the accessible demand pool and improve matching\nefficiency, but often at the cost of reduced profit margins. This study aims to\ndynamically manage drivers' acceptance of Discount Express from the perspective\nof individual platforms. The lack of historical data under the new business\nmodel necessitates online learning. However, early-stage exploration through\ntrial and error can be costly in practice, highlighting the need for reliable\nearly-stage performance in real-world deployment. To address these challenges,\nthis study formulates the decision regarding the proportion of drivers'\nacceptance behavior as a continuous control task. In response to the high\nstochasticity, the opaque matching mechanisms employed by third-party\nintegrator, and the limited availability of historical data, we propose a\npolicy-improved deep deterministic policy gradient (pi-DDPG) framework. The\nproposed framework incorporates a refiner module to boost policy performance\nduring the early training phase, leverages a convolutional long short-term\nmemory network to effectively capture complex spatiotemporal patterns, and\nadopts a prioritized experience replay mechanism to enhance learning\nefficiency. A simulator based on a real-world dataset is developed to validate\nthe effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate\nthat pi-DDPG achieves superior learning efficiency and significantly reduces\nearly-stage training losses.\n","authors":["Hanwen Dai","Chang Gao","Fang He","Congyuan Ji","Yanni Yang"],"pdf_url":"https://arxiv.org/pdf/2507.11865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10543v2","updated":"2025-07-16T03:02:57Z","published":"2024-12-13T20:39:30Z","title":"METIS: Fast Quality-Aware RAG Systems with Configuration Adaptation","summary":"  RAG (Retrieval Augmented Generation) allows LLMs (large language models) to\ngenerate better responses with external knowledge, but using more external\nknowledge often improves generation quality at the expense of response delay.\nPrior work either reduces the response delay (through better scheduling of RAG\nqueries) or strives to maximize quality (which involves tuning the RAG\nworkflow), but they fall short in optimizing the tradeoff between the delay and\nquality of RAG responses. This paper presents METIS, the first RAG system that\njointly schedules queries and adapts the key RAG configurations of each query,\nsuch as the number of retrieved text chunks and synthesis methods, in order to\nbalance quality optimization and response delay reduction. Using 4 popular\nRAG-QA datasets, we show that compared with the state-of-the-art RAG\noptimization schemes, METIS reduces the generation latency by $1.64-2.54\\times$\nwithout sacrificing generation quality.\n","authors":["Siddhant Ray","Rui Pan","Zhuohan Gu","Kuntai Du","Shaoting Feng","Ganesh Ananthanarayanan","Ravi Netravali","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.10543v2.pdf","comment":"17 pages, 18 figures"},{"id":"http://arxiv.org/abs/2507.11855v1","updated":"2025-07-16T02:40:01Z","published":"2025-07-16T02:40:01Z","title":"OrdShap: Feature Position Importance for Sequential Black-Box Models","summary":"  Sequential deep learning models excel in domains with temporal or sequential\ndependencies, but their complexity necessitates post-hoc feature attribution\nmethods for understanding their predictions. While existing techniques quantify\nfeature importance, they inherently assume fixed feature ordering - conflating\nthe effects of (1) feature values and (2) their positions within input\nsequences. To address this gap, we introduce OrdShap, a novel attribution\nmethod that disentangles these effects by quantifying how a model's predictions\nchange in response to permuting feature position. We establish a game-theoretic\nconnection between OrdShap and Sanchez-Berganti\\~nos values, providing a\ntheoretically grounded approach to position-sensitive attribution. Empirical\nresults from health, natural language, and synthetic datasets highlight\nOrdShap's effectiveness in capturing feature value and feature position\nattributions, and provide deeper insight into model behavior.\n","authors":["Davin Hill","Brian L. Hill","Aria Masoomi","Vijay S. Nori","Robert E. Tillman","Jennifer Dy"],"pdf_url":"https://arxiv.org/pdf/2507.11855v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.12060v1","updated":"2025-07-16T09:16:51Z","published":"2025-07-16T09:16:51Z","title":"InstructFLIP: Exploring Unified Vision-Language Model for Face\n  Anti-spoofing","summary":"  Face anti-spoofing (FAS) aims to construct a robust system that can withstand\ndiverse attacks. While recent efforts have concentrated mainly on cross-domain\ngeneralization, two significant challenges persist: limited semantic\nunderstanding of attack types and training redundancy across domains. We\naddress the first by integrating vision-language models (VLMs) to enhance the\nperception of visual input. For the second challenge, we employ a meta-domain\nstrategy to learn a unified model that generalizes well across multiple\ndomains. Our proposed InstructFLIP is a novel instruction-tuned framework that\nleverages VLMs to enhance generalization via textual guidance trained solely on\na single domain. At its core, InstructFLIP explicitly decouples instructions\ninto content and style components, where content-based instructions focus on\nthe essential semantics of spoofing, and style-based instructions consider\nvariations related to the environment and camera characteristics. Extensive\nexperiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA\nmodels in accuracy and substantially reducing training redundancy across\ndiverse domains in FAS. Project website is available at\nhttps://kunkunlin1221.github.io/InstructFLIP.\n","authors":["Kun-Hsiang Lin","Yu-Wen Tseng","Kang-Yang Huang","Jhih-Ciang Wu","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2507.12060v1.pdf","comment":"Accepted by MM'25"},{"id":"http://arxiv.org/abs/2507.12042v1","updated":"2025-07-16T08:59:09Z","published":"2025-07-16T08:59:09Z","title":"Stereo Sound Event Localization and Detection with Onscreen/offscreen\n  Classification","summary":"  This paper presents the objective, dataset, baseline, and metrics of Task 3\nof the DCASE2025 Challenge on sound event localization and detection (SELD). In\nprevious editions, the challenge used four-channel audio formats of first-order\nAmbisonics (FOA) and microphone array. In contrast, this year's challenge\ninvestigates SELD with stereo audio data (termed stereo SELD). This change\nshifts the focus from more specialized 360{\\deg} audio and audiovisual scene\nanalysis to more commonplace audio and media scenarios with limited\nfield-of-view (FOV). Due to inherent angular ambiguities in stereo audio data,\nthe task focuses on direction-of-arrival (DOA) estimation in the azimuth plane\n(left-right axis) along with distance estimation. The challenge remains divided\ninto two tracks: audio-only and audiovisual, with the audiovisual track\nintroducing a new sub-task of onscreen/offscreen event classification\nnecessitated by the limited FOV. This challenge introduces the DCASE2025 Task3\nStereo SELD Dataset, whose stereo audio and perspective video clips are sampled\nand converted from the STARSS23 recordings. The baseline system is designed to\nprocess stereo audio and corresponding video frames as inputs. In addition to\nthe typical SELD event classification and localization, it integrates\nonscreen/offscreen classification for the audiovisual track. The evaluation\nmetrics have been modified to introduce an onscreen/offscreen accuracy metric,\nwhich assesses the models' ability to identify which sound sources are\nonscreen. In the experimental evaluation, the baseline system performs\nreasonably well with the stereo audio data.\n","authors":["Kazuki Shimada","Archontis Politis","Iran R. Roman","Parthasaarathy Sudarsanam","David Diaz-Guerra","Ruchi Pandey","Kengo Uchida","Yuichiro Koyama","Naoya Takahashi","Takashi Shibuya","Shusuke Takahashi","Tuomas Virtanen","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2507.12042v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.11939v1","updated":"2025-07-16T06:09:02Z","published":"2025-07-16T06:09:02Z","title":"POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual\n  Chart Question Answering","summary":"  Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.\n","authors":["Yichen Xu","Liangyu Chen","Liang Zhang","Wenxuan Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2507.11939v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2507.11903v1","updated":"2025-07-16T04:41:53Z","published":"2025-07-16T04:41:53Z","title":"Unveiling the Visual Rhetoric of Persuasive Cartography: A Case Study of\n  the Design of Octopus Maps","summary":"  When designed deliberately, data visualizations can become powerful\npersuasive tools, influencing viewers' opinions, values, and actions. While\nresearchers have begun studying this issue (e.g., to evaluate the effects of\npersuasive visualization), we argue that a fundamental mechanism of persuasion\nresides in rhetorical construction, a perspective inadequately addressed in\ncurrent visualization research. To fill this gap, we present a focused analysis\nof octopus maps, a visual genre that has maintained persuasive power across\ncenturies and achieved significant social impact. Employing rhetorical schema\ntheory, we collected and analyzed 90 octopus maps spanning from the 19th\ncentury to contemporary times. We closely examined how octopus maps implement\ntheir persuasive intents and constructed a design space that reveals how visual\nmetaphors are strategically constructed and what common rhetorical strategies\nare applied to components such as maps, octopus imagery, and text. Through the\nabove analysis, we also uncover a set of interesting findings. For instance,\ncontrary to the common perception that octopus maps are primarily a historical\nphenomenon, our research shows that they remain a lively design convention in\ntoday's digital age. Additionally, while most octopus maps stem from Western\ndiscourse that views the octopus as an evil symbol, some designs offer\nalternative interpretations, highlighting the dynamic nature of rhetoric across\ndifferent sociocultural settings. Lastly, drawing from the lessons provided by\noctopus maps, we discuss the associated ethical concerns of persuasive\nvisualization.\n","authors":["Daocheng Lin","Yifan Wang","Yutong Yang","Xingyu Lan"],"pdf_url":"https://arxiv.org/pdf/2507.11903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12571v1","updated":"2025-07-16T18:41:42Z","published":"2025-07-16T18:41:42Z","title":"Catching Dark Signals in Algorithms: Unveiling Audiovisual and Thematic\n  Markers of Unsafe Content Recommended for Children and Teenagers","summary":"  The prevalence of short form video platforms, combined with the\nineffectiveness of age verification mechanisms, raises concerns about the\npotential harms facing children and teenagers in an algorithm-moderated online\nenvironment. We conducted multimodal feature analysis and thematic topic\nmodeling of 4,492 short videos recommended to children and teenagers on\nInstagram Reels, TikTok, and YouTube Shorts, collected as a part of an\nalgorithm auditing experiment. This feature-level and content-level analysis\nrevealed that unsafe (i.e., problematic, mentally distressing) short videos (a)\npossess darker visual features and (b) contain explicitly harmful content and\nimplicit harm from anxiety-inducing ordinary content. We introduce a useful\nframework of online harm (i.e., explicit, implicit, unintended), providing a\nunique lens for understanding the dynamic, multifaceted online risks facing\nchildren and teenagers. The findings highlight the importance of protecting\nyounger audiences in critical developmental stages from both explicit and\nimplicit risks on social media, calling for nuanced content moderation, age\nverification, and platform regulation.\n","authors":["Haoning Xue","Brian Nishimine","Martin Hilbert","Drew Cingel","Samantha Vigil","Jane Shawcroft","Arti Thakur","Zubair Shafiq","Jingwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.12571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12096v4","updated":"2025-07-16T14:12:28Z","published":"2025-02-17T18:14:18Z","title":"Token Communications: A Large Model-Driven Framework for Cross-modal\n  Context-aware Semantic Communications","summary":"  In this paper, we introduce token communications (TokCom), a large\nmodel-driven framework to leverage cross-modal context information in\ngenerative semantic communications (GenSC). TokCom is a new paradigm, motivated\nby the recent success of generative foundation models and multimodal large\nlanguage models (GFM/MLLMs), where the communication units are tokens, enabling\nefficient transformer-based token processing at the transmitter and receiver.\nIn this paper, we introduce the potential opportunities and challenges of\nleveraging context in GenSC, explore how to integrate GFM/MLLMs-based token\nprocessing into semantic communication systems to leverage cross-modal context\neffectively at affordable complexity, present the key principles for efficient\nTokCom at various layers in future wireless networks. In a typical image\nsemantic communication setup, we demonstrate a significant improvement of the\nbandwidth efficiency, achieved by TokCom by leveraging the context information\namong tokens. Finally, the potential research directions are identified to\nfacilitate adoption of TokCom in future wireless networks.\n","authors":["Li Qiao","Mahdi Boloursaz Mashhadi","Zhen Gao","Rahim Tafazolli","Mehdi Bennis","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2502.12096v4.pdf","comment":"Accepted at IEEE Wireless Communications Magazine"},{"id":"http://arxiv.org/abs/2507.12498v1","updated":"2025-07-16T01:54:06Z","published":"2025-07-16T01:54:06Z","title":"Wavelet-GS: 3D Gaussian Splatting with Wavelet Decomposition","summary":"  3D Gaussian Splatting (3DGS) has revolutionized 3D scene reconstruction,\nwhich effectively balances rendering quality, efficiency, and speed. However,\nexisting 3DGS approaches usually generate plausible outputs and face\nsignificant challenges in complex scene reconstruction, manifesting as\nincomplete holistic structural outlines and unclear local lighting effects. To\naddress these issues simultaneously, we propose a novel decoupled optimization\nframework, which integrates wavelet decomposition into 3D Gaussian Splatting\nand 2D sampling. Technically, through 3D wavelet decomposition, our approach\ndivides point clouds into high-frequency and low-frequency components, enabling\ntargeted optimization for each. The low-frequency component captures global\nstructural outlines and manages the distribution of Gaussians through\nvoxelization. In contrast, the high-frequency component restores intricate\ngeometric and textural details while incorporating a relight module to mitigate\nlighting artifacts and enhance photorealistic rendering. Additionally, a 2D\nwavelet decomposition is applied to the training images, simulating radiance\nvariations. This provides critical guidance for high-frequency detail\nreconstruction, ensuring seamless integration of details with the global\nstructure. Extensive experiments on challenging datasets demonstrate our method\nachieves state-of-the-art performance across various metrics, surpassing\nexisting approaches and advancing the field of 3D scene reconstruction.\n","authors":["Beizhen Zhao","Yifan Zhou","Sicheng Yu","Zijian Wang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2507.12498v1.pdf","comment":"9 pages"}]},"2025-07-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2507.13348v1","updated":"2025-07-17T17:59:55Z","published":"2025-07-17T17:59:55Z","title":"VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning","summary":"  Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.\n","authors":["Senqiao Yang","Junyi Li","Xin Lai","Bei Yu","Hengshuang Zhao","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2507.13348v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/VisionThink"},{"id":"http://arxiv.org/abs/2410.01772v2","updated":"2025-07-17T17:58:50Z","published":"2024-10-02T17:29:34Z","title":"DeFine: Decision-Making with Analogical Reasoning over Factor Profiles","summary":"  LLMs are ideal for decision-making thanks to their ability to reason over\nlong contexts. However, challenges arise when processing speech transcripts\nthat describe complex scenarios, as they are verbose and include repetition,\nhedging, and vagueness. E.g., during a company's earnings call, an executive\nmight project a positive revenue outlook to reassure investors, despite\nuncertainty regarding future earnings. It is crucial for LLMs to incorporate\nthis uncertainty systematically when making decisions. In this paper, we\nintroduce \\textsc{DeFine}, a modular framework that constructs probabilistic\nfactor profiles from complex scenarios. It then integrates these profiles with\nanalogical reasoning, leveraging insights from similar past experiences to\nguide LLMs in making critical decisions in new situations. Our framework\nseparates the tasks of quantifying uncertainty and incorporating it into LLM\ndecision-making. This approach is particularly useful in areas such as\nconsulting and financial deliberation, where making decisions under uncertainty\nis vital.\n","authors":["Yebowen Hu","Xiaoyang Wang","Wenlin Yao","Yiming Lu","Daoan Zhang","Hassan Foroosh","Dong Yu","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01772v2.pdf","comment":"Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025), Vienna, Austria"},{"id":"http://arxiv.org/abs/2507.13335v1","updated":"2025-07-17T17:51:20Z","published":"2025-07-17T17:51:20Z","title":"Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour\n  Understanding from Traditional Puns to Topical Jokes","summary":"  Humour, as a complex language form, is derived from myriad aspects of life,\nwhilst existing work on computational humour has focussed almost exclusively on\nshort pun-based jokes. In this work, we investigate whether the ability of\nLarge Language Models (LLMs) to explain humour depends on the particular humour\nform. We compare models on simple puns and more complex topical humour that\nrequires knowledge of real-world entities and events. In doing so, we curate a\ndataset of 600 jokes split across 4 joke types and manually write high-quality\nexplanations. These jokes include heterographic and homographic puns,\ncontemporary internet humour, and topical jokes, where understanding relies on\nreasoning beyond \"common sense\", rooted instead in world knowledge regarding\nnews events and pop culture. Using this dataset, we compare the zero-shot\nabilities of a range of LLMs to accurately and comprehensively explain jokes of\ndifferent types, identifying key research gaps in the task of humour\nexplanation. We find that none of the tested models (inc. reasoning models) are\ncapable of reliably generating adequate explanations of all joke types, further\nhighlighting the narrow focus of most works in computational humour on overly\nsimple joke forms.\n","authors":["Tyler Loakman","William Thorne","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2507.13335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13334v1","updated":"2025-07-17T17:50:36Z","published":"2025-07-17T17:50:36Z","title":"A Survey of Context Engineering for Large Language Models","summary":"  The performance of Large Language Models (LLMs) is fundamentally determined\nby the contextual information provided during inference. This survey introduces\nContext Engineering, a formal discipline that transcends simple prompt design\nto encompass the systematic optimization of information payloads for LLMs. We\npresent a comprehensive taxonomy decomposing Context Engineering into its\nfoundational components and the sophisticated implementations that integrate\nthem into intelligent systems. We first examine the foundational components:\ncontext retrieval and generation, context processing and context management. We\nthen explore how these components are architecturally integrated to create\nsophisticated system implementations: retrieval-augmented generation (RAG),\nmemory systems and tool-integrated reasoning, and multi-agent systems. Through\nthis systematic analysis of over 1300 research papers, our survey not only\nestablishes a technical roadmap for the field but also reveals a critical\nresearch gap: a fundamental asymmetry exists between model capabilities. While\ncurrent models, augmented by advanced context engineering, demonstrate\nremarkable proficiency in understanding complex contexts, they exhibit\npronounced limitations in generating equally sophisticated, long-form outputs.\nAddressing this gap is a defining priority for future research. Ultimately,\nthis survey provides a unified framework for both researchers and engineers\nadvancing context-aware AI.\n","authors":["Lingrui Mei","Jiayu Yao","Yuyao Ge","Yiwei Wang","Baolong Bi","Yujun Cai","Jiazhi Liu","Mingyu Li","Zhong-Zhi Li","Duzhen Zhang","Chenlin Zhou","Jiayi Mao","Tianze Xia","Jiafeng Guo","Shenghua Liu"],"pdf_url":"https://arxiv.org/pdf/2507.13334v1.pdf","comment":"ongoing work; 165 pages, 1401 citations"},{"id":"http://arxiv.org/abs/2507.13332v1","updated":"2025-07-17T17:50:07Z","published":"2025-07-17T17:50:07Z","title":"The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner","summary":"  Length generalization, the ability to solve problems of longer sequences than\nthose observed during training, poses a core challenge of Transformer-based\nlarge language models (LLM). Although existing studies have predominantly\nfocused on data-driven approaches for arithmetic operations and symbolic\nmanipulation tasks, these approaches tend to be task-specific with limited\noverall performance. To pursue a more general solution, this paper focuses on a\nbroader case of reasoning problems that are computable, i.e., problems that\nalgorithms can solve, thus can be solved by the Turing Machine. From this\nperspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to\nimprove the length generalization ability of LLMs. TAIL synthesizes\nchain-of-thoughts (CoT) data that imitate the execution process of a Turing\nMachine by computer programs, which linearly expands the reasoning steps into\natomic states to alleviate shortcut learning and explicit memory fetch\nmechanism to reduce the difficulties of dynamic and long-range data access in\nelementary operations. To validate the reliability and universality of TAIL, we\nconstruct a challenging synthetic dataset covering 8 classes of algorithms and\n18 tasks. Without bells and whistles, TAIL significantly improves the length\ngeneralization ability as well as the performance of Qwen2.5-7B on various\ntasks using only synthetic data, surpassing previous methods and DeepSeek-R1.\nThe experimental results reveal that the key concepts in the Turing Machine,\ninstead of the thinking styles, are indispensable for TAIL for length\ngeneralization, through which the model exhibits read-and-write behaviors\nconsistent with the properties of the Turing Machine in their attention layers.\nThis work provides a promising direction for future research in the learning of\nLLM reasoning from synthetic data.\n","authors":["Zhouqi Hua","Wenwei Zhang","Chengqi Lyu","Yuzhe Gu","Songyang Gao","Kuikun Liu","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2507.13332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13328v1","updated":"2025-07-17T17:47:47Z","published":"2025-07-17T17:47:47Z","title":"Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does\n  Not Fundamentally Alter It","summary":"  Does vision-and-language (VL) training change the linguistic representations\nof language models in meaningful ways? Most results in the literature have\nshown inconsistent or marginal differences, both behaviorally and\nrepresentationally. In this work, we start from the hypothesis that the domain\nin which VL training could have a significant effect is lexical-conceptual\nknowledge, in particular its taxonomic organization. Through comparing minimal\npairs of text-only LMs and their VL-trained counterparts, we first show that\nthe VL models often outperform their text-only counterparts on a text-only\nquestion-answering task that requires taxonomic understanding of concepts\nmentioned in the questions. Using an array of targeted behavioral and\nrepresentational analyses, we show that the LMs and VLMs do not differ\nsignificantly in terms of their taxonomic knowledge itself, but they differ in\nhow they represent questions that contain concepts in a taxonomic relation vs.\na non-taxonomic relation. This implies that the taxonomic knowledge itself does\nnot change substantially through additional VL training, but VL training does\nimprove the deployment of this knowledge in the context of a specific task,\neven when the presentation of the task is purely linguistic.\n","authors":["Yulu Qin","Dheeraj Varghese","Adam Dahlgren Lindström","Lucia Donatelli","Kanishka Misra","Najoung Kim"],"pdf_url":"https://arxiv.org/pdf/2507.13328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13325v1","updated":"2025-07-17T17:44:33Z","published":"2025-07-17T17:44:33Z","title":"Social and Political Framing in Search Engine Results","summary":"  Search engines play a crucial role in shaping public discourse by influencing\nhow information is accessed and framed. While prior research has extensively\nexamined various dimensions of search bias -- such as content prioritization,\nindexical bias, political polarization, and sources of bias -- an important\nquestion remains underexplored: how do search engines and\nideologically-motivated user queries contribute to bias in search results. This\nstudy analyzes the outputs of major search engines using a dataset of political\nand social topics. The findings reveal that search engines not only prioritize\ncontent in ways that reflect underlying biases but also that\nideologically-driven user queries exacerbate these biases, resulting in the\namplification of specific narratives. Moreover, significant differences were\nobserved across search engines in terms of the sources they prioritize. These\nresults suggest that search engines may play a pivotal role in shaping public\nperceptions by reinforcing ideological divides, thereby contributing to the\nbroader issue of information polarization.\n","authors":["Amrit Poudel","Tim Weninger"],"pdf_url":"https://arxiv.org/pdf/2507.13325v1.pdf","comment":"Accepted to ICWSM 2026"},{"id":"http://arxiv.org/abs/2507.13318v1","updated":"2025-07-17T17:37:24Z","published":"2025-07-17T17:37:24Z","title":"HapticCap: A Multimodal Dataset and Task for Understanding User\n  Experience of Vibration Haptic Signals","summary":"  Haptic signals, from smartphone vibrations to virtual reality touch feedback,\ncan effectively convey information and enhance realism, but designing signals\nthat resonate meaningfully with users is challenging. To facilitate this, we\nintroduce a multimodal dataset and task, of matching user descriptions to\nvibration haptic signals, and highlight two primary challenges: (1) lack of\nlarge haptic vibration datasets annotated with textual descriptions as\ncollecting haptic descriptions is time-consuming, and (2) limited capability of\nexisting tasks and models to describe vibration signals in text. To advance\nthis area, we create HapticCap, the first fully human-annotated\nhaptic-captioned dataset, containing 92,070 haptic-text pairs for user\ndescriptions of sensory, emotional, and associative attributes of vibrations.\nBased on HapticCap, we propose the haptic-caption retrieval task and present\nthe results of this task from a supervised contrastive learning framework that\nbrings together text representations within specific categories and vibrations.\nOverall, the combination of language model T5 and audio model AST yields the\nbest performance in the haptic-caption retrieval task, especially when\nseparately trained for each description category.\n","authors":["Guimin Hu","Daniel Hershcovich","Hasti Seifi"],"pdf_url":"https://arxiv.org/pdf/2507.13318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12774v2","updated":"2025-07-17T17:27:11Z","published":"2024-10-16T17:49:45Z","title":"Identifying Task Groupings for Multi-Task Learning Using Pointwise\n  V-Usable Information","summary":"  The success of multi-task learning can depend heavily on which tasks are\ngrouped together. Naively grouping all tasks or a random set of tasks can\nresult in negative transfer, with the multi-task models performing worse than\nsingle-task models. Though many efforts have been made to identify task\ngroupings and to measure the relatedness among different tasks, it remains a\nchallenging research topic to define a metric to identify the best task\ngrouping out of a pool of many potential task combinations. We propose a metric\nof task relatedness based on task difficulty measured by pointwise V-usable\ninformation (PVI). PVI is a recently proposed metric to estimate how much\nusable information a dataset contains given a model. We hypothesize that tasks\nwith not statistically different PVI estimates are similar enough to benefit\nfrom the joint learning process. We conduct comprehensive experiments to\nevaluate the feasibility of this metric for task grouping on 15 NLP datasets in\nthe general, biomedical, and clinical domains. We compare the results of the\njoint learners against single learners, existing baseline methods, and recent\nlarge language models, including Llama 2 and GPT-4. The results show that by\ngrouping tasks with similar PVI estimates, the joint learners yielded\ncompetitive results with fewer total parameters, with consistent performance\nacross domains.\n","authors":["Yingya Li","Timothy Miller","Steven Bethard","Guergana Savova"],"pdf_url":"https://arxiv.org/pdf/2410.12774v2.pdf","comment":"main paper 12 pages, Appendix 7 pages, 1 figure, 18 tables"},{"id":"http://arxiv.org/abs/2507.13302v1","updated":"2025-07-17T17:11:14Z","published":"2025-07-17T17:11:14Z","title":"The Generative Energy Arena (GEA): Incorporating Energy Awareness in\n  Large Language Model (LLM) Human Evaluations","summary":"  The evaluation of large language models is a complex task, in which several\napproaches have been proposed. The most common is the use of automated\nbenchmarks in which LLMs have to answer multiple-choice questions of different\ntopics. However, this method has certain limitations, being the most\nconcerning, the poor correlation with the humans. An alternative approach, is\nto have humans evaluate the LLMs. This poses scalability issues as there is a\nlarge and growing number of models to evaluate making it impractical (and\ncostly) to run traditional studies based on recruiting a number of evaluators\nand having them rank the responses of the models. An alternative approach is\nthe use of public arenas, such as the popular LM arena, on which any user can\nfreely evaluate models on any question and rank the responses of two models.\nThe results are then elaborated into a model ranking. An increasingly important\naspect of LLMs is their energy consumption and, therefore, evaluating how\nenergy awareness influences the decisions of humans in selecting a model is of\ninterest. In this paper, we present GEA, the Generative Energy Arena, an arena\nthat incorporates information on the energy consumption of the model in the\nevaluation process. Preliminary results obtained with GEA are also presented,\nshowing that for most questions, when users are aware of the energy\nconsumption, they favor smaller and more energy efficient models. This suggests\nthat for most user interactions, the extra cost and energy incurred by the more\ncomplex and top-performing models do not provide an increase in the perceived\nquality of the responses that justifies their use.\n","authors":["Carlos Arriaga","Gonzalo Martínez","Eneko Sendin","Javier Conde","Pedro Reviriego"],"pdf_url":"https://arxiv.org/pdf/2507.13302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13300v1","updated":"2025-07-17T17:09:22Z","published":"2025-07-17T17:09:22Z","title":"AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research","summary":"  We introduce AbGen, the first benchmark designed to evaluate the capabilities\nof LLMs in designing ablation studies for scientific research. AbGen consists\nof 1,500 expert-annotated examples derived from 807 NLP papers. In this\nbenchmark, LLMs are tasked with generating detailed ablation study designs for\na specified module or process based on the given research context. Our\nevaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a\nsignificant performance gap between these models and human experts in terms of\nthe importance, faithfulness, and soundness of the ablation study designs.\nMoreover, we demonstrate that current automated evaluation methods are not\nreliable for our task, as they show a significant discrepancy when compared to\nhuman assessment. To better investigate this, we develop AbGen-Eval, a\nmeta-evaluation benchmark designed to assess the reliability of commonly used\nautomated evaluation systems in measuring LLM performance on our task. We\ninvestigate various LLM-as-Judge systems on AbGen-Eval, providing insights for\nfuture research on developing more effective and reliable LLM-based evaluation\nsystems for complex scientific tasks.\n","authors":["Yilun Zhao","Weiyuan Chen","Zhijian Xu","Manasi Patwardhan","Yixin Liu","Chengye Wang","Lovekesh Vig","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2507.13300v1.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2507.13285v1","updated":"2025-07-17T16:50:07Z","published":"2025-07-17T16:50:07Z","title":"Multi-Agent Synergy-Driven Iterative Visual Narrative Synthesis","summary":"  Automated generation of high-quality media presentations is challenging,\nrequiring robust content extraction, narrative planning, visual design, and\noverall quality optimization. Existing methods often produce presentations with\nlogical inconsistencies and suboptimal layouts, thereby struggling to meet\nprofessional standards. To address these challenges, we introduce RCPS\n(Reflective Coherent Presentation Synthesis), a novel framework integrating\nthree key components: (1) Deep Structured Narrative Planning; (2) Adaptive\nLayout Generation; (3) an Iterative Optimization Loop. Additionally, we propose\nPREVAL, a preference-based evaluation framework employing rationale-enhanced\nmulti-dimensional models to assess presentation quality across Content,\nCoherence, and Design. Experimental results demonstrate that RCPS significantly\noutperforms baseline methods across all quality dimensions, producing\npresentations that closely approximate human expert standards. PREVAL shows\nstrong correlation with human judgments, validating it as a reliable automated\ntool for assessing presentation quality.\n","authors":["Wang Xi","Quan Shi","Tian Yu","Yujie Peng","Jiayi Sun","Mengxing Ren","Zenghui Ding","Ningguang Yao"],"pdf_url":"https://arxiv.org/pdf/2507.13285v1.pdf","comment":"22 pages, 7 figures, 3 tables. Submitted to an ACL-style conference"},{"id":"http://arxiv.org/abs/2505.23121v2","updated":"2025-07-17T16:49:08Z","published":"2025-05-29T05:41:26Z","title":"ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal\n  Conversations","summary":"  Multi-modal large language models have demonstrated remarkable zero-shot\nabilities and powerful image-understanding capabilities. However, the existing\nopen-source multi-modal models suffer from the weak capability of multi-turn\ninteraction, especially for long contexts. To address the issue, we first\nintroduce a context modeling module, termed ContextQFormer, which utilizes a\nmemory block to enhance the presentation of contextual information.\nFurthermore, to facilitate further research, we carefully build a new\nmulti-turn multi-modal dialogue dataset (TMDialog) for pre-training,\ninstruction-tuning, and evaluation, which will be open-sourced lately. Compared\nwith other multi-modal dialogue datasets, TMDialog contains longer\nconversations, which supports the research of multi-turn multi-modal dialogue.\nIn addition, ContextQFormer is compared with three baselines on TMDialog and\nexperimental results illustrate that ContextQFormer achieves an improvement of\n2%-4% in available rate over baselines.\n","authors":["Yiming Lei","Zhizheng Yang","Zeming Liu","Haitao Leng","Shaoguo Liu","Tingting Gao","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.23121v2.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.13275v1","updated":"2025-07-17T16:33:57Z","published":"2025-07-17T16:33:57Z","title":"Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for\n  Human Capital Management","summary":"  Advances in natural language processing and large language models are driving\na major transformation in Human Capital Management, with a growing interest in\nbuilding smart systems based on language technologies for talent acquisition,\nupskilling strategies, and workforce planning. However, the adoption and\nprogress of these technologies critically depend on the development of reliable\nand fair models, properly evaluated on public data and open benchmarks, which\nhave so far been unavailable in this domain.\n  To address this gap, we present TalentCLEF 2025, the first evaluation\ncampaign focused on skill and job title intelligence. The lab consists of two\ntasks: Task A - Multilingual Job Title Matching, covering English, Spanish,\nGerman, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.\nBoth corpora were built from real job applications, carefully anonymized, and\nmanually annotated to reflect the complexity and diversity of real-world labor\nmarket data, including linguistic variability and gender-marked expressions.\n  The evaluations included monolingual and cross-lingual scenarios and covered\nthe evaluation of gender bias.\n  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most\nsystems relied on information retrieval techniques built with multilingual\nencoder-based models fine-tuned with contrastive learning, and several of them\nincorporated large language models for data augmentation or re-ranking. The\nresults show that the training strategies have a larger effect than the size of\nthe model alone. TalentCLEF provides the first public benchmark in this field\nand encourages the development of robust, fair, and transferable language\ntechnologies for the labor market.\n","authors":["Luis Gasco","Hermenegildo Fabregat","Laura García-Sardiña","Paula Estrella","Daniel Deniz","Alvaro Rodrigo","Rabih Zbib"],"pdf_url":"https://arxiv.org/pdf/2507.13275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13425v2","updated":"2025-07-17T16:32:46Z","published":"2025-04-18T02:51:29Z","title":"Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with\n  Security Filtering","summary":"  Existing Retrieval-Augmented Generation (RAG) systems face challenges in\nenterprise settings due to limited retrieval scope and data security risks.\nWhen relevant internal documents are unavailable, the system struggles to\ngenerate accurate and complete responses. Additionally, using closed-source\nLarge Language Models (LLMs) raises concerns about exposing proprietary\ninformation. To address these issues, we propose the Secure Multifaceted-RAG\n(SecMulti-RAG) framework, which retrieves not only from internal documents but\nalso from two supplementary sources: pre-generated expert knowledge for\nanticipated queries and on-demand external LLM-generated knowledge. To mitigate\nsecurity risks, we adopt a local open-source generator and selectively utilize\nexternal LLMs only when prompts are deemed safe by a filtering mechanism. This\napproach enhances completeness, prevents data leakage, and reduces costs. In\nour evaluation on a report generation task in the automotive industry,\nSecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9\npercent win rates across correctness, richness, and helpfulness in LLM-based\nevaluation, and 56.3 to 70.4 percent in human evaluation. This highlights\nSecMulti-RAG as a practical and secure solution for enterprise RAG.\n","authors":["Grace Byun","Shinsun Lee","Nayoung Choi","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2504.13425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13266v1","updated":"2025-07-17T16:21:47Z","published":"2025-07-17T16:21:47Z","title":"QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation","summary":"  Reinforcement learning (RL) has become a key component in training large\nlanguage reasoning models (LLMs). However, recent studies questions its\neffectiveness in improving multi-step reasoning-particularly on hard problems.\nTo address this challenge, we propose a simple yet effective strategy via\nQuestion Augmentation: introduce partial solutions during training to reduce\nproblem difficulty and provide more informative learning signals. Our method,\nQuestA, when applied during RL training on math reasoning tasks, not only\nimproves pass@1 but also pass@k-particularly on problems where standard RL\nstruggles to make progress. This enables continual improvement over strong\nopen-source models such as DeepScaleR and OpenMath Nemotron, further enhancing\ntheir reasoning capabilities. We achieve new state-of-the-art results on math\nbenchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)\non AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical\nexplanations that QuestA improves sample efficiency, offering a practical and\ngeneralizable pathway for expanding reasoning capability through RL.\n","authors":["Jiazheng Li","Hong Lu","Kaiyue Wen","Zaiwen Yang","Jiaxuan Gao","Hongzhou Lin","Yi Wu","Jingzhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.13266v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2507.13255v1","updated":"2025-07-17T16:04:55Z","published":"2025-07-17T16:04:55Z","title":"Automating Steering for Safe Multimodal Large Language Models","summary":"  Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.\n","authors":["Lyucheng Wu","Mengru Wang","Ziwen Xu","Tri Cao","Nay Oo","Bryan Hooi","Shumin Deng"],"pdf_url":"https://arxiv.org/pdf/2507.13255v1.pdf","comment":"Working in progress. 22 pages (8+ for main); 25 figures; 1 table"},{"id":"http://arxiv.org/abs/2504.16394v3","updated":"2025-07-17T15:50:27Z","published":"2025-04-23T03:42:46Z","title":"ConTextual: Improving Clinical Text Summarization in LLMs with\n  Context-preserving Token Filtering and Knowledge Graphs","summary":"  Unstructured clinical data can serve as a unique and rich source of\ninformation that can meaningfully inform clinical practice. Extracting the most\npertinent context from such data is critical for exploiting its true potential\ntoward optimal and timely decision-making in patient care. While prior research\nhas explored various methods for clinical text summarization, most prior\nstudies either process all input tokens uniformly or rely on heuristic-based\nfilters, which can overlook nuanced clinical cues and fail to prioritize\ninformation critical for decision-making. In this study, we propose Contextual,\na novel framework that integrates a Context-Preserving Token Filtering method\nwith a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By\npreserving context-specific important tokens and enriching them with structured\nknowledge, ConTextual improves both linguistic coherence and clinical fidelity.\nOur extensive empirical evaluations on two public benchmark datasets\ndemonstrate that ConTextual consistently outperforms other baselines. Our\nproposed approach highlights the complementary role of token-level filtering\nand structured retrieval in enhancing both linguistic and clinical integrity,\nas well as offering a scalable solution for improving precision in clinical\ntext generation.\n","authors":["Fahmida Liza Piya","Rahmatollah Beheshti"],"pdf_url":"https://arxiv.org/pdf/2504.16394v3.pdf","comment":"Accepted for MLHC 2025"},{"id":"http://arxiv.org/abs/2507.13238v1","updated":"2025-07-17T15:47:49Z","published":"2025-07-17T15:47:49Z","title":"HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language\n  Models","summary":"  Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi.\n","authors":["Ashray Gupta","Rohan Joseph","Sunny Rai"],"pdf_url":"https://arxiv.org/pdf/2507.13238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13236v1","updated":"2025-07-17T15:47:22Z","published":"2025-07-17T15:47:22Z","title":"Enhancing Cross-task Transfer of Large Language Models via Activation\n  Steering","summary":"  Large language models (LLMs) have shown impressive abilities in leveraging\npretrained knowledge through prompting, but they often struggle with unseen\ntasks, particularly in data-scarce scenarios. While cross-task in-context\nlearning offers a direct solution for transferring knowledge across tasks, it\nstill faces critical challenges in terms of robustness, scalability, and\nefficiency. In this paper, we investigate whether cross-task transfer can be\nachieved via latent space steering without parameter updates or input\nexpansion. Through an analysis of activation patterns in the latent space of\nLLMs, we observe that the enhanced activations induced by in-context examples\nhave consistent patterns across different tasks. Inspired by these findings, we\npropose CAST, a novel Cross-task Activation Steering Transfer framework that\nenables effective transfer by manipulating the model's internal activation\nstates. Our approach first selects influential and diverse samples from\nhigh-resource tasks, then utilizes their contrastive representation-enhanced\nactivations to adapt LLMs to low-resource tasks. Extensive experiments across\nboth cross-domain and cross-lingual transfer settings show that our method\noutperforms competitive baselines and demonstrates superior scalability and\nlower computational costs.\n","authors":["Xinyu Tang","Zhihao Lv","Xiaoxue Cheng","Junyi Li","Wayne Xin Zhao","Zujie Wen","Zhiqiang Zhang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.13236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13733v2","updated":"2025-07-17T15:31:25Z","published":"2025-03-17T21:41:37Z","title":"CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual,\n  Multi-Generator and Multi-Domain Settings","summary":"  Large language models (LLMs) have revolutionized code generation, automating\nprogramming with remarkable efficiency. However, these advancements challenge\nprogramming skills, ethics, and assessment integrity, making the detection of\nLLM-generated code essential for maintaining accountability and standards.\nWhile, there has been some research on this problem, it generally lacks domain\ncoverage and robustness, and only covers a small number of programming\nlanguages. To this end, we propose a framework capable of distinguishing\nbetween human- and LLM-written code across multiple programming languages, code\ngenerators, and domains. We use a large-scale dataset from renowned platforms\nand LLM-based code generators, alongside applying rigorous data quality checks,\nfeature engineering, and comparative analysis using evaluation of traditional\nmachine learning models, pre-trained language models (PLMs), and LLMs for code\ndetection. We perform an evaluation on out-of-domain scenarios, such as\ndetecting the authorship and hybrid authorship of generated code and\ngeneralizing to unseen models, domains, and programming languages. Moreover,\nour extensive experiments show that our framework effectively distinguishes\nhuman- from LLM-written code and sets a new benchmark for this task.\n","authors":["Daniil Orel","Dilshod Azizov","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2503.13733v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12039v2","updated":"2025-07-17T15:27:29Z","published":"2025-07-16T08:56:19Z","title":"A Comparative Approach to Assessing Linguistic Creativity of Large\n  Language Models and Humans","summary":"  The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.\n","authors":["Anca Dinu","Andra-Maria Florescu","Alina Resceanu"],"pdf_url":"https://arxiv.org/pdf/2507.12039v2.pdf","comment":"Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)"},{"id":"http://arxiv.org/abs/2507.13205v1","updated":"2025-07-17T15:15:43Z","published":"2025-07-17T15:15:43Z","title":"Automatically assessing oral narratives of Afrikaans and isiXhosa\n  children","summary":"  Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning.\n","authors":["R. Louw","E. Sharratt","F. de Wet","C. Jacobs","A. Smith","H. Kamper"],"pdf_url":"https://arxiv.org/pdf/2507.13205v1.pdf","comment":"Accepted to SLaTE 2025"},{"id":"http://arxiv.org/abs/2507.13190v1","updated":"2025-07-17T14:59:20Z","published":"2025-07-17T14:59:20Z","title":"GEMMAS: Graph-based Evaluation Metrics for Multi Agent Systems","summary":"  Multi-agent systems built on language models have shown strong performance on\ncollaborative reasoning tasks. However, existing evaluations focus only on the\ncorrectness of the final output, overlooking how inefficient communication and\npoor coordination contribute to redundant reasoning and higher computational\ncosts. We introduce GEMMAS, a graph-based evaluation framework that analyzes\nthe internal collaboration process by modeling agent interactions as a directed\nacyclic graph. To capture collaboration quality, we propose two process-level\nmetrics: Information Diversity Score (IDS) to measure semantic variation in\ninter-agent messages, and Unnecessary Path Ratio (UPR) to quantify redundant\nreasoning paths. We evaluate GEMMAS across five benchmarks and highlight\nresults on GSM8K, where systems with only a 2.1% difference in accuracy differ\nby 12.8% in IDS and 80% in UPR, revealing substantial variation in internal\ncollaboration. These findings demonstrate that outcome-only metrics are\ninsufficient for evaluating multi-agent performance and highlight the\nimportance of process-level diagnostics in designing more interpretable and\nresource-efficient collaborative AI systems.\n","authors":["Jisoo Lee","Raeyoung Chang","Dongwook Kwon","Harmanpreet Singh","Nikhil Verma"],"pdf_url":"https://arxiv.org/pdf/2507.13190v1.pdf","comment":"4 figures, 1 algorithm, 2 tables, 6 pages, under review at EMNLP\n  Industry track 2025"},{"id":"http://arxiv.org/abs/2507.13164v1","updated":"2025-07-17T14:31:32Z","published":"2025-07-17T14:31:32Z","title":"Feature-based analysis of oral narratives from Afrikaans and isiXhosa\n  children","summary":"  Oral narrative skills are strong predictors of later literacy development.\nThis study examines the features of oral narratives from children who were\nidentified by experts as requiring intervention. Using simple machine learning\nmethods, we analyse recorded stories from four- and five-year-old Afrikaans-\nand isiXhosa-speaking children. Consistent with prior research, we identify\nlexical diversity (unique words) and length-based features (mean utterance\nlength) as indicators of typical development, but features like articulation\nrate prove less informative. Despite cross-linguistic variation in\npart-of-speech patterns, the use of specific verbs and auxiliaries associated\nwith goal-directed storytelling is correlated with a reduced likelihood of\nrequiring intervention. Our analysis of two linguistically distinct languages\nreveals both language-specific and shared predictors of narrative proficiency,\nwith implications for early assessment in multilingual contexts.\n","authors":["Emma Sharratt","Annelien Smith","Retief Louw","Daleen Klop","Febe de Wet","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2507.13164v1.pdf","comment":"SLaTE 2025 in Nijmegen, Netherlands"},{"id":"http://arxiv.org/abs/2507.13158v1","updated":"2025-07-17T14:22:24Z","published":"2025-07-17T14:22:24Z","title":"Inverse Reinforcement Learning Meets Large Language Model Post-Training:\n  Basics, Advances, and Opportunities","summary":"  In the era of Large Language Models (LLMs), alignment has emerged as a\nfundamental yet challenging problem in the pursuit of more reliable,\ncontrollable, and capable machine intelligence. The recent success of reasoning\nmodels and conversational AI systems has underscored the critical role of\nreinforcement learning (RL) in enhancing these systems, driving increased\nresearch interest at the intersection of RL and LLM alignment. This paper\nprovides a comprehensive review of recent advances in LLM alignment through the\nlens of inverse reinforcement learning (IRL), emphasizing the distinctions\nbetween RL techniques employed in LLM alignment and those in conventional RL\ntasks. In particular, we highlight the necessity of constructing neural reward\nmodels from human data and discuss the formal and practical implications of\nthis paradigm shift. We begin by introducing fundamental concepts in RL to\nprovide a foundation for readers unfamiliar with the field. We then examine\nrecent advances in this research agenda, discussing key challenges and\nopportunities in conducting IRL for LLM alignment. Beyond methodological\nconsiderations, we explore practical aspects, including datasets, benchmarks,\nevaluation metrics, infrastructure, and computationally efficient training and\ninference techniques. Finally, we draw insights from the literature on\nsparse-reward RL to identify open questions and potential research directions.\nBy synthesizing findings from diverse studies, we aim to provide a structured\nand critical overview of the field, highlight unresolved challenges, and\noutline promising future directions for improving LLM alignment through RL and\nIRL techniques.\n","authors":["Hao Sun","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2507.13158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13142v1","updated":"2025-07-17T14:06:19Z","published":"2025-07-17T14:06:19Z","title":"From Roots to Rewards: Dynamic Tree Reasoning with RL","summary":"  Modern language models address complex questions through chain-of-thought\n(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,\n2021), yet struggle with error propagation and knowledge integration.\nTree-structured reasoning methods, particularly the Probabilistic\nTree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues\nby decomposing questions into hierarchical structures and selecting answers\nthrough confidence-weighted aggregation of parametric and retrieved knowledge\n(Yao et al., 2023). However, ProbTree's static implementation introduces two\nkey limitations: (1) the reasoning tree is fixed during the initial\nconstruction phase, preventing dynamic adaptation to intermediate results, and\n(2) each node requires exhaustive evaluation of all possible solution\nstrategies, creating computational inefficiency. We present a dynamic\nreinforcement learning (Sutton and Barto, 2018) framework that transforms\ntree-based reasoning into an adaptive process. Our approach incrementally\nconstructs the reasoning tree based on real-time confidence estimates, while\nlearning optimal policies for action selection (decomposition, retrieval, or\naggregation). This maintains ProbTree's probabilistic rigor while improving\nboth solution quality and computational efficiency through selective expansion\nand focused resource allocation. The work establishes a new paradigm for\ntreestructured reasoning that balances the reliability of probabilistic\nframeworks with the flexibility required for real-world question answering\nsystems.\n","authors":["Ahmed Bahloul","Simon Malberg"],"pdf_url":"https://arxiv.org/pdf/2507.13142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11059v2","updated":"2025-07-17T14:04:07Z","published":"2025-07-15T07:52:33Z","title":"SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language\n  Models on Software Engineering Tasks","summary":"  The rapid advancement of Large Language Models (LLMs) in software engineering\nhas revealed critical limitations in existing benchmarks, particularly the\nwidely used SWE-bench dataset. Recent studies have uncovered severe data\ncontamination issues, e.g. SWE-bench reports 32.67% of successful patches\ninvolve direct solution leakage and 31.08% pass due to inadequate test cases.\nWe introduce SWE-MERA, a dynamic, continuously updated benchmark designed to\naddress these fundamental challenges through an automated collection of\nreal-world GitHub issues and rigorous quality validation. Our approach\nimplements a reliable pipeline that ensures quality while minimizing\ncontamination risks, resulting in approximately 10,000 potential tasks with 300\nsamples currently available. Evaluation using the Aider coding agent\ndemonstrates strong discriminative power in state-of-the-art models. We report\nperformance across a dozen recent LLMs evaluated on tasks collected between\nSeptember 2024 and June 2025.\n","authors":["Pavel Adamenko","Mikhail Ivanov","Aidar Valeev","Rodion Levichev","Pavel Zadorozhny","Ivan Lopatin","Dmitry Babayev","Alena Fenogenova","Valentin Malykh"],"pdf_url":"https://arxiv.org/pdf/2507.11059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13138v1","updated":"2025-07-17T14:00:13Z","published":"2025-07-17T14:00:13Z","title":"Assessing the Reliability of LLMs Annotations in the Context of\n  Demographic Bias and Model Explanation","summary":"  Understanding the sources of variability in annotations is crucial for\ndeveloping fair NLP systems, especially for tasks like sexism detection where\ndemographic bias is a concern. This study investigates the extent to which\nannotator demographic features influence labeling decisions compared to text\ncontent. Using a Generalized Linear Mixed Model, we quantify this inf luence,\nfinding that while statistically present, demographic factors account for a\nminor fraction ( 8%) of the observed variance, with tweet content being the\ndominant factor. We then assess the reliability of Generative AI (GenAI) models\nas annotators, specifically evaluating if guiding them with demographic\npersonas improves alignment with human judgments. Our results indicate that\nsimplistic persona prompting often fails to enhance, and sometimes degrades,\nperformance compared to baseline models. Furthermore, explainable AI (XAI)\ntechniques reveal that model predictions rely heavily on content-specific\ntokens related to sexism, rather than correlates of demographic\ncharacteristics. We argue that focusing on content-driven explanations and\nrobust annotation protocols offers a more reliable path towards fairness than\npotentially persona simulation.\n","authors":["Hadi Mohammadi","Tina Shahedi","Pablo Mosteiro","Massimo Poesio","Ayoub Bagheri","Anastasia Giachanou"],"pdf_url":"https://arxiv.org/pdf/2507.13138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13115v1","updated":"2025-07-17T13:31:04Z","published":"2025-07-17T13:31:04Z","title":"A Computational Framework to Identify Self-Aspects in Text","summary":"  This Ph.D. proposal introduces a plan to develop a computational framework to\nidentify Self-aspects in text. The Self is a multifaceted construct and it is\nreflected in language. While it is described across disciplines like cognitive\nscience and phenomenology, it remains underexplored in natural language\nprocessing (NLP). Many of the aspects of the Self align with psychological and\nother well-researched phenomena (e.g., those related to mental health),\nhighlighting the need for systematic NLP-based analysis. In line with this, we\nplan to introduce an ontology of Self-aspects and a gold-standard annotated\ndataset. Using this foundation, we will develop and evaluate conventional\ndiscriminative models, generative large language models, and embedding-based\nretrieval approaches against four main criteria: interpretability, ground-truth\nadherence, accuracy, and computational efficiency. Top-performing models will\nbe applied in case studies in mental health and empirical phenomenology.\n","authors":["Jaya Caporusso","Matthew Purver","Senja Pollak"],"pdf_url":"https://arxiv.org/pdf/2507.13115v1.pdf","comment":"Accepted to ACL SRW 2025"},{"id":"http://arxiv.org/abs/2504.07389v2","updated":"2025-07-17T13:24:18Z","published":"2025-04-10T02:19:03Z","title":"Task-Circuit Quantization: Leveraging Knowledge Localization and\n  Interpretability for Compression","summary":"  Post-training quantization (PTQ) reduces a model's memory footprint by\nmapping full precision weights into low bit weights without costly retraining,\nbut can degrade its downstream performance especially in low 2- to 3-bit\nsettings. We develop a new mixed-precision PTQ approach, Task-Circuit\nQuantization (TaCQ), that draws parallels to automated circuit discovery,\ndirectly conditioning the quantization process on specific weight circuits --\nwhich we define as sets of weights associated with downstream task performance.\nThese weights are kept as 16-bit weights, while others are quantized,\nmaintaining performance while only adding a marginal memory cost. Specifically,\nTaCQ contrasts unquantized model weights with a uniformly-quantized model to\nestimate the expected change in weights due to quantization and uses gradient\ninformation to predict the resulting impact on task performance, allowing us to\npreserve task-specific weights. We compare TaCQ-based quantization to existing\nmixed-precision quantization methods when conditioning both on general-purpose\nand task-specific data. Across QA, math reasoning, and text-to-SQL tasks for\nboth Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the\nsame calibration data and a lower weight budget, achieving major improvements\nin the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of\nLlama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25%\nabsolute improvement over SPQR. We also observe consistently large gains over\nexisting methods in the 2-bit regime, with an average gain of 14.74% over the\nstrongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without\nconditioning on specific tasks, showing TaCQ's ability to identify important\nweights is not limited to task-conditioned settings.\n","authors":["Hanqi Xiao","Yi-Lin Sung","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2504.07389v2.pdf","comment":"COLM 2025 Camera Ready. Code:\n  https://github.com/The-Inscrutable-X/TACQ"},{"id":"http://arxiv.org/abs/2507.13105v1","updated":"2025-07-17T13:19:50Z","published":"2025-07-17T13:19:50Z","title":"SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated\n  Summaries For Scientific Abstracts","summary":"  We introduce SemCSE, an unsupervised method for learning semantic embeddings\nof scientific texts. Building on recent advances in contrastive learning for\ntext embeddings, our approach leverages LLM-generated summaries of scientific\nabstracts to train a model that positions semantically related summaries closer\ntogether in the embedding space. This resulting objective ensures that the\nmodel captures the true semantic content of a text, in contrast to traditional\ncitation-based approaches that do not necessarily reflect semantic similarity.\nTo validate this, we propose a novel benchmark designed to assess a model's\nability to understand and encode the semantic content of scientific texts,\ndemonstrating that our method enforces a stronger semantic separation within\nthe embedding space. Additionally, we evaluate SemCSE on the comprehensive\nSciRepEval benchmark for scientific text embeddings, where it achieves\nstate-of-the-art performance among models of its size, thus highlighting the\nbenefits of a semantically focused training approach.\n","authors":["Marc Brinner","Sina Zarriess"],"pdf_url":"https://arxiv.org/pdf/2507.13105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23114v4","updated":"2025-07-17T13:18:06Z","published":"2024-10-30T15:25:06Z","title":"Unified Triplet-Level Hallucination Evaluation for Large Vision-Language\n  Models","summary":"  Despite the outstanding performance in vision-language reasoning, Large\nVision-Language Models (LVLMs) might generate hallucinated contents that do not\nexist in the given image. Most existing LVLM hallucination benchmarks are\nconstrained to evaluate the object-related hallucinations. However, the\npotential hallucination on the relations between two objects, i.e., relation\nhallucination, still lacks investigation. To remedy that, we design a unified\nframework to measure the object and relation hallucination in LVLMs\nsimultaneously. The core idea of our framework is to evaluate hallucinations\nvia (object, relation, object) triplets extracted from LVLMs' responses, making\nit easily generalizable to different vision-language tasks. Based on our\nframework, we further introduce Tri-HE, a novel Triplet-level Hallucination\nEvaluation benchmark which can be used to study both object and relation\nhallucination at the same time. With comprehensive evaluations on Tri-HE, we\nobserve that the relation hallucination issue is even more serious than object\nhallucination among existing LVLMs, highlighting a previously neglected problem\ntowards reliable LVLMs. Moreover, based on our findings, we design a simple\ntraining-free approach that effectively mitigates hallucinations for LVLMs. Our\ndataset and code for the reproduction of our experiments are available publicly\nat https://github.com/wujunjie1998/Tri-HE.\n","authors":["Junjie Wu","Tsz Ting Chung","Kai Chen","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2410.23114v4.pdf","comment":"Accepted by TMLR 2025. Project Page:\n  https://kaichen1998.github.io/projects/tri-he/"},{"id":"http://arxiv.org/abs/2507.04348v2","updated":"2025-07-17T13:07:41Z","published":"2025-07-06T11:21:47Z","title":"SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level\n  Length Control","summary":"  Large reasoning models (LRMs) have exhibited remarkable reasoning\ncapabilities through inference-time scaling, but this progress has also\nintroduced considerable redundancy and inefficiency into their reasoning\nprocesses, resulting in substantial computational waste. Previous work has\nattempted to mitigate this issue by penalizing the overall length of generated\nsamples during reinforcement learning (RL), with the goal of encouraging a more\nconcise chains of thought. However, we observe that such global length penalty\noften lead to excessive compression of critical reasoning steps while\npreserving unnecessary details in simpler ones, yielding a suboptimal trade-off\nbetween accuracy and efficiency. To address this issue, we propose\nSmartThinker, a two-stage learnable framework designed to enable fine-grained\ncontrol over the length of reasoning chains based on the importance of each\nindividual step. In the first stage, SmartThinker adapts a reasoning model to a\nshort-form reasoning mode through rejection sampling combined with supervised\nfine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length\nControl Policy Optimization (SCPO) to refine the model output distribution,\nwhich increases the proportion of length allocated to critical steps while\nreducing redundancy in less important ones. SCPO consists of four core\ncomponents: an online importance estimator, a step-level length control reward\nfunction, a step-level generalized advantage estimation (S-GAE) and a\ndifficulty-adaptive clipping strategy. Working in concert, these components\nenable SCPO to implement differentiated length control across reasoning steps.\nEmpirical results across multiple reasoning benchmarks and various backbone\nmodels demonstrate that SmartThinker significantly reduces redundant reasoning\nwhile achieving comparable or even superior performance to existing methods.\n","authors":["Xingyang He","Xiao Ling","Jie Liu"],"pdf_url":"https://arxiv.org/pdf/2507.04348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12284v2","updated":"2025-07-17T12:55:32Z","published":"2025-07-16T14:31:33Z","title":"MERA Code: A Unified Framework for Evaluating Code Generation Across\n  Tasks","summary":"  Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.\n","authors":["Artem Chervyakov","Alexander Kharitonov","Pavel Zadorozhny","Adamenko Pavel","Rodion Levichev","Dmitrii Vorobev","Dmitrii Salikhov","Aidar Valeev","Alena Pestova","Maria Dziuba","Ilseyar Alimova","Artem Zavgorodnev","Aleksandr Medvedev","Stanislav Moiseev","Elena Bruches","Daniil Grebenkin","Roman Derunets","Vikulov Vladimir","Anton Emelyanov","Dmitrii Babaev","Vladimir V. Ivanov","Valentin Malykh","Alena Fenogenova"],"pdf_url":"https://arxiv.org/pdf/2507.12284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13076v1","updated":"2025-07-17T12:45:37Z","published":"2025-07-17T12:45:37Z","title":"Formalizing Attack Scenario Description: A Proposed Model","summary":"  Organizations face an ever-changing threat landscape. They must continuously\ndedicate significant efforts to protect their assets, making their adoption of\nincreased cybersecurity automation inevitable. However, process automation\nrequires formalization of input data. Through this paper, we address this need\nfor processes that use attack scenarios as input. Among these processes, one\ncan mention both the generation of scripts for attack simulation and training\npurposes, as well as the analysis of attacks. Therefore, the paper's main\nresearch contribution is a novel formal model that encompasses the attack's\ncontext description and its scenario. It is abstracted using UML class model.\nOnce the description of our model done, we will show how it could serve an\nupstream attack analysis process. We will show also its use for an automatic\ngeneration of attack scripts in the context of cybersecurity training. These\ntwo uses cases constitute the second contribution of this present research\nwork.\n","authors":["Quentin Goux","Nadira Lammari"],"pdf_url":"https://arxiv.org/pdf/2507.13076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13019v1","updated":"2025-07-17T11:46:00Z","published":"2025-07-17T11:46:00Z","title":"Rethinking the Embodied Gap in Vision-and-Language Navigation: A\n  Holistic Study of Physical and Visual Disparities","summary":"  Recent Vision-and-Language Navigation (VLN) advancements are promising, but\ntheir idealized assumptions about robot movement and control fail to reflect\nphysically embodied deployment challenges. To bridge this gap, we introduce\nVLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and\nwheeled robots. For the first time, we systematically evaluate several\nego-centric VLN methods in physical robotic settings across different technical\npipelines, including classification models for single-step discrete action\nprediction, a diffusion model for dense waypoint prediction, and a train-free,\nmap-based large language model (LLM) integrated with path planning. Our results\nreveal significant performance degradation due to limited robot observation\nspace, environmental lighting variations, and physical challenges like\ncollisions and falls. This also exposes locomotion constraints for legged\nrobots in complex environments. VLN-PE is highly extensible, allowing seamless\nintegration of new scenes beyond MP3D, thereby enabling more comprehensive VLN\nevaluation. Despite the weak generalization of current models in physical\ndeployment, VLN-PE provides a new pathway for improving cross-embodiment's\noverall adaptability. We hope our findings and tools inspire the community to\nrethink VLN limitations and advance robust, practical VLN models. The code is\navailable at https://crystalsixone.github.io/vln_pe.github.io/.\n","authors":["Liuyi Wang","Xinyuan Xia","Hui Zhao","Hanqing Wang","Tai Wang","Yilun Chen","Chengju Liu","Qijun Chen","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2507.13019v1.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.12990v1","updated":"2025-07-17T10:57:49Z","published":"2025-07-17T10:57:49Z","title":"Teach Old SAEs New Domain Tricks with Boosting","summary":"  Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs.\n","authors":["Nikita Koriagin","Yaroslav Aksenov","Daniil Laptev","Gleb Gerasimov","Nikita Balagansky","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2507.12990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03580v2","updated":"2025-07-17T10:42:09Z","published":"2025-07-04T13:49:14Z","title":"Learning to Translate Ambiguous Terminology by Preference Optimization\n  on Post-Edits","summary":"  In real world translation scenarios, terminology is rarely one-to-one.\nInstead, multiple valid translations may appear in a terminology dictionary,\nbut correctness of a translation depends on corporate style guides and context.\nThis can be challenging for neural machine translation (NMT) systems. Luckily,\nin a corporate context, many examples of human post-edits of valid but\nincorrect terminology exist. The goal of this work is to learn how to\ndisambiguate our terminology based on these corrections. Our approach is based\non preference optimization, using the term post-edit as the knowledge to be\npreferred. While previous work had to rely on unambiguous translation\ndictionaries to set hard constraints during decoding, or to add soft\nconstraints in the input, our framework requires neither one-to-one\ndictionaries nor human intervention at decoding time. We report results on\nEnglish-German post-edited data and find that the optimal combination of\nsupervised fine-tuning and preference optimization, with both term-specific and\nfull sequence objectives, yields statistically significant improvements in term\naccuracy over a strong NMT baseline without significant losses in COMET score.\nAdditionally, we release test sets from our post-edited data and terminology\ndictionary.\n","authors":["Nathaniel Berger","Johannes Eschbach-Dymanus","Miriam Exel","Matthias Huck","Stefan Riezler"],"pdf_url":"https://arxiv.org/pdf/2507.03580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12981v1","updated":"2025-07-17T10:33:36Z","published":"2025-07-17T10:33:36Z","title":"MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with\n  Multiple Steps","summary":"  This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas\ny Respuestas sobre Tablas en Espa\\~nol (Questions and Answers about Tables in\nSpanish). Our solution obtains answers to the questions by implementing Python\ncode generation with LLMs that is used to filter and process the table. This\nsolution evolves from the MRT implementation for the Semeval 2025 related task.\nThe process consists of multiple steps: analyzing and understanding the content\nof the table, selecting the useful columns, generating instructions in natural\nlanguage, translating these instructions to code, running it, and handling\npotential errors or exceptions. These steps use open-source LLMs and\nfine-grained optimized prompts for each step. With this approach, we achieved\nan accuracy score of 85\\% in the task.\n","authors":["Maximiliano Hormazábal Lagos","Álvaro Bueno Sáez","Héctor Cerezo-Costas","Pedro Alonso Doval","Jorge Alcalde Vesteiro"],"pdf_url":"https://arxiv.org/pdf/2507.12981v1.pdf","comment":"Accepted as an official challenge paper in the PRESTA: Questions and\n  Answers over Tabular Data shared task at IberLEF 2025, colocated with the\n  41st SEPLN Conference in Zaragoza, Spain"},{"id":"http://arxiv.org/abs/2507.12951v1","updated":"2025-07-17T09:45:49Z","published":"2025-07-17T09:45:49Z","title":"UniSLU: Unified Spoken Language Understanding from Heterogeneous\n  Cross-Task Datasets","summary":"  Spoken Language Understanding (SLU) plays a crucial role in speech-centric\nmultimedia applications, enabling machines to comprehend spoken language in\nscenarios such as meetings, interviews, and customer service interactions. SLU\nencompasses multiple tasks, including Automatic Speech Recognition (ASR),\nspoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).\nHowever, existing methods often rely on separate model architectures for\nindividual tasks such as spoken NER and SA, which increases system complexity,\nlimits cross-task interaction, and fails to fully exploit heterogeneous\ndatasets available across tasks. To address these limitations, we propose\nUniSLU, a unified framework that jointly models multiple SLU tasks within a\nsingle architecture. Specifically, we propose a unified representation for\ndiverse SLU tasks, enabling full utilization of heterogeneous datasets across\nmultiple tasks. Built upon this representation, we propose a unified generative\nmethod that jointly models ASR, spoken NER, and SA tasks, enhancing task\ninteractions and enabling seamless integration with large language models to\nharness their powerful generative capabilities. Extensive experiments on public\nSLU datasets demonstrate the effectiveness of our approach, achieving superior\nSLU performance compared to several benchmark methods, making it well-suited\nfor real-world speech-based multimedia scenarios. We will release all code and\nmodels at github to facilitate future research.\n","authors":["Zhichao Sheng","Shilin Zhou","Chen Gong","Zhenghua Li"],"pdf_url":"https://arxiv.org/pdf/2507.12951v1.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.12948v1","updated":"2025-07-17T09:40:56Z","published":"2025-07-17T09:40:56Z","title":"Probabilistic Soundness Guarantees in LLM Reasoning Chains","summary":"  In reasoning chains generated by large language models (LLMs), initial errors\noften propagate and undermine the reliability of the final conclusion. Current\nLLM-based error detection methods often fail to detect propagated errors\nbecause they do not properly account for how earlier errors might corrupt\njudgments of downstream reasoning. To better detect such propagated errors, we\nintroduce Autoregressive Reasoning Entailment Stability (ARES), a novel\nprobabilistic framework that prevents error propagation by judging each claim\nbased only on previously-assessed sound premises. This inductive method yields\na nuanced score for each step and provides certified statistical guarantees of\nits soundness, rather than a brittle binary label. ARES achieves\nstate-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2\npoints) and demonstrates superior robustness on very long synthetic reasoning\nchains, where it excels at detecting propagated errors (90.3% F1, +27.6\npoints).\n","authors":["Weiqiu You","Anton Xue","Shreya Havaldar","Delip Rao","Helen Jin","Chris Callison-Burch","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2507.12948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08161v4","updated":"2025-07-17T09:34:49Z","published":"2025-03-11T08:26:37Z","title":"OASIS: Order-Augmented Strategy for Improved Code Search","summary":"  Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training.\n","authors":["Zuchen Gao","Zizheng Zhan","Xianming Li","Erxin Yu","Ziqi Zhan","Haotian Zhang","Bin Chen","Yuqun Zhang","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2503.08161v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12930v1","updated":"2025-07-17T09:09:53Z","published":"2025-07-17T09:09:53Z","title":"Making Language Model a Hierarchical Classifier and Generator","summary":"  Decoder-only language models, such as GPT and LLaMA, generally decode on the\nlast layer. Motivated by human's hierarchical thinking capability, we propose\nthat a hierarchical decoder architecture could be built with different layers\ndecoding texts simultaneously. Due to limited time and computationally\nresources, we choose to adapt a pretrained language model into this form of\nhierarchical decoder. Language heads of the last layer are copied to different\nselected intermediate layers, and fine-tuned with different task inputs. By\nthorough experiments, we validate that these selective intermediate layers\ncould be adapted to speak meaningful and reasonable contents, and this paradigm\nof hierarchical decoder can obtain state-of-the-art performances on multiple\ntasks such as hierarchical text classification, classification-guided\ngeneration, and hierarchical text generation. This study suggests the\npossibility of a generalized hierarchical reasoner, pretraining from scratch.\n","authors":["Yihong Wang","Zhonglin Jiang","Ningyuan Xi","Yue Zhao","Qingqing Gu","Xiyuan Chen","Hao Wu","Sheng Xu","Hange Zhou","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2507.12930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15841v2","updated":"2025-07-17T08:53:48Z","published":"2025-06-18T19:44:46Z","title":"MEM1: Learning to Synergize Memory and Reasoning for Efficient\n  Long-Horizon Agents","summary":"  Modern language agents must operate over long-horizon, multi-turn\ninteractions, where they retrieve external information, adapt to observations,\nand answer interdependent queries. Yet, most LLM systems rely on full-context\nprompting, appending all past turns regardless of their relevance. This leads\nto unbounded memory growth, increased computational costs, and degraded\nreasoning performance on out-of-distribution input lengths. We introduce MEM1,\nan end-to-end reinforcement learning framework that enables agents to operate\nwith constant memory across long multi-turn tasks. At each turn, MEM1 updates a\ncompact shared internal state that jointly supports memory consolidation and\nreasoning. This state integrates prior memory with new observations from the\nenvironment while strategically discarding irrelevant or redundant information.\nTo support training in more realistic and compositional settings, we propose a\nsimple yet effective and scalable approach to constructing multi-turn\nenvironments by composing existing datasets into arbitrarily complex task\nsequences. Experiments across three domains, including internal retrieval QA,\nopen-domain web QA, and multi-turn web shopping, show that MEM1-7B improves\nperformance by 3.5x while reducing memory usage by 3.7x compared to\nQwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes\nbeyond the training horizon. Our results demonstrate the promise of\nreasoning-driven memory consolidation as a scalable alternative to existing\nsolutions for training long-horizon interactive agents, where both efficiency\nand performance are optimized.\n","authors":["Zijian Zhou","Ao Qu","Zhaoxuan Wu","Sunghwan Kim","Alok Prakash","Daniela Rus","Jinhua Zhao","Bryan Kian Hsiang Low","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2506.15841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.13886v3","updated":"2025-07-17T08:46:29Z","published":"2025-05-20T03:47:44Z","title":"Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General\n  Reasoning","summary":"  Visual-language Chain-of-Thought (CoT) data resources are relatively scarce\ncompared to text-only counterparts, limiting the improvement of reasoning\ncapabilities in Vision Language Models (VLMs). However, high-quality\nvision-language reasoning data is expensive and labor-intensive to annotate. To\naddress this issue, we leverage a promising resource: game code, which\nnaturally contains logical structures and state transition processes.\nTherefore, we propose Code2Logic, a novel game-code-driven approach for\nmultimodal reasoning data synthesis. Our approach leverages Large Language\nModels (LLMs) to adapt game code, enabling automatic acquisition of reasoning\nprocesses and results through code execution. Using the Code2Logic approach, we\ndeveloped the GameQA dataset to train and evaluate VLMs. GameQA is\ncost-effective and scalable, offers controllable difficulty gradation and is\ndiverse with 30 games and 158 tasks. Surprisingly, despite training solely on\ngame data, VLMs demonstrated out of domain generalization, specifically\nQwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language\nbenchmarks. Our code, dataset and models are available at\nhttps://github.com/tongjingqi/Code2Logic.\n","authors":["Jingqi Tong","Jixin Tang","Hangcheng Li","Yurong Mou","Ming Zhang","Jun Zhao","Yanbo Wen","Fan Song","Jiahao Zhan","Yuyang Lu","Chaoran Tao","Zhiyuan Guo","Jizhou Yu","Tianhao Cheng","Changhao Jiang","Zhen Wang","Tao Liang","Zhihui Fei","Mingyang Wan","Guojun Ma","Weifeng Ge","Guanhua Chen","Tao Gui","Xipeng Qiu","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2505.13886v3.pdf","comment":"63 pages, 23 figures, submitted to NeurIPS 2025"},{"id":"http://arxiv.org/abs/2411.06208v3","updated":"2025-07-17T08:39:11Z","published":"2024-11-09T15:12:43Z","title":"IOPO: Empowering LLMs with Complex Instruction Following via\n  Input-Output Preference Optimization","summary":"  In the realm of large language models (LLMs), the ability of models to\naccurately follow instructions is paramount as more agents and applications\nleverage LLMs for construction, where the complexity of instructions are\nrapidly increasing. However, on the one hand, there is only a certain amount of\ncomplex instruction evaluation data; on the other hand, there are no dedicated\nalgorithms to improve the ability to follow complex instructions. To this end,\nthis paper introduces TRACE, a benchmark for improving and evaluating the\ncomplex instructionfollowing ability, which consists of 120K training data and\n1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference\nOptimization) alignment method which takes both input and output preference\npairs into consideration, where LLMs not only rapidly align with response\npreferences but also meticulously explore the instruction preferences.\nExtensive experiments on both in-domain and outof-domain datasets confirm the\neffectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and\n6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.\n","authors":["Xinghua Zhang","Haiyang Yu","Cheng Fu","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2411.06208v3.pdf","comment":"ACL 2025"},{"id":"http://arxiv.org/abs/2404.04631v2","updated":"2025-07-17T08:20:16Z","published":"2024-04-06T13:38:15Z","title":"On the Limitations of Large Language Models (LLMs): False Attribution","summary":"  In this work, we introduce a new hallucination metric - Simple Hallucination\nIndex (SHI) and provide insight into one important limitation of the parametric\nknowledge of large language models (LLMs), i.e. false attribution. The task of\nautomatic author attribution for relatively small chunks of text is an\nimportant NLP task but can be challenging. We empirically evaluate the power of\n3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and\nLLaMA-2-13B). We acquired the top 10 most popular books of a month, according\nto Project Gutenberg, divided each one into equal chunks of 400 words, and\nprompted each LLM to predict the author. We then randomly sampled 162 chunks\nper book for human evaluation, based on the error margin of 7% and a confidence\nlevel of 95%. The average results show that Mixtral 8x7B has the highest\nprediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724,\n0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B.\nHowever, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as\nhigh as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong\nnegative correlation of accuracy and SHI, given by r, demonstrates the fidelity\nof the new hallucination metric, which may generalize to other tasks. We also\nshow that prediction accuracies correlate positively with the frequencies of\nWikipedia instances of the book titles instead of the downloads and we perform\nerror analyses of predictions. We publicly release the annotated chunks of data\nand our codes to aid the reproducibility and evaluation of other models.\n","authors":["Tosin Adewumi","Nudrat Habib","Lama Alkhaled","Elisa Barney"],"pdf_url":"https://arxiv.org/pdf/2404.04631v2.pdf","comment":"This paper was accepted for presentation by Recent Advances in NLP\n  (RANLP) 2025 conference"},{"id":"http://arxiv.org/abs/2507.06261v3","updated":"2025-07-17T08:16:43Z","published":"2025-07-07T17:36:04Z","title":"Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality,\n  Long Context, and Next Generation Agentic Capabilities","summary":"  In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and\nGemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite\nmodels. Gemini 2.5 Pro is our most capable model yet, achieving SoTA\nperformance on frontier coding and reasoning benchmarks. In addition to its\nincredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that\nexcels at multimodal understanding and it is now able to process up to 3 hours\nof video content. Its unique combination of long context, multimodal and\nreasoning capabilities can be combined to unlock new agentic workflows. Gemini\n2.5 Flash provides excellent reasoning abilities at a fraction of the compute\nand latency requirements and Gemini 2.0 Flash and Flash-Lite provide high\nperformance at low latency and cost. Taken together, the Gemini 2.X model\ngeneration spans the full Pareto frontier of model capability vs cost, allowing\nusers to explore the boundaries of what is possible with complex agentic\nproblem solving.\n","authors":["Gheorghe Comanici","Eric Bieber","Mike Schaekermann","Ice Pasupat","Noveen Sachdeva","Inderjit Dhillon","Marcel Blistein","Ori Ram","Dan Zhang","Evan Rosen","Luke Marris","Sam Petulla","Colin Gaffney","Asaf Aharoni","Nathan Lintz","Tiago Cardal Pais","Henrik Jacobsson","Idan Szpektor","Nan-Jiang Jiang","Krishna Haridasan","Ahmed Omran","Nikunj Saunshi","Dara Bahri","Gaurav Mishra","Eric Chu","Toby Boyd","Brad Hekman","Aaron Parisi","Chaoyi Zhang","Kornraphop Kawintiranon","Tania Bedrax-Weiss","Oliver Wang","Ya Xu","Ollie Purkiss","Uri Mendlovic","Ilaï Deutel","Nam Nguyen","Adam Langley","Flip Korn","Lucia Rossazza","Alexandre Ramé","Sagar Waghmare","Helen Miller","Vaishakh Keshava","Ying Jian","Xiaofan Zhang","Raluca Ada Popa","Kedar Dhamdhere","Blaž Bratanič","Kyuyeun Kim","Terry Koo","Ferran Alet","Yi-ting Chen","Arsha Nagrani","Hannah Muckenhirn","Zhiyuan Zhang","Corbin Quick","Filip Pavetić","Duc Dung Nguyen","Joao Carreira","Michael Elabd","Haroon Qureshi","Fabian Mentzer","Yao-Yuan Yang","Danielle Eisenbud","Anmol Gulati","Ellie Talius","Eric Ni","Sahra Ghalebikesabi","Edouard Yvinec","Alaa Saade","Thatcher Ulrich","Lorenzo Blanco","Dan A. Calian","Muhuan Huang","Aäron van den Oord","Naman Goyal","Terry Chen","Praynaa Rawlani","Christian Schallhart","Swachhand Lokhande","Xianghong Luo","Jyn Shan","Ceslee Montgomery","Victoria Krakovna","Federico Piccinini","Omer Barak","Jingyu Cui","Yiling Jia","Mikhail Dektiarev","Alexey Kolganov","Shiyu Huang","Zhe Chen","Xingyu Wang","Jessica Austin","Peter de Boursac","Evgeny Sluzhaev","Frank Ding","Huijian Li","Surya Bhupatiraju","Mohit Agarwal","Sławek Kwasiborski","Paramjit Sandhu","Patrick Siegler","Ahmet Iscen","Eyal Ben-David","Shiraz Butt","Miltos Allamanis","Seth Benjamin","Robert Busa-Fekete","Felix Hernandez-Campos","Sasha Goldshtein","Matt Dibb","Weiyang Zhang","Annie Marsden","Carey Radebaugh","Stephen Roller","Abhishek Nayyar","Jacob Austin","Tayfun Terzi","Bhargav Kanagal Shamanna","Pete Shaw","Aayush Singh","Florian Luisier","Artur Mendonça","Vaibhav Aggarwal","Larisa Markeeva","Claudio Fantacci","Sergey Brin","HyunJeong Choe","Guanyu Wang","Hartwig Adam","Avigail Dabush","Tatsuya Kiyono","Eyal Marcus","Jeremy Cole","Theophane Weber","Hongrae Lee","Ronny Huang","Alex Muzio","Leandro Kieliger","Maigo Le","Courtney Biles","Long Le","Archit Sharma","Chengrun Yang","Avery Lamp","Dave Dopson","Nate Hurley"," Katrina"," Xu","Zhihao Shan","Shuang Song","Jiewen Tan","Alexandre Senges","George Zhang","Chong You","Yennie Jun","David Raposo","Susanna Ricco","Xuan Yang","Weijie Chen","Prakhar Gupta","Arthur Szlam","Kevin Villela","Chun-Sung Ferng","Daniel Kasenberg","Chen Liang","Rui Zhu","Arunachalam Narayanaswamy","Florence Perot","Paul Pucciarelli","Anna Shekhawat","Alexey Stern","Rishikesh Ingale","Stefani Karp","Sanaz Bahargam","Adrian Goedeckemeyer","Jie Han","Sicheng Li","Andrea Tacchetti","Dian Yu","Abhishek Chakladar","Zhiying Zhang","Mona El Mahdy","Xu Gao","Dale Johnson","Samrat Phatale","AJ Piergiovanni","Hyeontaek Lim","Clement Farabet","Carl Lebsack","Theo Guidroz","John Blitzer","Nico Duduta","David Madras","Steve Li","Daniel von Dincklage","Xin Li","Mahdis Mahdieh","George Tucker","Ganesh Jawahar","Owen Xiao","Danny Tarlow","Robert Geirhos","Noam Velan","Daniel Vlasic","Kalesha Bullard","SK Park","Nishesh Gupta","Kellie Webster","Ayal Hitron","Jieming Mao","Julian Eisenschlos","Laurel Prince","Nina D'Souza","Kelvin Zheng","Sara Nasso","Gabriela Botea","Carl Doersch","Caglar Unlu","Chris Alberti","Alexey Svyatkovskiy","Ankita Goel","Krzysztof Choromanski","Pan-Pan Jiang","Richard Nguyen","Four Flynn","Daria Ćurko","Peter Chen","Nicholas Roth","Kieran Milan","Caleb Habtegebriel","Shashi Narayan","Michael Moffitt","Jake Marcus","Thomas Anthony","Brendan McMahan","Gowoon Cheon","Ruibo Liu","Megan Barnes","Lukasz Lew","Rebeca Santamaria-Fernandez","Mayank Upadhyay","Arjun Akula","Arnar Mar Hrafnkelsson","Alvaro Caceres","Andrew Bunner","Michal Sokolik","Subha Puttagunta","Lawrence Moore","Berivan Isik","Jay Hartford","Lawrence Chan","Pradeep Shenoy","Dan Holtmann-Rice","Jane Park","Fabio Viola","Alex Salcianu","Sujeevan Rajayogam","Ian Stewart-Binks","Zelin Wu","Richard Everett","Xi Xiong","Pierre-Antoine Manzagol","Gary Leung","Carl Saroufim","Bo Pang","Dawid Wegner","George Papamakarios","Jennimaria Palomaki","Helena Pankov","Guangda Lai","Guilherme Tubone","Shubin Zhao","Theofilos Strinopoulos","Seth Neel","Mingqiu Wang","Joe Kelley","Li Li","Pingmei Xu","Anitha Vijayakumar","Andrea D'olimpio","Omer Levy","Massimo Nicosia","Grigory Rozhdestvenskiy","Ni Lao","Sirui Xie","Yash Katariya","Jon Simon","Sanjiv Kumar","Florian Hartmann","Michael Kilgore","Jinhyuk Lee","Aroma Mahendru","Roman Ring","Tom Hennigan","Fiona Lang","Colin Cherry","David Steiner","Dawsen Hwang","Ray Smith","Pidong Wang","Jeremy Chen","Ming-Hsuan Yang","Sam Kwei","Philippe Schlattner","Donnie Kim","Ganesh Poomal Girirajan","Nikola Momchev","Ayushi Agarwal","Xingyi Zhou","Ilkin Safarli","Zachary Garrett","AJ Pierigiovanni","Sarthak Jauhari","Alif Raditya Rochman","Shikhar Vashishth","Quan Yuan","Christof Angermueller","Jon Blanton","Xinying Song","Nitesh Bharadwaj Gundavarapu","Thi Avrahami","Maxine Deines","Subhrajit Roy","Manish Gupta","Christopher Semturs","Shobha Vasudevan","Aditya Srikanth Veerubhotla","Shriya Sharma","Josh Jacob","Zhen Yang","Andreas Terzis","Dan Karliner","Auriel Wright","Tania Rojas-Esponda","Ashley Brown","Abhijit Guha Roy","Pawan Dogra","Andrei Kapishnikov","Peter Young","Wendy Kan","Vinodh Kumar Rajendran","Maria Ivanova","Salil Deshmukh","Chia-Hua Ho","Mike Kwong","Stav Ginzburg","Annie Louis","KP Sawhney","Slav Petrov","Jing Xie","Yunfei Bai","Georgi Stoyanov","Alex Fabrikant","Rajesh Jayaram","Yuqi Li","Joe Heyward","Justin Gilmer","Yaqing Wang","Radu Soricut","Luyang Liu","Qingnan Duan","Jamie Hayes","Maura O'Brien","Gaurav Singh Tomar","Sivan Eiger","Bahar Fatemi","Jeffrey Hui","Catarina Barros","Adaeze Chukwuka","Alena Butryna","Saksham Thakur","Austin Huang","Zhufeng Pan","Haotian Tang","Serkan Cabi","Tulsee Doshi","Michiel Bakker","Sumit Bagri","Ruy Ley-Wild","Adam Lelkes","Jennie Lees","Patrick Kane","David Greene","Shimu Wu","Jörg Bornschein","Gabriela Surita","Sarah Hodkinson","Fangtao Li","Chris Hidey","Sébastien Pereira","Sean Ammirati","Phillip Lippe","Adam Kraft","Pu Han","Sebastian Gerlach","Zifeng Wang","Liviu Panait","Feng Han","Brian Farris","Yingying Bi","Hannah DeBalsi","Miaosen Wang","Gladys Tyen","James Cohan","Susan Zhang","Jarred Barber","Da-Woon Chung","Jaeyoun Kim","Markus Kunesch","Steven Pecht","Nami Akazawa","Abe Friesen","James Lyon","Ali Eslami","Junru Wu","Jie Tan","Yue Song","Ravi Kumar","Chris Welty","Ilia Akolzin","Gena Gibson","Sean Augenstein","Arjun Pillai","Nancy Yuen","Du Phan","Xin Wang","Iain Barr","Heiga Zen","Nan Hua","Casper Liu"," Jilei"," Wang","Tanuj Bhatia","Hao Xu","Oded Elyada","Pushmeet Kohli","Mirek Olšák","Ke Chen","Azalia Mirhoseini","Noam Shazeer","Shoshana Jakobovits","Maggie Tran","Nolan Ramsden","Tarun Bharti","Fred Alcober","Yunjie Li","Shilpa Shetty","Jing Chen","Dmitry Kalashnikov","Megha Nawhal","Sercan Arik","Hanwen Chen","Michiel Blokzijl","Shubham Gupta","James Rubin","Rigel Swavely","Sophie Bridgers","Ian Gemp","Chen Su","Arun Suggala","Juliette Pluto","Mary Cassin","Alain Vaucher","Kaiyang Ji","Jiahao Cai","Andrew Audibert","Animesh Sinha","David Tian","Efrat Farkash","Amy Hua","Jilin Chen","Duc-Hieu Tran","Edward Loper","Nicole Brichtova","Lara McConnaughey","Ballie Sandhu","Robert Leland","Doug DeCarlo","Andrew Over","James Huang","Xing Wu","Connie Fan","Eric Li","Yun Lei","Deepak Sharma","Cosmin Paduraru","Luo Yu","Matko Bošnjak","Phuong Dao","Min Choi","Sneha Kudugunta","Jakub Adamek","Carlos Guía","Ali Khodaei","Jie Feng","Wenjun Zeng","David Welling","Sandeep Tata","Christina Butterfield","Andrey Vlasov","Seliem El-Sayed","Swaroop Mishra","Tara Sainath","Shentao Yang","RJ Skerry-Ryan","Jeremy Shar","Robert Berry","Arunkumar Rajendran","Arun Kandoor","Andrea Burns","Deepali Jain","Tom Stone","Wonpyo Park","Shibo Wang","Albin Cassirer","Guohui Wang","Hayato Kobayashi","Sergey Rogulenko","Vineetha Govindaraj","Mikołaj Rybiński","Nadav Olmert","Colin Evans","Po-Sen Huang","Kelvin Xu","Premal Shah","Terry Thurk","Caitlin Sikora","Mu Cai","Jin Xie","Elahe Dabir","Saloni Shah","Norbert Kalb","Carrie Zhang","Shruthi Prabhakara","Amit Sabne","Artiom Myaskovsky","Vikas Raunak","Blanca Huergo","Behnam Neyshabur","Jon Clark","Ye Zhang","Shankar Krishnan","Eden Cohen","Dinesh Tewari","James Lottes","Yumeya Yamamori"," Hui"," Li","Mohamed Elhawaty","Ada Maksutaj Oflazer","Adrià Recasens","Sheryl Luo","Duy Nguyen","Taylor Bos","Kalyan Andra","Ana Salazar","Ed Chi","Jeongwoo Ko","Matt Ginsberg","Anders Andreassen","Anian Ruoss","Todor Davchev","Elnaz Davoodi","Chenxi Liu","Min Kim","Santiago Ontanon","Chi Ming To","Dawei Jia","Rosemary Ke","Jing Wang","Anna Korsun","Moran Ambar","Ilya Kornakov","Irene Giannoumis","Toni Creswell","Denny Zhou","Yi Su","Ishaan Watts","Aleksandr Zaks","Evgenii Eltyshev","Ziqiang Feng","Sidharth Mudgal","Alex Kaskasoli","Juliette Love","Kingshuk Dasgupta","Sam Shleifer","Richard Green","Sungyong Seo","Chansoo Lee","Dale Webster","Prakash Shroff","Ganna Raboshchuk","Isabel Leal","James Manyika","Sofia Erell","Daniel Murphy","Zhisheng Xiao","Anton Bulyenov","Julian Walker","Mark Collier","Matej Kastelic","Nelson George","Sushant Prakash","Sailesh Sidhwani","Alexey Frolov","Steven Hansen","Petko Georgiev","Tiberiu Sosea","Chris Apps","Aishwarya Kamath","David Reid","Emma Cooney","Charlotte Magister","Oriana Riva","Alec Go","Pu-Chin Chen","Sebastian Krause","Nir Levine","Marco Fornoni","Ilya Figotin","Nick Roy","Parsa Mahmoudieh","Vladimir Magay","Mukundan Madhavan","Jin Miao","Jianmo Ni","Yasuhisa Fujii","Ian Chou","George Scrivener","Zak Tsai","Siobhan Mcloughlin","Jeremy Selier","Sandra Lefdal","Jeffrey Zhao","Abhijit Karmarkar","Kushal Chauhan","Shivanker Goel","Zhaoyi Zhang","Vihan Jain","Parisa Haghani","Mostafa Dehghani","Jacob Scott","Erin Farnese","Anastasija Ilić","Steven Baker","Julia Pawar","Li Zhong","Josh Camp","Yoel Zeldes","Shravya Shetty","Anand Iyer","Vít Listík","Jiaxian Guo","Luming Tang","Mark Geller","Simon Bucher","Yifan Ding","Hongzhi Shi","Carrie Muir","Dominik Grewe","Ramy Eskander","Octavio Ponce","Boqing Gong","Derek Gasaway","Samira Khan","Umang Gupta","Angelos Filos","Weicheng Kuo","Klemen Kloboves","Jennifer Beattie","Christian Wright","Leon Li","Alicia Jin","Sandeep Mariserla","Miteyan Patel","Jens Heitkaemper","Dilip Krishnan","Vivek Sharma","David Bieber","Christian Frank","John Lambert","Paul Caron","Martin Polacek","Mai Giménez","Himadri Choudhury","Xing Yu","Sasan Tavakkol","Arun Ahuja","Franz Och","Rodolphe Jenatton","Wojtek Skut","Bryan Richter","David Gaddy","Andy Ly","Misha Bilenko","Megh Umekar","Ethan Liang","Martin Sevenich","Mandar Joshi","Hassan Mansoor","Rebecca Lin","Sumit Sanghai","Abhimanyu Singh","Xiaowei Li","Sudheendra Vijayanarasimhan","Zaheer Abbas","Yonatan Bitton","Hansa Srinivasan","Manish Reddy Vuyyuru","Alexander Frömmgen","Yanhua Sun","Ralph Leith","Alfonso Castaño","DJ Strouse","Le Yan","Austin Kyker","Satish Kambala","Mary Jasarevic","Thibault Sellam","Chao Jia","Alexander Pritzel","Raghavender R","Huizhong Chen","Natalie Clay","Sudeep Gandhe","Sean Kirmani","Sayna Ebrahimi","Hannah Kirkwood","Jonathan Mallinson","Chao Wang","Adnan Ozturel","Kuo Lin","Shyam Upadhyay","Vincent Cohen-Addad","Sean Purser-haskell","Yichong Xu","Ebrahim Songhori","Babi Seal","Alberto Magni","Almog Gueta","Tingting Zou","Guru Guruganesh","Thais Kagohara","Hung Nguyen","Khalid Salama","Alejandro Cruzado Ruiz","Justin Frye","Zhenkai Zhu","Matthias Lochbrunner","Simon Osindero","Wentao Yuan","Lisa Lee","Aman Prasad","Lam Nguyen Thiet","Daniele Calandriello","Victor Stone","Qixuan Feng","Han Ke","Maria Voitovich","Geta Sampemane","Lewis Chiang","Ling Wu","Alexander Bykovsky","Matt Young","Luke Vilnis","Ishita Dasgupta","Aditya Chawla","Qin Cao","Bowen Liang","Daniel Toyama","Szabolcs Payrits","Anca Stefanoiu","Dimitrios Vytiniotis","Ankesh Anand","Tianxiao Shen","Blagoj Mitrevski","Michael Tschannen","Sreenivas Gollapudi","Aishwarya P S","José Leal","Zhe Shen","Han Fu","Wei Wang","Arvind Kannan","Doron Kukliansky","Sergey Yaroshenko","Svetlana Grant","Umesh Telang","David Wood","Alexandra Chronopoulou","Alexandru Ţifrea","Tao Zhou"," Tony"," Nguy\\~ên","Muge Ersoy","Anima Singh","Meiyan Xie","Emanuel Taropa","Woohyun Han","Eirikur Agustsson","Andrei Sozanschi","Hui Peng","Alex Chen","Yoel Drori","Efren Robles","Yang Gao","Xerxes Dotiwalla","Ying Chen","Anudhyan Boral","Alexei Bendebury","John Nham","Chris Tar","Luis Castro","Jiepu Jiang","Canoee Liu","Felix Halim","Jinoo Baek","Andy Wan","Jeremiah Liu","Yuan Cao","Shengyang Dai","Trilok Acharya","Ruoxi Sun","Fuzhao Xue","Saket Joshi","Morgane Lustman","Yongqin Xian","Rishabh Joshi","Deep Karkhanis","Nora Kassner","Jamie Hall","Xiangzhuo Ding","Gan Song","Gang Li","Chen Zhu","Yana Kulizhskaya","Bin Ni","Alexey Vlaskin","Solomon Demmessie","Lucio Dery","Salah Zaiem","Yanping Huang","Cindy Fan","Felix Gimeno","Ananth Balashankar","Koji Kojima","Hagai Taitelbaum","Maya Meng","Dero Gharibian","Sahil Singla","Wei Chen","Ambrose Slone","Guanjie Chen","Sujee Rajayogam","Max Schumacher","Suyog Kotecha","Rory Blevins","Qifei Wang","Mor Hazan Taege","Alex Morris","Xin Liu","Fayaz Jamil","Richard Zhang","Pratik Joshi","Ben Ingram","Tyler Liechty","Ahmed Eleryan","Scott Baird","Alex Grills","Gagan Bansal","Shan Han","Kiran Yalasangi","Shawn Xu","Majd Al Merey","Isabel Gao","Felix Weissenberger","Igor Karpov","Robert Riachi","Ankit Anand","Gautam Prasad","Kay Lamerigts","Reid Hayes","Jamie Rogers","Mandy Guo","Ashish Shenoy"," Qiong"," Hu","Kyle He","Yuchen Liu","Polina Zablotskaia","Sagar Gubbi","Yifan Chang","Jay Pavagadhi","Kristian Kjems","Archita Vadali","Diego Machado","Yeqing Li","Renshen Wang","Dipankar Ghosh","Aahil Mehta","Dana Alon","George Polovets","Alessio Tonioni","Nate Kushman","Joel D'sa","Lin Zhuo","Allen Wu","Rohin Shah","John Youssef","Jiayu Ye","Justin Snyder","Karel Lenc","Senaka Buthpitiya","Matthew Tung","Jichuan Chang","Tao Chen","David Saxton","Jenny Lee","Lydia Lihui Zhang","James Qin","Prabakar Radhakrishnan","Maxwell Chen","Piotr Ambroszczyk","Metin Toksoz-Exley","Yan Zhong","Nitzan Katz","Brendan O'Donoghue","Tamara von Glehn","Adi Gerzi Rosenthal","Aga Świetlik","Xiaokai Zhao","Nick Fernando","Jinliang Wei","Jieru Mei","Sergei Vassilvitskii","Diego Cedillo","Pranjal Awasthi","Hui Zheng","Koray Kavukcuoglu","Itay Laish","Joseph Pagadora","Marc Brockschmidt","Christopher A. Choquette-Choo","Arunkumar Byravan","Yifeng Lu","Xu Chen","Mia Chen","Kenton Lee","Rama Pasumarthi","Sijal Bhatnagar","Aditya Shah","Qiyin Wu","Zhuoyuan Chen","Zack Nado","Bartek Perz","Zixuan Jiang","David Kao","Ganesh Mallya","Nino Vieillard","Lantao Mei","Sertan Girgin","Mandy Jordan","Yeongil Ko","Alekh Agarwal","Yaxin Liu","Yasemin Altun","Raoul de Liedekerke","Anastasios Kementsietsidis","Daiyi Peng","Dangyi Liu","Utku Evci","Peter Humphreys","Austin Tarango","Xiang Deng","Yoad Lewenberg","Kevin Aydin","Chengda Wu","Bhavishya Mittal","Tsendsuren Munkhdalai","Kleopatra Chatziprimou","Rodrigo Benenson","Uri First","Xiao Ma","Jinning Li","Armand Joulin","Hamish Tomlinson","Tingnan Zhang","Milad Nasr","Zhi Hong","Michaël Sander","Lisa Anne Hendricks","Anuj Sharma","Andrew Bolt","Eszter Vértes","Jiri Simsa","Tomer Levinboim","Olcan Sercinoglu","Divyansh Shukla","Austin Wu","Craig Swanson","Danny Vainstein","Fan Bu","Bo Wang","Ryan Julian","Charles Yoon","Sergei Lebedev","Antonious Girgis","Bernd Bandemer","David Du","Todd Wang","Xi Chen","Ying Xiao","Peggy Lu","Natalie Ha","Vlad Ionescu","Simon Rowe","Josip Matak","Federico Lebron","Andreas Steiner","Lalit Jain","Manaal Faruqui","Nicolas Lacasse","Georgie Evans","Neesha Subramaniam","Dean Reich","Giulia Vezzani","Aditya Pandey","Joe Stanton","Tianhao Zhou","Liam McCafferty","Henry Griffiths","Verena Rieser","Soheil Hassas Yeganeh","Eleftheria Briakou","Lu Huang","Zichuan Wei","Liangchen Luo","Erik Jue","Gabby Wang","Victor Cotruta","Myriam Khan","Jongbin Park","Qiuchen Guo","Peiran Li","Rong Rong","Diego Antognini","Anastasia Petrushkina","Chetan Tekur","Eli Collins","Parul Bhatia","Chester Kwak","Wenhu Chen","Arvind Neelakantan","Immanuel Odisho","Sheng Peng","Vincent Nallatamby","Vaibhav Tulsyan","Fabian Pedregosa","Peng Xu","Raymond Lin","Yulong Wang","Emma Wang","Sholto Douglas","Reut Tsarfaty","Elena Gribovskaya","Renga Aravamudhan","Manu Agarwal","Mara Finkelstein","Qiao Zhang","Elizabeth Cole","Phil Crone","Sarmishta Velury","Anil Das","Chris Sauer","Luyao Xu","Danfeng Qin","Chenjie Gu","Dror Marcus","CJ Zheng","Wouter Van Gansbeke","Sobhan Miryoosefi","Haitian Sun","YaGuang Li","Charlie Chen","Jae Yoo","Pavel Dubov","Alex Tomala","Adams Yu","Paweł Wesołowski","Alok Gunjan","Eddie Cao","Jiaming Luo","Nikhil Sethi","Arkadiusz Socala","Laura Graesser","Tomas Kocisky","Arturo BC","Minmin Chen","Edward Lee","Sophie Wang","Weize Kong","Qiantong Xu","Nilesh Tripuraneni","Yiming Li","Xinxin Yu","Allen Porter","Paul Voigtlaender","Biao Zhang","Arpi Vezer","Sarah York","Qing Wei","Geoffrey Cideron","Mark Kurzeja","Seungyeon Kim","Benny Li","Angéline Pouget","Hyo Lee","Kaspar Daugaard","Yang Li","Dave Uthus","Aditya Siddhant","Paul Cavallaro","Sriram Ganapathy","Maulik Shah","Rolf Jagerman","Jeff Stanway","Piermaria Mendolicchio","Li Xiao","Kayi Lee","Tara Thompson","Shubham Milind Phal","Jason Chase","Sun Jae Lee","Adrian N Reyes","Disha Shrivastava","Zhen Qin","Roykrong Sukkerd","Seth Odoom","Lior Madmoni","John Aslanides","Jonathan Herzig","Elena Pochernina","Sheng Zhang","Parker Barnes","Daisuke Ikeda","Qiujia Li","Shuo-yiin Chang","Shakir Mohamed","Jim Sproch","Richard Powell","Bidisha Samanta","Domagoj Ćevid","Anton Kovsharov","Shrestha Basu Mallick","Srinivas Tadepalli","Anne Zheng","Kareem Ayoub","Andreas Noever","Christian Reisswig","Zhuo Xu","Junhyuk Oh","Martin Matysiak","Tim Blyth","Shereen Ashraf","Julien Amelot","Boone Severson","Michele Bevilacqua","Motoki Sano","Ethan Dyer","Ofir Roval","Anu Sinha","Yin Zhong","Sagi Perel","Tea Sabolić","Johannes Mauerer","Willi Gierke","Mauro Verzetti","Rodrigo Cabrera","Alvin Abdagic","Steven Hemingray","Austin Stone","Jong Lee","Farooq Ahmad","Karthik Raman","Lior Shani","Jonathan Lai","Orhan Firat","Nathan Waters","Eric Ge","Mo Shomrat","Himanshu Gupta","Rajeev Aggarwal","Tom Hudson","Bill Jia","Simon Baumgartner","Palak Jain","Joe Kovac","Junehyuk Jung","Ante Žužul","Will Truong","Morteza Zadimoghaddam","Songyou Peng","Marco Liang","Rachel Sterneck","Balaji Lakshminarayanan","Machel Reid","Oliver Woodman","Tong Zhou","Jianling Wang","Vincent Coriou","Arjun Narayanan","Jay Hoover","Yenai Ma","Apoorv Jindal","Clayton Sanford","Doug Reid","Swaroop Ramaswamy","Alex Kurakin","Roland Zimmermann","Yana Lunts","Dragos Dena","Zalán Borsos","Vered Cohen","Shujian Zhang","Will Grathwohl","Robert Dadashi","Morgan Redshaw","Joshua Kessinger","Julian Odell","Silvano Bonacina","Zihang Dai","Grace Chen","Ayush Dubey","Pablo Sprechmann","Mantas Pajarskas","Wenxuan Zhou","Niharika Ahuja","Tara Thomas","Martin Nikoltchev","Matija Kecman","Bharath Mankalale","Andrey Ryabtsev","Jennifer She","Christian Walder","Jiaming Shen","Lu Li","Carolina Parada","Sheena Panthaplackel","Okwan Kwon","Matt Lawlor","Utsav Prabhu","Yannick Schroecker","Marc'aurelio Ranzato","Pete Blois","Iurii Kemaev","Ting Yu","Dmitry Lepikhin","Hao Xiong","Sahand Sharifzadeh","Oleaser Johnson","Jeremiah Willcock","Rui Yao","Greg Farquhar","Sujoy Basu","Hidetoshi Shimokawa","Nina Anderson","Haiguang Li","Khiem Pham","Yizhong Liang","Sebastian Borgeaud","Alexandre Moufarek","Hideto Kazawa","Blair Kutzman","Marcin Sieniek","Sara Smoot","Ruth Wang","Natalie Axelsson","Nova Fallen","Prasha Sundaram","Yuexiang Zhai","Varun Godbole","Petros Maniatis","Alek Wang","Ilia Shumailov","Santhosh Thangaraj","Remi Crocker","Nikita Gupta","Gang Wu","Phil Chen","Gellért Weisz","Celine Smith","Mojtaba Seyedhosseini","Boya Fang","Xiyang Luo","Roey Yogev","Zeynep Cankara","Andrew Hard","Helen Ran","Rahul Sukthankar","George Necula","Gaël Liu","Honglong Cai","Praseem Banzal","Daniel Keysers","Sanjay Ghemawat","Connie Tao","Emma Dunleavy","Aditi Chaudhary","Wei Li","Maciej Mikuła","Chen-Yu Lee","Tiziana Refice","Krishna Somandepalli","Alexandre Fréchette","Dan Bahir","John Karro","Keith Rush","Sarah Perrin","Bill Rosgen","Xiaomeng Yang","Clara Huiyi Hu","Mahmoud Alnahlawi","Justin Mao-Jones","Roopal Garg","Hoang Nguyen","Bat-Orgil Batsaikhan","Iñaki Iturrate","Anselm Levskaya","Avi Singh","Ashyana Kachra","Tony Lu","Denis Petek","Zheng Xu","Mark Graham","Lukas Zilka","Yael Karov","Marija Kostelac","Fangyu Liu","Yaohui Guo","Weiyue Wang","Bernd Bohnet","Emily Pitler","Tony Bruguier","Keisuke Kinoshita","Chrysovalantis Anastasiou","Nilpa Jha","Ting Liu","Jerome Connor","Phil Wallis","Philip Pham","Eric Bailey","Shixin Li","Heng-Tze Cheng","Sally Ma","Haiqiong Li","Akanksha Maurya","Kate Olszewska","Manfred Warmuth","Christy Koh","Dominik Paulus","Siddhartha Reddy Jonnalagadda","Enrique Piqueras","Ali Elqursh","Geoff Brown","Hadar Shemtov","Loren Maggiore","Fei Xia","Ryan Foley","Beka Westberg","George van den Driessche","Livio Baldini Soares","Arjun Kar","Michael Quinn","Siqi Zuo","Jialin Wu","Kyle Kastner","Anna Bortsova","Aijun Bai","Ales Mikhalap","Luowei Zhou","Jennifer Brennan","Vinay Ramasesh","Honglei Zhuang","John Maggs","Johan Schalkwyk","Yuntao Xu","Hui Huang","Andrew Howard","Sasha Brown","Linting Xue","Gloria Shen","Brian Albert","Neha Jha","Daniel Zheng","Varvara Krayvanova","Spurthi Amba Hombaiah","Olivier Lacombe","Gautam Vasudevan","Dan Graur","Tian Xie","Meet Gandhi","Bangju Wang","Dustin Zelle","Harman Singh","Dahun Kim","Sébastien Cevey","Victor Ungureanu","Natasha Noy","Fei Liu","Annie Xie","Fangxiaoyu Feng","Katerina Tsihlas","Daniel Formoso","Neera Vats","Quentin Wellens","Yinan Wang","Niket Kumar Bhumihar","Samrat Ghosh","Matt Hoffman","Tom Lieber","Oran Lang","Kush Bhatia","Tom Paine","Aroonalok Pyne","Ronny Votel","Madeleine Clare Elish","Benoit Schillings","Alex Panagopoulos","Haichuan Yang","Adam Raveret","Zohar Yahav","Shuang Liu","Dalia El Badawy","Nishant Agrawal","Mohammed Badawi","Mahdi Mirzazadeh","Carla Bromberg","Fan Ye","Chang Liu","Tatiana Sholokhova","George-Cristian Muraru","Gargi Balasubramaniam","Jonathan Malmaud","Alen Carin","Danilo Martins","Irina Jurenka","Pankil Botadra","Dave Lacey","Richa Singh","Mariano Schain","Dan Zheng","Isabelle Guyon","Victor Lavrenko","Seungji Lee","Xiang Zhou","Demis Hassabis","Jeshwanth Challagundla","Derek Cheng","Nikhil Mehta","Matthew Mauger","Michela Paganini","Pushkar Mishra","Kate Lee","Zhang Li","Lexi Baugher","Ondrej Skopek","Max Chang","Amir Zait","Gaurav Menghani","Lizzetth Bellot","Guangxing Han","Jean-Michel Sarr","Sharat Chikkerur","Himanshu Sahni","Rohan Anil","Arun Narayanan","Chandu Thekkath","Daniele Pighin","Hana Strejček","Marko Velic","Fred Bertsch","Manuel Tragut","Keran Rong","Alicia Parrish","Kai Bailey","Jiho Park","Isabela Albuquerque","Abhishek Bapna","Rajesh Venkataraman","Alec Kosik","Johannes Griesser","Zhiwei Deng","Alek Andreev","Qingyun Dou","Kevin Hui","Fanny Wei","Xiaobin Yu","Lei Shu","Avia Aharon","David Barker","Badih Ghazi","Sebastian Flennerhag","Chris Breaux","Yuchuan Liu","Matthew Bilotti","Josh Woodward","Uri Alon","Stephanie Winkler","Tzu-Kuo Huang","Kostas Andriopoulos","João Gabriel Oliveira","Penporn Koanantakool","Berkin Akin","Michael Wunder","Cicero Nogueira dos Santos","Mohammad Hossein Bateni","Lin Yang","Dan Horgan","Beer Changpinyo","Keyvan Amiri","Min Ma","Dayeong Lee","Lihao Liang","Anirudh Baddepudi","Tejasi Latkar","Raia Hadsell","Jun Xu","Hairong Mu","Michael Han","Aedan Pope","Snchit Grover","Frank Kim","Ankit Bhagatwala","Guan Sun","Yamini Bansal","Amir Globerson","Alireza Nazari","Samira Daruki","Hagen Soltau","Jane Labanowski","Laurent El Shafey","Matt Harvey","Yanif Ahmad","Elan Rosenfeld","William Kong","Etienne Pot","Yi-Xuan Tan","Aurora Wei","Victoria Langston","Marcel Prasetya","Petar Veličković","Richard Killam","Robin Strudel","Darren Ni","Zhenhai Zhu","Aaron Archer","Kavya Kopparapu","Lynn Nguyen","Emilio Parisotto","Hussain Masoom","Sravanti Addepalli","Jordan Grimstad","Hexiang Hu","Joss Moore","Avinatan Hassidim","Le Hou","Mukund Raghavachari","Jared Lichtarge","Adam R. Brown","Hilal Dib","Natalia Ponomareva","Justin Fu","Yujing Zhang","Altaf Rahman","Joana Iljazi","Edouard Leurent","Gabriel Dulac-Arnold","Cosmo Du","Chulayuth Asawaroengchai","Larry Jin","Ela Gruzewska","Ziwei Ji","Benigno Uria","Daniel De Freitas","Paul Barham","Lauren Beltrone","Víctor Campos","Jun Yan","Neel Kovelamudi","Arthur Nguyen","Elinor Davies","Zhichun Wu","Zoltan Egyed","Kristina Toutanova","Nithya Attaluri","Hongliang Fei","Peter Stys","Siddhartha Brahma","Martin Izzard","Siva Velusamy","Scott Lundberg","Vincent Zhuang","Kevin Sequeira","Adam Santoro","Ehsan Amid","Ophir Aharoni","Shuai Ye","Mukund Sundararajan","Lijun Yu","Yu-Cheng Ling","Stephen Spencer","Hugo Song","Josip Djolonga","Christo Kirov","Sonal Gupta","Alessandro Bissacco","Clemens Meyer","Mukul Bhutani","Andrew Dai","Weiyi Wang","Siqi Liu","Ashwin Sreevatsa","Qijun Tan","Maria Wang","Lucy Kim","Yicheng Wang","Alex Irpan","Yang Xiao","Stanislav Fort","Yifan He","Alex Gurney","Bryan Gale","Yue Ma","Monica Roy","Viorica Patraucean","Taylan Bilal","Golnaz Ghiasi","Anahita Hosseini","Melvin Johnson","Zhuowan Li","Yi Tay","Benjamin Beyret","Katie Millican","Josef Broder","Mayank Lunayach","Danny Swisher","Eugen Vušak","David Parkinson","MH Tessler","Adi Mayrav Gilady","Richard Song","Allan Dafoe","Yves Raimond","Masa Yamaguchi","Itay Karo","Elizabeth Nielsen","Kevin Kilgour","Mike Dusenberry","Rajiv Mathews","Jiho Choi","Siyuan Qiao","Harsh Mehta","Sahitya Potluri","Chris Knutsen","Jialu Liu","Tat Tan","Kuntal Sengupta","Keerthana Gopalakrishnan","Abodunrinwa Toki","Mencher Chiang","Mike Burrows","Grace Vesom","Zafarali Ahmed","Ilia Labzovsky","Siddharth Vashishtha","Preeti Singh","Ankur Sharma","Ada Ma","Jinyu Xie","Pranav Talluri","Hannah Forbes-Pollard","Aarush Selvan","Joel Wee","Loic Matthey","Tom Funkhouser","Parthasarathy Gopavarapu","Lev Proleev","Cheng Li","Matt Thomas","Kashyap Kolipaka","Zhipeng Jia","Ashwin Kakarla","Srinivas Sunkara","Joan Puigcerver","Suraj Satishkumar Sheth","Emily Graves","Chen Wang","Sadh MNM Khan","Kai Kang","Shyamal Buch","Fred Zhang","Omkar Savant","David Soergel","Kevin Lee","Linda Friso","Xuanyi Dong","Rahul Arya","Shreyas Chandrakaladharan","Connor Schenck","Greg Billock","Tejas Iyer","Anton Bakalov","Leslie Baker","Alex Ruiz","Angad Chandorkar","Trieu Trinh","Matt Miecnikowski","Yanqi Zhou","Yangsibo Huang","Jiazhong Nie","Ali Shah","Ashish Thapliyal","Sam Haves","Lun Wang","Uri Shaham","Patrick Morris-Suzuki","Soroush Radpour","Leonard Berrada","Thomas Strohmann","Chaochao Yan","Jingwei Shen","Sonam Goenka","Tris Warkentin","Petar Dević","Dan Belov","Albert Webson","Madhavi Yenugula","Puranjay Datta","Jerry Chang","Nimesh Ghelani","Aviral Kumar","Vincent Perot","Jessica Lo","Yang Song","Herman Schmit","Jianmin Chen","Vasilisa Bashlovkina","Xiaoyue Pan","Diana Mincu","Paul Roit","Isabel Edkins","Andy Davis","Yujia Li","Ben Horn","Xinjian Li","Pradeep Kumar S","Eric Doi","Wanzheng Zhu","Sri Gayatri Sundara Padmanabhan","Siddharth Verma","Jasmine Liu","Heng Chen","Mihajlo Velimirović","Malcolm Reynolds","Priyanka Agrawal","Nick Sukhanov","Abhinit Modi","Siddharth Goyal","John Palowitch","Nima Khajehnouri","Wing Lowe","David Klinghoffer","Sharon Silver","Vinh Tran","Candice Schumann","Francesco Piccinno","Xi Liu","Mario Lučić","Xiaochen Yang","Sandeep Kumar","Ajay Kannan","Ragha Kotikalapudi","Mudit Bansal","Fabian Fuchs","Mohammad Javad Hosseini","Abdelrahman Abdelhamed","Dawn Bloxwich","Tianhe Yu","Ruoxin Sang","Gregory Thornton","Karan Gill","Yuchi Liu","Virat Shejwalkar","Jason Lin","Zhipeng Yan","Kehang Han","Thomas Buschmann","Michael Pliskin","Zhi Xing","Susheel Tatineni","Junlin Zhang","Sissie Hsiao","Gavin Buttimore","Marcus Wu","Zefei Li","Geza Kovacs","Legg Yeung","Tao Huang","Aaron Cohen","Bethanie Brownfield","Averi Nowak","Mikel Rodriguez","Tianze Shi","Hado van Hasselt","Kevin Cen","Deepanway Ghoshal","Kushal Majmundar","Weiren Yu"," Warren"," Chen","Danila Sinopalnikov","Hao Zhang","Vlado Galić","Di Lu","Zeyu Zheng","Maggie Song","Gary Wang","Gui Citovsky","Swapnil Gawde","Isaac Galatzer-Levy","David Silver","Ivana Balazevic","Dipanjan Das","Kingshuk Majumder","Yale Cong","Praneet Dutta","Dustin Tran","Hui Wan","Junwei Yuan","Daniel Eppens","Alanna Walton","Been Kim","Harry Ragan","James Cobon-Kerr","Lu Liu","Weijun Wang","Bryce Petrini","Jack Rae","Rakesh Shivanna","Yan Xiong","Chace Lee","Pauline Coquinot","Yiming Gu","Lisa Patel","Blake Hechtman","Aviel Boag","Orion Jankowski","Alex Wertheim","Alex Lee","Paul Covington","Hila Noga","Sam Sobell","Shanthal Vasanth","William Bono","Chirag Nagpal","Wei Fan","Xavier Garcia","Kedar Soparkar","Aybuke Turker","Nathan Howard","Sachit Menon","Yuankai Chen","Vikas Verma","Vladimir Pchelin","Harish Rajamani","Valentin Dalibard","Ana Ramalho","Yang Guo","Kartikeya Badola","Seojin Bang","Nathalie Rauschmayr","Julia Proskurnia","Sudeep Dasari","Xinyun Chen","Mikhail Sushkov","Anja Hauth","Pauline Sho","Abhinav Singh","Bilva Chandra","Allie Culp","Max Dylla","Olivier Bachem","James Besley","Heri Zhao","Timothy Lillicrap","Wei Wei","Wael Al Jishi","Ning Niu","Alban Rrustemi","Raphaël Lopez Kaufman","Ryan Poplin","Jewel Zhao","Minh Truong","Shikhar Bharadwaj","Ester Hlavnova","Eli Stickgold","Cordelia Schmid","Georgi Stephanov","Zhaoqi Leng","Frederick Liu","Léonard Hussenot","Shenil Dodhia","Juliana Vicente Franco","Lesley Katzen","Abhanshu Sharma","Sarah Cogan","Zuguang Yang","Aniket Ray","Sergi Caelles","Shen Yan","Ravin Kumar","Daniel Gillick","Renee Wong","Joshua Ainslie","Jonathan Hoech","Séb Arnold","Dan Abolafia","Anca Dragan","Ben Hora","Grace Hu","Alexey Guseynov","Yang Lu","Chas Leichner","Jinmeng Rao","Abhimanyu Goyal","Nagabhushan Baddi","Daniel Hernandez Diaz","Tim McConnell","Max Bain","Jake Abernethy","Qiqi Yan","Rylan Schaeffer","Paul Vicol","Will Thompson","Montse Gonzalez Arenas","Mathias Bellaiche","Pablo Barrio","Stefan Zinke","Riccardo Patana","Pulkit Mehta","JK Kearns","Avraham Ruderman","Scott Pollom","David D'Ambrosio","Cath Hope","Yang Yu","Andrea Gesmundo","Kuang-Huei Lee","Aviv Rosenberg","Yiqian Zhou","Yaoyiran Li","Drew Garmon","Yonghui Wu","Safeen Huda","Gil Fidel","Martin Baeuml","Jian Li","Phoebe Kirk","Rhys May","Tao Tu","Sara Mc Carthy","Toshiyuki Fukuzawa","Miranda Aperghis","Chih-Kuan Yeh","Toshihiro Yoshino","Bo Li","Austin Myers","Kaisheng Yao","Ben Limonchik","Changwan Ryu","Rohun Saxena","Alex Goldin","Ruizhe Zhao","Rocky Rhodes","Tao Zhu","Divya Tyam","Heidi Howard","Nathan Byrd","Hongxu Ma","Yan Wu","Ryan Mullins","Qingze Wang","Aida Amini","Sebastien Baur","Yiran Mao","Subhashini Venugopalan","Will Song","Wen Ding","Paul Collins","Sashank Reddi","Megan Shum","Andrei Rusu","Luisa Zintgraf","Kelvin Chan","Sheela Goenka","Mathieu Blondel","Michael Collins","Renke Pan","Marissa Giustina","Nikolai Chinaev","Christian Schuler","Ce Zheng","Jonas Valfridsson","Alyssa Loo","Alex Yakubovich","Jamie Smith","Tao Jiang","Rich Munoz","Gabriel Barcik","Rishabh Bansal","Mingyao Yang","Yilun Du","Pablo Duque","Mary Phuong","Alexandra Belias","Kunal Lad","Zeyu Liu","Tal Schuster","Karthik Duddu","Jieru Hu","Paige Kunkle","Matthew Watson","Jackson Tolins","Josh Smith","Denis Teplyashin","Garrett Bingham","Marvin Ritter","Marco Andreetto","Divya Pitta","Mohak Patel","Shashank Viswanadha","Trevor Strohman","Catalin Ionescu","Jincheng Luo","Yogesh Kalley","Jeremy Wiesner","Dan Deutsch","Derek Lockhart","Peter Choy","Rumen Dangovski","Chawin Sitawarin","Cat Graves","Tanya Lando","Joost van Amersfoort","Ndidi Elue","Zhouyuan Huo","Pooya Moradi","Jean Tarbouriech","Henryk Michalewski","Wenting Ye","Eunyoung Kim","Alex Druinsky","Florent Altché","Xinyi Chen","Artur Dwornik","Da-Cheng Juan","Rivka Moroshko","Horia Toma","Jarrod Kahn","Hai Qian","Maximilian Sieb","Irene Cai","Roman Goldenberg","Praneeth Netrapalli","Sindhu Raghuram","Yuan Gong","Lijie Fan","Evan Palmer","Yossi Matias","Valentin Gabeur","Shreya Pathak","Tom Ouyang","Don Metzler","Geoff Bacon","Srinivasan Venkatachary","Sridhar Thiagarajan","Alex Cullum","Eran Ofek","Vytenis Sakenas","Mohamed Hammad","Cesar Magalhaes","Mayank Daswani","Oscar Chang","Ashok Popat","Ruichao Li","Komal Jalan","Yanhan Hou","Josh Lipschultz","Antoine He","Wenhao Jia","Pier Giuseppe Sessa","Prateek Kolhar","William Wong","Sumeet Singh","Lukas Haas","Jay Whang","Hanna Klimczak-Plucińska","Georges Rotival","Grace Chung","Yiqing Hua","Anfal Siddiqui","Nicolas Serrano","Dongkai Chen","Billy Porter","Libin Bai","Keshav Shivam","Sho Arora","Partha Talukdar","Tom Cobley","Sangnie Bhardwaj","Evgeny Gladchenko","Simon Green","Kelvin Guu","Felix Fischer","Xiao Wu","Eric Wang","Achintya Singhal","Tatiana Matejovicova","James Martens","Hongji Li","Roma Patel","Elizabeth Kemp","Jiaqi Pan","Lily Wang","Blake JianHang Chen","Jean-Baptiste Alayrac","Navneet Potti","Erika Gemzer","Eugene Ie","Kay McKinney","Takaaki Saeki","Edward Chou","Pascal Lamblin","SQ Mah","Zach Fisher","Martin Chadwick","Jon Stritar","Obaid Sarvana","Andrew Hogue","Artem Shtefan","Hadi Hashemi","Yang Xu","Jindong Gu","Sharad Vikram","Chung-Ching Chang","Sabela Ramos","Logan Kilpatrick","Weijuan Xi","Jenny Brennan","Yinghao Sun","Abhishek Jindal","Ionel Gog","Dawn Chen","Felix Wu","Jason Lee","Sudhindra Kopalle","Srinadh Bhojanapalli","Oriol Vinyals","Natan Potikha","Burcu Karagol Ayan","Yuan Yuan","Michael Riley","Piotr Stanczyk","Sergey Kishchenko","Bing Wang","Dan Garrette","Antoine Yang","Vlad Feinberg","CJ Carey","Javad Azizi","Viral Shah","Erica Moreira","Chongyang Shi","Josh Feldman","Elizabeth Salesky","Thomas Lampe","Aneesh Pappu","Duhyeon Kim","Jonas Adler","Avi Caciularu","Brian Walker","Yunhan Xu","Yochai Blau","Dylan Scandinaro","Terry Huang","Sam El-Husseini","Abhishek Sinha","Lijie Ren","Taylor Tobin","Patrik Sundberg","Tim Sohn","Vikas Yadav","Mimi Ly","Emily Xue","Jing Xiong","Afzal Shama Soudagar","Sneha Mondal","Nikhil Khadke","Qingchun Ren","Ben Vargas","Stan Bileschi","Sarah Chakera","Cindy Wang","Boyu Wang","Yoni Halpern","Joe Jiang","Vikas Sindhwani","Petre Petrov","Pranavaraj Ponnuramu","Sanket Vaibhav Mehta","Yu Watanabe","Betty Chan","Matheus Wisniewski","Trang Pham","Jingwei Zhang","Conglong Li","Dario de Cesare","Art Khurshudov","Alex Vasiloff","Melissa Tan","Zoe Ashwood","Bobak Shahriari","Maryam Majzoubi","Garrett Tanzer","Olga Kozlova","Robin Alazard","James Lee-Thorp","Nguyet Minh Phu","Isaac Tian","Junwhan Ahn","Andy Crawford","Lauren Lax","Yuan Shangguan","Iftekhar Naim","David Ross","Oleksandr Ferludin","Tongfei Guo","Andrea Banino","Hubert Soyer","Xiaoen Ju","Dominika Rogozińska","Ishaan Malhi","Marcella Valentine","Daniel Balle","Apoorv Kulshreshtha","Maciej Kula","Yiwen Song","Sophia Austin","John Schultz","Roy Hirsch","Arthur Douillard","Apoorv Reddy","Michael Fink","Summer Yue","Khyatti Gupta","Adam Zhang","Norman Rink","Daniel McDuff","Lei Meng","András György","Yasaman Razeghi","Ricky Liang","Kazuki Osawa","Aviel Atias","Matan Eyal","Tyrone Hill","Nikolai Grigorev","Zhengdong Wang","Nitish Kulkarni","Rachel Soh","Ivan Lobov","Zachary Charles","Sid Lall","Kazuma Hashimoto","Ido Kessler","Victor Gomes","Zelda Mariet","Danny Driess","Alessandro Agostini","Canfer Akbulut","Jingcao Hu","Marissa Ikonomidis","Emily Caveness","Kartik Audhkhasi","Saurabh Agrawal","Ioana Bica","Evan Senter","Jayaram Mudigonda","Kelly Chen","Jingchen Ye","Xuanhui Wang","James Svensson","Philipp Fränken","Josh Newlan","Li Lao","Eva Schnider","Sami Alabed","Joseph Kready","Jesse Emond","Afief Halumi","Tim Zaman","Chengxi Ye","Naina Raisinghani","Vilobh Meshram","Bo Chang","Ankit Singh Rawat","Axel Stjerngren","Sergey Levi","Rui Wang","Xiangzhu Long","Mitchelle Rasquinha","Steven Hand","Aditi Mavalankar","Lauren Agubuzu","Sudeshna Roy","Junquan Chen","Jarek Wilkiewicz","Hao Zhou","Michal Jastrzebski","Qiong Hu","Agustin Dal Lago","Ramya Sree Boppana","Wei-Jen Ko","Jennifer Prendki","Yao Su","Zhi Li","Eliza Rutherford","Girish Ramchandra Rao","Ramona Comanescu","Adrià Puigdomènech","Qihang Chen","Dessie Petrova","Christine Chan","Vedrana Milutinovic","Felipe Tiengo Ferreira","Chin-Yi Cheng","Ming Zhang","Tapomay Dey","Sherry Yang","Ramesh Sampath","Quoc Le","Howard Zhou","Chu-Cheng Lin","Hoi Lam","Christine Kaeser-Chen","Kai Hui","Dean Hirsch","Tom Eccles","Basil Mustafa","Shruti Rijhwani","Morgane Rivière","Yuanzhong Xu","Junjie Wang","Xinyang Geng","Xiance Si","Arjun Khare","Cheolmin Kim","Vahab Mirrokni","Kamyu Lee","Khuslen Baatarsukh","Nathaniel Braun","Lisa Wang","Pallavi LV","Richard Tanburn"," Yuvein"," Zhu","Fangda Li","Setareh Ariafar","Dan Goldberg","Ken Burke","Daniil Mirylenka","Meiqi Guo","Olaf Ronneberger","Hadas Natalie Vogel","Liqun Cheng","Nishita Shetty","Johnson Jia","Thomas Jimma","Corey Fry","Ted Xiao","Martin Sundermeyer","Ryan Burnell","Yannis Assael","Mario Pinto","JD Chen","Rohit Sathyanarayana","Donghyun Cho","Jing Lu","Rishabh Agarwal","Sugato Basu","Lucas Gonzalez","Dhruv Shah","Meng Wei","Dre Mahaarachchi","Rohan Agrawal","Tero Rissa","Yani Donchev","Ramiro Leal-Cavazos","Adrian Hutter","Markus Mircea","Alon Jacovi","Faruk Ahmed","Jiageng Zhang","Shuguang Hu","Bo-Juen Chen","Jonni Kanerva","Guillaume Desjardins","Andrew Lee","Nikos Parotsidis","Asier Mujika","Tobias Weyand","Jasper Snoek","Jo Chick","Kai Chen","Paul Chang","Ethan Mahintorabi","Zi Wang","Tolly Powell","Orgad Keller","Abhirut Gupta","Claire Sha","Kanav Garg","Nicolas Heess","Ágoston Weisz","Cassidy Hardin","Bartek Wydrowski","Ben Coleman","Karina Zainullina","Pankaj Joshi","Alessandro Epasto","Terry Spitz","Binbin Xiong","Kai Zhao","Arseniy Klimovskiy","Ivy Zheng","Johan Ferret","Itay Yona","Waleed Khawaja","Jean-Baptiste Lespiau","Maxim Krikun","Siamak Shakeri","Timothee Cour","Bonnie Li","Igor Krivokon","Dan Suh","Alex Hofer","Jad Al Abdallah","Nikita Putikhin","Oscar Akerlund","Silvio Lattanzi","Anurag Kumar","Shane Settle","Himanshu Srivastava","Folawiyo Campbell-Ajala","Edouard Rosseel","Mihai Dorin Istin","Nishanth Dikkala","Anand Rao","Nick Young","Kate Lin","Dhruva Bhaswar","Yiming Wang","Jaume Sanchez Elias","Kritika Muralidharan","James Keeling","Dayou Du","Siddharth Gopal","Gregory Dibb","Charles Blundell","Manolis Delakis","Jacky Liang","Marco Tulio Ribeiro","Georgi Karadzhov","Guillermo Garrido","Ankur Bapna","Jiawei Cao","Adam Sadovsky","Pouya Tafti","Arthur Guez","Coline Devin","Yixian Di","Jinwei Xing"," Chuqiao"," Xu","Hanzhao Lin","Chun-Te Chu","Sameera Ponda","Wesley Helmholz","Fan Yang","Yue Gao","Sara Javanmardi","Wael Farhan","Alex Ramirez","Ricardo Figueira","Khe Chai Sim","Yuval Bahat","Ashwin Vaswani","Liangzhe Yuan","Gufeng Zhang","Leland Rechis","Hanjun Dai","Tayo Oguntebi","Alexandra Cordell","Eugénie Rives","Kaan Tekelioglu","Naveen Kumar","Bing Zhang","Aurick Zhou","Nikolay Savinov","Andrew Leach","Alex Tudor","Sanjay Ganapathy","Yanyan Zheng","Mirko Rossini","Vera Axelrod","Arnaud Autef","Yukun Zhu","Zheng Zheng","Mingda Zhang","Baochen Sun","Jie Ren","Nenad Tomasev","Nithish Kannen","Amer Sinha","Charles Chen","Louis O'Bryan","Alex Pak","Aditya Kusupati","Weel Yang","Deepak Ramachandran","Patrick Griffin","Seokhwan Kim","Philipp Neubeck","Craig Schiff","Tammo Spalink","Mingyang Ling","Arun Nair","Ga-Young Joung","Linda Deng","Avishkar Bhoopchand","Lora Aroyo","Tom Duerig","Jordan Griffith","Gabe Barth-Maron","Jake Ades","Alex Haig","Ankur Taly","Yunting Song","Paul Michel","Dave Orr","Dean Weesner","Corentin Tallec","Carrie Grimes Bostock","Paul Niemczyk","Andy Twigg","Mudit Verma","Rohith Vallu","Henry Wang","Marco Gelmi","Kiranbir Sodhia","Aleksandr Chuklin","Omer Goldman","Jasmine George","Liang Bai","Kelvin Zhang","Petar Sirkovic","Efrat Nehoran","Golan Pundak","Jiaqi Mu","Alice Chen","Alex Greve","Paulo Zacchello","David Amos","Heming Ge","Eric Noland","Colton Bishop","Jeffrey Dudek","Youhei Namiki","Elena Buchatskaya","Jing Li","Dorsa Sadigh","Masha Samsikova","Dan Malkin","Damien Vincent","Robert David","Rob Willoughby","Phoenix Meadowlark","Shawn Gao","Yan Li","Raj Apte","Amit Jhindal","Stein Xudong Lin","Alex Polozov","Zhicheng Wang","Tomas Mery","Anirudh GP","Varun Yerram","Sage Stevens","Tianqi Liu","Noah Fiedel","Charles Sutton","Matthew Johnson","Xiaodan Song","Kate Baumli","Nir Shabat","Muqthar Mohammad","Hao Liu","Marco Selvi","Yichao Zhou","Mehdi Hafezi Manshadi","Chu-ling Ko","Anthony Chen","Michael Bendersky","Jorge Gonzalez Mendez","Nisarg Kothari","Amir Zandieh","Yiling Huang","Daniel Andor","Ellie Pavlick","Idan Brusilovsky","Jitendra Harlalka","Sally Goldman","Andrew Lampinen","Guowang Li","Asahi Ushio","Somit Gupta","Lei Zhang","Chuyuan Kelly Fu","Madhavi Sewak","Timo Denk","Jed Borovik","Brendan Jou","Avital Zipori","Prateek Jain","Junwen Bai","Thang Luong","Jonathan Tompson","Alice Li","Li Liu","George Powell","Jiajun Shen","Alex Feng","Grishma Chole","Da Yu","Yinlam Chow","Tongxin Yin","Eric Malmi","Kefan Xiao","Yash Pande","Shachi Paul","Niccolò Dal Santo","Adil Dostmohamed","Sergio Guadarrama","Aaron Phillips","Thanumalayan Sankaranarayana Pillai","Gal Yona","Amin Ghafouri","Preethi Lahoti","Benjamin Lee","Dhruv Madeka","Eren Sezener","Simon Tokumine","Adrian Collister","Nicola De Cao","Richard Shin","Uday Kalra","Parker Beak","Emily Nottage","Ryo Nakashima","Ivan Jurin","Vikash Sehwag","Meenu Gaba","Junhao Zeng","Kevin R. McKee","Fernando Pereira","Tamar Yakar","Amayika Panda","Arka Dhar","Peilin Zhong","Daniel Sohn","Mark Brand","Lars Lowe Sjoesund","Viral Carpenter","Sharon Lin","Shantanu Thakoor","Marcus Wainwright","Ashwin Chaugule","Pranesh Srinivasan","Muye Zhu","Bernett Orlando","Jack Weber","Ayzaan Wahid","Gilles Baechler","Apurv Suman","Jovana Mitrović","Gabe Taubman","Honglin Yu","Helen King","Josh Dillon","Cathy Yip","Dhriti Varma","Tomas Izo","Levent Bolelli","Borja De Balle Pigem","Julia Di Trapani","Fotis Iliopoulos","Adam Paszke","Nishant Ranka","Joe Zou","Francesco Pongetti","Jed McGiffin","Alex Siegman","Rich Galt","Ross Hemsley","Goran Žužić","Victor Carbune","Tao Li","Myle Ott","Félix de Chaumont Quitry","David Vilar Torres","Yuri Chervonyi","Tomy Tsai","Prem Eruvbetine","Samuel Yang","Matthew Denton","Jake Walker","Slavica Andačić","Idan Heimlich Shtacher","Vittal Premachandran","Harshal Tushar Lehri","Cip Baetu","Damion Yates","Lampros Lamprou","Mariko Iinuma","Ioana Mihailescu","Ben Albrecht","Shachi Dave","Susie Sargsyan","Bryan Perozzi","Lucas Manning","Chiyuan Zhang","Denis Vnukov","Igor Mordatch","Raia Hadsell Wolfgang Macherey","Ryan Kappedal","Jim Stephan","Aditya Tripathi","Klaus Macherey","Jun Qian","Abhishek Bhowmick","Shekoofeh Azizi","Rémi Leblond","Shiva Mohan Reddy Garlapati","Timothy Knight","Matthew Wiethoff","Wei-Chih Hung","Anelia Angelova","Georgios Evangelopoulos","Pawel Janus","Dimitris Paparas","Matthew Rahtz","Ken Caluwaerts","Vivek Sampathkumar","Daniel Jarrett","Shadi Noghabi","Antoine Miech","Chak Yeung","Geoff Clark","Henry Prior","Fei Zheng","Jean Pouget-Abadie","Indro Bhattacharya","Kalpesh Krishna","Will Bishop","Zhe Yuan","Yunxiao Deng","Ashutosh Sathe","Kacper Krasowiak","Ciprian Chelba","Cho-Jui Hsieh","Kiran Vodrahalli","Buhuang Liu","Thomas Köppe","Amr Khalifa","Lubo Litchev","Pichi Charoenpanit","Reed Roberts","Sachin Yadav","Yasumasa Onoe","Desi Ivanov","Megha Mohabey","Vighnesh Birodkar","Nemanja Rakićević","Pierre Sermanet","Vaibhav Mehta","Krishan Subudhi","Travis Choma","Will Ng","Luheng He","Kathie Wang","Tasos Kementsietsidis","Shane Gu","Mansi Gupta","Andrew Nystrom","Mehran Kazemi","Timothy Chung","Nacho Cano","Nikhil Dhawan","Yufei Wang","Jiawei Xia","Trevor Yacovone","Eric Jia","Mingqing Chen","Simeon Ivanov","Ashrith Sheshan","Sid Dalmia","Paweł Stradomski","Pengcheng Yin","Salem Haykal","Congchao Wang","Dennis Duan","Neslihan Bulut","Greg Kochanski","Liam MacDermed","Namrata Godbole","Shitao Weng","Jingjing Chen","Rachana Fellinger","Ramin Mehran","Daniel Suo","Hisham Husain","Tong He","Kaushal Patel","Joshua Howland","Randall Parker","Kelvin Nguyen","Sharath Maddineni","Chris Rawles","Mina Khan","Shlomi Cohen-Ganor","Amol Mandhane","Xinyi Wu","Chenkai Kuang","Iulia Comşa","Ramya Ganeshan","Hanie Sedghi","Adam Bloniarz","Nuo Wang Pierse","Anton Briukhov","Petr Mitrichev","Anita Gergely","Serena Zhan","Allan Zhou","Nikita Saxena","Eva Lu","Josef Dean","Ashish Gupta","Nicolas Perez-Nieves","Renjie Wu","Cory McLean","Wei Liang","Disha Jindal","Anton Tsitsulin","Wenhao Yu","Kaiz Alarakyia","Tom Schaul","Piyush Patil","Peter Sung","Elijah Peake","Hongkun Yu","Feryal Behbahani","JD Co-Reyes","Alan Ansell","Sean Sun","Clara Barbu","Jonathan Lee","Seb Noury","James Allingham","Bilal Piot","Mohit Sharma","Christopher Yew","Ivan Korotkov","Bibo Xu","Demetra Brady","Goran Petrovic","Shibl Mourad","Claire Cui","Aditya Gupta","Parker Schuh","Saarthak Khanna","Anna Goldie","Abhinav Arora","Vadim Zubov","Amy Stuart","Mark Epstein","Yun Zhu","Jianqiao Liu","Yury Stuken","Ziyue Wang","Karolis Misiunas","Dee Guo","Ashleah Gill","Ale Hartman","Zaid Nabulsi","Aurko Roy","Aleksandra Faust","Jason Riesa","Ben Withbroe","Mengchao Wang","Marco Tagliasacchi","Andreea Marzoca","James Noraky","Serge Toropov","Malika Mehrotra","Bahram Raad","Sanja Deur","Steve Xu","Marianne Monteiro","Zhongru Wu","Yi Luan","Sam Ritter","Nick Li","Håvard Garnes","Yanzhang He","Martin Zlocha","Jifan Zhu","Matteo Hessel","Will Wu","Spandana Raj Babbula","Chizu Kawamoto","Yuanzhen Li","Mehadi Hassen","Yan Wang","Brian Wieder","James Freedman","Yin Zhang","Xinyi Bai","Tianli Yu","David Reitter","XiangHai Sheng","Mateo Wirth","Aditya Kini","Dima Damen","Mingcen Gao","Rachel Hornung","Michael Voznesensky","Brian Roark","Adhi Kuncoro","Yuxiang Zhou","Rushin Shah","Anthony Brohan","Kuangyuan Chen","James Wendt","David Rim","Paul Kishan Rubenstein","Jonathan Halcrow","Michelle Liu","Ty Geri","Yunhsuan Sung","Jane Shapiro","Shaan Bijwadia","Chris Duvarney","Christina Sorokin","Paul Natsev","Reeve Ingle","Pramod Gupta","Young Maeng","Ndaba Ndebele","Kexin Zhu","Valentin Anklin","Katherine Lee","Yuan Liu","Yaroslav Akulov","Shaleen Gupta","Guolong Su","Flavien Prost","Tianlin Liu","Vitaly Kovalev","Pol Moreno","Martin Scholz","Sam Redmond","Zongwei Zhou","Alex Castro-Ros","André Susano Pinto","Dia Kharrat","Michal Yarom","Rachel Saputro","Jannis Bulian","Ben Caine","Ji Liu","Abbas Abdolmaleki","Shariq Iqbal","Tautvydas Misiunas","Mikhail Sirotenko","Shefali Garg","Guy Bensky","Huan Gui","Xuezhi Wang","Raphael Koster","Mike Bernico","Da Huang","Romal Thoppilan","Trevor Cohn","Ben Golan","Wenlei Zhou","Andrew Rosenberg","Markus Freitag","Tynan Gangwani","Vincent Tsang","Anand Shukla","Xiaoqi Ren","Minh Giang","Chi Zou","Andre Elisseeff","Charline Le Lan","Dheeru Dua","Shuba Lall","Pranav Shyam","Frankie Garcia","Sarah Nguyen","Michael Guzman","AJ Maschinot","Marcello Maggioni","Ming-Wei Chang","Karol Gregor","Lotte Weerts","Kumaran Venkatesan","Bogdan Damoc","Leon Liu","Jan Wassenberg","Lewis Ho","Becca Roelofs","Majid Hadian","François-Xavier Aubet","Yu Liang","Sami Lachgar","Danny Karmon","Yong Cheng","Amelio Vázquez-Reina","Angie Chen","Zhuyun Dai","Andy Brock","Shubham Agrawal","Chenxi Pang","Peter Garst","Mariella Sanchez-Vargas","Ivor Rendulic","Aditya Ayyar","Andrija Ražnatović","Olivia Ma","Roopali Vij","Neha Sharma","Ashwin Balakrishna","Bingyuan Liu","Ian Mackinnon","Sorin Baltateanu","Petra Poklukar","Gabriel Ibagon","Colin Ji","Hongyang Jiao","Isaac Noble","Wojciech Stokowiec","Zhihao Li","Jeff Dean","David Lindner","Mark Omernick","Kristen Chiafullo","Mason Dimarco","Vitor Rodrigues","Vittorio Selo","Garrett Honke"," Xintian"," Wu","Wei He","Adam Hillier","Anhad Mohananey","Vihari Piratla","Chang Ye","Chase Malik","Sebastian Riedel","Samuel Albanie","Zi Yang","Kenny Vassigh","Maria Bauza","Sheng Li","Yiqing Tao","Nevan Wichers","Andrii Maksai","Abe Ittycheriah","Ross Mcilroy","Bryan Seybold","Noah Goodman","Romina Datta","Steven M. Hernandez","Tian Shi","Yony Kochinski","Anna Bulanova","Ken Franko","Mikita Sazanovich","Nicholas FitzGerald","Praneeth Kacham","Shubha Srinivas Raghvendra","Vincent Hellendoorn","Alexander Grushetsky","Julian Salazar","Angeliki Lazaridou","Jason Chang","Jan-Thorsten Peter","Sushant Kafle","Yann Dauphin","Abhishek Rao","Filippo Graziano","Izhak Shafran","Yuguo Liao","Tianli Ding","Geng Yan","Grace Chu","Zhao Fu","Vincent Roulet","Gabriel Rasskin","Duncan Williams","Shahar Drath","Alex Mossin","Raphael Hoffmann","Jordi Orbay","Francesco Bertolini","Hila Sheftel","Justin Chiu","Siyang Xue","Yuheng Kuang","Ferjad Naeem","Swaroop Nath","Nana Nti","Phil Culliton","Kashyap Krishnakumar","Michael Isard","Pei Sun","Ayan Chakrabarti","Nathan Clement","Regev Cohen","Arissa Wongpanich","GS Oh","Ashwin Murthy","Hao Zheng","Jessica Hamrick","Oskar Bunyan","Suhas Ganesh","Nitish Gupta","Roy Frostig","John Wieting","Yury Malkov","Pierre Marcenac"," Zhixin"," Lai","Xiaodan Tang","Mohammad Saleh","Fedir Zubach","Chinmay Kulkarni","Huanjie Zhou","Vicky Zayats","Nan Ding","Anshuman Tripathi","Arijit Pramanik","Patrik Zochbauer","Harish Ganapathy","Vedant Misra","Zach Behrman","Hugo Vallet","Mingyang Zhang","Mukund Sridhar","Ye Jin","Mohammad Babaeizadeh","Siim Põder","Megha Goel","Divya Jain","Tajwar Nasir","Shubham Mittal","Tim Dozat","Diego Ardila","Aliaksei Severyn","Fabio Pardo","Sammy Jerome","Siyang Qin","Louis Rouillard","Amir Yazdanbakhsh","Zizhao Zhang","Shivani Agrawal","Kaushik Shivakumar","Caden Lu","Praveen Kallakuri","Rachita Chhaparia","Kanishka Rao","Charles Kwong","Asya Fadeeva","Shitij Nigam","Yan Virin","Yuan Zhang","Balaji Venkatraman","Beliz Gunel","Marc Wilson","Huiyu Wang","Abhinav Gupta","Xiaowei Xu","Adrien Ali Taïga","Kareem Mohamed","Doug Fritz","Daniel Rodriguez","Zoubin Ghahramani","Harry Askham","Lior Belenki","James Zhao","Rahul Gupta","Krzysztof Jastrzębski","Takahiro Kosakai","Kaan Katircioglu","Jon Schneider","Rina Panigrahy","Konstantinos Bousmalis","Peter Grabowski","Prajit Ramachandran","Chaitra Hegde","Mihaela Rosca","Angelo Scorza Scarpati","Kyriakos Axiotis","Ying Xu","Zach Gleicher","Assaf Hurwitz Michaely","Mandar Sharma","Sanil Jain","Christoph Hirnschall","Tal Marian","Xuhui Jia","Kevin Mather","Kilol Gupta","Linhai Qiu","Nigamaa Nayakanti","Lucian Ionita","Steven Zheng","Lucia Loher","Kurt Shuster","Igor Petrovski","Roshan Sharma","Rahma Chaabouni","Angel Yeh","James An","Arushi Gupta","Steven Schwarcz","Seher Ellis","Sam Conway-Rahman","Javier Snaider","Alex Zhai","James Atwood","Daniel Golovin","Liqian Peng","Te I","Vivian Xia","Salvatore Scellato","Mahan Malihi","Arthur Bražinskas","Vlad-Doru Ion","Younghoon Jun","James Swirhun","Soroosh Mariooryad","Jiao Sun","Steve Chien","Rey Coaguila","Ariel Brand","Yi Gao","Tom Kwiatkowski","Roee Aharoni","Cheng-Chun Lee","Mislav Žanić","Yichi Zhang","Dan Ethier","Vitaly Nikolaev","Pranav Nair","Yoav Ben Shalom","Hen Fitoussi","Jai Gupta","Hongbin Liu","Dee Cattle","Tolga Bolukbasi","Ben Murdoch","Fantine Huot","Yin Li","Chris Hahn","Urvashi Khandelwal","Frederik Benzing","Arthur Conmy","Andrey Simanovsky","Françoise Beaufays","Eugene Weinstein","Tongzhou Chen","Luke Leonhard","Bhuvana Ramabhadran"],"pdf_url":"https://arxiv.org/pdf/2507.06261v3.pdf","comment":"72 pages, 17 figures"},{"id":"http://arxiv.org/abs/2312.16054v2","updated":"2025-07-17T08:13:07Z","published":"2023-12-26T13:54:00Z","title":"A Logically Consistent Chain-of-Thought Approach for Stance Detection","summary":"  Zero-shot stance detection (ZSSD) aims to detect stances toward unseen\ntargets. Incorporating background knowledge to enhance transferability between\nseen and unseen targets constitutes the primary approach of ZSSD. However,\nthese methods often struggle with a knowledge-task disconnect and lack logical\nconsistency in their predictions. To address these issues, we introduce a novel\napproach named Logically Consistent Chain-of-Thought (LC-CoT) for ZSSD, which\nimproves stance detection by ensuring relevant and logically sound knowledge\nextraction. LC-CoT employs a three-step process. Initially, it assesses whether\nsupplementary external knowledge is necessary. Subsequently, it uses API calls\nto retrieve this knowledge, which can be processed by a separate LLM. Finally,\na manual exemplar guides the LLM to infer stance categories, using an if-then\nlogical structure to maintain relevance and logical coherence. This structured\napproach to eliciting background knowledge enhances the model's capability,\noutperforming traditional supervised methods without relying on labeled data.\n","authors":["Bowen Zhang","Daijun Ding","Liwen Jing","Hu Huang"],"pdf_url":"https://arxiv.org/pdf/2312.16054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.21773v2","updated":"2025-07-17T08:03:43Z","published":"2025-04-30T16:17:53Z","title":"MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced\n  Knowledge Boundary Awareness","summary":"  With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision.\n","authors":["Junsheng Huang","Zhitao He","Yucheng Huang","Sandeep Polisetty","Qingyun Wang","May Fung"],"pdf_url":"https://arxiv.org/pdf/2504.21773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08898v3","updated":"2025-07-17T08:01:44Z","published":"2025-07-11T05:15:35Z","title":"SEALGuard: Safeguarding the Multilingual Conversations in Southeast\n  Asian Languages for LLM Software Systems","summary":"  Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. We release our pre-trained model and\nbenchmark at https://github.com/awsm-research/SEALGuard to support further\nresearch.\n","authors":["Wenliang Shan","Michael Fu","Rui Yang","Chakkrit Tantithamthavorn"],"pdf_url":"https://arxiv.org/pdf/2507.08898v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12838v1","updated":"2025-07-17T06:55:15Z","published":"2025-07-17T06:55:15Z","title":"Are Knowledge and Reference in Multilingual Language Models\n  Cross-Lingually Consistent?","summary":"  Cross-lingual consistency should be considered to assess cross-lingual\ntransferability, maintain the factuality of the model knowledge across\nlanguages, and preserve the parity of language model performance. We are thus\ninterested in analyzing, evaluating, and interpreting cross-lingual consistency\nfor factual knowledge. We examine code-mixed coreferential statements conveyed\nidentical knowledge across languages to study cross-lingual knowledge\nconsistency. We use some interpretability approaches to analyze the behavior of\na model in cross-lingual contexts, discovering that multilingual models show\ndifferent levels of consistency, subject to language families, linguistic\nfactors, and a bottleneck in cross-lingual consistency on a particular layer.\nIn addition, we evaluate common strategies aimed at improving multilingual\nperformance to observe whether these strategies can improve knowledge\nconsistency at the same time. While knowledge is not cross-lingual consistency\nin many cases, code-switching training and cross-lingual word alignment\nobjectives show the most promising results, emphasizing the noteworthiness of\ncross-lingual alignment supervision and code-switching training for both\nmultilingual performance and cross-lingual consistency enhancement.\n","authors":["Xi Ai","Mahardika Krisna Ihsani","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2507.12838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12820v1","updated":"2025-07-17T06:24:20Z","published":"2025-07-17T06:24:20Z","title":"Emotional Support with LLM-based Empathetic Dialogue Generation","summary":"  Emotional Support Conversation (ESC) aims to provide empathetic and effective\nemotional assistance through dialogue, addressing the growing demand for mental\nhealth support. This paper presents our solution for the NLPCC 2025 Task 8 ESC\nevaluation, where we leverage large-scale language models enhanced by prompt\nengineering and finetuning techniques. We explore both parameter-efficient\nLow-Rank Adaptation and full-parameter fine-tuning strategies to improve the\nmodel's ability to generate supportive and contextually appropriate responses.\nOur best model ranked second in the competition, highlighting the potential of\ncombining LLMs with effective adaptation methods for ESC tasks. Future work\nwill focus on further enhancing emotional understanding and response\npersonalization to build more practical and reliable emotional support systems.\n","authors":["Shiquan Wang","Ruiyu Fang","Zhongjiang He","Shuangyong Song","Yongxiang Li"],"pdf_url":"https://arxiv.org/pdf/2507.12820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12808v1","updated":"2025-07-17T05:48:45Z","published":"2025-07-17T05:48:45Z","title":"Large Language Models' Internal Perception of Symbolic Music","summary":"  Large language models (LLMs) excel at modeling relationships between strings\nin natural language and have shown promise in extending to other symbolic\ndomains like coding or mathematics. However, the extent to which they\nimplicitly model symbolic music remains underexplored. This paper investigates\nhow LLMs represent musical concepts by generating symbolic music data from\ntextual prompts describing combinations of genres and styles, and evaluating\ntheir utility through recognition and generation tasks. We produce a dataset of\nLLM-generated MIDI files without relying on explicit musical training. We then\ntrain neural networks entirely on this LLM-generated MIDI dataset and perform\ngenre and style classification as well as melody completion, benchmarking their\nperformance against established models. Our results demonstrate that LLMs can\ninfer rudimentary musical structures and temporal relationships from text,\nhighlighting both their potential to implicitly encode musical patterns and\ntheir limitations due to a lack of explicit musical context, shedding light on\ntheir generative capabilities for symbolic music.\n","authors":["Andrew Shin","Kunitake Kaneko"],"pdf_url":"https://arxiv.org/pdf/2507.12808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12806v1","updated":"2025-07-17T05:46:27Z","published":"2025-07-17T05:46:27Z","title":"MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models","summary":"  The rapid rise of Large Language Models (LLMs)-based intelligent agents\nunderscores the need for robust, scalable evaluation frameworks. Existing\nmethods rely on static benchmarks and labor-intensive data collection, limiting\npractical assessment. We introduce \\oursystemname, an open-source Model Context\nProtocol (MCP)-based framework that automates end-to-end task generation and\ndeep evaluation of LLM agents across diverse domains. MCPEval standardizes\nmetrics, seamlessly integrates with native agent tools, and eliminates manual\neffort in building evaluation pipelines. Empirical results across five\nreal-world domains show its effectiveness in revealing nuanced, domain-specific\nperformance. We publicly release MCPEval\nhttps://github.com/SalesforceAIResearch/MCPEval to promote reproducible and\nstandardized LLM agent evaluation.\n","authors":["Zhiwei Liu","Jielin Qiu","Shiyu Wang","Jianguo Zhang","Zuxin Liu","Roshan Ram","Haolin Chen","Weiran Yao","Huan Wang","Shelby Heinecke","Silvio Savarese","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2507.12806v1.pdf","comment":"https://github.com/SalesforceAIResearch/MCPEval"},{"id":"http://arxiv.org/abs/2507.12805v1","updated":"2025-07-17T05:46:08Z","published":"2025-07-17T05:46:08Z","title":"PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for\n  Large-Scale Genomics Database","summary":"  Learning-based lossless compressors play a crucial role in large-scale\ngenomic database backup, storage, transmission, and management. However, their\n1) inadequate compression ratio, 2) low compression \\& decompression\nthroughput, and 3) poor compression robustness limit their widespread adoption\nand application in both industry and academia. To solve those challenges, we\npropose a novel \\underline{P}arallel \\underline{M}ulti-\\underline{K}nowledge\n\\underline{L}earning-based \\underline{C}ompressor (PMKLC) with four crucial\ndesigns: 1) We propose an automated multi-knowledge learning-based compression\nframework as compressors' backbone to enhance compression ratio and robustness;\n2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression\nthroughput and computing resource usage; 3) we introduce data block\npartitioning and Step-wise Model Passing (SMP) mechanisms for parallel\nacceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet\nthe complex application scenarios, where the former runs on a\nresource-constrained single GPU and the latter is multi-GPU accelerated. We\nbenchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15\nreal-world datasets with different species and data sizes. Compared to\nbaselines on the testing datasets, PMKLC-S/M achieve the average compression\nratio improvement up to 73.609\\% and 73.480\\%, the average throughput\nimprovement up to 3.036$\\times$ and 10.710$\\times$, respectively. Besides,\nPMKLC-S/M also achieve the best robustness and competitive memory cost,\nindicating its greater stability against datasets with different probability\ndistribution perturbations, and its strong ability to run on memory-constrained\ndevices.\n","authors":["Hui Sun","Yanfeng Ding","Liping Yi","Huidong Ma","Gang Wang","Xiaoguang Liu","Cheng Zhong","Wentong Cai"],"pdf_url":"https://arxiv.org/pdf/2507.12805v1.pdf","comment":"Accepted via KDD-25"},{"id":"http://arxiv.org/abs/2506.20495v2","updated":"2025-07-17T05:31:07Z","published":"2025-06-25T14:41:13Z","title":"ReCode: Updating Code API Knowledge with Reinforcement Learning","summary":"  Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.\n","authors":["Haoze Wu","Yunzhi Yao","Wenhao Yu","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.20495v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.18699v2","updated":"2025-07-17T05:29:09Z","published":"2025-02-25T23:22:12Z","title":"MPO: An Efficient Post-Processing Framework for Mixing Diverse\n  Preference Alignment","summary":"  Reinforcement Learning from Human Feedback (RLHF) has shown promise in\naligning large language models (LLMs). Yet its reliance on a singular reward\nmodel often overlooks the diversity of human preferences. Recent approaches\naddress this limitation by leveraging multi-dimensional feedback to fine-tune\ncorresponding reward models and train LLMs using reinforcement learning.\nHowever, the process is costly and unstable, especially given the competing and\nheterogeneous nature of human preferences. In this paper, we propose Mixing\nPreference Optimization (MPO), a post-processing framework for aggregating\nsingle-objective policies as an alternative to both multi-objective RLHF\n(MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it\nlog-linearly combines existing policies into a unified one with the weight of\neach policy computed via a batch stochastic mirror descent. Empirical results\ndemonstrate that MPO achieves balanced performance across diverse preferences,\noutperforming or matching existing models with significantly reduced\ncomputational costs.\n","authors":["Tianze Wang","Dongnan Gui","Yifan Hu","Shuhang Lin","Linjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.18699v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2507.12782v1","updated":"2025-07-17T04:48:54Z","published":"2025-07-17T04:48:54Z","title":"Learning Robust Negation Text Representations","summary":"  Despite rapid adoption of autoregressive large language models, smaller text\nencoders still play an important role in text understanding tasks that require\nrich contextualized representations. Negation is an important semantic function\nthat is still not properly captured by such methods, affecting many downstream\napplications relying on text embeddings. We propose a strategy to improve\nnegation robustness of text encoders, by distilling data from large language\nmodels using diverse patterns of negation and hedging. We adopt a standard\ncontrastive learning strategy to finetune a strong BERT-based model, and\nobserve large improvement in negation understanding capabilities while\nmaintaining competitive performance on general benchmarks. In addition, we also\nshow that our method can be adapted to LLMs, leading to improved performance on\nnegation benchmarks.\n","authors":["Thinh Hung Truong","Karin Verspoor","Trevor Cohn","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2507.12782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12774v1","updated":"2025-07-17T04:31:55Z","published":"2025-07-17T04:31:55Z","title":"A Comprehensive Survey of Electronic Health Record Modeling: From Deep\n  Learning Approaches to Large Language Models","summary":"  Artificial intelligence (AI) has demonstrated significant potential in\ntransforming healthcare through the analysis and modeling of electronic health\nrecords (EHRs). However, the inherent heterogeneity, temporal irregularity, and\ndomain-specific nature of EHR data present unique challenges that differ\nfundamentally from those in vision and natural language tasks. This survey\noffers a comprehensive overview of recent advancements at the intersection of\ndeep learning, large language models (LLMs), and EHR modeling. We introduce a\nunified taxonomy that spans five key design dimensions: data-centric\napproaches, neural architecture design, learning-focused strategies, multimodal\nlearning, and LLM-based modeling systems. Within each dimension, we review\nrepresentative methods addressing data quality enhancement, structural and\ntemporal representation, self-supervised learning, and integration with\nclinical knowledge. We further highlight emerging trends such as foundation\nmodels, LLM-driven clinical agents, and EHR-to-text translation for downstream\nreasoning. Finally, we discuss open challenges in benchmarking, explainability,\nclinical alignment, and generalization across diverse clinical settings. This\nsurvey aims to provide a structured roadmap for advancing AI-driven EHR\nmodeling and clinical decision support. For a comprehensive list of EHR-related\nmethods, kindly refer to https://survey-on-tabular-data.github.io/.\n","authors":["Weijieying Ren","Jingxi Zhu","Zehao Liu","Tianxiang Zhao","Vasant Honavar"],"pdf_url":"https://arxiv.org/pdf/2507.12774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03106v4","updated":"2025-07-17T04:08:03Z","published":"2025-06-03T17:39:02Z","title":"Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback","summary":"  Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of spontaneous self-reflection, and\npersistent failures. We then demonstrate that RL-finetuned models, even after\nexhibiting performance plateaus, can generate correct refinements on\npersistently failed problems by leveraging natural language feedback in the\nform of critiques. Building on this insight, we propose Critique-GRPO, an\nonline RL framework that integrates both natural language and numerical\nfeedback for effective policy optimization. Critique-GRPO enables LLMs to learn\nfrom initial responses and critique-guided self-refinements simultaneously\nwhile maintaining exploration. Additionally, we employ a shaping function to\namplify learning from correct, especially unfamiliar, refinements and penalize\nincorrect ones. Extensive experiments with Qwen2.5-7B-Base,\nQwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently\noutperforms supervised learning and RL-based fine-tuning methods across eight\nchallenging mathematical, STEM, and general reasoning tasks, improving average\npass@1 scores by approximately 4.4% and 3.8% on Qwen2.5-7B-Base and Qwen3-8B,\nrespectively. Notably, Critique-GRPO enables effective self-improvement through\nself-critiquing and weak-to-strong generalization, achieving consistent gains\nover GRPO, such as 16.7% and 10.0% pass@1 improvements on AIME 2024,\nrespectively.\n","authors":["Xiaoying Zhang","Hao Sun","Yipeng Zhang","Kaituo Feng","Chaochao Lu","Chao Yang","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2506.03106v4.pdf","comment":"52 pages, updated with new experimental results and implementation\n  details"},{"id":"http://arxiv.org/abs/2507.12769v1","updated":"2025-07-17T04:01:28Z","published":"2025-07-17T04:01:28Z","title":"Synergy: End-to-end Concept Model","summary":"  In this paper, we present Synergy, a language model that bridges different\nlevels of abstraction in an end-to-end fashion through a learned routing\nmechanism. Focusing on low-level linguistic abstraction, we trained our model\nas a byte-level language model. Our model spontaneously learns to tokenize\nbytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)\ntokenizers while keeping comparable performance. By comparing with Llama3, we\nobserved an advantage of Synergy under the same model scale and training\ndataset size. Further studies show that the middle part (the higher abstraction\npart) of our model performs better when positional encodings are removed,\nsuggesting the emergence of position-independent concepts. These findings\ndemonstrate the feasibility of tokenizer-free architectures, paving the way for\nmore robust and flexible pipelines.\n","authors":["Keli Zheng","Zerong Xie"],"pdf_url":"https://arxiv.org/pdf/2507.12769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.21582v2","updated":"2025-07-17T03:52:15Z","published":"2025-06-17T05:24:58Z","title":"VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation\n  of Text Analytics with Intelligent Agents","summary":"  Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems.\n","authors":["Sam Yu-Te Lee","Chengyang Ji","Shicheng Wen","Lifu Huang","Dongyu Liu","Kwan-Liu Ma"],"pdf_url":"https://arxiv.org/pdf/2506.21582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12759v1","updated":"2025-07-17T03:31:36Z","published":"2025-07-17T03:31:36Z","title":"Logit Arithmetic Elicits Long Reasoning Capabilities Without Training","summary":"  Large reasoning models (LRMs) can do complex reasoning via long\nchain-of-thought (CoT) involving cognitive strategies such as backtracking and\nself-correction. Recent studies suggest that some models inherently possess\nthese long reasoning abilities, which may be unlocked via extra training. Our\nwork first investigates whether we can elicit such behavior without any\ntraining. To this end, we propose a decoding-time approach, ThinkLogit, which\nutilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for\nlong reasoning using a substantially smaller model as guider. We then show that\nwe can further boost performance by training the guider model with preference\noptimization over correct/incorrect reasoning pairs sampled from both the\ntarget and guider model -- a setup we refer to as ThinkLogit-DPO. Our\nexperiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative\nimprovement in pass@1 by 26% and 29%, respectively, over four mathematical\ndatasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model\n21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills\nacquired through reinforcement learning, improving pass@1 by 13% relative\ncompared to the Qwen2.5-32B base model. Our work presents a\ncomputationally-efficient method to elicit long reasoning in large models with\nminimal or no additional training.\n","authors":["Yunxiang Zhang","Muhammad Khalifa","Lechen Zhang","Xin Liu","Ayoung Lee","Xinliang Frederick Zhang","Farima Fatahi Bayat","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2507.12759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12732v1","updated":"2025-07-17T02:27:45Z","published":"2025-07-17T02:27:45Z","title":"Strategy Adaptation in Large Language Model Werewolf Agents","summary":"  This study proposes a method to improve the performance of Werewolf agents by\nswitching between predefined strategies based on the attitudes of other players\nand the context of conversations. While prior works of Werewolf agents using\nprompt engineering have employed methods where effective strategies are\nimplicitly defined, they cannot adapt to changing situations. In this research,\nwe propose a method that explicitly selects an appropriate strategy based on\nthe game context and the estimated roles of other players. We compare the\nstrategy adaptation Werewolf agents with baseline agents using implicit or\nfixed strategies and verify the effectiveness of our proposed method.\n","authors":["Fuya Nakamori","Yin Jou Huang","Fei Cheng"],"pdf_url":"https://arxiv.org/pdf/2507.12732v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.12724v1","updated":"2025-07-17T02:02:54Z","published":"2025-07-17T02:02:54Z","title":"TransEvalnia: Reasoning-based Evaluation and Ranking of Translations","summary":"  We present TransEvalnia, a prompting-based translation evaluation and ranking\nsystem that uses reasoning in performing its evaluations and ranking. This\nsystem presents fine-grained evaluations based on a subset of the\nMultidimensional Quality Metrics (https://themqm.org/), returns an assessment\nof which translation it deems the best, and provides numerical scores for the\nvarious dimensions and for the overall translation. We show that TransEvalnia\nperforms as well as or better than the state-of-the-art MT-Ranker (Moosa et al.\n2024) on our own English-Japanese data as well as several language pairs from\nvarious WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and\nQwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations\nreturned are deemed highly acceptable to human raters, and that the scores\nassigned to the translations by Sonnet, as well as other LLMs, correlate well\nwith scores assigned by the human raters. We also note the sensitivity of our\nsystem -- as well as MT-Ranker -- to the order in which the translations are\npresented, and we propose methods to address this position bias. All data,\nincluding the system's evaluation and reasoning, human assessments, as well as\ncode is released.\n","authors":["Richard Sproat","Tianyu Zhao","Llion Jones"],"pdf_url":"https://arxiv.org/pdf/2507.12724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12720v1","updated":"2025-07-17T01:55:41Z","published":"2025-07-17T01:55:41Z","title":"FLEXITOKENS: Flexible Tokenization for Evolving Language Models","summary":"  Language models (LMs) are challenging to adapt to new data distributions by\nsimple finetuning. This is due to the rigidity of their subword tokenizers,\nwhich typically remain unchanged during adaptation. This inflexibility often\nleads to inefficient tokenization, causing overfragmentation of\nout-of-distribution domains, unseen languages, or scripts. In this work, we\ndevelop byte-level LMs with learnable tokenizers to make tokenization adaptive.\nOur models include a submodule that learns to predict boundaries between the\ninput byte sequence, encoding it into variable-length segments. Existing\ntokenizer-free methods train this boundary predictor using an auxiliary loss\nthat enforces a fixed compression rate across the training corpus, introducing\na new kind of rigidity. We propose FLEXITOKENS, a simplified training objective\nthat enables significantly greater flexibility during adaptation. Evaluating\nacross multiple multilingual benchmarks, morphologically diverse tasks, and\ndomains, we demonstrate that FLEXITOKENS consistently reduces token\nover-fragmentation and achieves up to 10\\% improvements on downstream task\nperformance compared to subword and other gradient-based tokenizers. Code and\ndata for our experiments will be released at\nhttps://github.com/owos/flexitokens\n","authors":["Abraham Toluase Owodunni","Orevaoghene Ahia","Sachin Kumar"],"pdf_url":"https://arxiv.org/pdf/2507.12720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07919v2","updated":"2025-07-17T01:50:49Z","published":"2025-03-10T23:50:30Z","title":"BEARCUBS: A benchmark for computer-using web agents","summary":"  Modern web agents possess computer use abilities that allow them to interact\nwith webpages by sending commands to a virtual keyboard and mouse. While such\nagents have considerable potential to assist human users with complex tasks,\nevaluating their capabilities in real-world settings poses a major challenge.\nTo this end, we introduce BEARCUBS, a \"small but mighty\" benchmark of 111\ninformation-seeking questions designed to evaluate a web agent's ability to\nsearch, browse, and identify factual information from the web. Unlike prior web\nagent benchmarks, solving BEARCUBS requires (1) accessing live web content\nrather than synthetic or simulated pages, which captures the unpredictability\nof real-world web interactions; and (2) performing a broad range of multimodal\ninteractions (e.g., video understanding, 3D navigation) that cannot be bypassed\nvia text-based workarounds. Each question in BEARCUBS has a corresponding\nshort, unambiguous answer and a human-validated browsing trajectory, allowing\nfor transparent evaluation of agent performance and strategies. A human study\nconfirms that BEARCUBS questions are solvable but non-trivial (84.7% human\naccuracy), revealing domain knowledge gaps and overlooked details as common\nfailure points. By contrast, state-of-the-art computer-using agents\nunderperform, with the best-scoring system (OpenAI's Operator) reaching only\n23.4% accuracy. These results highlight critical areas for improvement,\nincluding reliable source selection and more powerful multimodal capabilities.\nTo facilitate future research, BEARCUBS will be updated periodically to replace\ninvalid or contaminated questions, keeping the benchmark fresh for future\ngenerations of web agents.\n","authors":["Yixiao Song","Katherine Thai","Chau Minh Pham","Yapei Chang","Mazin Nadaf","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2503.07919v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2503.12347v2","updated":"2025-07-17T01:39:41Z","published":"2025-03-16T04:00:32Z","title":"Synthesizing Privacy-Preserving Text Data via Finetuning without\n  Finetuning Billion-Scale LLMs","summary":"  Synthetic data offers a promising path to train models while preserving data\nprivacy. Differentially private (DP) finetuning of large language models (LLMs)\nas data generator is effective, but is impractical when computation resources\nare limited. Meanwhile, prompt-based methods such as private evolution depend\nheavily on the manual prompts, and ineffectively use private information in\ntheir iterative data selection process. To overcome these limitations, we\npropose CTCL (Data Synthesis with ConTrollability and CLustering), a novel\nframework for generating privacy-preserving synthetic data without extensive\nprompt engineering or billion-scale LLM finetuning. CTCL pretrains a\nlightweight 140M conditional generator and a clustering-based topic model on\nlarge-scale public data. To further adapt to the private domain, the generator\nis DP finetuned on private data for fine-grained textual information, while the\ntopic model extracts a DP histogram representing distributional information.\nThe DP generator then samples according to the DP histogram to synthesize a\ndesired number of data examples. Evaluation across five diverse domains\ndemonstrates the effectiveness of our framework, particularly in the strong\nprivacy regime. Systematic ablation validates the design of each framework\ncomponent and highlights the scalability of our approach.\n","authors":["Bowen Tan","Zheng Xu","Eric Xing","Zhiting Hu","Shanshan Wu"],"pdf_url":"https://arxiv.org/pdf/2503.12347v2.pdf","comment":"Code available at https://github.com/tanyuqian/synthetic-private-data"},{"id":"http://arxiv.org/abs/2409.05028v2","updated":"2025-07-17T01:36:27Z","published":"2024-09-08T08:46:05Z","title":"GUI Test Migration via Abstraction and Concretization","summary":"  GUI test migration aims to produce test cases with events and assertions to\ntest specific functionalities of a target app. Existing migration approaches\ntypically focus on the widget-mapping paradigm that maps widgets from source\napps to target apps. However, since different apps may implement the same\nfunctionality in different ways, direct mapping may result in incomplete or\nbuggy test cases, thus significantly impacting the effectiveness of testing\ntarget functionality and the practical applicability of migration approaches.\n  In this paper, we propose a new migration paradigm (i.e., the\nabstraction-concretization paradigm) that first abstracts the test logic for\nthe target functionality and then utilizes this logic to generate the concrete\nGUI test case. Furthermore, we introduce MACdroid, the first approach that\nmigrates GUI test cases based on this paradigm. Specifically, we propose an\nabstraction technique that utilizes source test cases from source apps\ntargeting the same functionality to extract a general test logic for that\nfunctionality. Then, we propose a concretization technique that utilizes the\ngeneral test logic to guide an LLM in generating the corresponding GUI test\ncase (including events and assertions) for the target app. We evaluate MACdroid\non two widely-used datasets (including 31 apps, 34 functionalities, and 123\ntest cases). On the FrUITeR dataset, the test cases generated by MACdroid\nsuccessfully test 64% of the target functionalities, improving the baselines by\n191%. On the Lin dataset, MACdroid successfully tests 75% of the target\nfunctionalities, outperforming the baselines by 42%. These results underscore\nthe effectiveness of MACdroid in GUI test migration.\n","authors":["Yakun Zhang","Chen Liu","Xiaofei Xie","Yun Lin","Jin Song Dong","Dan Hao","Lu Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05028v2.pdf","comment":"This paper has been accepted for publication in ACM Transactions on\n  Software Engineering and Methodology (TOSEM) in 2025. The official\n  publication link is: https://dl.acm.org/doi/10.1145/3726525"},{"id":"http://arxiv.org/abs/2507.11548v2","updated":"2025-07-17T01:30:09Z","published":"2025-07-11T16:57:13Z","title":"Fairness Is Not Enough: Auditing Competence and Intersectional Bias in\n  AI-powered Resume Screening","summary":"  The increasing use of generative AI for resume screening is predicated on the\nassumption that it offers an unbiased alternative to biased human\ndecision-making. However, this belief fails to address a critical question: are\nthese AI systems fundamentally competent at the evaluative tasks they are meant\nto perform?\n  This study investigates the question of competence through a two-part audit\nof eight major AI platforms. Experiment 1 confirmed complex, contextual racial\nand gender biases, with some models penalizing candidates merely for the\npresence of demographic signals. Experiment 2, which evaluated core competence,\nprovided a critical insight: some models that appeared unbiased were, in fact,\nincapable of performing a substantive evaluation, relying instead on\nsuperficial keyword matching.\n  This paper introduces the \"Illusion of Neutrality\" to describe this\nphenomenon, where an apparent lack of bias is merely a symptom of a model's\ninability to make meaningful judgments. This study recommends that\norganizations and regulators adopt a dual-validation framework, auditing AI\nhiring tools for both demographic bias and demonstrable competence to ensure\nthey are both equitable and effective.\n","authors":["Kevin T Webster"],"pdf_url":"https://arxiv.org/pdf/2507.11548v2.pdf","comment":"34 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.22673v3","updated":"2025-07-17T01:19:22Z","published":"2025-03-28T17:58:33Z","title":"ActionStudio: A Lightweight Framework for Data and Training of Large\n  Action Models","summary":"  Large Action models are essential for enabling autonomous agents to perform\ncomplex tasks. However, training such models remains challenging due to the\ndiversity of agent environments and the complexity of noisy agentic data.\nExisting infrastructure offers limited support for scalable, agent-specific\nfine-tuning and standardized agent data processing. We introduce ActionStudio,\na lightweight and extensible data and training framework designed for large\naction models. ActionStudio unifies diverse agent trajectories using our\nproposed Unified Format 2.0, supports a range of training workflows with\noptimized multi-node distributed setup, and integrates robust preprocessing and\nreal-time verification tools. ActionStudio demonstrates up to 9x higher\nthroughput compared to existing agentic training frameworks, and our trained\nmodels yield top performances across public and realistic agent benchmarks. To\nsupport the broader research community, we open-source the ActionStudio\nframework and release actionstudio-98k, a curated dataset of 98k high-quality\ntrajectories. Code: https://github.com/SalesforceAIResearch/xLAM.\n","authors":["Jianguo Zhang","Thai Hoang","Ming Zhu","Zuxin Liu","Shiyu Wang","Tulika Awalgaonkar","Akshara Prabhakar","Haolin Chen","Weiran Yao","Zhiwei Liu","Juntao Tan","Juan Carlos Niebles","Shelby Heinecke","Huan Wang","Silvio Savarese","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.22673v3.pdf","comment":"16 pages; large action models; xLAM; ActionStudio"},{"id":"http://arxiv.org/abs/2506.17088v2","updated":"2025-07-17T00:41:37Z","published":"2025-06-20T15:49:37Z","title":"Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language\n  Models: An Empirical Evaluation","summary":"  Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://anonymous.4open.science/r/cot-hallu-detect.\n","authors":["Jiahao Cheng","Tiancheng Su","Jia Yuan","Guoxiu He","Jiawei Liu","Xinqi Tao","Jingwen Xie","Huaxia Li"],"pdf_url":"https://arxiv.org/pdf/2506.17088v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12705v1","updated":"2025-07-17T00:39:18Z","published":"2025-07-17T00:39:18Z","title":"AudioJudge: Understanding What Works in Large Audio Model Based Speech\n  Evaluation","summary":"  Current speech evaluation suffers from two critical limitations: the need and\ndifficulty of designing specialized systems targeting individual audio\ncharacteristics, and poor correlation between automatic evaluation methods and\nhuman preferences. This work presents a systematic study of Large Audio Model\n(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified\nevaluation framework that addresses both challenges. We systematically explore\nAudioJudge across audio characteristic detection tasks, including\npronunciation, speaking rate, speaker identification and speech quality, and\nsystem-level human preference simulation for automated benchmarking. We\ninvestigate different prompt engineering strategies, finding that audio\nconcatenation combined with in-context learning significantly improves\nperformance across both audio characteristic detection and human preference\nsimulation tasks. We further introduce a multi-aspect ensemble AudioJudge to\nenable general-purpose multi-aspect audio evaluation. This method decomposes\nspeech assessment into specialized judges for lexical content, speech quality,\nand paralinguistic features, achieving up to 0.91 Spearman correlation with\nhuman preferences on our system ranking benchmark. Robustness analysis reveals\nthat while LAMs maintain strong performance under acoustic noise, they exhibit\nsignificant verbosity and positional biases that require careful mitigation.\n","authors":["Potsawee Manakul","Woody Haosheng Gan","Michael J. Ryan","Ali Sartaz Khan","Warit Sirichotedumrong","Kunat Pipatanakul","William Held","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2507.12705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13722v2","updated":"2025-07-17T00:24:33Z","published":"2024-02-21T11:33:09Z","title":"Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment\n  Analysis","summary":"  Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem\nthat entails the extraction of multifaceted aspects, opinions, and sentiments\nfrom the given text. Both standalone and compound ABSA tasks have been\nextensively used in the literature to examine the nuanced information present\nin online reviews and social media posts. Current ABSA methods often rely on\nstatic hyperparameters for attention-masking mechanisms, which can struggle\nwith context adaptation and may overlook the unique relevance of words in\nvaried situations. This leads to challenges in accurately analyzing complex\nsentences containing multiple aspects with differing sentiments. In this work,\nwe present adaptive masking methods that remove irrelevant tokens based on\ncontext to assist in Aspect Term Extraction and Aspect Sentiment Classification\nsubtasks of ABSA. We show with our experiments that the proposed methods\noutperform the baseline methods in terms of accuracy and F1 scores on four\nbenchmark online review datasets. Further, we show that the proposed methods\ncan be extended with multiple adaptations and demonstrate a qualitative\nanalysis of the proposed approach using sample text for aspect term extraction.\n","authors":["S M Rafiuddin","Mohammed Rakib","Sadia Kamal","Arunkumar Bagavathi"],"pdf_url":"https://arxiv.org/pdf/2402.13722v2.pdf","comment":"12 pages, 4 figures, Accepted at PAKDD 2024"},{"id":"http://arxiv.org/abs/2507.12695v1","updated":"2025-07-17T00:06:43Z","published":"2025-07-17T00:06:43Z","title":"AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based\n  Sentiment Analysis","summary":"  We introduce AdaptiSent, a new framework for Multimodal Aspect-Based\nSentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms\nto improve sentiment classification and aspect term extraction from both text\nand images. Our model integrates dynamic modality weighting and\ncontext-adaptive attention, enhancing the extraction of sentiment and\naspect-related information by focusing on how textual cues and visual context\ninteract. We tested our approach against several baselines, including\ntraditional text-based models and other multimodal methods. Results from\nstandard Twitter datasets show that AdaptiSent surpasses existing models in\nprecision, recall, and F1 score, and is particularly effective in identifying\nnuanced inter-modal relationships that are crucial for accurate sentiment and\naspect term extraction. This effectiveness comes from the model's ability to\nadjust its focus dynamically based on the context's relevance, improving the\ndepth and accuracy of sentiment analysis across various multimodal data sets.\nAdaptiSent sets a new standard for MABSA, significantly outperforming current\nmethods, especially in understanding complex multimodal information.\n","authors":["S M Rafiuddin","Sadia Kamal","Mohammed Rakib","Arunkumar Bagavathi","Atriya Sen"],"pdf_url":"https://arxiv.org/pdf/2507.12695v1.pdf","comment":"12 pages (including references), 2 figures (Fig. 1 overview, Fig. 2\n  hyperparameter sensitivity with two subplots), 6 tables (performance,\n  ablation, dataset stats, case studies, etc.), accepted at ASONAM 2025 (Social\n  Network Analysis and Mining)"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2507.13353v1","updated":"2025-07-17T17:59:59Z","published":"2025-07-17T17:59:59Z","title":"VideoITG: Multimodal Video Understanding with Instructed Temporal\n  Grounding","summary":"  Recent studies have revealed that selecting informative and relevant video\nframes can significantly improve the performance of Video Large Language Models\n(Video-LLMs). Current methods, such as reducing inter-frame redundancy,\nemploying separate models for image-text relevance assessment, or utilizing\ntemporal video grounding for event localization, substantially adopt\nunsupervised learning paradigms, whereas they struggle to address the complex\nscenarios in long video understanding. We propose Instructed Temporal Grounding\nfor Videos (VideoITG), featuring customized frame sampling aligned with user\ninstructions. The core of VideoITG is the VidThinker pipeline, an automated\nannotation framework that explicitly mimics the human annotation process.\nFirst, it generates detailed clip-level captions conditioned on the\ninstruction; then, it retrieves relevant video segments through\ninstruction-guided reasoning; finally, it performs fine-grained frame selection\nto pinpoint the most informative visual evidence. Leveraging VidThinker, we\nconstruct the VideoITG-40K dataset, containing 40K videos and 500K instructed\ntemporal grounding annotations. We then design a plug-and-play VideoITG model,\nwhich takes advantage of visual language alignment and reasoning capabilities\nof Video-LLMs, for effective frame selection in a discriminative manner.\nCoupled with Video-LLMs, VideoITG achieves consistent performance improvements\nacross multiple multimodal video understanding benchmarks, showing its\nsuperiority and great potentials for video understanding.\n","authors":["Shihao Wang","Guo Chen","De-an Huang","Zhiqi Li","Minghan Li","Guilin Li","Jose M. Alvarez","Lei Zhang","Zhiding Yu"],"pdf_url":"https://arxiv.org/pdf/2507.13353v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2507.13350v1","updated":"2025-07-17T17:59:56Z","published":"2025-07-17T17:59:56Z","title":"Hierarchical Rectified Flow Matching with Mini-Batch Couplings","summary":"  Flow matching has emerged as a compelling generative modeling approach that\nis widely used across domains. To generate data via a flow matching model, an\nordinary differential equation (ODE) is numerically solved via forward\nintegration of the modeled velocity field. To better capture the multi-modality\nthat is inherent in typical velocity fields, hierarchical flow matching was\nrecently introduced. It uses a hierarchy of ODEs that are numerically\nintegrated when generating data. This hierarchy of ODEs captures the\nmulti-modal velocity distribution just like vanilla flow matching is capable of\nmodeling a multi-modal data distribution. While this hierarchy enables to model\nmulti-modal velocity distributions, the complexity of the modeled distribution\nremains identical across levels of the hierarchy. In this paper, we study how\nto gradually adjust the complexity of the distributions across different levels\nof the hierarchy via mini-batch couplings. We show the benefits of mini-batch\ncouplings in hierarchical rectified flow matching via compelling results on\nsynthetic and imaging data. Code is available at\nhttps://riccizz.github.io/HRF_coupling.\n","authors":["Yichi Zhang","Yici Yan","Alex Schwing","Zhizhen Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.13350v1.pdf","comment":"Project Page: https://riccizz.github.io/HRF_coupling"},{"id":"http://arxiv.org/abs/2507.13348v1","updated":"2025-07-17T17:59:55Z","published":"2025-07-17T17:59:55Z","title":"VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning","summary":"  Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.\n","authors":["Senqiao Yang","Junyi Li","Xin Lai","Bei Yu","Hengshuang Zhao","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2507.13348v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/VisionThink"},{"id":"http://arxiv.org/abs/2507.13347v1","updated":"2025-07-17T17:59:53Z","published":"2025-07-17T17:59:53Z","title":"$π^3$: Scalable Permutation-Equivariant Visual Geometry Learning","summary":"  We introduce $\\pi^3$, a feed-forward neural network that offers a novel\napproach to visual geometry reconstruction, breaking the reliance on a\nconventional fixed reference view. Previous methods often anchor their\nreconstructions to a designated viewpoint, an inductive bias that can lead to\ninstability and failures if the reference is suboptimal. In contrast, $\\pi^3$\nemploys a fully permutation-equivariant architecture to predict\naffine-invariant camera poses and scale-invariant local point maps without any\nreference frames. This design makes our model inherently robust to input\nordering and highly scalable. These advantages enable our simple and bias-free\napproach to achieve state-of-the-art performance on a wide range of tasks,\nincluding camera pose estimation, monocular/video depth estimation, and dense\npoint map reconstruction. Code and models are publicly available.\n","authors":["Yifan Wang","Jianjun Zhou","Haoyi Zhu","Wenzheng Chang","Yang Zhou","Zizun Li","Junyi Chen","Jiangmiao Pang","Chunhua Shen","Tong He"],"pdf_url":"https://arxiv.org/pdf/2507.13347v1.pdf","comment":"Project page: https://yyfz.github.io/pi3/"},{"id":"http://arxiv.org/abs/2507.13345v1","updated":"2025-07-17T17:59:47Z","published":"2025-07-17T17:59:47Z","title":"Imbalance in Balance: Online Concept Balancing in Generation Models","summary":"  In visual generation tasks, the responses and combinations of complex\nconcepts often lack stability and are error-prone, which remains an\nunder-explored area. In this paper, we attempt to explore the causal factors\nfor poor concept responses through elaborately designed experiments. We also\ndesign a concept-wise equalization loss function (IMBA loss) to address this\nissue. Our proposed method is online, eliminating the need for offline dataset\nprocessing, and requires minimal code changes. In our newly proposed complex\nconcept benchmark Inert-CompBench and two other public test sets, our method\nsignificantly enhances the concept response capability of baseline models and\nyields highly competitive results with only a few codes.\n","authors":["Yukai Shi","Jiarong Ou","Rui Chen","Haotian Yang","Jiahao Wang","Xin Tao","Pengfei Wan","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2507.13345v1.pdf","comment":"Accepted by ICCV2025"},{"id":"http://arxiv.org/abs/2507.13346v1","updated":"2025-07-17T17:59:47Z","published":"2025-07-17T17:59:47Z","title":"AutoPartGen: Autogressive 3D Part Generation and Discovery","summary":"  We introduce AutoPartGen, a model that generates objects composed of 3D parts\nin an autoregressive manner. This model can take as input an image of an\nobject, 2D masks of the object's parts, or an existing 3D object, and generate\na corresponding compositional 3D reconstruction. Our approach builds upon\n3DShape2VecSet, a recent latent 3D representation with powerful geometric\nexpressiveness. We observe that this latent space exhibits strong compositional\nproperties, making it particularly well-suited for part-based generation tasks.\nSpecifically, AutoPartGen generates object parts autoregressively, predicting\none part at a time while conditioning on previously generated parts and\nadditional inputs, such as 2D images, masks, or 3D objects. This process\ncontinues until the model decides that all parts have been generated, thus\ndetermining automatically the type and number of parts. The resulting parts can\nbe seamlessly assembled into coherent objects or scenes without requiring\nadditional optimization. We evaluate both the overall 3D generation\ncapabilities and the part-level generation quality of AutoPartGen,\ndemonstrating that it achieves state-of-the-art performance in 3D part\ngeneration.\n","authors":["Minghao Chen","Jianyuan Wang","Roman Shapovalov","Tom Monnier","Hyunyoung Jung","Dilin Wang","Rakesh Ranjan","Iro Laina","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2507.13346v1.pdf","comment":"Project page: https://silent-chen.github.io/AutoPartGen/"},{"id":"http://arxiv.org/abs/2507.13344v1","updated":"2025-07-17T17:59:17Z","published":"2025-07-17T17:59:17Z","title":"Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos\n  with Spatio-Temporal Diffusion Models","summary":"  This paper addresses the challenge of high-fidelity view synthesis of humans\nwith sparse-view videos as input. Previous methods solve the issue of\ninsufficient observation by leveraging 4D diffusion models to generate videos\nat novel viewpoints. However, the generated videos from these models often lack\nspatio-temporal consistency, thus degrading view synthesis quality. In this\npaper, we propose a novel sliding iterative denoising process to enhance the\nspatio-temporal consistency of the 4D diffusion model. Specifically, we define\na latent grid in which each latent encodes the image, camera pose, and human\npose for a certain viewpoint and timestamp, then alternately denoising the\nlatent grid along spatial and temporal dimensions with a sliding window, and\nfinally decode the videos at target viewpoints from the corresponding denoised\nlatents. Through the iterative sliding, information flows sufficiently across\nthe latent grid, allowing the diffusion model to obtain a large receptive field\nand thus enhance the 4D consistency of the output, while making the GPU memory\nconsumption affordable. The experiments on the DNA-Rendering and ActorsHQ\ndatasets demonstrate that our method is able to synthesize high-quality and\nconsistent novel-view videos and significantly outperforms the existing\napproaches. See our project page for interactive demos and video results:\nhttps://diffuman4d.github.io/ .\n","authors":["Yudong Jin","Sida Peng","Xuan Wang","Tao Xie","Zhen Xu","Yifan Yang","Yujun Shen","Hujun Bao","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.13344v1.pdf","comment":"Project page: https://diffuman4d.github.io/"},{"id":"http://arxiv.org/abs/2507.13343v1","updated":"2025-07-17T17:59:10Z","published":"2025-07-17T17:59:10Z","title":"Taming Diffusion Transformer for Real-Time Mobile Video Generation","summary":"  Diffusion Transformers (DiT) have shown strong performance in video\ngeneration tasks, but their high computational cost makes them impractical for\nresource-constrained devices like smartphones, and real-time generation is even\nmore challenging. In this work, we propose a series of novel optimizations to\nsignificantly accelerate video generation and enable real-time performance on\nmobile platforms. First, we employ a highly compressed variational autoencoder\n(VAE) to reduce the dimensionality of the input data without sacrificing visual\nquality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning\nstrategy to shrink the model size to suit mobile platform while preserving\ncritical performance characteristics. Third, we develop an adversarial step\ndistillation technique tailored for DiT, which allows us to reduce the number\nof inference steps to four. Combined, these optimizations enable our model to\nachieve over 10 frames per second (FPS) generation on an iPhone 16 Pro Max,\ndemonstrating the feasibility of real-time, high-quality video generation on\nmobile devices.\n","authors":["Yushu Wu","Yanyu Li","Anil Kag","Ivan Skorokhodov","Willi Menapace","Ke Ma","Arpit Sahni","Ju Hu","Aliaksandr Siarohin","Dhritiman Sagar","Yanzhi Wang","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2507.13343v1.pdf","comment":"9 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2507.13339v1","updated":"2025-07-17T17:57:18Z","published":"2025-07-17T17:57:18Z","title":"SpectraLift: Physics-Guided Spectral-Inversion Network for\n  Self-Supervised Hyperspectral Image Super-Resolution","summary":"  High-spatial-resolution hyperspectral images (HSI) are essential for\napplications such as remote sensing and medical imaging, yet HSI sensors\ninherently trade spatial detail for spectral richness. Fusing\nhigh-spatial-resolution multispectral images (HR-MSI) with\nlow-spatial-resolution hyperspectral images (LR-HSI) is a promising route to\nrecover fine spatial structures without sacrificing spectral fidelity. Most\nstate-of-the-art methods for HSI-MSI fusion demand point spread function (PSF)\ncalibration or ground truth high resolution HSI (HR-HSI), both of which are\nimpractical to obtain in real world settings. We present SpectraLift, a fully\nself-supervised framework that fuses LR-HSI and HR-MSI inputs using only the\nMSI's Spectral Response Function (SRF). SpectraLift trains a lightweight\nper-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic\nlow-spatial-resolution multispectral image (LR-MSI) obtained by applying the\nSRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an\n$\\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as\nthe optimization objective. At inference, SpectraLift uses the trained network\nto map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in\nminutes, is agnostic to spatial blur and resolution, and outperforms\nstate-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks.\n","authors":["Ritik Shah","Marco F. Duarte"],"pdf_url":"https://arxiv.org/pdf/2507.13339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13326v1","updated":"2025-07-17T17:45:09Z","published":"2025-07-17T17:45:09Z","title":"A Real-Time System for Egocentric Hand-Object Interaction Detection in\n  Industrial Domains","summary":"  Hand-object interaction detection remains an open challenge in real-time\napplications, where intuitive user experiences depend on fast and accurate\ndetection of interactions with surrounding objects. We propose an efficient\napproach for detecting hand-objects interactions from streaming egocentric\nvision that operates in real time. Our approach consists of an action\nrecognition module and an object detection module for identifying active\nobjects upon confirmed interaction. Our Mamba model with EfficientNetV2 as\nbackbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark\nat 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object.\nWe implement our models in a cascaded architecture where the action recognition\nand object detection modules operate sequentially. When the action recognition\npredicts a contact state, it activates the object detection module, which in\nturn performs inference on the relevant frame to detect and classify the active\nobject.\n","authors":["Antonio Finocchiaro","Alessandro Sebastiano Catinello","Michele Mazzamuto","Rosario Leonardi","Antonino Furnari","Giovanni Maria Farinella"],"pdf_url":"https://arxiv.org/pdf/2507.13326v1.pdf","comment":"12 pages, 4 figures, In International Conference on Image Analysis\n  and Processing"},{"id":"http://arxiv.org/abs/2507.13314v1","updated":"2025-07-17T17:33:11Z","published":"2025-07-17T17:33:11Z","title":"Revisiting Reliability in the Reasoning-based Pose Estimation Benchmark","summary":"  The reasoning-based pose estimation (RPE) benchmark has emerged as a widely\nadopted evaluation standard for pose-aware multimodal large language models\n(MLLMs). Despite its significance, we identified critical reproducibility and\nbenchmark-quality issues that hinder fair and consistent quantitative\nevaluations. Most notably, the benchmark utilizes different image indices from\nthose of the original 3DPW dataset, forcing researchers into tedious and\nerror-prone manual matching processes to obtain accurate ground-truth (GT)\nannotations for quantitative metrics (\\eg, MPJPE, PA-MPJPE). Furthermore, our\nanalysis reveals several inherent benchmark-quality limitations, including\nsignificant image redundancy, scenario imbalance, overly simplistic poses, and\nambiguous textual descriptions, collectively undermining reliable evaluations\nacross diverse scenarios. To alleviate manual effort and enhance\nreproducibility, we carefully refined the GT annotations through meticulous\nvisual matching and publicly release these refined annotations as an\nopen-source resource, thereby promoting consistent quantitative evaluations and\nfacilitating future advancements in human pose-aware multimodal reasoning.\n","authors":["Junsu Kim","Naeun Kim","Jaeho Lee","Incheol Park","Dongyoon Han","Seungryul Baek"],"pdf_url":"https://arxiv.org/pdf/2507.13314v1.pdf","comment":"To be presented as a poster at MMFM 2025"},{"id":"http://arxiv.org/abs/2507.12440v2","updated":"2025-07-17T17:30:47Z","published":"2025-07-16T17:27:44Z","title":"EgoVLA: Learning Vision-Language-Action Models from Egocentric Human\n  Videos","summary":"  Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Ego\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA\n","authors":["Ruihan Yang","Qinxi Yu","Yecheng Wu","Rui Yan","Borui Li","An-Chieh Cheng","Xueyan Zou","Yunhao Fang","Hongxu Yin","Sifei Liu","Song Han","Yao Lu","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2507.12440v2.pdf","comment":"More videos can be found on our website:\n  https://rchalyang.github.io/EgoVLA"},{"id":"http://arxiv.org/abs/2507.13311v1","updated":"2025-07-17T17:30:29Z","published":"2025-07-17T17:30:29Z","title":"FashionPose: Text to Pose to Relight Image Generation for Personalized\n  Fashion Visualization","summary":"  Realistic and controllable garment visualization is critical for fashion\ne-commerce, where users expect personalized previews under diverse poses and\nlighting conditions. Existing methods often rely on predefined poses, limiting\nsemantic flexibility and illumination adaptability. To address this, we\nintroduce FashionPose, the first unified text-to-pose-to-relighting generation\nframework. Given a natural language description, our method first predicts a 2D\nhuman pose, then employs a diffusion model to generate high-fidelity person\nimages, and finally applies a lightweight relighting module, all guided by the\nsame textual input. By replacing explicit pose annotations with text-driven\nconditioning, FashionPose enables accurate pose alignment, faithful garment\nrendering, and flexible lighting control. Experiments demonstrate fine-grained\npose synthesis and efficient, consistent relighting, providing a practical\nsolution for personalized virtual fashion display.\n","authors":["Chuancheng Shi","Yixiang Chen","Burong Lei","Jichao Chen"],"pdf_url":"https://arxiv.org/pdf/2507.13311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14048v2","updated":"2025-07-17T17:06:39Z","published":"2025-01-23T19:29:34Z","title":"SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with\n  Equivariant Neural Networks","summary":"  Modern neural networks (NNs) often do not generalize well in the presence of\na \"covariate shift\"; that is, in situations where the training and test data\ndistributions differ, but the conditional distribution of classification labels\nremains unchanged. In such cases, NN generalization can be reduced to a problem\nof learning more domain-invariant features. Domain adaptation (DA) methods\ninclude a range of techniques aimed at achieving this; however, these methods\nhave struggled with the need for extensive hyperparameter tuning, which then\nincurs significant computational costs. In this work, we introduce SIDDA, an\nout-of-the-box DA training algorithm built upon the Sinkhorn divergence, that\ncan achieve effective domain alignment with minimal hyperparameter tuning and\ncomputational overhead. We demonstrate the efficacy of our method on multiple\nsimulated and real datasets of varying complexity, including simple shapes,\nhandwritten digits, and real astronomical observations. SIDDA is compatible\nwith a variety of NN architectures, and it works particularly well in improving\nclassification accuracy and model calibration when paired with equivariant\nneural networks (ENNs). We find that SIDDA enhances the generalization\ncapabilities of NNs, achieving up to a $\\approx40\\%$ improvement in\nclassification accuracy on unlabeled target data. We also study the efficacy of\nDA on ENNs with respect to the varying group orders of the dihedral group\n$D_N$, and find that the model performance improves as the degree of\nequivariance increases. Finally, we find that SIDDA enhances model calibration\non both source and target data--achieving over an order of magnitude\nimprovement in the ECE and Brier score. SIDDA's versatility, combined with its\nautomated approach to domain alignment, has the potential to advance\nmulti-dataset studies by enabling the development of highly generalizable\nmodels.\n","authors":["Sneh Pandya","Purvik Patel","Brian D. Nord","Mike Walmsley","Aleksandra Ćiprijanović"],"pdf_url":"https://arxiv.org/pdf/2501.14048v2.pdf","comment":"25 pages, 5 figures, 4 tables. code available at:\n  https://github.com/deepskies/SIDDA"},{"id":"http://arxiv.org/abs/2507.13292v1","updated":"2025-07-17T16:58:02Z","published":"2025-07-17T16:58:02Z","title":"DiffClean: Diffusion-based Makeup Removal for Accurate Age Estimation","summary":"  Accurate age verification can protect underage users from unauthorized access\nto online platforms and e-commerce sites that provide age-restricted services.\nHowever, accurate age estimation can be confounded by several factors,\nincluding facial makeup that can induce changes to alter perceived identity and\nage to fool both humans and machines. In this work, we propose DiffClean which\nerases makeup traces using a text-guided diffusion model to defend against\nmakeup attacks. DiffClean improves age estimation (minor vs. adult accuracy by\n4.8%) and face verification (TMR by 8.9% at FMR=0.01%) over competing baselines\non digitally simulated and real makeup images.\n","authors":["Ekta Balkrishna Gavas","Chinmay Hegde","Nasir Memon","Sudipta Banerjee"],"pdf_url":"https://arxiv.org/pdf/2507.13292v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2507.13336v1","updated":"2025-07-17T17:53:50Z","published":"2025-07-17T17:53:50Z","title":"SGCL: Unifying Self-Supervised and Supervised Learning for Graph\n  Recommendation","summary":"  Recommender systems (RecSys) are essential for online platforms, providing\npersonalized suggestions to users within a vast sea of information.\nSelf-supervised graph learning seeks to harness high-order collaborative\nfiltering signals through unsupervised augmentation on the user-item bipartite\ngraph, primarily leveraging a multi-task learning framework that includes both\nsupervised recommendation loss and self-supervised contrastive loss. However,\nthis separate design introduces additional graph convolution processes and\ncreates inconsistencies in gradient directions due to disparate losses,\nresulting in prolonged training times and sub-optimal performance. In this\nstudy, we introduce a unified framework of Supervised Graph Contrastive\nLearning for recommendation (SGCL) to address these issues. SGCL uniquely\ncombines the training of recommendation and unsupervised contrastive losses\ninto a cohesive supervised contrastive learning loss, aligning both tasks\nwithin a single optimization direction for exceptionally fast training.\nExtensive experiments on three real-world datasets show that SGCL outperforms\nstate-of-the-art methods, achieving superior accuracy and efficiency.\n","authors":["Weizhi Zhang","Liangwei Yang","Zihe Song","Henrry Peng Zou","Ke Xu","Yuanjie Zhu","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2507.13336v1.pdf","comment":"Accepted in RecSys 2025. arXiv admin note: substantial text overlap\n  with arXiv:2404.15954"},{"id":"http://arxiv.org/abs/2507.13296v1","updated":"2025-07-17T17:04:18Z","published":"2025-07-17T17:04:18Z","title":"Efficiently Constructing Sparse Navigable Graphs","summary":"  Graph-based nearest neighbor search methods have seen a surge of popularity\nin recent years, offering state-of-the-art performance across a wide variety of\napplications. Central to these methods is the task of constructing a sparse\nnavigable search graph for a given dataset endowed with a distance function.\nUnfortunately, doing so is computationally expensive, so heuristics are\nuniversally used in practice.\n  In this work, we initiate the study of fast algorithms with provable\nguarantees for search graph construction. For a dataset with $n$ data points,\nthe problem of constructing an optimally sparse navigable graph can be framed\nas $n$ separate but highly correlated minimum set cover instances. This yields\na naive $O(n^3)$ time greedy algorithm that returns a navigable graph whose\nsparsity is at most $O(\\log n)$ higher than optimal. We improve significantly\non this baseline, taking advantage of correlation between the set cover\ninstances to leverage techniques from streaming and sublinear-time set cover\nalgorithms. Combined with problem-specific pre-processing techniques, we\npresent an $\\tilde{O}(n^2)$ time algorithm for constructing an $O(\\log\nn)$-approximate sparsest navigable graph under any distance function.\n  The runtime of our method is optimal up to logarithmic factors under the\nStrong Exponential Time Hypothesis via a reduction from Monochromatic Closest\nPair. Moreover, we prove that, as with general set cover, obtaining better than\nan $O(\\log n)$-approximation is NP-hard, despite the significant additional\nstructure present in the navigable graph problem. Finally, we show that our\ntechniques can also beat cubic time for the closely related and practically\nimportant problems of constructing $\\alpha$-shortcut reachable and\n$\\tau$-monotonic graphs, which are also used for nearest neighbor search. For\nsuch graphs, we obtain $\\tilde{O}(n^{2.5})$ time or better algorithms.\n","authors":["Alex Conway","Laxman Dhulipala","Martin Farach-Colton","Rob Johnson","Ben Landrum","Christopher Musco","Yarin Shechter","Torsten Suel","Richard Wen"],"pdf_url":"https://arxiv.org/pdf/2507.13296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13275v1","updated":"2025-07-17T16:33:57Z","published":"2025-07-17T16:33:57Z","title":"Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for\n  Human Capital Management","summary":"  Advances in natural language processing and large language models are driving\na major transformation in Human Capital Management, with a growing interest in\nbuilding smart systems based on language technologies for talent acquisition,\nupskilling strategies, and workforce planning. However, the adoption and\nprogress of these technologies critically depend on the development of reliable\nand fair models, properly evaluated on public data and open benchmarks, which\nhave so far been unavailable in this domain.\n  To address this gap, we present TalentCLEF 2025, the first evaluation\ncampaign focused on skill and job title intelligence. The lab consists of two\ntasks: Task A - Multilingual Job Title Matching, covering English, Spanish,\nGerman, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.\nBoth corpora were built from real job applications, carefully anonymized, and\nmanually annotated to reflect the complexity and diversity of real-world labor\nmarket data, including linguistic variability and gender-marked expressions.\n  The evaluations included monolingual and cross-lingual scenarios and covered\nthe evaluation of gender bias.\n  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most\nsystems relied on information retrieval techniques built with multilingual\nencoder-based models fine-tuned with contrastive learning, and several of them\nincorporated large language models for data augmentation or re-ranking. The\nresults show that the training strategies have a larger effect than the size of\nthe model alone. TalentCLEF provides the first public benchmark in this field\nand encourages the development of robust, fair, and transferable language\ntechnologies for the labor market.\n","authors":["Luis Gasco","Hermenegildo Fabregat","Laura García-Sardiña","Paula Estrella","Daniel Deniz","Alvaro Rodrigo","Rabih Zbib"],"pdf_url":"https://arxiv.org/pdf/2507.13275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13255v1","updated":"2025-07-17T16:04:55Z","published":"2025-07-17T16:04:55Z","title":"Automating Steering for Safe Multimodal Large Language Models","summary":"  Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.\n","authors":["Lyucheng Wu","Mengru Wang","Ziwen Xu","Tri Cao","Nay Oo","Bryan Hooi","Shumin Deng"],"pdf_url":"https://arxiv.org/pdf/2507.13255v1.pdf","comment":"Working in progress. 22 pages (8+ for main); 25 figures; 1 table"},{"id":"http://arxiv.org/abs/2503.23033v2","updated":"2025-07-17T15:06:16Z","published":"2025-03-29T10:36:54Z","title":"Imagine All The Relevance: Scenario-Profiled Indexing with Knowledge\n  Expansion for Dense Retrieval","summary":"  Existing dense retrieval models struggle with reasoning-intensive retrieval\ntask as they fail to capture implicit relevance that requires reasoning beyond\nsurface-level semantic information. To address these challenges, we propose\nScenario-Profiled Indexing with Knowledge Expansion (SPIKE), a dense retrieval\nframework that explicitly indexes implicit relevance by decomposing documents\ninto scenario-based retrieval units. SPIKE organizes documents into scenario,\nwhich encapsulates the reasoning process necessary to uncover implicit\nrelationships between hypothetical information needs and document content.\nSPIKE constructs a scenario-augmented dataset using a powerful teacher large\nlanguage model (LLM), then distills these reasoning capabilities into a\nsmaller, efficient scenario generator. During inference, SPIKE incorporates\nscenario-level relevance alongside document-level relevance, enabling\nreasoning-aware retrieval. Extensive experiments demonstrate that SPIKE\nconsistently enhances retrieval performance across various query types and\ndense retrievers. It also enhances the retrieval experience for users through\nscenario and offers valuable contextual information for LLMs in\nretrieval-augmented generation (RAG).\n","authors":["Sangam Lee","Ryang Heo","SeongKu Kang","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2503.23033v2.pdf","comment":"Accepted to COLM 2025"},{"id":"http://arxiv.org/abs/2507.10917v2","updated":"2025-07-17T15:01:08Z","published":"2025-07-15T02:13:54Z","title":"LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation","summary":"  Recently, much effort has been devoted to modeling users' multi-interests\nbased on their behaviors or auxiliary signals. However, existing methods often\nrely on heuristic assumptions, e.g., co-occurring items indicate the same\ninterest of users, failing to capture user multi-interests aligning with\nreal-world scenarios. While large language models (LLMs) show significant\npotential for multi-interest analysis due to their extensive knowledge and\npowerful reasoning capabilities, two key challenges remain. First, the\ngranularity of LLM-driven multi-interests is agnostic, possibly leading to\noverly fine or coarse interest grouping. Second, individual user analysis\nprovides limited insights due to the data sparsity issue. In this paper, we\npropose an LLM-driven dual-level multi-interest modeling framework for more\neffective recommendation. At the user-individual level, we exploit LLMs to\nflexibly allocate items engaged by users into different semantic clusters,\nindicating their diverse and distinct interests. To alleviate the agnostic\ngeneration of LLMs, we adaptively assign these semantic clusters to users'\ncollaborative multi-interests learned from global user-item interactions,\nallowing the granularity to be automatically adjusted according to the user's\nbehaviors using an alignment module. To alleviate the limited insights derived\nfrom individual users' behaviors, at the user-crowd level, we propose\naggregating user cliques into synthesized users with rich behaviors for more\ncomprehensive LLM-driven multi-interest analysis. We formulate a max covering\nproblem to ensure the compactness and representativeness of synthesized users'\nbehaviors, and then conduct contrastive learning based on their LLM-driven\nmulti-interests to disentangle item representations among different interests.\nExperiments on real-world datasets show the superiority of our approach against\nstate-of-the-art methods.\n","authors":["Ziyan Wang","Yingpeng Du","Zhu Sun","Jieyi Bi","Haoyan Chua","Tianjun Wei","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.10917v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2507.13105v1","updated":"2025-07-17T13:19:50Z","published":"2025-07-17T13:19:50Z","title":"SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated\n  Summaries For Scientific Abstracts","summary":"  We introduce SemCSE, an unsupervised method for learning semantic embeddings\nof scientific texts. Building on recent advances in contrastive learning for\ntext embeddings, our approach leverages LLM-generated summaries of scientific\nabstracts to train a model that positions semantically related summaries closer\ntogether in the embedding space. This resulting objective ensures that the\nmodel captures the true semantic content of a text, in contrast to traditional\ncitation-based approaches that do not necessarily reflect semantic similarity.\nTo validate this, we propose a novel benchmark designed to assess a model's\nability to understand and encode the semantic content of scientific texts,\ndemonstrating that our method enforces a stronger semantic separation within\nthe embedding space. Additionally, we evaluate SemCSE on the comprehensive\nSciRepEval benchmark for scientific text embeddings, where it achieves\nstate-of-the-art performance among models of its size, thus highlighting the\nbenefits of a semantically focused training approach.\n","authors":["Marc Brinner","Sina Zarriess"],"pdf_url":"https://arxiv.org/pdf/2507.13105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08161v4","updated":"2025-07-17T09:34:49Z","published":"2025-03-11T08:26:37Z","title":"OASIS: Order-Augmented Strategy for Improved Code Search","summary":"  Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training.\n","authors":["Zuchen Gao","Zizheng Zhan","Xianming Li","Erxin Yu","Ziqi Zhan","Haotian Zhang","Bin Chen","Yuqun Zhang","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2503.08161v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.15841v2","updated":"2025-07-17T08:53:48Z","published":"2025-06-18T19:44:46Z","title":"MEM1: Learning to Synergize Memory and Reasoning for Efficient\n  Long-Horizon Agents","summary":"  Modern language agents must operate over long-horizon, multi-turn\ninteractions, where they retrieve external information, adapt to observations,\nand answer interdependent queries. Yet, most LLM systems rely on full-context\nprompting, appending all past turns regardless of their relevance. This leads\nto unbounded memory growth, increased computational costs, and degraded\nreasoning performance on out-of-distribution input lengths. We introduce MEM1,\nan end-to-end reinforcement learning framework that enables agents to operate\nwith constant memory across long multi-turn tasks. At each turn, MEM1 updates a\ncompact shared internal state that jointly supports memory consolidation and\nreasoning. This state integrates prior memory with new observations from the\nenvironment while strategically discarding irrelevant or redundant information.\nTo support training in more realistic and compositional settings, we propose a\nsimple yet effective and scalable approach to constructing multi-turn\nenvironments by composing existing datasets into arbitrarily complex task\nsequences. Experiments across three domains, including internal retrieval QA,\nopen-domain web QA, and multi-turn web shopping, show that MEM1-7B improves\nperformance by 3.5x while reducing memory usage by 3.7x compared to\nQwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes\nbeyond the training horizon. Our results demonstrate the promise of\nreasoning-driven memory consolidation as a scalable alternative to existing\nsolutions for training long-horizon interactive agents, where both efficiency\nand performance are optimized.\n","authors":["Zijian Zhou","Ao Qu","Zhaoxuan Wu","Sunghwan Kim","Alok Prakash","Daniela Rus","Jinhua Zhao","Bryan Kian Hsiang Low","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2506.15841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09617v2","updated":"2025-07-17T07:51:28Z","published":"2024-02-14T23:12:09Z","title":"LLM-Enhanced User-Item Interactions: Leveraging Edge Information for\n  Optimized Recommendations","summary":"  Graph recommendation methods, representing a connected interaction\nperspective, reformulate user-item interactions as graphs to leverage graph\nstructure and topology to recommend and have proved practical effectiveness at\nscale. Large language models, representing a textual generative perspective,\nexcel at modeling user languages, understanding behavioral contexts, capturing\nuser-item semantic relationships, analyzing textual sentiments, and generating\ncoherent and contextually relevant texts as recommendations. However, there is\na gap between the connected graph perspective and the text generation\nperspective as the task formulations are different. A research question arises:\nhow can we effectively integrate the two perspectives for more personalized\nrecsys? To fill this gap, we propose to incorporate graph-edge information into\nLLMs via prompt and attention innovations. We reformulate recommendations as a\nprobabilistic generative problem using prompts. We develop a framework to\nincorporate graph edge information from the prompt and attention mechanisms for\ngraph-structured LLM recommendations. We develop a new prompt design that\nbrings in both first-order and second-order graph relationships; we devise an\nimproved LLM attention mechanism to embed direct the spatial and connectivity\ninformation of edges. Our evaluation of real-world datasets demonstrates the\nframework's ability to understand connectivity information in graph data and to\nimprove the relevance and quality of recommendation results.\n","authors":["Xinyuan Wang","Liang Wu","Liangjie Hong","Hao Liu","Yanjie Fu"],"pdf_url":"https://arxiv.org/pdf/2402.09617v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12871v1","updated":"2025-07-17T07:44:05Z","published":"2025-07-17T07:44:05Z","title":"Generative Multi-Target Cross-Domain Recommendation","summary":"  Recently, there has been a surge of interest in Multi-Target Cross-Domain\nRecommendation (MTCDR), which aims to enhance recommendation performance across\nmultiple domains simultaneously. Existing MTCDR methods primarily rely on\ndomain-shared entities (\\eg users or items) to fuse and transfer cross-domain\nknowledge, which may be unavailable in non-overlapped recommendation scenarios.\nSome studies model user preferences and item features as domain-sharable\nsemantic representations, which can be utilized to tackle the MTCDR task.\nNevertheless, they often require extensive auxiliary data for pre-training.\nDeveloping more effective solutions for MTCDR remains an important area for\nfurther exploration.\n  Inspired by recent advancements in generative recommendation, this paper\nintroduces GMC, a generative paradigm-based approach for multi-target\ncross-domain recommendation. The core idea of GMC is to leverage semantically\nquantized discrete item identifiers as a medium for integrating multi-domain\nknowledge within a unified generative model. GMC first employs an item\ntokenizer to generate domain-shared semantic identifiers for each item, and\nthen formulates item recommendation as a next-token generation task by training\na domain-unified sequence-to-sequence model. To further leverage the domain\ninformation to enhance performance, we incorporate a domain-aware contrastive\nloss into the semantic identifier learning, and perform domain-specific\nfine-tuning on the unified recommender. Extensive experiments on five public\ndatasets demonstrate the effectiveness of GMC compared to a range of baseline\nmethods.\n","authors":["Jinqiu Jin","Yang Zhang","Junwei Pan","Fuli Feng","Hua Lu","Haijie Gu","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2507.12871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12844v1","updated":"2025-07-17T07:10:27Z","published":"2025-07-17T07:10:27Z","title":"Machine-Readable Ads: Accessibility and Trust Patterns for AI Web Agents\n  interacting with Online Advertisements","summary":"  Autonomous multimodal language models are rapidly evolving into web agents\nthat can browse, click, and purchase items on behalf of users, posing a threat\nto display advertising designed for human eyes. Yet little is known about how\nthese agents interact with ads or which design principles ensure reliable\nengagement. To address this, we ran a controlled experiment using a faithful\nclone of the news site TT.com, seeded with diverse ads: static banners, GIFs,\ncarousels, videos, cookie dialogues, and paywalls. We ran 300 initial trials\nplus follow-ups using the Document Object Model (DOM)-centric Browser Use\nframework with GPT-4o, Claude 3.7 Sonnet, Gemini 2.0 Flash, and the pixel-based\nOpenAI Operator, across 10 realistic user tasks. Our results show these agents\ndisplay severe satisficing: they never scroll beyond two viewports and ignore\npurely visual calls to action, clicking banners only when semantic button\noverlays or off-screen text labels are present. Critically, when sweepstake\nparticipation required a purchase, GPT-4o and Claude 3.7 Sonnet subscribed in\n100% of trials, and Gemini 2.0 Flash in 70%, revealing gaps in cost-benefit\nanalysis. We identified five actionable design principles-semantic overlays,\nhidden labels, top-left placement, static frames, and dialogue replacement,\nthat make human-centric creatives machine-detectable without harming user\nexperience. We also evaluated agent trustworthiness through \"behavior patterns\"\nsuch as cookie consent handling and subscription choices, highlighting\nmodel-specific risk boundaries and the urgent need for robust trust evaluation\nframeworks in real-world advertising.\n","authors":["Joel Nitu","Heidrun Mühle","Andreas Stöckl"],"pdf_url":"https://arxiv.org/pdf/2507.12844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12840v1","updated":"2025-07-17T06:59:52Z","published":"2025-07-17T06:59:52Z","title":"Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better\n  Understand Public Concerns about Vaccines","summary":"  Vaccine hesitancy threatens public health, leading to delayed or rejected\nvaccines. Social media is a vital source for understanding public concerns, and\ntraditional methods like topic modelling often struggle to capture nuanced\nopinions. Though trained for query answering, large Language Models (LLMs)\noften miss current events and community concerns. Additionally, hallucinations\nin LLMs can compromise public health communication. To address these\nlimitations, we developed a tool (VaxPulse Query Corner) using the Retrieval\nAugmented Generation technique. It addresses complex queries about public\nvaccine concerns on various online platforms, aiding public health\nadministrators and stakeholders in understanding public concerns and\nimplementing targeted interventions to boost vaccine confidence. Analysing\n35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and\nrelevance (0.94).\n","authors":["Muhammad Javed","Sedigh Khademi Habibabadi","Christopher Palmer","Hazel Clothier","Jim Buttery","Gerardo Luis Dimaguila"],"pdf_url":"https://arxiv.org/pdf/2507.12840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.20495v2","updated":"2025-07-17T05:31:07Z","published":"2025-06-25T14:41:13Z","title":"ReCode: Updating Code API Knowledge with Reinforcement Learning","summary":"  Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.\n","authors":["Haoze Wu","Yunzhi Yao","Wenhao Yu","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.20495v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2501.19232v2","updated":"2025-07-17T01:08:34Z","published":"2025-01-31T15:43:21Z","title":"LLM-RecG: A Semantic Bias-Aware Framework for Zero-Shot Sequential\n  Recommendation","summary":"  Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions\nin unseen domains without additional training or fine-tuning, addressing the\nlimitations of traditional models in sparse data environments. Recent\nadvancements in large language models (LLMs) have significantly enhanced ZCDSR\nby facilitating cross-domain knowledge transfer through rich, pretrained\nrepresentations. Despite this progress, domain semantic bias -- arising from\ndifferences in vocabulary and content focus between domains -- remains a\npersistent challenge, leading to misaligned item embeddings and reduced\ngeneralization across domains. To address this, we propose a novel semantic\nbias-aware framework that enhances LLM-based ZCDSR by improving cross-domain\nalignment at both the item and sequential levels. At the item level, we\nintroduce a generalization loss that aligns the embeddings of items across\ndomains (inter-domain compactness), while preserving the unique characteristics\nof each item within its own domain (intra-domain diversity). This ensures that\nitem embeddings can be transferred effectively between domains without\ncollapsing into overly generic or uniform representations. At the sequential\nlevel, we develop a method to transfer user behavioral patterns by clustering\nsource domain user sequences and applying attention-based aggregation during\ntarget domain inference. We dynamically adapt user embeddings to unseen\ndomains, enabling effective zero-shot recommendations without requiring\ntarget-domain interactions...\n","authors":["Yunzhe Li","Junting Wang","Hari Sundaram","Zhining Liu"],"pdf_url":"https://arxiv.org/pdf/2501.19232v2.pdf","comment":"10 pages, Recsys'25 Spotlight Oral"},{"id":"http://arxiv.org/abs/2507.12704v1","updated":"2025-07-17T00:37:59Z","published":"2025-07-17T00:37:59Z","title":"PinFM: Foundation Model for User Activity Sequences at a Billion-scale\n  Visual Discovery Platform","summary":"  User activity sequences have emerged as one of the most important signals in\nrecommender systems. We present a foundational model, PinFM, for understanding\nuser activity sequences across multiple applications at a billion-scale visual\ndiscovery platform. We pretrain a transformer model with 20B+ parameters using\nextensive user activity data, then fine-tune it for specific applications,\nefficiently coupling it with existing models. While this\npretraining-and-fine-tuning approach has been popular in other domains, such as\nVision and NLP, its application in industrial recommender systems presents\nnumerous challenges. The foundational model must be scalable enough to score\nmillions of items every second while meeting tight cost and latency constraints\nimposed by these systems. Additionally, it should capture the interactions\nbetween user activities and other features and handle new items that were not\npresent during the pretraining stage.\n  We developed innovative techniques to address these challenges. Our\ninfrastructure and algorithmic optimizations, such as the Deduplicated\nCross-Attention Transformer (DCAT), improved our throughput by 600% on\nPinterest internal data. We demonstrate that PinFM can learn interactions\nbetween user sequences and candidate items by altering input sequences, leading\nto a 20% increase in engagement with new items. PinFM is now deployed to help\nimprove the experience of more than a half billion users across various\napplications.\n","authors":["Xiangyi Chen","Kousik Rajesh","Matthew Lawhon","Zelun Wang","Hanyu Li","Haomiao Li","Saurabh Vishwas Joshi","Pong Eksombatchai","Jaewon Yang","Yi-Ping Hsu","Jiajing Xu","Charles Rosenberg"],"pdf_url":"https://arxiv.org/pdf/2507.12704v1.pdf","comment":"RecSys 2025"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2507.13350v1","updated":"2025-07-17T17:59:56Z","published":"2025-07-17T17:59:56Z","title":"Hierarchical Rectified Flow Matching with Mini-Batch Couplings","summary":"  Flow matching has emerged as a compelling generative modeling approach that\nis widely used across domains. To generate data via a flow matching model, an\nordinary differential equation (ODE) is numerically solved via forward\nintegration of the modeled velocity field. To better capture the multi-modality\nthat is inherent in typical velocity fields, hierarchical flow matching was\nrecently introduced. It uses a hierarchy of ODEs that are numerically\nintegrated when generating data. This hierarchy of ODEs captures the\nmulti-modal velocity distribution just like vanilla flow matching is capable of\nmodeling a multi-modal data distribution. While this hierarchy enables to model\nmulti-modal velocity distributions, the complexity of the modeled distribution\nremains identical across levels of the hierarchy. In this paper, we study how\nto gradually adjust the complexity of the distributions across different levels\nof the hierarchy via mini-batch couplings. We show the benefits of mini-batch\ncouplings in hierarchical rectified flow matching via compelling results on\nsynthetic and imaging data. Code is available at\nhttps://riccizz.github.io/HRF_coupling.\n","authors":["Yichi Zhang","Yici Yan","Alex Schwing","Zhizhen Zhao"],"pdf_url":"https://arxiv.org/pdf/2507.13350v1.pdf","comment":"Project Page: https://riccizz.github.io/HRF_coupling"},{"id":"http://arxiv.org/abs/2507.13348v1","updated":"2025-07-17T17:59:55Z","published":"2025-07-17T17:59:55Z","title":"VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning","summary":"  Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.\n","authors":["Senqiao Yang","Junyi Li","Xin Lai","Bei Yu","Hengshuang Zhao","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2507.13348v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/VisionThink"},{"id":"http://arxiv.org/abs/2507.13340v1","updated":"2025-07-17T17:57:57Z","published":"2025-07-17T17:57:57Z","title":"Latent Policy Steering with Embodiment-Agnostic Pretrained World Models","summary":"  Learning visuomotor policies via imitation has proven effective across a wide\nrange of robotic domains. However, the performance of these policies is heavily\ndependent on the number of training demonstrations, which requires expensive\ndata collection in the real world. In this work, we aim to reduce data\ncollection efforts when learning visuomotor robot policies by leveraging\nexisting or cost-effective data from a wide range of embodiments, such as\npublic robot datasets and the datasets of humans playing with objects (human\ndata from play). Our approach leverages two key insights. First, we use optic\nflow as an embodiment-agnostic action representation to train a World Model\n(WM) across multi-embodiment datasets, and finetune it on a small amount of\nrobot data from the target embodiment. Second, we develop a method, Latent\nPolicy Steering (LPS), to improve the output of a behavior-cloned policy by\nsearching in the latent space of the WM for better action sequences. In real\nworld experiments, we observe significant improvements in the performance of\npolicies trained with a small amount of data (over 50% relative improvement\nwith 30 demonstrations and over 20% relative improvement with 50\ndemonstrations) by combining the policy with a WM pretrained on two thousand\nepisodes sampled from the existing Open X-embodiment dataset across different\nrobots or a cost-effective human dataset from play.\n","authors":["Yiqi Wang","Mrinal Verghese","Jeff Schneider"],"pdf_url":"https://arxiv.org/pdf/2507.13340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13338v1","updated":"2025-07-17T17:55:00Z","published":"2025-07-17T17:55:00Z","title":"Training Transformers with Enforced Lipschitz Constants","summary":"  Neural networks are often highly sensitive to input and weight perturbations.\nThis sensitivity has been linked to pathologies such as vulnerability to\nadversarial examples, divergent training, and overfitting. To combat these\nproblems, past research has looked at building neural networks entirely from\nLipschitz components. However, these techniques have not matured to the point\nwhere researchers have trained a modern architecture such as a transformer with\na Lipschitz certificate enforced beyond initialization. To explore this gap, we\nbegin by developing and benchmarking novel, computationally-efficient tools for\nmaintaining norm-constrained weight matrices. Applying these tools, we are able\nto train transformer models with Lipschitz bounds enforced throughout training.\nWe find that optimizer dynamics matter: switching from AdamW to Muon improves\nstandard methods -- weight decay and spectral normalization -- allowing models\nto reach equal performance with a lower Lipschitz bound. Inspired by Muon's\nupdate having a fixed spectral norm, we co-design a weight constraint method\nthat improves the Lipschitz vs. performance tradeoff on MLPs and 2M parameter\ntransformers. Our 2-Lipschitz transformer on Shakespeare text reaches\nvalidation accuracy 60%. Scaling to 145M parameters, our 10-Lipschitz\ntransformer reaches 21% accuracy on internet text. However, to match the\nNanoGPT baseline validation accuracy of 39.4%, our Lipschitz upper bound\nincreases to 10^264. Nonetheless, our Lipschitz transformers train without\nstability measures such as layer norm, QK norm, and logit tanh softcapping.\n","authors":["Laker Newhouse","R. Preston Hess","Franz Cesista","Andrii Zahorodnii","Jeremy Bernstein","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2507.13338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13323v1","updated":"2025-07-17T17:42:29Z","published":"2025-07-17T17:42:29Z","title":"GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic\n  Estimation using LLM","summary":"  Socio-economic indicators like regional GDP, population, and education\nlevels, are crucial to shaping policy decisions and fostering sustainable\ndevelopment. This research introduces GeoReg a regression model that integrates\ndiverse data sources, including satellite imagery and web-based geospatial\ninformation, to estimate these indicators even for data-scarce regions such as\ndeveloping countries. Our approach leverages the prior knowledge of large\nlanguage model (LLM) to address the scarcity of labeled data, with the LLM\nfunctioning as a data engineer by extracting informative features to enable\neffective estimation in few-shot settings. Specifically, our model obtains\ncontextual relationships between data features and the target indicator,\ncategorizing their correlations as positive, negative, mixed, or irrelevant.\nThese features are then fed into the linear estimator with tailored weight\nconstraints for each category. To capture nonlinear patterns, the model also\nidentifies meaningful feature interactions and integrates them, along with\nnonlinear transformations. Experiments across three countries at different\nstages of development demonstrate that our model outperforms baselines in\nestimating socio-economic indicators, even for low-income countries with\nlimited data availability.\n","authors":["Kyeongjin Ahn","Sungwon Han","Seungeon Lee","Donghyun Ahn","Hyoshin Kim","Jungwon Kim","Jihee Kim","Sangyoon Park","Meeyoung Cha"],"pdf_url":"https://arxiv.org/pdf/2507.13323v1.pdf","comment":"15 pages, 13 figures, 7 tables"},{"id":"http://arxiv.org/abs/2504.17703v2","updated":"2025-07-17T17:36:34Z","published":"2025-04-24T16:10:29Z","title":"Federated Learning: A Survey on Privacy-Preserving Collaborative\n  Intelligence","summary":"  Federated Learning (FL) has emerged as a transformative paradigm in the field\nof distributed machine learning, enabling multiple clients such as mobile\ndevices, edge nodes, or organizations to collaboratively train a shared global\nmodel without the need to centralize sensitive data. This decentralized\napproach addresses growing concerns around data privacy, security, and\nregulatory compliance, making it particularly attractive in domains such as\nhealthcare, finance, and smart IoT systems. This survey provides a concise yet\ncomprehensive overview of Federated Learning, beginning with its core\narchitecture and communication protocol. We discuss the standard FL lifecycle,\nincluding local training, model aggregation, and global updates. A particular\nemphasis is placed on key technical challenges such as handling non-IID\n(non-independent and identically distributed) data, mitigating system and\nhardware heterogeneity, reducing communication overhead, and ensuring privacy\nthrough mechanisms like differential privacy and secure aggregation.\nFurthermore, we examine emerging trends in FL research, including personalized\nFL, cross-device versus cross-silo settings, and integration with other\nparadigms such as reinforcement learning and quantum computing. We also\nhighlight real-world applications and summarize benchmark datasets and\nevaluation metrics commonly used in FL research. Finally, we outline open\nresearch problems and future directions to guide the development of scalable,\nefficient, and trustworthy FL systems.\n","authors":["Nusrat Jahan","Ratun Rahman","Michel Wang"],"pdf_url":"https://arxiv.org/pdf/2504.17703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12440v2","updated":"2025-07-17T17:30:47Z","published":"2025-07-16T17:27:44Z","title":"EgoVLA: Learning Vision-Language-Action Models from Egocentric Human\n  Videos","summary":"  Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Ego\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Ego Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA\n","authors":["Ruihan Yang","Qinxi Yu","Yecheng Wu","Rui Yan","Borui Li","An-Chieh Cheng","Xueyan Zou","Yunhao Fang","Hongxu Yin","Sifei Liu","Song Han","Yao Lu","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2507.12440v2.pdf","comment":"More videos can be found on our website:\n  https://rchalyang.github.io/EgoVLA"},{"id":"http://arxiv.org/abs/2502.14819v2","updated":"2025-07-17T17:29:11Z","published":"2025-02-20T18:39:41Z","title":"Learning from Reward-Free Offline Data: A Case for Planning with Latent\n  Dynamics Models","summary":"  A long-standing goal in AI is to build agents that can solve a variety of\ntasks across different environments, including previously unseen ones. Two\ndominant approaches tackle this challenge: (i) reinforcement learning (RL),\nwhich learns policies through trial and error, and (ii) optimal control, which\nplans actions using a learned or known dynamics model. However, their relative\nstrengths and weaknesses remain underexplored in the setting where agents must\nlearn from offline trajectories without reward annotations. In this work, we\nsystematically analyze the performance of different RL and control-based\nmethods under datasets of varying quality. On the RL side, we consider\ngoal-conditioned and zero-shot approaches. On the control side, we train a\nlatent dynamics model using the Joint Embedding Predictive Architecture (JEPA)\nand use it for planning. We study how dataset properties-such as data\ndiversity, trajectory quality, and environment variability-affect the\nperformance of these approaches. Our results show that model-free RL excels\nwhen abundant, high-quality data is available, while model-based planning\nexcels in generalization to novel environment layouts, trajectory stitching,\nand data-efficiency. Notably, planning with a latent dynamics model emerges as\na promising approach for zero-shot generalization from suboptimal data.\n","authors":["Vlad Sobal","Wancong Zhang","Kynghyun Cho","Randall Balestriero","Tim G. J. Rudner","Yann LeCun"],"pdf_url":"https://arxiv.org/pdf/2502.14819v2.pdf","comment":"Project web page: https://latent-planning.github.io/"},{"id":"http://arxiv.org/abs/2507.13305v1","updated":"2025-07-17T17:22:41Z","published":"2025-07-17T17:22:41Z","title":"Boosting Team Modeling through Tempo-Relational Representation Learning","summary":"  Team modeling remains a fundamental challenge at the intersection of\nArtificial Intelligence and the Social Sciences. Social Science research\nemphasizes the need to jointly model dynamics and relations, while practical\napplications demand unified models capable of inferring multiple team\nconstructs simultaneously, providing interpretable insights and actionable\nrecommendations to enhance team performance. However, existing works do not\nmeet these practical demands. To bridge this gap, we present TRENN, a novel\ntempo-relational architecture that integrates: (i) an automatic temporal graph\nextractor, (ii) a tempo-relational encoder, (iii) a decoder for team construct\nprediction, and (iv) two complementary explainability modules. TRENN jointly\ncaptures relational and temporal team dynamics, providing a solid foundation\nfor MT-TRENN, which extends TReNN by replacing the decoder with a multi-task\nhead, enabling the model to learn shared Social Embeddings and simultaneously\npredict multiple team constructs, including Emergent Leadership, Leadership\nStyle, and Teamwork components. Experimental results demonstrate that our\napproach significantly outperforms approaches that rely exclusively on temporal\nor relational information. Additionally, experimental evaluation has shown that\nthe explainability modules integrated in MT-TRENN yield interpretable insights\nand actionable suggestions to support team improvement. These capabilities make\nour approach particularly well-suited for Human-Centered AI applications, such\nas intelligent decision-support systems in high-stakes collaborative\nenvironments.\n","authors":["Vincenzo Marco De Luca","Giovanna Varni","Andrea Passerini"],"pdf_url":"https://arxiv.org/pdf/2507.13305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08589v3","updated":"2025-07-17T17:20:34Z","published":"2024-10-11T07:36:14Z","title":"Retraining-Free Merging of Sparse MoE via Hierarchical Clustering","summary":"  Sparse Mixture-of-Experts (SMoE) models represent a significant advancement\nin large language model (LLM) development through their efficient parameter\nutilization. These models achieve substantial performance improvements at\nreduced inference costs. However, the deployment of SMoE models faces\nconstraints from extensive memory requirements of expert components in\nresource-limited environments. To address these limitations, this paper\nintroduces Hierarchical Clustering for Sparsely activated Mixture of Experts\n(HC-SMoE), a task-agnostic expert merging framework for parameter reduction\nwithout retraining. HC-SMoE introduces a novel hierarchical clustering approach\nbased on expert outputs to ensure merging robustness independent of routing\ndecisions. The proposed output-based clustering method enables effective\ncapture of functional relationships between experts for large-scale\narchitectures. We provide theoretical analysis and comprehensive evaluations\nacross multiple zero-shot language tasks to demonstrate HC-SMoE's effectiveness\nin state-of-the-art models including Qwen and Mixtral. The experimental results\nvalidate HC-SMoE's superior performance and practical applicability for\nreal-world deployments.\n","authors":["I-Chun Chen","Hsu-Shen Liu","Wei-Fang Sun","Chen-Hao Chao","Yen-Chang Hsu","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2410.08589v3.pdf","comment":"Code: https://github.com/wazenmai/HC-SMoE"},{"id":"http://arxiv.org/abs/2505.01455v2","updated":"2025-07-17T17:20:09Z","published":"2025-04-30T19:42:16Z","title":"Advancing Seasonal Prediction of Tropical Cyclone Activity with a Hybrid\n  AI-Physics Climate Model","summary":"  Machine learning (ML) models are successful with weather forecasting and have\nshown progress in climate simulations, yet leveraging them for useful climate\npredictions needs exploration. Here we show this feasibility using Neural\nGeneral Circulation Model (NeuralGCM), a hybrid ML-physics atmospheric model\ndeveloped by Google, for seasonal predictions of large-scale atmospheric\nvariability and Northern Hemisphere tropical cyclone (TC) activity. Inspired by\nphysical model studies, we simplify boundary conditions, assuming sea surface\ntemperature (SST) and sea ice follow their climatological cycle but persist\nanomalies present at the initialization time. With such forcings, NeuralGCM can\ngenerate 100 simulation days in ~8 minutes with a single Graphics Processing\nUnit (GPU), while simulating realistic atmospheric circulation and TC\nclimatology patterns. This configuration yields useful seasonal predictions\n(July to November) for the tropical atmosphere and various TC activity metrics.\nNotably, the predicted and observed TC frequency in the North Atlantic and East\nPacific basins are significantly correlated during 1990 to 2023 (r=~0.7),\nsuggesting prediction skill comparable to existing physical GCMs. Despite\nchallenges associated with model resolution and simplified boundary forcings,\nthe model-predicted interannual variations demonstrate significant correlations\nwith the observation, including the sub-basin TC tracks (p<0.1) and basin-wide\naccumulated cyclone energy (p<0.01) of the North Atlantic and North Pacific\nbasins. These findings highlight the promise of leveraging ML models with\nphysical insights to model TC risks and deliver seamless weather-climate\npredictions.\n","authors":["Gan Zhang","Megha Rao","Janni Yuval","Ming Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.01455v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14048v2","updated":"2025-07-17T17:06:39Z","published":"2025-01-23T19:29:34Z","title":"SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with\n  Equivariant Neural Networks","summary":"  Modern neural networks (NNs) often do not generalize well in the presence of\na \"covariate shift\"; that is, in situations where the training and test data\ndistributions differ, but the conditional distribution of classification labels\nremains unchanged. In such cases, NN generalization can be reduced to a problem\nof learning more domain-invariant features. Domain adaptation (DA) methods\ninclude a range of techniques aimed at achieving this; however, these methods\nhave struggled with the need for extensive hyperparameter tuning, which then\nincurs significant computational costs. In this work, we introduce SIDDA, an\nout-of-the-box DA training algorithm built upon the Sinkhorn divergence, that\ncan achieve effective domain alignment with minimal hyperparameter tuning and\ncomputational overhead. We demonstrate the efficacy of our method on multiple\nsimulated and real datasets of varying complexity, including simple shapes,\nhandwritten digits, and real astronomical observations. SIDDA is compatible\nwith a variety of NN architectures, and it works particularly well in improving\nclassification accuracy and model calibration when paired with equivariant\nneural networks (ENNs). We find that SIDDA enhances the generalization\ncapabilities of NNs, achieving up to a $\\approx40\\%$ improvement in\nclassification accuracy on unlabeled target data. We also study the efficacy of\nDA on ENNs with respect to the varying group orders of the dihedral group\n$D_N$, and find that the model performance improves as the degree of\nequivariance increases. Finally, we find that SIDDA enhances model calibration\non both source and target data--achieving over an order of magnitude\nimprovement in the ECE and Brier score. SIDDA's versatility, combined with its\nautomated approach to domain alignment, has the potential to advance\nmulti-dataset studies by enabling the development of highly generalizable\nmodels.\n","authors":["Sneh Pandya","Purvik Patel","Brian D. Nord","Mike Walmsley","Aleksandra Ćiprijanović"],"pdf_url":"https://arxiv.org/pdf/2501.14048v2.pdf","comment":"25 pages, 5 figures, 4 tables. code available at:\n  https://github.com/deepskies/SIDDA"},{"id":"http://arxiv.org/abs/2504.09085v2","updated":"2025-07-17T17:00:33Z","published":"2025-04-12T05:36:16Z","title":"crowd-hpo: Realistic Hyperparameter Optimization and Benchmarking for\n  Learning from Crowds with Noisy Labels","summary":"  Crowdworking is a cost-efficient solution for acquiring class labels. Since\nthese labels are subject to noise, various approaches to learning from crowds\nhave been proposed. Typically, these approaches are evaluated with default\nhyperparameter configurations, resulting in unfair and suboptimal performance,\nor with hyperparameter configurations tuned via a validation set with ground\ntruth class labels, representing an often unrealistic scenario. Moreover, both\nsetups can produce different approach rankings, complicating study comparisons.\nTherefore, we introduce crowd-hpo as a framework for evaluating approaches to\nlearning from crowds in combination with criteria to select well-performing\nhyperparameter configurations with access only to noisy crowd-labeled\nvalidation data. Extensive experiments with neural networks demonstrate that\nthese criteria select hyperparameter configurations, which improve the learning\nfrom crowd approaches' generalization performances, measured on separate test\nsets with ground truth labels. Hence, incorporating such criteria into\nexperimental studies is essential for enabling fairer and more realistic\nbenchmarking.\n","authors":["Marek Herde","Lukas Lührs","Denis Huseljic","Bernhard Sick"],"pdf_url":"https://arxiv.org/pdf/2504.09085v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2507.13287v1","updated":"2025-07-17T16:53:31Z","published":"2025-07-17T16:53:31Z","title":"Optimal Empirical Risk Minimization under Temporal Distribution Shifts","summary":"  Temporal distribution shifts pose a key challenge for machine learning models\ntrained and deployed in dynamically evolving environments. This paper\nintroduces RIDER (RIsk minimization under Dynamically Evolving Regimes) which\nderives optimally-weighted empirical risk minimization procedures under\ntemporal distribution shifts. Our approach is theoretically grounded in the\nrandom distribution shift model, where random shifts arise as a superposition\nof numerous unpredictable changes in the data-generating process. We show that\ncommon weighting schemes, such as pooling all data, exponentially weighting\ndata, and using only the most recent data, emerge naturally as special cases in\nour framework. We demonstrate that RIDER consistently improves out-of-sample\npredictive performance when applied as a fine-tuning step on the Yearbook\ndataset, across a range of benchmark methods in Wild-Time. Moreover, we show\nthat RIDER outperforms standard weighting strategies in two other real-world\ntasks: predicting stock market volatility and forecasting ride durations in NYC\ntaxi data.\n","authors":["Yujin Jeong","Ramesh Johari","Dominik Rothenhäusler","Emily Fox"],"pdf_url":"https://arxiv.org/pdf/2507.13287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13283v1","updated":"2025-07-17T16:48:45Z","published":"2025-07-17T16:48:45Z","title":"Stochastic Weakly Convex Optimization Under Heavy-Tailed Noises","summary":"  An increasing number of studies have focused on stochastic first-order\nmethods (SFOMs) under heavy-tailed gradient noises, which have been observed in\nthe training of practical deep learning models. In this paper, we focus on two\ntypes of gradient noises: one is sub-Weibull noise, and the other is noise\nunder the assumption that it has a bounded $p$-th central moment ($p$-BCM) with\n$p\\in (1, 2]$. The latter is more challenging due to the occurrence of infinite\nvariance when $p\\in (1, 2)$. Under these two gradient noise assumptions, the\nin-expectation and high-probability convergence of SFOMs have been extensively\nstudied in the contexts of convex optimization and standard smooth\noptimization. However, for weakly convex objectives-a class that includes all\nLipschitz-continuous convex objectives and smooth objectives-our understanding\nof the in-expectation and high-probability convergence of SFOMs under these two\ntypes of noises remains incomplete. We investigate the high-probability\nconvergence of the vanilla stochastic subgradient descent (SsGD) method under\nsub-Weibull noises, as well as the high-probability and in-expectation\nconvergence of clipped SsGD under the $p$-BCM noises. Both analyses are\nconducted in the context of weakly convex optimization. For weakly convex\nobjectives that may be non-convex and non-smooth, our results demonstrate that\nthe theoretical dependence of vanilla SsGD on the failure probability and\nnumber of iterations under sub-Weibull noises does not degrade compared to the\ncase of smooth objectives. Under $p$-BCM noises, our findings indicate that the\nnon-smoothness and non-convexity of weakly convex objectives do not impact the\ntheoretical dependence of clipped SGD on the failure probability relative to\nthe smooth case; however, the sample complexity we derived is worse than a\nwell-known lower bound for smooth optimization.\n","authors":["Tianxi Zhu","Yi Xu","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2507.13283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20277v2","updated":"2025-07-17T16:38:32Z","published":"2025-04-28T21:44:31Z","title":"Generative Diffusion Models for Resource Allocation in Wireless Networks","summary":"  This paper proposes a supervised training algorithm for learning stochastic\nresource allocation policies with generative diffusion models (GDMs). We\nformulate the allocation problem as the maximization of an ergodic utility\nfunction subject to ergodic Quality of Service (QoS) constraints. Given samples\nfrom a stochastic expert policy that yields a near-optimal solution to the\nconstrained optimization problem, we train a GDM policy to imitate the expert\nand generate new samples from the optimal distribution. We achieve near-optimal\nperformance through the sequential execution of the generated samples. To\nenable generalization to a family of network configurations, we parameterize\nthe backward diffusion process with a graph neural network (GNN) architecture.\nWe present numerical results in a case study of power control.\n","authors":["Yigit Berkay Uslu","Samar Hadou","Shirin Saeedi Bidokhti","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2504.20277v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13277v1","updated":"2025-07-17T16:38:14Z","published":"2025-07-17T16:38:14Z","title":"Evaluating Reinforcement Learning Algorithms for Navigation in Simulated\n  Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour","summary":"  Robots are increasingly integrated across industries, particularly in\nhealthcare. However, many valuable applications for quadrupedal robots remain\noverlooked. This research explores the effectiveness of three reinforcement\nlearning algorithms in training a simulated quadruped robot for autonomous\nnavigation and obstacle avoidance. The goal is to develop a robotic guide dog\nsimulation capable of path following and obstacle avoidance, with long-term\npotential for real-world assistance to guide dogs and visually impaired\nindividuals. It also seeks to expand research into medical 'pets', including\nrobotic guide and alert dogs.\n  A comparative analysis of thirteen related research papers shaped key\nevaluation criteria, including collision detection, pathfinding algorithms,\nsensor usage, robot type, and simulation platforms. The study focuses on sensor\ninputs, collision frequency, reward signals, and learning progression to\ndetermine which algorithm best supports robotic navigation in complex\nenvironments.\n  Custom-made environments were used to ensure fair evaluation of all three\nalgorithms under controlled conditions, allowing consistent data collection.\nResults show that Proximal Policy Optimization (PPO) outperformed Deep\nQ-Network (DQN) and Q-learning across all metrics, particularly in average and\nmedian steps to goal per episode.\n  By analysing these results, this study contributes to robotic navigation, AI\nand medical robotics, offering insights into the feasibility of AI-driven\nquadruped mobility and its role in assistive robotics.\n","authors":["Emma M. A. Harrison"],"pdf_url":"https://arxiv.org/pdf/2507.13277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09701v3","updated":"2025-07-17T16:35:19Z","published":"2023-08-18T17:52:12Z","title":"Do you know what q-means?","summary":"  Clustering is one of the most important tools for analysis of large datasets,\nand perhaps the most popular clustering algorithm is Lloyd's algorithm for\n$k$-means. This algorithm takes $n$ vectors\n$V=[v_1,\\dots,v_n]\\in\\mathbb{R}^{d\\times n}$ and outputs $k$ centroids\n$c_1,\\dots,c_k\\in\\mathbb{R}^d$; these partition the vectors into clusters based\non which centroid is closest to a particular vector. We present a classical\n$\\varepsilon$-$k$-means algorithm that performs an approximate version of one\niteration of Lloyd's algorithm with time complexity\n$\\tilde{O}\\big(\\frac{\\|V\\|_F^2}{n}\\frac{k^{2}d}{\\varepsilon^2}(k +\n\\log{n})\\big)$, exponentially improving the dependence on the data size $n$ and\nmatching that of the \"$q$-means\" quantum algorithm originally proposed by\nKerenidis, Landman, Luongo, and Prakash (NeurIPS'19). Moreover, we propose an\nimproved $q$-means quantum algorithm with time complexity\n$\\tilde{O}\\big(\\frac{\\|V\\|_F}{\\sqrt{n}}\\frac{k^{3/2}d}{\\varepsilon}(\\sqrt{k}+\\sqrt{d})(\\sqrt{k}\n+ \\log{n})\\big)$ that quadratically improves the runtime of our classical\n$\\varepsilon$-$k$-means algorithm in several parameters. Our quantum algorithm\ndoes not rely on quantum linear algebra primitives of prior work, but instead\nonly uses QRAM to prepare simple states based on the current iteration's\nclusters and multivariate quantum amplitude estimation. Finally, we provide\nclassical and quantum query lower bounds, showing that our algorithms are\noptimal in most parameters.\n","authors":["Arjan Cornelissen","Joao F. Doriguello","Alessandro Luongo","Ewin Tang"],"pdf_url":"https://arxiv.org/pdf/2308.09701v3.pdf","comment":"21 pages. v2: improved the quantum complexity, references added; v3:\n  new co-author added, new algorithms and upper bounds, improved old upper\n  bounds, new lower bounds, references added"},{"id":"http://arxiv.org/abs/2507.13263v1","updated":"2025-07-17T16:12:39Z","published":"2025-07-17T16:12:39Z","title":"Merge Kernel for Bayesian Optimization on Permutation Space","summary":"  Bayesian Optimization (BO) algorithm is a standard tool for black-box\noptimization problems. The current state-of-the-art BO approach for permutation\nspaces relies on the Mallows kernel-an $\\Omega(n^2)$ representation that\nexplicitly enumerates every pairwise comparison. Inspired by the close\nrelationship between the Mallows kernel and pairwise comparison, we propose a\nnovel framework for generating kernel functions on permutation space based on\nsorting algorithms. Within this framework, the Mallows kernel can be viewed as\na special instance derived from bubble sort. Further, we introduce the\n\\textbf{Merge Kernel} constructed from merge sort, which replaces the quadratic\ncomplexity with $\\Theta(n\\log n)$ to achieve the lowest possible complexity.\nThe resulting feature vector is significantly shorter, can be computed in\nlinearithmic time, yet still efficiently captures meaningful permutation\ndistances. To boost robustness and right-invariance without sacrificing\ncompactness, we further incorporate three lightweight, task-agnostic\ndescriptors: (1) a shift histogram, which aggregates absolute element\ndisplacements and supplies a global misplacement signal; (2) a split-pair line,\nwhich encodes selected long-range comparisons by aligning elements across the\ntwo halves of the whole permutation; and (3) sliding-window motifs, which\nsummarize local order patterns that influence near-neighbor objectives. Our\nempirical evaluation demonstrates that the proposed kernel consistently\noutperforms the state-of-the-art Mallows kernel across various permutation\noptimization benchmarks. Results confirm that the Merge Kernel provides a more\ncompact yet more effective solution for Bayesian optimization in permutation\nspace.\n","authors":["Zikai Xie","Linjiang Chen"],"pdf_url":"https://arxiv.org/pdf/2507.13263v1.pdf","comment":"8 pages, submitted to AAAI-26"},{"id":"http://arxiv.org/abs/2507.13255v1","updated":"2025-07-17T16:04:55Z","published":"2025-07-17T16:04:55Z","title":"Automating Steering for Safe Multimodal Large Language Models","summary":"  Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.\n","authors":["Lyucheng Wu","Mengru Wang","Ziwen Xu","Tri Cao","Nay Oo","Bryan Hooi","Shumin Deng"],"pdf_url":"https://arxiv.org/pdf/2507.13255v1.pdf","comment":"Working in progress. 22 pages (8+ for main); 25 figures; 1 table"},{"id":"http://arxiv.org/abs/2507.11623v2","updated":"2025-07-17T16:00:19Z","published":"2025-07-15T18:01:49Z","title":"A Roadmap for Climate-Relevant Robotics Research","summary":"  Climate change is one of the defining challenges of the 21st century, and\nmany in the robotics community are looking for ways to contribute. This paper\npresents a roadmap for climate-relevant robotics research, identifying\nhigh-impact opportunities for collaboration between roboticists and experts\nacross climate domains such as energy, the built environment, transportation,\nindustry, land use, and Earth sciences. These applications include problems\nsuch as energy systems optimization, construction, precision agriculture,\nbuilding envelope retrofits, autonomous trucking, and large-scale environmental\nmonitoring. Critically, we include opportunities to apply not only physical\nrobots but also the broader robotics toolkit - including planning, perception,\ncontrol, and estimation algorithms - to climate-relevant problems. A central\ngoal of this roadmap is to inspire new research directions and collaboration by\nhighlighting specific, actionable problems at the intersection of robotics and\nclimate. This work represents a collaboration between robotics researchers and\ndomain experts in various climate disciplines, and it serves as an invitation\nto the robotics community to bring their expertise to bear on urgent climate\npriorities.\n","authors":["Alan Papalia","Charles Dawson","Laurentiu L. Anton","Norhan Magdy Bayomi","Bianca Champenois","Jung-Hoon Cho","Levi Cai","Joseph DelPreto","Kristen Edwards","Bilha-Catherine Githinji","Cameron Hickert","Vindula Jayawardana","Matthew Kramer","Shreyaa Raghavan","David Russell","Shide Salimi","Jingnan Shi","Soumya Sudhakar","Yanwei Wang","Shouyi Wang","Luca Carlone","Vijay Kumar","Daniela Rus","John E. Fernandez","Cathy Wu","George Kantor","Derek Young","Hanumant Singh"],"pdf_url":"https://arxiv.org/pdf/2507.11623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13250v1","updated":"2025-07-17T15:59:00Z","published":"2025-07-17T15:59:00Z","title":"Leveraging Asynchronous Cross-border Market Data for Improved Day-Ahead\n  Electricity Price Forecasting in European Markets","summary":"  Accurate short-term electricity price forecasting is crucial for\nstrategically scheduling demand and generation bids in day-ahead markets. While\ndata-driven techniques have shown considerable prowess in achieving high\nforecast accuracy in recent years, they rely heavily on the quality of input\ncovariates. In this paper, we investigate whether asynchronously published\nprices as a result of differing gate closure times (GCTs) in some bidding zones\ncan improve forecasting accuracy in other markets with later GCTs. Using a\nstate-of-the-art ensemble of models, we show significant improvements of 22%\nand 9% in forecast accuracy in the Belgian (BE) and Swedish bidding zones (SE3)\nrespectively, when including price data from interconnected markets with\nearlier GCT (Germany-Luxembourg, Austria, and Switzerland). This improvement\nholds for both general as well as extreme market conditions. Our analysis also\nyields further important insights: frequent model recalibration is necessary\nfor maximum accuracy but comes at substantial additional computational costs,\nand using data from more markets does not always lead to better performance - a\nfact we delve deeper into with interpretability analysis of the forecast\nmodels. Overall, these findings provide valuable guidance for market\nparticipants and decision-makers aiming to optimize bidding strategies within\nincreasingly interconnected and volatile European energy markets.\n","authors":["Maria Margarida Mascarenhas","Jilles De Blauwe","Mikael Amelin","Hussain Kazmi"],"pdf_url":"https://arxiv.org/pdf/2507.13250v1.pdf","comment":"Both Maria Margarida Mascarenhas and Jilles De Blauwe contributed\n  equally to the paper"},{"id":"http://arxiv.org/abs/2408.10996v2","updated":"2025-07-17T15:55:31Z","published":"2024-08-20T16:43:45Z","title":"Approximation Rates for Shallow ReLU$^k$ Neural Networks on Sobolev\n  Spaces via the Radon Transform","summary":"  Let $\\Omega\\subset \\mathbb{R}^d$ be a bounded domain. We consider the problem\nof how efficiently shallow neural networks with the ReLU$^k$ activation\nfunction can approximate functions from Sobolev spaces $W^s(L_p(\\Omega))$ with\nerror measured in the $L_q(\\Omega)$-norm. Utilizing the Radon transform and\nrecent results from discrepancy theory, we provide a simple proof of nearly\noptimal approximation rates in a variety of cases, including when $q\\leq p$,\n$p\\geq 2$, and $s \\leq k + (d+1)/2$. The rates we derive are optimal up to\nlogarithmic factors, and significantly generalize existing results. An\ninteresting consequence is that the adaptivity of shallow ReLU$^k$ neural\nnetworks enables them to obtain optimal approximation rates for smoothness up\nto order $s = k + (d+1)/2$, even though they represent piecewise polynomials of\nfixed degree $k$.\n","authors":["Tong Mao","Jonathan W. Siegel","Jinchao Xu"],"pdf_url":"https://arxiv.org/pdf/2408.10996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13246v1","updated":"2025-07-17T15:55:02Z","published":"2025-07-17T15:55:02Z","title":"The carbon cost of materials discovery: Can machine learning really\n  accelerate the discovery of new photovoltaics?","summary":"  Computational screening has become a powerful complement to experimental\nefforts in the discovery of high-performance photovoltaic (PV) materials. Most\nworkflows rely on density functional theory (DFT) to estimate electronic and\noptical properties relevant to solar energy conversion. Although more efficient\nthan laboratory-based methods, DFT calculations still entail substantial\ncomputational and environmental costs. Machine learning (ML) models have\nrecently gained attention as surrogates for DFT, offering drastic reductions in\nresource use with competitive predictive performance. In this study, we\nreproduce a canonical DFT-based workflow to estimate the maximum efficiency\nlimit and progressively replace its components with ML surrogates. By\nquantifying the CO$_2$ emissions associated with each computational strategy,\nwe evaluate the trade-offs between predictive efficacy and environmental cost.\nOur results reveal multiple hybrid ML/DFT strategies that optimize different\npoints along the accuracy--emissions front. We find that direct prediction of\nscalar quantities, such as maximum efficiency, is significantly more tractable\nthan using predicted absorption spectra as an intermediate step. Interestingly,\nML models trained on DFT data can outperform DFT workflows using alternative\nexchange--correlation functionals in screening applications, highlighting the\nconsistency and utility of data-driven approaches. We also assess strategies to\nimprove ML-driven screening through expanded datasets and improved model\narchitectures tailored to PV-relevant features. This work provides a\nquantitative framework for building low-emission, high-throughput discovery\npipelines.\n","authors":["Matthew Walker","Keith T. Butler"],"pdf_url":"https://arxiv.org/pdf/2507.13246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19530v2","updated":"2025-07-17T15:52:54Z","published":"2025-03-25T10:36:27Z","title":"VectorFit : Adaptive Singular & Bias Vector Fine-Tuning of Pre-trained\n  Foundation Models","summary":"  Popular PEFT methods reduce trainable parameter count for fine-tuning by\nparameterizing new low-rank or sparse trainable weights in parallel to the\nfrozen pre-trained weights $W$. However, these weights are trained from\nscratch, and there exists a performance gap between these methods and full\nfine-tuning, especially in low-budget settings. We introduce VectorFit, a new\nway of parameterization that efficiently utilizes the existing knowledge\nembedded in $W$ by adaptively training their singular vectors and biases. We\nshow that utilizing the structural and transformational properties of $W$ in\nthis way can lead to high-rank incremental weight matrices $\\Delta W$,\ncomparable to that of full fine-tuning. VectorFit delivers superior results\nwith \\textbf{9$\\boldsymbol\\times$} fewer trainable parameters than the leading\nPEFT methods. Through comprehensive experiments across 19 datasets covering a\nwide range of language and vision tasks such as natural language understanding\nand generation, question answering, image classification, and image generation,\nwe demonstrate that VectorFit surpasses baselines in terms of performance as a\nfunction of parameter-efficiency.\n","authors":["Suhas G Hegde","Shilpy Kaur","Aruna Tiwari"],"pdf_url":"https://arxiv.org/pdf/2503.19530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.03225v2","updated":"2025-07-17T15:41:03Z","published":"2025-06-03T11:19:21Z","title":"Multiple-Frequencies Population-Based Training","summary":"  Reinforcement Learning's high sensitivity to hyperparameters is a source of\ninstability and inefficiency, creating significant challenges for\npractitioners. Hyperparameter Optimization (HPO) algorithms have been developed\nto address this issue, among them Population-Based Training (PBT) stands out\nfor its ability to generate hyperparameters schedules instead of fixed\nconfigurations. PBT trains a population of agents, each with its own\nhyperparameters, frequently ranking them and replacing the worst performers\nwith mutations of the best agents. These intermediate selection steps can cause\nPBT to focus on short-term improvements, leading it to get stuck in local\noptima and eventually fall behind vanilla Random Search over longer timescales.\nThis paper studies how this greediness issue is connected to the choice of\nevolution frequency, the rate at which the selection is done. We propose\nMultiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm\nthat addresses greediness by employing sub-populations, each evolving at\ndistinct frequencies. MF-PBT introduces a migration process to transfer\ninformation between sub-populations, with an asymmetric design to balance short\nand long-term optimization. Extensive experiments on the Brax suite demonstrate\nthat MF-PBT improves sample efficiency and long-term performance, even without\nactually tuning hyperparameters.\n","authors":["Waël Doulazmi","Auguste Lehuger","Marin Toromanoff","Valentin Charraut","Thibault Buhet","Fabien Moutarde"],"pdf_url":"https://arxiv.org/pdf/2506.03225v2.pdf","comment":"RLC25 - Camera-ready"},{"id":"http://arxiv.org/abs/2507.13222v1","updated":"2025-07-17T15:35:36Z","published":"2025-07-17T15:35:36Z","title":"Computational-Statistical Tradeoffs from NP-hardness","summary":"  A central question in computer science and statistics is whether efficient\nalgorithms can achieve the information-theoretic limits of statistical\nproblems. Many computational-statistical tradeoffs have been shown under\naverage-case assumptions, but since statistical problems are average-case in\nnature, it has been a challenge to base them on standard worst-case\nassumptions.\n  In PAC learning where such tradeoffs were first studied, the question is\nwhether computational efficiency can come at the cost of using more samples\nthan information-theoretically necessary. We base such tradeoffs on\n$\\mathsf{NP}$-hardness and obtain:\n  $\\circ$ Sharp computational-statistical tradeoffs assuming $\\mathsf{NP}$\nrequires exponential time: For every polynomial $p(n)$, there is an $n$-variate\nclass $C$ with VC dimension $1$ such that the sample complexity of\ntime-efficiently learning $C$ is $\\Theta(p(n))$.\n  $\\circ$ A characterization of $\\mathsf{RP}$ vs. $\\mathsf{NP}$ in terms of\nlearning: $\\mathsf{RP} = \\mathsf{NP}$ iff every $\\mathsf{NP}$-enumerable class\nis learnable with $O(\\mathrm{VCdim}(C))$ samples in polynomial time. The\nforward implication has been known since (Pitt and Valiant, 1988); we prove the\nreverse implication.\n  Notably, all our lower bounds hold against improper learners. These are the\nfirst $\\mathsf{NP}$-hardness results for improperly learning a subclass of\npolynomial-size circuits, circumventing formal barriers of Applebaum, Barak,\nand Xiao (2008).\n","authors":["Guy Blanc","Caleb Koch","Carmen Strassle","Li-Yang Tan"],"pdf_url":"https://arxiv.org/pdf/2507.13222v1.pdf","comment":"To appear at FOCS 2025"},{"id":"http://arxiv.org/abs/2503.08388v3","updated":"2025-07-17T15:30:27Z","published":"2025-03-11T12:53:24Z","title":"V-Max: A Reinforcement Learning Framework for Autonomous Driving","summary":"  Learning-based decision-making has the potential to enable generalizable\nAutonomous Driving (AD) policies, reducing the engineering overhead of\nrule-based approaches. Imitation Learning (IL) remains the dominant paradigm,\nbenefiting from large-scale human demonstration datasets, but it suffers from\ninherent limitations such as distribution shift and imitation gaps.\nReinforcement Learning (RL) presents a promising alternative, yet its adoption\nin AD remains limited due to the lack of standardized and efficient research\nframeworks. To this end, we introduce V-Max, an open research framework\nproviding all the necessary tools to make RL practical for AD. V-Max is built\non Waymax, a hardware-accelerated AD simulator designed for large-scale\nexperimentation. We extend it using ScenarioNet's approach, enabling the fast\nsimulation of diverse AD datasets.\n","authors":["Valentin Charraut","Waël Doulazmi","Thomas Tournaire","Thibault Buhet"],"pdf_url":"https://arxiv.org/pdf/2503.08388v3.pdf","comment":"RLC 25 - Camera-ready"},{"id":"http://arxiv.org/abs/2507.12318v2","updated":"2025-07-17T15:27:20Z","published":"2025-07-16T15:12:17Z","title":"Compositional Discrete Latent Code for High Fidelity, Productive\n  Diffusion Models","summary":"  We argue that diffusion models' success in modeling complex distributions is,\nfor the most part, coming from their input conditioning. This paper\ninvestigates the representation used to condition diffusion models from the\nperspective that ideal representations should improve sample fidelity, be easy\nto generate, and be compositional to allow out-of-training samples generation.\nWe introduce Discrete Latent Code (DLC), an image representation derived from\nSimplicial Embeddings trained with a self-supervised learning objective. DLCs\nare sequences of discrete tokens, as opposed to the standard continuous image\nembeddings. They are easy to generate and their compositionality enables\nsampling of novel images beyond the training distribution. Diffusion models\ntrained with DLCs have improved generation fidelity, establishing a new\nstate-of-the-art for unconditional image generation on ImageNet. Additionally,\nwe show that composing DLCs allows the image generator to produce\nout-of-distribution samples that coherently combine the semantics of images in\ndiverse ways. Finally, we showcase how DLCs can enable text-to-image generation\nby leveraging large-scale pretrained language models. We efficiently finetune a\ntext diffusion language model to generate DLCs that produce novel samples\noutside of the image generator training distribution.\n","authors":["Samuel Lavoie","Michael Noukhovitch","Aaron Courville"],"pdf_url":"https://arxiv.org/pdf/2507.12318v2.pdf","comment":"In submission, 22 pages, 7 tables, 12 figures"},{"id":"http://arxiv.org/abs/2507.13207v1","updated":"2025-07-17T15:16:30Z","published":"2025-07-17T15:16:30Z","title":"MoTM: Towards a Foundation Model for Time Series Imputation based on\n  Continuous Modeling","summary":"  Recent years have witnessed a growing interest for time series foundation\nmodels, with a strong emphasis on the forecasting task. Yet, the crucial task\nof out-of-domain imputation of missing values remains largely underexplored. We\npropose a first step to fill this gap by leveraging implicit neural\nrepresentations (INRs). INRs model time series as continuous functions and\nnaturally handle various missing data scenarios and sampling rates. While they\nhave shown strong performance within specific distributions, they struggle\nunder distribution shifts. To address this, we introduce MoTM (Mixture of\nTimeflow Models), a step toward a foundation model for time series imputation.\nBuilding on the idea that a new time series is a mixture of previously seen\npatterns, MoTM combines a basis of INRs, each trained independently on a\ndistinct family of time series, with a ridge regressor that adapts to the\nobserved context at inference. We demonstrate robust in-domain and\nout-of-domain generalization across diverse imputation scenarios (e.g., block\nand pointwise missingness, variable sampling rates), paving the way for\nadaptable foundation imputation models.\n","authors":["Etienne Le Naour","Tahar Nabil","Ghislain Agoua"],"pdf_url":"https://arxiv.org/pdf/2507.13207v1.pdf","comment":"10th Workshop on Advanced Analytics and Learning on Temporal Data\n  (AALTD), ECML 2025"},{"id":"http://arxiv.org/abs/2506.13916v2","updated":"2025-07-17T15:05:36Z","published":"2025-06-16T18:51:44Z","title":"Branching Stein Variational Gradient Descent for sampling multimodal\n  distributions","summary":"  We propose a novel particle-based variational inference method designed to\nwork with multimodal distributions. Our approach, referred to as Branched Stein\nVariational Gradient Descent (BSVGD), extends the classical Stein Variational\nGradient Descent (SVGD) algorithm by incorporating a random branching mechanism\nthat encourages the exploration of the state space. In this work, a theoretical\nguarantee for the convergence in distribution is presented, as well as\nnumerical experiments to validate the suitability of our algorithm. Performance\ncomparisons between the BSVGD and the SVGD are presented using the Wasserstein\ndistance between samples and the corresponding computational times.\n","authors":["Isaías Bañales","Arturo Jaramillo","Joshué Helí Ricalde-Guerrero"],"pdf_url":"https://arxiv.org/pdf/2506.13916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13194v1","updated":"2025-07-17T15:03:25Z","published":"2025-07-17T15:03:25Z","title":"Relation-Aware Slicing in Cross-Domain Alignment","summary":"  The Sliced Gromov-Wasserstein (SGW) distance, aiming to relieve the\ncomputational cost of solving a non-convex quadratic program that is the\nGromov-Wasserstein distance, utilizes projecting directions sampled uniformly\nfrom unit hyperspheres. This slicing mechanism incurs unnecessary computational\ncosts due to uninformative directions, which also affects the representative\npower of the distance. However, finding a more appropriate distribution over\nthe projecting directions (slicing distribution) is often an optimization\nproblem in itself that comes with its own computational cost. In addition, with\nmore intricate distributions, the sampling itself may be expensive. As a\nremedy, we propose an optimization-free slicing distribution that provides fast\nsampling for the Monte Carlo approximation. We do so by introducing the\nRelation-Aware Projecting Direction (RAPD), effectively capturing the pairwise\nassociation of each of two pairs of random vectors, each following their\nambient law. This enables us to derive the Relation-Aware Slicing Distribution\n(RASD), a location-scale law corresponding to sampled RAPDs. Finally, we\nintroduce the RASGW distance and its variants, e.g., IWRASGW (Importance\nWeighted RASGW), which overcome the shortcomings experienced by SGW. We\ntheoretically analyze its properties and substantiate its empirical prowess\nusing extensive experiments on various alignment tasks.\n","authors":["Dhruv Sarkar","Aprameyo Chakrabartty","Anish Chakrabarty","Swagatam Das"],"pdf_url":"https://arxiv.org/pdf/2507.13194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11192v2","updated":"2025-07-17T15:00:34Z","published":"2025-07-15T10:52:57Z","title":"Recent Advances in Simulation-based Inference for Gravitational Wave\n  Data Analysis","summary":"  The detection of gravitational waves by the LIGO-Virgo-KAGRA collaboration\nhas ushered in a new era of observational astronomy, emphasizing the need for\nrapid and detailed parameter estimation and population-level analyses.\nTraditional Bayesian inference methods, particularly Markov chain Monte Carlo,\nface significant computational challenges when dealing with the\nhigh-dimensional parameter spaces and complex noise characteristics inherent in\ngravitational wave data. This review examines the emerging role of\nsimulation-based inference methods in gravitational wave astronomy, with a\nfocus on approaches that leverage machine-learning techniques such as\nnormalizing flows and neural posterior estimation. We provide a comprehensive\noverview of the theoretical foundations underlying various simulation-based\ninference methods, including neural posterior estimation, neural ratio\nestimation, neural likelihood estimation, flow matching, and consistency\nmodels. We explore the applications of these methods across diverse\ngravitational wave data processing scenarios, from single-source parameter\nestimation and overlapping signal analysis to testing general relativity and\nconducting population studies. Although these techniques demonstrate speed\nimprovements over traditional methods in controlled studies, their\nmodel-dependent nature and sensitivity to prior assumptions are barriers to\ntheir widespread adoption. Their accuracy, which is similar to that of\nconventional methods, requires further validation across broader parameter\nspaces and noise conditions.\n","authors":["Bo Liang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2507.11192v2.pdf","comment":"30 pages, 6 figures, 1 table. Minor clarifications added on page 3.\n  Literature covered up to early 2025"},{"id":"http://arxiv.org/abs/2507.13191v1","updated":"2025-07-17T14:59:24Z","published":"2025-07-17T14:59:24Z","title":"GradNetOT: Learning Optimal Transport Maps with GradNets","summary":"  Monotone gradient functions play a central role in solving the Monge\nformulation of the optimal transport problem, which arises in modern\napplications ranging from fluid dynamics to robot swarm control. When the\ntransport cost is the squared Euclidean distance, Brenier's theorem guarantees\nthat the unique optimal map is the gradient of a convex function, namely a\nmonotone gradient map, and it satisfies a Monge-Amp\\`ere equation. In\n[arXiv:2301.10862] [arXiv:2404.07361], we proposed Monotone Gradient Networks\n(mGradNets), neural networks that directly parameterize the space of monotone\ngradient maps. In this work, we leverage mGradNets to directly learn the\noptimal transport mapping by minimizing a training loss function defined using\nthe Monge-Amp\\`ere equation. We empirically show that the structural bias of\nmGradNets facilitates the learning of optimal transport maps and employ our\nmethod for a robot swarm control problem.\n","authors":["Shreyas Chaudhari","Srinivasa Pranav","José M. F. Moura"],"pdf_url":"https://arxiv.org/pdf/2507.13191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14890v3","updated":"2025-07-17T14:56:17Z","published":"2023-10-20T07:49:10Z","title":"Bounding the Worst-class Error: A Boosting Approach","summary":"  This paper tackles the problem of the worst-class error rate, instead of the\nstandard error rate averaged over all classes. For example, a three-class\nclassification task with class-wise error rates of 10%, 10%, and 40% has a\nworst-class error rate of 40%, whereas the average is 20% under the\nclass-balanced condition. The worst-class error is important in many\napplications. For example, in a medical image classification task, it would not\nbe acceptable for the malignant tumor class to have a 40% error rate, while the\nbenign and healthy classes have a 10% error rates. To avoid overfitting in\nworst-class error minimization using Deep Neural Networks (DNNs), we design a\nproblem formulation for bounding the worst-class error instead of achieving\nzero worst-class error. Moreover, to correctly bound the worst-class error, we\npropose a boosting approach which ensembles DNNs. We give training and\ngeneralization worst-class-error bound. Experimental results show that the\nalgorithm lowers worst-class test error rates while avoiding overfitting to the\ntraining set. This code is available at\nhttps://github.com/saito-yuya/Bounding-the-Worst-class-error-A-Boosting-Approach.\n","authors":["Yuya Saito","Shinnosuke Matsuo","Seiichi Uchida","Daiki Suehiro"],"pdf_url":"https://arxiv.org/pdf/2310.14890v3.pdf","comment":"Accepted at IJCNN2025"},{"id":"http://arxiv.org/abs/2507.13181v1","updated":"2025-07-17T14:50:52Z","published":"2025-07-17T14:50:52Z","title":"Spectral Bellman Method: Unifying Representation and Exploration in RL","summary":"  The effect of representation has been demonstrated in reinforcement learning,\nfrom both theoretical and empirical successes. However, the existing\nrepresentation learning mainly induced from model learning aspects, misaligning\nwith our RL tasks. This work introduces Spectral Bellman Representation, a\nnovel framework derived from the Inherent Bellman Error (IBE) condition, which\naligns with the fundamental structure of Bellman updates across a space of\npossible value functions, therefore, directly towards value-based RL. Our key\ninsight is the discovery of a fundamental spectral relationship: under the\nzero-IBE condition, the transformation of a distribution of value functions by\nthe Bellman operator is intrinsically linked to the feature covariance\nstructure. This spectral connection yields a new, theoretically-grounded\nobjective for learning state-action features that inherently capture this\nBellman-aligned covariance. Our method requires a simple modification to\nexisting algorithms. We demonstrate that our learned representations enable\nstructured exploration, by aligning feature covariance with Bellman dynamics,\nand improve overall performance, particularly in challenging hard-exploration\nand long-horizon credit assignment tasks. Our framework naturally extends to\npowerful multi-step Bellman operators, further broadening its impact. Spectral\nBellman Representation offers a principled and effective path toward learning\nmore powerful and structurally sound representations for value-based\nreinforcement learning.\n","authors":["Ofir Nabati","Bo Dai","Shie Mannor","Guy Tennenholtz"],"pdf_url":"https://arxiv.org/pdf/2507.13181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13170v1","updated":"2025-07-17T14:33:54Z","published":"2025-07-17T14:33:54Z","title":"SHIELD: A Secure and Highly Enhanced Integrated Learning for Robust\n  Deepfake Detection against Adversarial Attacks","summary":"  Audio plays a crucial role in applications like speaker verification,\nvoice-enabled smart devices, and audio conferencing. However, audio\nmanipulations, such as deepfakes, pose significant risks by enabling the spread\nof misinformation. Our empirical analysis reveals that existing methods for\ndetecting deepfake audio are often vulnerable to anti-forensic (AF) attacks,\nparticularly those attacked using generative adversarial networks. In this\narticle, we propose a novel collaborative learning method called SHIELD to\ndefend against generative AF attacks. To expose AF signatures, we integrate an\nauxiliary generative model, called the defense (DF) generative model, which\nfacilitates collaborative learning by combining input and output. Furthermore,\nwe design a triplet model to capture correlations for real and AF attacked\naudios with real-generated and attacked-generated audios using auxiliary\ngenerative models. The proposed SHIELD strengthens the defense against\ngenerative AF attacks and achieves robust performance across various generative\nmodels. The proposed AF significantly reduces the average detection accuracy\nfrom 95.49% to 59.77% for ASVspoof2019, from 99.44% to 38.45% for In-the-Wild,\nand from 98.41% to 51.18% for HalfTruth for three different generative models.\nThe proposed SHIELD mechanism is robust against AF attacks and achieves an\naverage accuracy of 98.13%, 98.58%, and 99.57% in match, and 98.78%, 98.62%,\nand 98.85% in mismatch settings for the ASVspoof2019, In-the-Wild, and\nHalfTruth datasets, respectively.\n","authors":["Kutub Uddin","Awais Khan","Muhammad Umar Farooq","Khalid Malik"],"pdf_url":"https://arxiv.org/pdf/2507.13170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13162v1","updated":"2025-07-17T14:29:34Z","published":"2025-07-17T14:29:34Z","title":"Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World\n  Models","summary":"  Existing world models for autonomous driving struggle with long-horizon\ngeneration and generalization to challenging scenarios. In this work, we\ndevelop a model using simple design choices, and without additional supervision\nor sensors, such as maps, depth, or multiple cameras. We show that our model\nyields state-of-the-art performance, despite having only 469M parameters and\nbeing trained on 280h of video data. It particularly stands out in difficult\nscenarios like turning maneuvers and urban traffic. We test whether discrete\ntoken models possibly have advantages over continuous models based on flow\nmatching. To this end, we set up a hybrid tokenizer that is compatible with\nboth approaches and allows for a side-by-side comparison. Our study concludes\nin favor of the continuous autoregressive model, which is less brittle on\nindividual design choices and more powerful than the model built on discrete\ntokens. Code, models and qualitative results are publicly available at\nhttps://lmb-freiburg.github.io/orbis.github.io/.\n","authors":["Arian Mousakhan","Sudhanshu Mittal","Silvio Galesso","Karim Farid","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2507.13162v1.pdf","comment":"Project page: https://lmb-freiburg.github.io/orbis.github.io/"},{"id":"http://arxiv.org/abs/2507.13158v1","updated":"2025-07-17T14:22:24Z","published":"2025-07-17T14:22:24Z","title":"Inverse Reinforcement Learning Meets Large Language Model Post-Training:\n  Basics, Advances, and Opportunities","summary":"  In the era of Large Language Models (LLMs), alignment has emerged as a\nfundamental yet challenging problem in the pursuit of more reliable,\ncontrollable, and capable machine intelligence. The recent success of reasoning\nmodels and conversational AI systems has underscored the critical role of\nreinforcement learning (RL) in enhancing these systems, driving increased\nresearch interest at the intersection of RL and LLM alignment. This paper\nprovides a comprehensive review of recent advances in LLM alignment through the\nlens of inverse reinforcement learning (IRL), emphasizing the distinctions\nbetween RL techniques employed in LLM alignment and those in conventional RL\ntasks. In particular, we highlight the necessity of constructing neural reward\nmodels from human data and discuss the formal and practical implications of\nthis paradigm shift. We begin by introducing fundamental concepts in RL to\nprovide a foundation for readers unfamiliar with the field. We then examine\nrecent advances in this research agenda, discussing key challenges and\nopportunities in conducting IRL for LLM alignment. Beyond methodological\nconsiderations, we explore practical aspects, including datasets, benchmarks,\nevaluation metrics, infrastructure, and computationally efficient training and\ninference techniques. Finally, we draw insights from the literature on\nsparse-reward RL to identify open questions and potential research directions.\nBy synthesizing findings from diverse studies, we aim to provide a structured\nand critical overview of the field, highlight unresolved challenges, and\noutline promising future directions for improving LLM alignment through RL and\nIRL techniques.\n","authors":["Hao Sun","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2507.13158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13155v1","updated":"2025-07-17T14:17:40Z","published":"2025-07-17T14:17:40Z","title":"NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal\n  Vocalizations with Emotion Annotations for Text-to-Speech","summary":"  Current expressive speech synthesis models are constrained by the limited\navailability of open-source datasets containing diverse nonverbal vocalizations\n(NVs). In this work, we introduce NonverbalTTS (NVTTS), a 17-hour open-access\ndataset annotated with 10 types of NVs (e.g., laughter, coughs) and 8 emotional\ncategories. The dataset is derived from popular sources, VoxCeleb and Expresso,\nusing automated detection followed by human validation. We propose a\ncomprehensive pipeline that integrates automatic speech recognition (ASR), NV\ntagging, emotion classification, and a fusion algorithm to merge transcriptions\nfrom multiple annotators. Fine-tuning open-source text-to-speech (TTS) models\non the NVTTS dataset achieves parity with closed-source systems such as\nCosyVoice2, as measured by both human evaluation and automatic metrics,\nincluding speaker similarity and NV fidelity. By releasing NVTTS and its\naccompanying annotation guidelines, we address a key bottleneck in expressive\nTTS research. The dataset is available at\nhttps://huggingface.co/datasets/deepvk/NonverbalTTS.\n","authors":["Maksim Borisov","Egor Spirin","Daria Diatlova"],"pdf_url":"https://arxiv.org/pdf/2507.13155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13133v1","updated":"2025-07-17T13:54:42Z","published":"2025-07-17T13:54:42Z","title":"NGTM: Substructure-based Neural Graph Topic Model for Interpretable\n  Graph Generation","summary":"  Graph generation plays a pivotal role across numerous domains, including\nmolecular design and knowledge graph construction. Although existing methods\nachieve considerable success in generating realistic graphs, their\ninterpretability remains limited, often obscuring the rationale behind\nstructural decisions. To address this challenge, we propose the Neural Graph\nTopic Model (NGTM), a novel generative framework inspired by topic modeling in\nnatural language processing. NGTM represents graphs as mixtures of latent\ntopics, each defining a distribution over semantically meaningful\nsubstructures, which facilitates explicit interpretability at both local and\nglobal scales. The generation process transparently integrates these topic\ndistributions with a global structural variable, enabling clear semantic\ntracing of each generated graph. Experiments demonstrate that NGTM achieves\ncompetitive generation quality while uniquely enabling fine-grained control and\ninterpretability, allowing users to tune structural features or induce\nbiological properties through topic-level adjustments.\n","authors":["Yuanxin Zhuang","Dazhong Shen","Ying Sun"],"pdf_url":"https://arxiv.org/pdf/2507.13133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04018v2","updated":"2025-07-17T13:44:39Z","published":"2025-02-06T12:19:34Z","title":"PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data","summary":"  This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.\n","authors":["Keonvin Park","Jisu Kim","Jaemin Seo"],"pdf_url":"https://arxiv.org/pdf/2502.04018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13122v1","updated":"2025-07-17T13:38:02Z","published":"2025-07-17T13:38:02Z","title":"Search for Z/2 eigenfunctions on the sphere using machine learning","summary":"  We use machine learning to search for examples of Z/2 eigenfunctions on the\n2-sphere. For this we created a multivalued version of a feedforward deep\nneural network, and we implemented it using the JAX library. We found Z/2\neigenfunctions for three cases: In the first two cases we fixed the branch\npoints at the vertices of a tetrahedron and at a cube respectively. In a third\ncase, we allowed the AI to move the branch points around and, in the end, it\npositioned the branch points at the vertices of a squashed tetrahedron.\n","authors":["Andriy Haydys","Willem Adriaan Salm"],"pdf_url":"https://arxiv.org/pdf/2507.13122v1.pdf","comment":"14 pages, 12 pictures"},{"id":"http://arxiv.org/abs/2507.13120v1","updated":"2025-07-17T13:34:21Z","published":"2025-07-17T13:34:21Z","title":"RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects\n  in Remote Sensing Images","summary":"  Detecting tiny objects in remote sensing (RS) imagery has been a\nlong-standing challenge due to their extremely limited spatial information,\nweak feature representations, and dense distributions across complex\nbackgrounds. Despite numerous efforts devoted, mainstream detectors still\nunderperform in such scenarios. To bridge this gap, we introduce RS-TinyNet, a\nmulti-stage feature fusion and enhancement model explicitly tailored for RS\ntiny object detection in various RS scenarios. RS-TinyNet comes with two novel\ndesigns: tiny object saliency modeling and feature integrity reconstruction.\nGuided by these principles, we design three step-wise feature enhancement\nmodules. Among them, the multi-dimensional collaborative attention (MDCA)\nmodule employs multi-dimensional attention to enhance the saliency of tiny\nobjects. Additionally, the auxiliary reversible branch (ARB) and a progressive\nfusion detection head (PFDH) module are introduced to preserve information flow\nand fuse multi-level features to bridge semantic gaps and retain structural\ndetail. Comprehensive experiments on public RS dataset AI-TOD show that our\nRS-TinyNet surpasses existing state-of-the-art (SOTA) detectors by 4.0% AP and\n6.5% AP75. Evaluations on DIOR benchmark dataset further validate its superior\ndetection performance in diverse RS scenarios. These results demonstrate that\nthe proposed multi-stage feature fusion strategy offers an effective and\npractical solution for tiny object detection in complex RS environments.\n","authors":["Xiaozheng Jiang","Wei Zhang","Xuerui Mao"],"pdf_url":"https://arxiv.org/pdf/2507.13120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07389v2","updated":"2025-07-17T13:24:18Z","published":"2025-04-10T02:19:03Z","title":"Task-Circuit Quantization: Leveraging Knowledge Localization and\n  Interpretability for Compression","summary":"  Post-training quantization (PTQ) reduces a model's memory footprint by\nmapping full precision weights into low bit weights without costly retraining,\nbut can degrade its downstream performance especially in low 2- to 3-bit\nsettings. We develop a new mixed-precision PTQ approach, Task-Circuit\nQuantization (TaCQ), that draws parallels to automated circuit discovery,\ndirectly conditioning the quantization process on specific weight circuits --\nwhich we define as sets of weights associated with downstream task performance.\nThese weights are kept as 16-bit weights, while others are quantized,\nmaintaining performance while only adding a marginal memory cost. Specifically,\nTaCQ contrasts unquantized model weights with a uniformly-quantized model to\nestimate the expected change in weights due to quantization and uses gradient\ninformation to predict the resulting impact on task performance, allowing us to\npreserve task-specific weights. We compare TaCQ-based quantization to existing\nmixed-precision quantization methods when conditioning both on general-purpose\nand task-specific data. Across QA, math reasoning, and text-to-SQL tasks for\nboth Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the\nsame calibration data and a lower weight budget, achieving major improvements\nin the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of\nLlama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25%\nabsolute improvement over SPQR. We also observe consistently large gains over\nexisting methods in the 2-bit regime, with an average gain of 14.74% over the\nstrongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without\nconditioning on specific tasks, showing TaCQ's ability to identify important\nweights is not limited to task-conditioned settings.\n","authors":["Hanqi Xiao","Yi-Lin Sung","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2504.07389v2.pdf","comment":"COLM 2025 Camera Ready. Code:\n  https://github.com/The-Inscrutable-X/TACQ"},{"id":"http://arxiv.org/abs/2507.13106v1","updated":"2025-07-17T13:21:42Z","published":"2025-07-17T13:21:42Z","title":"Deep Learning-Based Fetal Lung Segmentation from Diffusion-weighted MRI\n  Images and Lung Maturity Evaluation for Fetal Growth Restriction","summary":"  Fetal lung maturity is a critical indicator for predicting neonatal outcomes\nand the need for post-natal intervention, especially for pregnancies affected\nby fetal growth restriction. Intra-voxel incoherent motion analysis has shown\npromising results for non-invasive assessment of fetal lung development, but\nits reliance on manual segmentation is time-consuming, thus limiting its\nclinical applicability. In this work, we present an automated lung maturity\nevaluation pipeline for diffusion-weighted magnetic resonance images that\nconsists of a deep learning-based fetal lung segmentation model and a\nmodel-fitting lung maturity assessment. A 3D nnU-Net model was trained on\nmanually segmented images selected from the baseline frames of 4D\ndiffusion-weighted MRI scans. The segmentation model demonstrated robust\nperformance, yielding a mean Dice coefficient of 82.14%. Next, voxel-wise model\nfitting was performed based on both the nnU-Net-predicted and manual lung\nsegmentations to quantify IVIM parameters reflecting tissue microstructure and\nperfusion. The results suggested no differences between the two. Our work shows\nthat a fully automated pipeline is possible for supporting fetal lung maturity\nassessment and clinical decision-making.\n","authors":["Zhennan Xiao","Katharine Brudkiewicz","Zhen Yuan","Rosalind Aughwane","Magdalena Sokolska","Joanna Chappell","Trevor Gaunt","Anna L. David","Andrew P. King","Andrew Melbourne"],"pdf_url":"https://arxiv.org/pdf/2507.13106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13105v1","updated":"2025-07-17T13:19:50Z","published":"2025-07-17T13:19:50Z","title":"SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated\n  Summaries For Scientific Abstracts","summary":"  We introduce SemCSE, an unsupervised method for learning semantic embeddings\nof scientific texts. Building on recent advances in contrastive learning for\ntext embeddings, our approach leverages LLM-generated summaries of scientific\nabstracts to train a model that positions semantically related summaries closer\ntogether in the embedding space. This resulting objective ensures that the\nmodel captures the true semantic content of a text, in contrast to traditional\ncitation-based approaches that do not necessarily reflect semantic similarity.\nTo validate this, we propose a novel benchmark designed to assess a model's\nability to understand and encode the semantic content of scientific texts,\ndemonstrating that our method enforces a stronger semantic separation within\nthe embedding space. Additionally, we evaluate SemCSE on the comprehensive\nSciRepEval benchmark for scientific text embeddings, where it achieves\nstate-of-the-art performance among models of its size, thus highlighting the\nbenefits of a semantically focused training approach.\n","authors":["Marc Brinner","Sina Zarriess"],"pdf_url":"https://arxiv.org/pdf/2507.13105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23114v4","updated":"2025-07-17T13:18:06Z","published":"2024-10-30T15:25:06Z","title":"Unified Triplet-Level Hallucination Evaluation for Large Vision-Language\n  Models","summary":"  Despite the outstanding performance in vision-language reasoning, Large\nVision-Language Models (LVLMs) might generate hallucinated contents that do not\nexist in the given image. Most existing LVLM hallucination benchmarks are\nconstrained to evaluate the object-related hallucinations. However, the\npotential hallucination on the relations between two objects, i.e., relation\nhallucination, still lacks investigation. To remedy that, we design a unified\nframework to measure the object and relation hallucination in LVLMs\nsimultaneously. The core idea of our framework is to evaluate hallucinations\nvia (object, relation, object) triplets extracted from LVLMs' responses, making\nit easily generalizable to different vision-language tasks. Based on our\nframework, we further introduce Tri-HE, a novel Triplet-level Hallucination\nEvaluation benchmark which can be used to study both object and relation\nhallucination at the same time. With comprehensive evaluations on Tri-HE, we\nobserve that the relation hallucination issue is even more serious than object\nhallucination among existing LVLMs, highlighting a previously neglected problem\ntowards reliable LVLMs. Moreover, based on our findings, we design a simple\ntraining-free approach that effectively mitigates hallucinations for LVLMs. Our\ndataset and code for the reproduction of our experiments are available publicly\nat https://github.com/wujunjie1998/Tri-HE.\n","authors":["Junjie Wu","Tsz Ting Chung","Kai Chen","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2410.23114v4.pdf","comment":"Accepted by TMLR 2025. Project Page:\n  https://kaichen1998.github.io/projects/tri-he/"},{"id":"http://arxiv.org/abs/2505.20755v2","updated":"2025-07-17T13:16:31Z","published":"2025-05-27T05:55:45Z","title":"Uni-Instruct: One-step Diffusion Model through Unified Diffusion\n  Divergence Instruction","summary":"  In this paper, we unify more than 10 existing one-step diffusion distillation\napproaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside a\ntheory-driven framework which we name the \\textbf{\\emph{Uni-Instruct}}.\nUni-Instruct is motivated by our proposed diffusion expansion theory of the\n$f$-divergence family. Then we introduce key theories that overcome the\nintractability issue of the original expanded $f$-divergence, resulting in an\nequivalent yet tractable loss that effectively trains one-step diffusion models\nby minimizing the expanded $f$-divergence family. The novel unification\nintroduced by Uni-Instruct not only offers new theoretical contributions that\nhelp understand existing approaches from a high-level perspective but also\nleads to state-of-the-art one-step diffusion generation performances. On the\nCIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet\nInception Distance (FID) values of \\textbf{\\emph{1.46}} for unconditional\ngeneration and \\textbf{\\emph{1.38}} for conditional generation. On the\nImageNet-$64\\times 64$ generation benchmark, Uni-Instruct achieves a new SoTA\none-step generation FID of \\textbf{\\emph{1.02}}, which outperforms its 79-step\nteacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).\nWe also apply Uni-Instruct on broader tasks like text-to-3D generation. For\ntext-to-3D generation, Uni-Instruct gives decent results, which slightly\noutperforms previous methods, such as SDS and VSD, in terms of both generation\nquality and diversity. Both the solid theoretical and empirical contributions\nof Uni-Instruct will potentially help future studies on one-step diffusion\ndistillation and knowledge transferring of diffusion models.\n","authors":["Yifei Wang","Weimin Bai","Colin Zhang","Debing Zhang","Weijian Luo","He Sun"],"pdf_url":"https://arxiv.org/pdf/2505.20755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13094v1","updated":"2025-07-17T13:06:24Z","published":"2025-07-17T13:06:24Z","title":"Unsupervised Ground Metric Learning","summary":"  Data classification without access to labeled samples remains a challenging\nproblem. It usually depends on an appropriately chosen distance between\nfeatures, a topic addressed in metric learning. Recently, Huizing, Cantini and\nPeyr\\'e proposed to simultaneously learn optimal transport (OT) cost matrices\nbetween samples and features of the dataset. This leads to the task of finding\npositive eigenvectors of a certain nonlinear function that maps cost matrices\nto OT distances. Having this basic idea in mind, we consider both the\nalgorithmic and the modeling part of unsupervised metric learning. First, we\nexamine appropriate algorithms and their convergence. In particular, we propose\nto use the stochastic random function iteration algorithm and prove that it\nconverges linearly for our setting, although our operators are not\nparacontractive as it was required for convergence so far. Second, we ask the\nnatural question if the OT distance can be replaced by other distances. We show\nhow Mahalanobis-like distances fit into our considerations. Further, we examine\nan approach via graph Laplacians. In contrast to the previous settings, we have\njust to deal with linear functions in the wanted matrices here, so that simple\nalgorithms from linear algebra can be applied.\n","authors":["Janis Auffenberg","Jonas Bresch","Oleh Melnyk","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2507.13094v1.pdf","comment":"10 figures, 1 table"},{"id":"http://arxiv.org/abs/2503.16395v4","updated":"2025-07-17T13:05:40Z","published":"2025-03-20T17:53:35Z","title":"Truthful Elicitation of Imprecise Forecasts","summary":"  The quality of probabilistic forecasts is crucial for decision-making under\nuncertainty. While proper scoring rules incentivize truthful reporting of\nprecise forecasts, they fall short when forecasters face epistemic uncertainty\nabout their beliefs, limiting their use in safety-critical domains where\ndecision-makers (DMs) prioritize proper uncertainty management. To address\nthis, we propose a framework for scoring imprecise forecasts -- forecasts given\nas a set of beliefs. Despite existing impossibility results for deterministic\nscoring rules, we enable truthful elicitation by drawing connection to social\nchoice theory and introducing a two-way communication framework where DMs first\nshare their aggregation rules (e.g., averaging or min-max) used in downstream\ndecisions for resolving forecast ambiguity. This, in turn, helps forecasters\nresolve indecision during elicitation. We further show that truthful\nelicitation of imprecise forecasts is achievable using proper scoring rules\nrandomized over the aggregation procedure. Our approach allows DM to elicit and\nintegrate the forecaster's epistemic uncertainty into their decision-making\nprocess, thus improving credibility.\n","authors":["Anurag Singh","Siu Lun Chau","Krikamol Muandet"],"pdf_url":"https://arxiv.org/pdf/2503.16395v4.pdf","comment":"Accepted at UAI 2025 for Oral Presentation (fixed formatting)"},{"id":"http://arxiv.org/abs/2507.13092v1","updated":"2025-07-17T13:03:20Z","published":"2025-07-17T13:03:20Z","title":"Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype\n  Learning for Multimodal Brain-Computer Interfaces","summary":"  Electroencephalography (EEG) is a fundamental modality for cognitive state\nmonitoring in brain-computer interfaces (BCIs). However, it is highly\nsusceptible to intrinsic signal errors and human-induced labeling errors, which\nlead to label noise and ultimately degrade model performance. To enhance EEG\nlearning, multimodal knowledge distillation (KD) has been explored to transfer\nknowledge from visual models with rich representations to EEG-based models.\nNevertheless, KD faces two key challenges: modality gap and soft label\nmisalignment. The former arises from the heterogeneous nature of EEG and visual\nfeature spaces, while the latter stems from label inconsistencies that create\ndiscrepancies between ground truth labels and distillation targets. This paper\naddresses semantic uncertainty caused by ambiguous features and weakly defined\nlabels. We propose a novel cross-modal knowledge distillation framework that\nmitigates both modality and label inconsistencies. It aligns feature semantics\nthrough a prototype-based similarity module and introduces a task-specific\ndistillation head to resolve label-induced inconsistency in supervision.\nExperimental results demonstrate that our approach improves EEG-based emotion\nregression and classification performance, outperforming both unimodal and\nmultimodal baselines on a public multimodal dataset. These findings highlight\nthe potential of our framework for BCI applications.\n","authors":["Hyo-Jeong Jang","Hye-Bin Shin","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2507.13092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19086v2","updated":"2025-07-17T13:01:22Z","published":"2024-07-26T21:07:17Z","title":"Super Resolution for Renewable Energy Resource Data With Wind From\n  Reanalysis Data and Application to Ukraine","summary":"  With a potentially increasing share of the electricity grid relying on wind\nto provide generating capacity and energy, there is an expanding global need\nfor historically accurate, spatiotemporally continuous, high-resolution wind\ndata. Conventional downscaling methods for generating these data based on\nnumerical weather prediction have a high computational burden and require\nextensive tuning for historical accuracy. In this work, we present a novel deep\nlearning-based spatiotemporal downscaling method using generative adversarial\nnetworks (GANs) for generating historically accurate high-resolution wind\nresource data from the European Centre for Medium-Range Weather Forecasting\nReanalysis version 5 data (ERA5). In contrast to previous approaches, which\nused coarsened high-resolution data as low-resolution training data, we use\ntrue low-resolution simulation outputs. We show that by training a GAN model\nwith ERA5 as the low-resolution input and Wind Integration National Dataset\nToolkit (WTK) data as the high-resolution target, we achieved results\ncomparable in historical accuracy and spatiotemporal variability to\nconventional dynamical downscaling. This GAN-based downscaling method\nadditionally reduces computational costs over dynamical downscaling by two\norders of magnitude. We applied this approach to downscale 30 km, hourly ERA5\ndata to 2 km, 5 min wind data for January 2000 through December 2023 at\nmultiple hub heights over Ukraine, Moldova, and part of Romania. This 24-year\ndata record is the first member of the super-resolution for renewable energy\nresource data with wind from the reanalysis data dataset (Sup3rWind).\n","authors":["Brandon N. Benton","Grant Buster","Pavlo Pinchuk","Andrew Glaws","Ryan N. King","Galen Maclaurin","Ilya Chernyakhovskiy"],"pdf_url":"https://arxiv.org/pdf/2407.19086v2.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2507.13090v1","updated":"2025-07-17T12:59:27Z","published":"2025-07-17T12:59:27Z","title":"MUPAX: Multidimensional Problem Agnostic eXplainable AI","summary":"  Robust XAI techniques should ideally be simultaneously deterministic, model\nagnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM\nAGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability\ntechnique, with guaranteed convergency. MUPAX measure theoretic formulation\ngives principled feature importance attribution through structured perturbation\nanalysis that discovers inherent input patterns and eliminates spurious\nrelationships. We evaluate MUPAX on an extensive range of data modalities and\ntasks: audio classification (1D), image classification (2D), volumetric medical\nimage analysis (3D), and anatomical landmark detection, demonstrating dimension\nagnostic effectiveness. The rigorous convergence guarantees extend to any loss\nfunction and arbitrary dimensions, making MUPAX applicable to virtually any\nproblem context for AI. By contrast with other XAI methods that typically\ndecrease performance when masking, MUPAX not only preserves but actually\nenhances model accuracy by capturing only the most important patterns of the\noriginal data. Extensive benchmarking against the state of the XAI art\ndemonstrates MUPAX ability to generate precise, consistent and understandable\nexplanations, a crucial step towards explainable and trustworthy AI systems.\nThe source code will be released upon publication.\n","authors":["Vincenzo Dentamaro","Felice Franchini","Giuseppe Pirlo","Irina Voiculescu"],"pdf_url":"https://arxiv.org/pdf/2507.13090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13079v1","updated":"2025-07-17T12:48:00Z","published":"2025-07-17T12:48:00Z","title":"DASViT: Differentiable Architecture Search for Vision Transformer","summary":"  Designing effective neural networks is a cornerstone of deep learning, and\nNeural Architecture Search (NAS) has emerged as a powerful tool for automating\nthis process. Among the existing NAS approaches, Differentiable Architecture\nSearch (DARTS) has gained prominence for its efficiency and ease of use,\ninspiring numerous advancements. Since the rise of Vision Transformers (ViT),\nresearchers have applied NAS to explore ViT architectures, often focusing on\nmacro-level search spaces and relying on discrete methods like evolutionary\nalgorithms. While these methods ensure reliability, they face challenges in\ndiscovering innovative architectural designs, demand extensive computational\nresources, and are time-intensive. To address these limitations, we introduce\nDifferentiable Architecture Search for Vision Transformer (DASViT), which\nbridges the gap in differentiable search for ViTs and uncovers novel designs.\nExperiments show that DASViT delivers architectures that break traditional\nTransformer encoder designs, outperform ViT-B/16 on multiple datasets, and\nachieve superior efficiency with fewer parameters and FLOPs.\n","authors":["Pengjin Wu","Ferrante Neri","Zhenhua Feng"],"pdf_url":"https://arxiv.org/pdf/2507.13079v1.pdf","comment":"Accepted to the International Joint Conference on Neural Networks\n  (IJCNN) 2025"},{"id":"http://arxiv.org/abs/2507.03404v2","updated":"2025-07-17T12:47:46Z","published":"2025-07-04T09:12:23Z","title":"On the Effectiveness of the z-Transform Method in Quadratic Optimization","summary":"  The z-transform of a sequence is a classical tool used within signal\nprocessing, control theory, computer science, and electrical engineering. It\nallows for studying sequences from their generating functions, with many\noperations that can be equivalently defined on the original sequence and its\n$z$-transform. In particular, the z-transform method focuses on asymptotic\nbehaviors and allows the use of Taylor expansions. We present a sequence of\nresults of increasing significance and difficulty for linear models and\noptimization algorithms, demonstrating the effectiveness and versatility of the\nz-transform method in deriving new asymptotic results. Starting from the\nsimplest gradient descent iterations in an infinite-dimensional Hilbert space,\nwe show how the spectral dimension characterizes the convergence behavior. We\nthen extend the analysis to Nesterov acceleration, averaging techniques, and\nstochastic gradient descent.\n","authors":["Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2507.03404v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02994v5","updated":"2025-07-17T12:30:16Z","published":"2024-07-03T10:49:21Z","title":"MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced\n  AI Applications with Retrieval Augmented Generation and Knowledge Graphs","summary":"  The increasing interest in developing Artificial Intelligence applications in\nthe medical domain, suffers from the lack of high-quality data set, mainly due\nto privacy-related issues. In addition, the recent increase in Vision Language\nModels (VLM) leads to the need for multimodal medical data sets, where clinical\nreports and findings are attached to the corresponding medical scans. This\npaper illustrates the entire workflow for building the MedPix 2.0 data set.\nStarting with the well-known multimodal data set\nMedPix\\textsuperscript{\\textregistered}, mainly used by physicians, nurses, and\nhealthcare students for Continuing Medical Education purposes, a semi-automatic\npipeline was developed to extract visual and textual data followed by a manual\ncuring procedure in which noisy samples were removed, thus creating a MongoDB\ndatabase. Along with the data set, we developed a Graphical User Interface\naimed at navigating efficiently the MongoDB instance and obtaining the raw data\nthat can be easily used for training and/or fine-tuning VLMs. To enforce this\npoint, in this work, we first recall DR-Minerva, a Retrieve Augmented\nGeneration-based VLM model trained upon MedPix 2.0. DR-Minerva predicts the\nbody part and the modality used to scan its input image. We also propose the\nextension of DR-Minerva with a Knowledge Graph that uses Llama 3.1 Instruct 8B,\nand leverages MedPix 2.0. The resulting architecture can be queried in a\nend-to-end manner, as a medical decision support system. MedPix 2.0 is\navailable on GitHub.\n","authors":["Irene Siragusa","Salvatore Contino","Massimo La Ciura","Rosario Alicata","Roberto Pirrone"],"pdf_url":"https://arxiv.org/pdf/2407.02994v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13054v1","updated":"2025-07-17T12:26:25Z","published":"2025-07-17T12:26:25Z","title":"On statistical learning of graphs","summary":"  We study PAC and online learnability of hypothesis classes formed by copies\nof a countably infinite graph G, where each copy is induced by permuting G's\nvertices. This corresponds to learning a graph's labeling, knowing its\nstructure and label set. We consider classes where permutations move only\nfinitely many vertices. Our main result shows that PAC learnability of all such\nfinite-support copies implies online learnability of the full isomorphism type\nof G, and is equivalent to the condition of automorphic triviality. We also\ncharacterize graphs where copies induced by swapping two vertices are not\nlearnable, using a relaxation of the extension property of the infinite random\ngraph. Finally, we show that, for all G and k>2, learnability for k-vertex\npermutations is equivalent to that for 2-vertex permutations, yielding a\nfour-class partition of infinite graphs, whose complexity we also determine\nusing tools coming from both descriptive set theory and computability theory.\n","authors":["Vittorio Cipriani","Valentino Delle Rose","Luca San Mauro","Giovanni Solda"],"pdf_url":"https://arxiv.org/pdf/2507.13054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17571v2","updated":"2025-07-17T12:24:49Z","published":"2024-11-26T16:34:24Z","title":"Uncertainty quantification for White Matter Hyperintensity segmentation\n  detects silent failures and improves automated Fazekas quantification","summary":"  White Matter Hyperintensities (WMH) are key neuroradiological markers of\nsmall vessel disease present in brain MRI. Assessment of WMH is important in\nresearch and clinics. However, WMH are challenging to segment due to their high\nvariability in shape, location, size, poorly defined borders, and similar\nintensity profile to other pathologies (e.g stroke lesions) and artefacts (e.g\nhead motion). In this work, we assess the utility and semantic properties of\nthe most effective techniques for uncertainty quantification (UQ) in\nsegmentation for the WMH segmentation task across multiple test-time data\ndistributions. We find UQ techniques reduce 'silent failure' by identifying in\nUQ maps small WMH clusters in the deep white matter that are unsegmented by the\nmodel. A combination of Stochastic Segmentation Networks with Deep Ensembles\nalso yields the highest Dice and lowest Absolute Volume Difference % (AVD)\nscore and can highlight areas where there is ambiguity between WMH and stroke\nlesions. We further demonstrate the downstream utility of UQ, proposing a novel\nmethod for classification of the clinical Fazekas score using spatial features\nextracted from voxelwise WMH probability and UQ maps. We show that\nincorporating WMH uncertainty information improves Fazekas classification\nperformance and calibration. Our model with (UQ and spatial WMH\nfeatures)/(spatial WMH features)/(WMH volume only) achieves a balanced accuracy\nscore of 0.74/0.67/0.62, and root brier score of 0.65/0.72/0.74 in the Deep WMH\nand balanced accuracy of 0.74/0.73/0.71 and root brier score of 0.64/0.66/0.68\nin the Periventricular region. We further demonstrate that stochastic UQ\ntechniques with high sample diversity can improve the detection of poor quality\nsegmentations.\n","authors":["Ben Philps","Maria del C. Valdes Hernandez","Chen Qin","Una Clancy","Eleni Sakka","Susana Munoz Maniega","Mark E. Bastin","Angela C. C. Jochems","Joanna M. Wardlaw","Miguel O. Bernabeu","Alzheimers Disease Neuroimaging Initiative"],"pdf_url":"https://arxiv.org/pdf/2411.17571v2.pdf","comment":"34 pages (or 19 not including appendix) 28 figures (or 10 not\n  including appendix)"},{"id":"http://arxiv.org/abs/2507.13043v1","updated":"2025-07-17T12:16:04Z","published":"2025-07-17T12:16:04Z","title":"The Power of Architecture: Deep Dive into Transformer Architectures for\n  Long-Term Time Series Forecasting","summary":"  Transformer-based models have recently become dominant in Long-term Time\nSeries Forecasting (LTSF), yet the variations in their architecture, such as\nencoder-only, encoder-decoder, and decoder-only designs, raise a crucial\nquestion: What Transformer architecture works best for LTSF tasks? However,\nexisting models are often tightly coupled with various time-series-specific\ndesigns, making it difficult to isolate the impact of the architecture itself.\nTo address this, we propose a novel taxonomy that disentangles these designs,\nenabling clearer and more unified comparisons of Transformer architectures. Our\ntaxonomy considers key aspects such as attention mechanisms, forecasting\naggregations, forecasting paradigms, and normalization layers. Through\nextensive experiments, we uncover several key insights: bi-directional\nattention with joint-attention is most effective; more complete forecasting\naggregation improves performance; and the direct-mapping paradigm outperforms\nautoregressive approaches. Furthermore, our combined model, utilizing optimal\narchitectural choices, consistently outperforms several existing models,\nreinforcing the validity of our conclusions. We hope these findings offer\nvaluable guidance for future research on Transformer architectural designs in\nLTSF. Our code is available at https://github.com/HALF111/TSF_architecture.\n","authors":["Lefei Shen","Mouxiang Chen","Han Fu","Xiaoxue Ren","Xiaoyun Joy Wang","Jianling Sun","Zhuo Li","Chenghao Liu"],"pdf_url":"https://arxiv.org/pdf/2507.13043v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.13034v1","updated":"2025-07-17T12:06:08Z","published":"2025-07-17T12:06:08Z","title":"Confidence-Filtered Relevance (CFR): An Interpretable and\n  Uncertainty-Aware Machine Learning Framework for Naturalness Assessment in\n  Satellite Imagery","summary":"  Protected natural areas play a vital role in ecological balance and ecosystem\nservices. Monitoring these regions at scale using satellite imagery and machine\nlearning is promising, but current methods often lack interpretability and\nuncertainty-awareness, and do not address how uncertainty affects naturalness\nassessment. In contrast, we propose Confidence-Filtered Relevance (CFR), a\ndata-centric framework that combines LRP Attention Rollout with Deep\nDeterministic Uncertainty (DDU) estimation to analyze how model uncertainty\ninfluences the interpretability of relevance heatmaps. CFR partitions the\ndataset into subsets based on uncertainty thresholds, enabling systematic\nanalysis of how uncertainty shapes the explanations of naturalness in satellite\nimagery. Applied to the AnthroProtect dataset, CFR assigned higher relevance to\nshrublands, forests, and wetlands, aligning with other research on naturalness\nassessment. Moreover, our analysis shows that as uncertainty increases, the\ninterpretability of these relevance heatmaps declines and their entropy grows,\nindicating less selective and more ambiguous attributions. CFR provides a\ndata-centric approach to assess the relevance of patterns to naturalness in\nsatellite imagery based on their associated certainty.\n","authors":["Ahmed Emam","Ribana Roscher"],"pdf_url":"https://arxiv.org/pdf/2507.13034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13033v1","updated":"2025-07-17T12:04:15Z","published":"2025-07-17T12:04:15Z","title":"(Exhaustive) Symbolic Regression and model selection by minimum\n  description length","summary":"  Symbolic regression is the machine learning method for learning functions\nfrom data. After a brief overview of the symbolic regression landscape, I will\ndescribe the two main challenges that traditional algorithms face: they have an\nunknown (and likely significant) probability of failing to find any given good\nfunction, and they suffer from ambiguity and poorly-justified assumptions in\ntheir function-selection procedure. To address these I propose an exhaustive\nsearch and model selection by the minimum description length principle, which\nallows accuracy and complexity to be directly traded off by measuring each in\nunits of information. I showcase the resulting publicly available Exhaustive\nSymbolic Regression algorithm on three open problems in astrophysics: the\nexpansion history of the universe, the effective behaviour of gravity in\ngalaxies and the potential of the inflaton field. In each case the algorithm\nidentifies many functions superior to the literature standards. This general\npurpose methodology should find widespread utility in science and beyond.\n","authors":["Harry Desmond"],"pdf_url":"https://arxiv.org/pdf/2507.13033v1.pdf","comment":"15 pages, 4 figures; Invited review for the Royal Society\n  Philosophical Transactions A special issue \"Symbolic regression in the\n  physical sciences\""},{"id":"http://arxiv.org/abs/2507.13024v1","updated":"2025-07-17T11:52:27Z","published":"2025-07-17T11:52:27Z","title":"When Pattern-by-Pattern Works: Theoretical and Empirical Insights for\n  Logistic Models with Missing Values","summary":"  Predicting a response with partially missing inputs remains a challenging\ntask even in parametric models, since parameter estimation in itself is not\nsufficient to predict on partially observed inputs. Several works study\nprediction in linear models. In this paper, we focus on logistic models, which\npresent their own difficulties. From a theoretical perspective, we prove that a\nPattern-by-Pattern strategy (PbP), which learns one logistic model per\nmissingness pattern, accurately approximates Bayes probabilities in various\nmissing data scenarios (MCAR, MAR and MNAR). Empirically, we thoroughly compare\nvarious methods (constant and iterative imputations, complete case analysis,\nPbP, and an EM algorithm) across classification, probability estimation,\ncalibration, and parameter inference. Our analysis provides a comprehensive\nview on the logistic regression with missing values. It reveals that mean\nimputation can be used as baseline for low sample sizes, and improved\nperformance is obtained via nonlinear multiple iterative imputation techniques\nwith the labels (MICE.RF.Y). For large sample sizes, PbP is the best method for\nGaussian mixtures, and we recommend MICE.RF.Y in presence of nonlinear\nfeatures.\n","authors":["Christophe Muller","Erwan Scornet","Julie Josse"],"pdf_url":"https://arxiv.org/pdf/2507.13024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.13022v1","updated":"2025-07-17T11:50:29Z","published":"2025-07-17T11:50:29Z","title":"Fault detection and diagnosis for the engine electrical system of a\n  space launcher based on a temporal convolutional autoencoder and calibrated\n  classifiers","summary":"  In the context of the health monitoring for the next generation of reusable\nspace launchers, we outline a first step toward developing an onboard fault\ndetection and diagnostic capability for the electrical system that controls the\nengine valves. Unlike existing approaches in the literature, our solution is\ndesigned to meet a broader range of key requirements. This includes estimating\nconfidence levels for predictions, detecting out-of-distribution (OOD) cases,\nand controlling false alarms. The proposed solution is based on a temporal\nconvolutional autoencoder to automatically extract low-dimensional features\nfrom raw sensor data. Fault detection and diagnosis are respectively carried\nout using a binary and a multiclass classifier trained on the autoencoder\nlatent and residual spaces. The classifiers are histogram-based gradient\nboosting models calibrated to output probabilities that can be interpreted as\nconfidence levels. A relatively simple technique, based on inductive conformal\nanomaly detection, is used to identify OOD data. We leverage other simple yet\neffective techniques, such as cumulative sum control chart (CUSUM) to limit the\nfalse alarms, and threshold moving to address class imbalance in fault\ndetection. The proposed framework is highly configurable and has been evaluated\non simulated data, covering both nominal and anomalous operational scenarios.\nThe results indicate that our solution is a promising first step, though\ntesting with real data will be necessary to ensure that it achieves the\nrequired maturity level for operational use.\n","authors":["Luis Basora","Louison Bocquet-Nouaille","Elinirina Robinson","Serge Le Gonidec"],"pdf_url":"https://arxiv.org/pdf/2507.13022v1.pdf","comment":"53 pages, 16 figures"},{"id":"http://arxiv.org/abs/2502.05668v3","updated":"2025-07-17T11:44:07Z","published":"2025-02-08T19:09:16Z","title":"The late-stage training dynamics of (stochastic) subgradient descent on\n  homogeneous neural networks","summary":"  We analyze the implicit bias of constant step stochastic subgradient descent\n(SGD). We consider the setting of binary classification with homogeneous neural\nnetworks - a large class of deep neural networks with ReLU-type activation\nfunctions such as MLPs and CNNs without biases. We interpret the dynamics of\nnormalized SGD iterates as an Euler-like discretization of a conservative field\nflow that is naturally associated to the normalized classification margin.\nOwing to this interpretation, we show that normalized SGD iterates converge to\nthe set of critical points of the normalized margin at late-stage training\n(i.e., assuming that the data is correctly classified with positive normalized\nmargin). Up to our knowledge, this is the first extension of the analysis of\nLyu and Li (2020) on the discrete dynamics of gradient descent to the nonsmooth\nand stochastic setting. Our main result applies to binary classification with\nexponential or logistic losses. We additionally discuss extensions to more\ngeneral settings.\n","authors":["Sholom Schechtman","Nicolas Schreuder"],"pdf_url":"https://arxiv.org/pdf/2502.05668v3.pdf","comment":"Accepted/presented at the 38th Annual Conference on Learning Theory\n  (COLT 2025)"},{"id":"http://arxiv.org/abs/2507.13001v1","updated":"2025-07-17T11:18:08Z","published":"2025-07-17T11:18:08Z","title":"SMART: Relation-Aware Learning of Geometric Representations for\n  Knowledge Graphs","summary":"  Knowledge graph representation learning approaches provide a mapping between\nsymbolic knowledge in the form of triples in a knowledge graph (KG) and their\nfeature vectors. Knowledge graph embedding (KGE) models often represent\nrelations in a KG as geometric transformations. Most state-of-the-art (SOTA)\nKGE models are derived from elementary geometric transformations (EGTs), such\nas translation, scaling, rotation, and reflection, or their combinations. These\ngeometric transformations enable the models to effectively preserve specific\nstructural and relational patterns of the KG. However, the current use of EGTs\nby KGEs remains insufficient without considering relation-specific\ntransformations. Although recent models attempted to address this problem by\nensembling SOTA baseline models in different ways, only a single or composite\nversion of geometric transformations are used by such baselines to represent\nall the relations. In this paper, we propose a framework that evaluates how\nwell each relation fits with different geometric transformations. Based on this\nranking, the model can: (1) assign the best-matching transformation to each\nrelation, or (2) use majority voting to choose one transformation type to apply\nacross all relations. That is, the model learns a single relation-specific EGT\nin low dimensional vector space through an attention mechanism. Furthermore, we\nuse the correlation between relations and EGTs, which are learned in a low\ndimension, for relation embeddings in a high dimensional vector space. The\neffectiveness of our models is demonstrated through comprehensive evaluations\non three benchmark KGs as well as a real-world financial KG, witnessing a\nperformance comparable to leading models\n","authors":["Kossi Amouzouvi","Bowen Song","Andrea Coletta","Luigi Bellomarini","Jens Lehmann","Sahar Vahdati"],"pdf_url":"https://arxiv.org/pdf/2507.13001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12998v1","updated":"2025-07-17T11:13:44Z","published":"2025-07-17T11:13:44Z","title":"Differential-informed Sample Selection Accelerates Multimodal\n  Contrastive Learning","summary":"  The remarkable success of contrastive-learning-based multimodal models has\nbeen greatly driven by training on ever-larger datasets with expensive compute\nconsumption. Sample selection as an alternative efficient paradigm plays an\nimportant direction to accelerate the training process. However, recent\nadvances on sample selection either mostly rely on an oracle model to offline\nselect a high-quality coreset, which is limited in the cold-start scenarios, or\nfocus on online selection based on real-time model predictions, which has not\nsufficiently or efficiently considered the noisy correspondence. To address\nthis dilemma, we propose a novel Differential-Informed Sample Selection\n(DISSect) method, which accurately and efficiently discriminates the noisy\ncorrespondence for training acceleration. Specifically, we rethink the impact\nof noisy correspondence on contrastive learning and propose that the\ndifferential between the predicted correlation of the current model and that of\na historical model is more informative to characterize sample quality. Based on\nthis, we construct a robust differential-based sample selection and analyze its\ntheoretical insights. Extensive experiments on three benchmark datasets and\nvarious downstream tasks demonstrate the consistent superiority of DISSect over\ncurrent state-of-the-art methods. Source code is available at:\nhttps://github.com/MediaBrain-SJTU/DISSect.\n","authors":["Zihua Zhao","Feng Hong","Mengxi Chen","Pengyi Chen","Benyuan Liu","Jiangchao Yao","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2507.12998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10015v3","updated":"2025-07-17T11:10:58Z","published":"2025-07-14T07:51:01Z","title":"(Almost) Free Modality Stitching of Foundation Models","summary":"  Foundation multi-modal models are often designed by stitching of multiple\nexisting pretrained uni-modal models: for example, an image classifier with an\ntext model. This stitching process is performed by training a connector module\nthat aims to align the representation spaces of these uni-modal models towards\na multi-modal objective. However, given the complexity of training such\nconnectors on large scale web-based datasets coupled with the ever-increasing\nnumber of available pretrained uni-modal models, the task of uni-modal models\nselection and subsequent connector module training becomes computationally\ndemanding. To address this under-studied critical problem, we propose\nHypernetwork Model Alignment (Hyma), a novel all-in-one solution for optimal\nuni-modal model selection and connector training by leveraging hypernetworks.\nSpecifically, our framework utilizes the parameter prediction capability of a\nhypernetwork to obtain jointly trained connector modules for $N \\times M$\ncombinations of uni-modal models. In our experiments, Hyma reduces the cost of\nsearching for the best performing uni-modal model pair by $10\\times$, while\nmatching the ranking and trained connector performance obtained via grid search\nacross a suite of diverse multi-modal benchmarks.\n","authors":["Jaisidh Singh","Diganta Misra","Boris Knyazev","Antonio Orvieto"],"pdf_url":"https://arxiv.org/pdf/2507.10015v3.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2507.12990v1","updated":"2025-07-17T10:57:49Z","published":"2025-07-17T10:57:49Z","title":"Teach Old SAEs New Domain Tricks with Boosting","summary":"  Sparse Autoencoders have emerged as powerful tools for interpreting the\ninternal representations of Large Language Models, yet they often fail to\ncapture domain-specific features not prevalent in their training corpora. This\npaper introduces a residual learning approach that addresses this feature\nblindness without requiring complete retraining. We propose training a\nsecondary SAE specifically to model the reconstruction error of a pretrained\nSAE on domain-specific texts, effectively capturing features missed by the\nprimary model. By summing the outputs of both models during inference, we\ndemonstrate significant improvements in both LLM cross-entropy and explained\nvariance metrics across multiple specialized domains. Our experiments show that\nthis method efficiently incorporates new domain knowledge into existing SAEs\nwhile maintaining their performance on general tasks. This approach enables\nresearchers to selectively enhance SAE interpretability for specific domains of\ninterest, opening new possibilities for targeted mechanistic interpretability\nof LLMs.\n","authors":["Nikita Koriagin","Yaroslav Aksenov","Daniil Laptev","Gleb Gerasimov","Nikita Balagansky","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2507.12990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12988v1","updated":"2025-07-17T10:54:17Z","published":"2025-07-17T10:54:17Z","title":"Variance-Based Pruning for Accelerating and Compressing Trained Networks","summary":"  Increasingly expensive training of ever larger models such as Vision\nTransfomers motivate reusing the vast library of already trained\nstate-of-the-art networks. However, their latency, high computational costs and\nmemory demands pose significant challenges for deployment, especially on\nresource-constrained hardware. While structured pruning methods can reduce\nthese factors, they often require costly retraining, sometimes for up to\nhundreds of epochs, or even training from scratch to recover the lost accuracy\nresulting from the structural modifications. Maintaining the provided\nperformance of trained models after structured pruning and thereby avoiding\nextensive retraining remains a challenge. To solve this, we introduce\nVariance-Based Pruning, a simple and structured one-shot pruning technique for\nefficiently compressing networks, with minimal finetuning. Our approach first\ngathers activation statistics, which are used to select neurons for pruning.\nSimultaneously the mean activations are integrated back into the model to\npreserve a high degree of performance. On ImageNet-1k recognition tasks, we\ndemonstrate that directly after pruning DeiT-Base retains over 70% of its\noriginal performance and requires only 10 epochs of fine-tuning to regain 99%\nof the original accuracy while simultaneously reducing MACs by 35% and model\nsize by 36%, thus speeding up the model by 1.44x.\n","authors":["Uranik Berisha","Jens Mehnert","Alexandru Paul Condurache"],"pdf_url":"https://arxiv.org/pdf/2507.12988v1.pdf","comment":"Accepted at IEEE/CVF International Conference on Computer Vision\n  (ICCV) 2025"},{"id":"http://arxiv.org/abs/2507.12983v1","updated":"2025-07-17T10:36:27Z","published":"2025-07-17T10:36:27Z","title":"FedGA: A Fair Federated Learning Framework Based on the Gini Coefficient","summary":"  Fairness has emerged as one of the key challenges in federated learning. In\nhorizontal federated settings, data heterogeneity often leads to substantial\nperformance disparities across clients, raising concerns about equitable model\nbehavior. To address this issue, we propose FedGA, a fairness-aware federated\nlearning algorithm. We first employ the Gini coefficient to measure the\nperformance disparity among clients. Based on this, we establish a relationship\nbetween the Gini coefficient $G$ and the update scale of the global model\n${U_s}$, and use this relationship to adaptively determine the timing of\nfairness intervention. Subsequently, we dynamically adjust the aggregation\nweights according to the system's real-time fairness status, enabling the\nglobal model to better incorporate information from clients with relatively\npoor performance.We conduct extensive experiments on the Office-Caltech-10,\nCIFAR-10, and Synthetic datasets. The results show that FedGA effectively\nimproves fairness metrics such as variance and the Gini coefficient, while\nmaintaining strong overall performance, demonstrating the effectiveness of our\napproach.\n","authors":["ShanBin Liu"],"pdf_url":"https://arxiv.org/pdf/2507.12983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12979v1","updated":"2025-07-17T10:31:31Z","published":"2025-07-17T10:31:31Z","title":"A Distributed Generative AI Approach for Heterogeneous Multi-Domain\n  Environments under Data Sharing constraints","summary":"  Federated Learning has gained increasing attention for its ability to enable\nmultiple nodes to collaboratively train machine learning models without sharing\ntheir raw data. At the same time, Generative AI -- particularly Generative\nAdversarial Networks (GANs) -- have achieved remarkable success across a wide\nrange of domains, such as healthcare, security, and Image Generation. However,\ntraining generative models typically requires large datasets and significant\ncomputational resources, which are often unavailable in real-world settings.\nAcquiring such resources can be costly and inefficient, especially when many\nunderutilized devices -- such as IoT devices and edge devices -- with varying\ncapabilities remain idle. Moreover, obtaining large datasets is challenging due\nto privacy concerns and copyright restrictions, as most devices are unwilling\nto share their data. To address these challenges, we propose a novel approach\nfor decentralized GAN training that enables the utilization of distributed data\nand underutilized, low-capability devices while not sharing data in its raw\nform. Our approach is designed to tackle key challenges in decentralized\nenvironments, combining KLD-weighted Clustered Federated Learning to address\nthe issues of data heterogeneity and multi-domain datasets, with Heterogeneous\nU-Shaped split learning to tackle the challenge of device heterogeneity under\nstrict data sharing constraints -- ensuring that no labels or raw data, whether\nreal or synthetic, are ever shared between nodes. Experimental results shows\nthat our approach demonstrates consistent and significant improvements across\nkey performance metrics, where it achieves 1.1x -- 2.2x higher image generation\nscores, an average 10% boost in classification metrics (up to 50% in\nmulti-domain non-IID settings), in much lower latency compared to several\nbenchmarks. Find our code at https://github.com/youssefga28/HuSCF-GAN.\n","authors":["Youssef Tawfilis","Hossam Amer","Minar El-Aasser","Tallal Elshabrawy"],"pdf_url":"https://arxiv.org/pdf/2507.12979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12969v1","updated":"2025-07-17T10:14:20Z","published":"2025-07-17T10:14:20Z","title":"WaveletInception Networks for Drive-by Vibration-Based Infrastructure\n  Health Monitoring","summary":"  This paper presents a novel deep learning-based framework for infrastructure\nhealth monitoring using drive-by vibration response signals. Recognizing the\nimportance of spectral and temporal information, we introduce the\nWaveletInception-BiLSTM network. The WaveletInception feature extractor\nutilizes a Learnable Wavelet Packet Transform (LWPT) as the stem for extracting\nvibration signal features, incorporating spectral information in the early\nnetwork layers. This is followed by 1D Inception networks that extract\nmulti-scale, high-level features at deeper layers. The extracted vibration\nsignal features are then integrated with operational conditions via a Long\nShort-term Memory (LSTM) layer. The resulting feature extraction network\neffectively analyzes drive-by vibration signals across various measurement\nspeeds without preprocessing and uses LSTM to capture interrelated temporal\ndependencies among different modes of information and to create feature vectors\nfor health condition estimation. The estimator head is designed with a\nsequential modeling architecture using bidirectional LSTM (BiLSTM) networks,\ncapturing bi-directional temporal relationships from drive-by measurements.\nThis architecture allows for a high-resolution, beam-level assessment of\ninfrastructure health conditions. A case study focusing on railway track\nstiffness estimation with simulated drive-by vibration signals shows that the\nmodel significantly outperforms state-of-the-art methods in estimating railway\nballast and railpad stiffness parameters. Results underscore the potential of\nthis approach for accurate, localized, and fully automated drive-by\ninfrastructure health monitoring.\n","authors":["Reza Riahi Samani","Alfredo Nunez","Bart De Schutter"],"pdf_url":"https://arxiv.org/pdf/2507.12969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12966v1","updated":"2025-07-17T10:06:43Z","published":"2025-07-17T10:06:43Z","title":"Investigating Forecasting Models for Pandemic Infections Using\n  Heterogeneous Data Sources: A 2-year Study with COVID-19","summary":"  Emerging in December 2019, the COVID-19 pandemic caused widespread health,\neconomic, and social disruptions. Rapid global transmission overwhelmed\nhealthcare systems, resulting in high infection rates, hospitalisations, and\nfatalities. To minimise the spread, governments implemented several\nnon-pharmaceutical interventions like lockdowns and travel restrictions. While\neffective in controlling transmission, these measures also posed significant\neconomic and societal challenges. Although the WHO declared COVID-19 no longer\na global health emergency in May 2023, its impact persists, shaping public\nhealth strategies. The vast amount of data collected during the pandemic offers\nvaluable insights into disease dynamics, transmission, and intervention\neffectiveness. Leveraging these insights can improve forecasting models,\nenhancing preparedness and response to future outbreaks while mitigating their\nsocial and economic impact. This paper presents a large-scale case study on\nCOVID-19 forecasting in Cyprus, utilising a two-year dataset that integrates\nepidemiological data, vaccination records, policy measures, and weather\nconditions. We analyse infection trends, assess forecasting performance, and\nexamine the influence of external factors on disease dynamics. The insights\ngained contribute to improved pandemic preparedness and response strategies.\n","authors":["Zacharias Komodromos","Kleanthis Malialis","Panayiotis Kolios"],"pdf_url":"https://arxiv.org/pdf/2507.12966v1.pdf","comment":"Keywords: epidemiology, pandemic forecasting, COVID-19, infections,\n  machine learning Accepted: IEEE Conference on Computational Intelligence in\n  Bioinformatics and Computational Biology (CIBCB) 2025"},{"id":"http://arxiv.org/abs/2507.12964v1","updated":"2025-07-17T10:03:57Z","published":"2025-07-17T10:03:57Z","title":"Demographic-aware fine-grained classification of pediatric wrist\n  fractures","summary":"  Wrist pathologies are frequently observed, particularly among children who\nconstitute the majority of fracture cases. However, diagnosing these conditions\nis time-consuming and requires specialized expertise. Computer vision presents\na promising avenue, contingent upon the availability of extensive datasets, a\nnotable challenge in medical imaging. Therefore, reliance solely on one\nmodality, such as images, proves inadequate, especially in an era of diverse\nand plentiful data types. In this study, we employ a multifaceted approach to\naddress the challenge of recognizing wrist pathologies using an extremely\nlimited dataset. Initially, we approach the problem as a fine-grained\nrecognition task, aiming to identify subtle X-ray pathologies that conventional\nCNNs overlook. Secondly, we enhance network performance by fusing patient\nmetadata with X-ray images. Thirdly, rather than pre-training on a\ncoarse-grained dataset like ImageNet, we utilize weights trained on a\nfine-grained dataset. While metadata integration has been used in other medical\ndomains, this is a novel application for wrist pathologies. Our results show\nthat a fine-grained strategy and metadata integration improve diagnostic\naccuracy by 2% with a limited dataset and by over 10% with a larger\nfracture-focused dataset.\n","authors":["Ammar Ahmed","Ali Shariq Imran","Zenun Kastrati","Sher Muhammad Daudpota"],"pdf_url":"https://arxiv.org/pdf/2507.12964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12963v1","updated":"2025-07-17T10:02:57Z","published":"2025-07-17T10:02:57Z","title":"A Spectral Interpretation of Redundancy in a Graph Reservoir","summary":"  Reservoir computing has been successfully applied to graphs as a\npreprocessing method to improve the training efficiency of Graph Neural\nNetworks (GNNs). However, a common issue that arises when repeatedly applying\nlayer operators on graphs is over-smoothing, which consists in the convergence\nof graph signals toward low-frequency components of the graph Laplacian. This\nwork revisits the definition of the reservoir in the Multiresolution Reservoir\nGraph Neural Network (MRGNN), a spectral reservoir model, and proposes a\nvariant based on a Fairing algorithm originally introduced in the field of\nsurface design in computer graphics. This algorithm provides a pass-band\nspectral filter that allows smoothing without shrinkage, and it can be adapted\nto the graph setting through the Laplacian operator. Given its spectral\nformulation, this method naturally connects to GNN architectures for tasks\nwhere smoothing, when properly controlled, can be beneficial,such as graph\nclassification. The core contribution of the paper lies in the theoretical\nanalysis of the algorithm from a random walks perspective. In particular, it\nshows how tuning the spectral coefficients can be interpreted as modulating the\ncontribution of redundant random walks. Exploratory experiments based on the\nMRGNN architecture illustrate the potential of this approach and suggest\npromising directions for future research.\n","authors":["Anna Bison","Alessandro Sperduti"],"pdf_url":"https://arxiv.org/pdf/2507.12963v1.pdf","comment":"This paper has been accepted for presentation at the 3rd\n  International Workshop on Reservoir Computing (RC 2025) at ICANN 2025"},{"id":"http://arxiv.org/abs/2407.20209v3","updated":"2025-07-17T10:02:31Z","published":"2024-07-29T17:40:04Z","title":"Characterizing Dynamical Stability of Stochastic Gradient Descent in\n  Overparameterized Learning","summary":"  For overparameterized optimization tasks, such as those found in modern\nmachine learning, global minima are generally not unique. In order to\nunderstand generalization in these settings, it is vital to study to which\nminimum an optimization algorithm converges. The possibility of having minima\nthat are unstable under the dynamics imposed by the optimization algorithm\nlimits the potential minima that the algorithm can find. In this paper, we\ncharacterize the global minima that are dynamically stable/unstable for both\ndeterministic and stochastic gradient descent (SGD). In particular, we\nintroduce a characteristic Lyapunov exponent that depends on the local dynamics\naround a global minimum and rigorously prove that the sign of this Lyapunov\nexponent determines whether SGD can accumulate at the respective global\nminimum.\n","authors":["Dennis Chemnitz","Maximilian Engel"],"pdf_url":"https://arxiv.org/pdf/2407.20209v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07195v2","updated":"2025-07-17T10:01:55Z","published":"2024-12-10T05:08:39Z","title":"A Progressive Image Restoration Network for High-order Degradation\n  Imaging in Remote Sensing","summary":"  Recently, deep learning methods have gained remarkable achievements in the\nfield of image restoration for remote sensing (RS). However, most existing RS\nimage restoration methods focus mainly on conventional first-order degradation\nmodels, which may not effectively capture the imaging mechanisms of remote\nsensing images. Furthermore, many RS image restoration approaches that use deep\nlearning are often criticized for their lacks of architecture transparency and\nmodel interpretability. To address these problems, we propose a novel\nprogressive restoration network for high-order degradation imaging (HDI-PRNet),\nto progressively restore different image degradation. HDI-PRNet is developed\nbased on the theoretical framework of degradation imaging, also Markov\nproperties of the high-order degradation process and Maximum a posteriori (MAP)\nestimation, offering the benefit of mathematical interpretability within the\nunfolding network. The framework is composed of three main components: a module\nfor image denoising that relies on proximal mapping prior learning, a module\nfor image deblurring that integrates Neumann series expansion with dual-domain\ndegradation learning, and a module for super-resolution. Extensive experiments\ndemonstrate that our method achieves superior performance on both synthetic and\nreal remote sensing images.\n","authors":["Yujie Feng","Yin Yang","Xiaohong Fan","Zhengpeng Zhang","Lijing Bu","Jianping Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.07195v2.pdf","comment":"17 pages, Accepted to Transactions on Geoscience and Remote Sensing\n  (TGRS), July 16, 2025"},{"id":"http://arxiv.org/abs/2507.09966v2","updated":"2025-07-17T09:49:45Z","published":"2025-07-14T06:32:59Z","title":"A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with\n  Cross-Modal Semantic Guidance and Multi-Level Feature Fusion","summary":"  Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is\nessential for neuro-oncology diagnosis and treatment planning. Despite advances\nin deep learning methods, automatic segmentation remains challenging due to\ntumor morphological heterogeneity and complex three-dimensional spatial\nrelationships. Current techniques primarily rely on visual features extracted\nfrom MRI sequences while underutilizing semantic knowledge embedded in medical\nreports. This research presents a multi-level fusion architecture that\nintegrates pixel-level, feature-level, and semantic-level information,\nfacilitating comprehensive processing from low-level data to high-level\nconcepts. The semantic-level fusion pathway combines the semantic understanding\ncapabilities of Contrastive Language-Image Pre-training (CLIP) models with the\nspatial feature extraction advantages of 3D U-Net through three mechanisms:\n3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based\nattention mechanisms. Experimental validation on the BraTS 2020 dataset\ndemonstrates that the proposed model achieves an overall Dice coefficient of\n0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with\na 7.3% Dice coefficient increase in the clinically important enhancing tumor\n(ET) region.\n","authors":["Mingda Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.09966v2.pdf","comment":"13 pages,6 figures"},{"id":"http://arxiv.org/abs/2507.12953v1","updated":"2025-07-17T09:48:53Z","published":"2025-07-17T09:48:53Z","title":"cIDIR: Conditioned Implicit Neural Representation for Regularized\n  Deformable Image Registration","summary":"  Regularization is essential in deformable image registration (DIR) to ensure\nthat the estimated Deformation Vector Field (DVF) remains smooth, physically\nplausible, and anatomically consistent. However, fine-tuning regularization\nparameters in learning-based DIR frameworks is computationally expensive, often\nrequiring multiple training iterations. To address this, we propose cIDI, a\nnovel DIR framework based on Implicit Neural Representations (INRs) that\nconditions the registration process on regularization hyperparameters. Unlike\nconventional methods that require retraining for each regularization\nhyperparameter setting, cIDIR is trained over a prior distribution of these\nhyperparameters, then optimized over the regularization hyperparameters by\nusing the segmentations masks as an observation. Additionally, cIDIR models a\ncontinuous and differentiable DVF, enabling seamless integration of advanced\nregularization techniques via automatic differentiation. Evaluated on the\nDIR-LAB dataset, $\\operatorname{cIDIR}$ achieves high accuracy and robustness\nacross the dataset.\n","authors":["Sidaty El Hadramy","Oumeymah Cherkaoui","Philippe C. Cattin"],"pdf_url":"https://arxiv.org/pdf/2507.12953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01840v2","updated":"2025-07-17T09:47:18Z","published":"2025-01-03T14:43:57Z","title":"Signal Recovery Using a Spiked Mixture Model","summary":"  We introduce the spiked mixture model (SMM) to address the problem of\nestimating a set of signals from many randomly scaled and noisy observations.\nSubsequently, we design a novel expectation-maximization (EM) algorithm to\nrecover all parameters of the SMM. Numerical experiments show that in low\nsignal-to-noise ratio regimes, and for data types where the SMM is relevant,\nSMM surpasses the more traditional Gaussian mixture model (GMM) in terms of\nsignal recovery performance. The broad relevance of the SMM and its\ncorresponding EM recovery algorithm is demonstrated by applying the technique\nto different data types. The first case study is a biomedical research\napplication, utilizing an imaging mass spectrometry dataset to explore the\nmolecular content of a rat brain tissue section at micrometer scale. The second\ncase study demonstrates SMM performance in a computer vision application,\nsegmenting a hyperspectral imaging dataset into underlying patterns. While the\nmeasurement modalities differ substantially, in both case studies SMM is shown\nto recover signals that were missed by traditional methods such as k-means\nclustering and GMM.\n","authors":["Paul-Louis Delacour","Sander Wahls","Jeffrey M. Spraggins","Lukasz Migas","Raf Van de Plas"],"pdf_url":"https://arxiv.org/pdf/2501.01840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11129v2","updated":"2025-07-17T09:45:51Z","published":"2025-07-15T09:29:29Z","title":"MMOne: Representing Multiple Modalities in One Scene","summary":"  Humans perceive the world through multimodal cues to understand and interact\nwith the environment. Learning a scene representation for multiple modalities\nenhances comprehension of the physical world. However, modality conflicts,\narising from inherent distinctions among different modalities, present two\ncritical challenges: property disparity and granularity disparity. To address\nthese challenges, we propose a general framework, MMOne, to represent multiple\nmodalities in one scene, which can be readily extended to additional\nmodalities. Specifically, a modality modeling module with a novel modality\nindicator is proposed to capture the unique properties of each modality.\nAdditionally, we design a multimodal decomposition mechanism to separate\nmulti-modal Gaussians into single-modal Gaussians based on modality\ndifferences. We address the essential distinctions among modalities by\ndisentangling multimodal information into shared and modality-specific\ncomponents, resulting in a more compact and efficient multimodal scene\nrepresentation. Extensive experiments demonstrate that our method consistently\nenhances the representation capability for each modality and is scalable to\nadditional modalities. The code is available at\nhttps://github.com/Neal2020GitHub/MMOne.\n","authors":["Zhifeng Gu","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2507.11129v2.pdf","comment":"Accepted to ICCV 2025"},{"id":"http://arxiv.org/abs/2507.12950v1","updated":"2025-07-17T09:43:20Z","published":"2025-07-17T09:43:20Z","title":"Insights into a radiology-specialised multimodal large language model\n  with sparse autoencoders","summary":"  Interpretability can improve the safety, transparency and trust of AI models,\nwhich is especially important in healthcare applications where decisions often\ncarry significant consequences. Mechanistic interpretability, particularly\nthrough the use of sparse autoencoders (SAEs), offers a promising approach for\nuncovering human-interpretable features within large transformer-based models.\nIn this study, we apply Matryoshka-SAE to the radiology-specialised multimodal\nlarge language model, MAIRA-2, to interpret its internal representations. Using\nlarge-scale automated interpretability of the SAE features, we identify a range\nof clinically relevant concepts - including medical devices (e.g., line and\ntube placements, pacemaker presence), pathologies such as pleural effusion and\ncardiomegaly, longitudinal changes and textual features. We further examine the\ninfluence of these features on model behaviour through steering, demonstrating\ndirectional control over generations with mixed success. Our results reveal\npractical and methodological challenges, yet they offer initial insights into\nthe internal concepts learned by MAIRA-2 - marking a step toward deeper\nmechanistic understanding and interpretability of a radiology-adapted\nmultimodal large language model, and paving the way for improved model\ntransparency. We release the trained SAEs and interpretations:\nhttps://huggingface.co/microsoft/maira-2-sae.\n","authors":["Kenza Bouzid","Shruthi Bannur","Daniel Coelho de Castro","Anton Schwaighofer","Javier Alvarez-Valle","Stephanie L. Hyland"],"pdf_url":"https://arxiv.org/pdf/2507.12950v1.pdf","comment":"Actionable Interpretability Workshop at ICML 2025. 24 pages, 7\n  figures, 5 tables"},{"id":"http://arxiv.org/abs/2507.12948v1","updated":"2025-07-17T09:40:56Z","published":"2025-07-17T09:40:56Z","title":"Probabilistic Soundness Guarantees in LLM Reasoning Chains","summary":"  In reasoning chains generated by large language models (LLMs), initial errors\noften propagate and undermine the reliability of the final conclusion. Current\nLLM-based error detection methods often fail to detect propagated errors\nbecause they do not properly account for how earlier errors might corrupt\njudgments of downstream reasoning. To better detect such propagated errors, we\nintroduce Autoregressive Reasoning Entailment Stability (ARES), a novel\nprobabilistic framework that prevents error propagation by judging each claim\nbased only on previously-assessed sound premises. This inductive method yields\na nuanced score for each step and provides certified statistical guarantees of\nits soundness, rather than a brittle binary label. ARES achieves\nstate-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2\npoints) and demonstrates superior robustness on very long synthetic reasoning\nchains, where it excels at detecting propagated errors (90.3% F1, +27.6\npoints).\n","authors":["Weiqiu You","Anton Xue","Shreya Havaldar","Delip Rao","Helen Jin","Chris Callison-Burch","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2507.12948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.12758v3","updated":"2025-07-17T09:28:28Z","published":"2025-05-19T06:35:11Z","title":"Global urban visual perception varies across demographics and\n  personalities","summary":"  Understanding people's preferences is crucial for urban planning, yet current\napproaches often combine responses from multi-cultural populations, obscuring\ndemographic differences and risking amplifying biases. We conducted a\nlarge-scale urban visual perception survey of streetscapes worldwide using\nstreet view imagery, examining how demographics -- including gender, age,\nincome, education, race and ethnicity, and, for the first time, personality\ntraits -- shape perceptions among 1,000 participants with balanced demographics\nfrom five countries and 45 nationalities. This dataset, Street Perception\nEvaluation Considering Socioeconomics (SPECS), reveals demographic- and\npersonality-based differences across six traditional indicators (safe, lively,\nwealthy, beautiful, boring, depressing) and four new ones (live nearby, walk,\ncycle, green). Location-based sentiments further shape these preferences.\nMachine learning models trained on existing global datasets tend to\noverestimate positive indicators and underestimate negative ones compared to\nhuman responses, underscoring the need for local context. Our study aspires to\nrectify the myopic treatment of street perception, which rarely considers\ndemographics or personality traits.\n","authors":["Matias Quintana","Youlong Gu","Xiucheng Liang","Yujun Hou","Koichi Ito","Yihan Zhu","Mahmoud Abdelrahman","Filip Biljecki"],"pdf_url":"https://arxiv.org/pdf/2505.12758v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2507.12935v1","updated":"2025-07-17T09:20:51Z","published":"2025-07-17T09:20:51Z","title":"MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov\n  Chain Monte Carlo Acceleration","summary":"  An increasing number of applications are exploiting sampling-based algorithms\nfor planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)\nalgorithms form the computational backbone of this emerging branch of machine\nlearning. Unfortunately, the high computational cost limits their feasibility\nfor large-scale problems and real-world applications, and the existing MCMC\nacceleration solutions are either limited in hardware flexibility or fail to\nmaintain efficiency at the system level across a variety of end-to-end\napplications. This paper introduces \\textbf{MC$^2$A}, an algorithm-hardware\nco-design framework, enabling efficient and flexible optimization for MCMC\nacceleration. Firstly, \\textbf{MC$^2$A} analyzes the MCMC workload diversity\nthrough an extension of the processor performance roofline model with a 3rd\ndimension to derive the optimal balance between the compute, sampling and\nmemory parameters. Secondly, \\textbf{MC$^2$A} proposes a parametrized hardware\naccelerator architecture with flexible and efficient support of MCMC kernels\nwith a pipeline of ISA-programmable tree-structured processing units,\nreconfigurable samplers and a crossbar interconnect to support irregular\naccess. Thirdly, the core of \\textbf{MC$^2$A} is powered by a novel Gumbel\nsampler that eliminates exponential and normalization operations. In the\nend-to-end case study, \\textbf{MC$^2$A} achieves an overall {$307.6\\times$,\n$1.4\\times$, $2.0\\times$, $84.2\\times$} speedup compared to the CPU, GPU, TPU\nand state-of-the-art MCMC accelerator. Evaluated on various representative MCMC\nworkloads, this work demonstrates and exploits the feasibility of general\nhardware acceleration to popularize MCMC-based solutions in diverse application\ndomains.\n","authors":["Shirui Zhao","Jun Yin","Lingyun Yao","Martin Andraud","Wannes Meert","Marian Verhelst"],"pdf_url":"https://arxiv.org/pdf/2507.12935v1.pdf","comment":"14 pages, 15 figures, IEEE journal paper"},{"id":"http://arxiv.org/abs/2507.12933v1","updated":"2025-07-17T09:15:29Z","published":"2025-07-17T09:15:29Z","title":"DMQ: Dissecting Outliers of Diffusion Models for Post-Training\n  Quantization","summary":"  Diffusion models have achieved remarkable success in image generation but\ncome with significant computational costs, posing challenges for deployment in\nresource-constrained environments. Recent post-training quantization (PTQ)\nmethods have attempted to mitigate this issue by focusing on the iterative\nnature of diffusion models. However, these approaches often overlook outliers,\nleading to degraded performance at low bit-widths. In this paper, we propose a\nDMQ which combines Learned Equivalent Scaling (LES) and channel-wise\nPower-of-Two Scaling (PTS) to effectively address these challenges. Learned\nEquivalent Scaling optimizes channel-wise scaling factors to redistribute\nquantization difficulty between weights and activations, reducing overall\nquantization error. Recognizing that early denoising steps, despite having\nsmall quantization errors, crucially impact the final output due to error\naccumulation, we incorporate an adaptive timestep weighting scheme to\nprioritize these critical steps during learning. Furthermore, identifying that\nlayers such as skip connections exhibit high inter-channel variance, we\nintroduce channel-wise Power-of-Two Scaling for activations. To ensure robust\nselection of PTS factors even with small calibration set, we introduce a voting\nalgorithm that enhances reliability. Extensive experiments demonstrate that our\nmethod significantly outperforms existing works, especially at low bit-widths\nsuch as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image\ngeneration quality and model stability. The code is available at\nhttps://github.com/LeeDongYeun/dmq.\n","authors":["Dongyeun Lee","Jiwan Hur","Hyounguk Shon","Jae Young Lee","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2507.12933v1.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2507.12931v1","updated":"2025-07-17T09:12:09Z","published":"2025-07-17T09:12:09Z","title":"From a Mixed-Policy Perspective: Improving Differentiable Automatic\n  Post-editing Optimization","summary":"  This paper introduces two novel modifications to the Differentiable Automatic\nPost-editing Optimization (DAPO) algorithm, approached from a mixed-policy\nperspective. Standard policy gradient methods can suffer from instability and\nsample inefficiency, particularly in sparse reward settings. To address this,\nwe first propose a method that incorporates a pre-trained, stable guiding\npolicy ($\\piphi$) to provide off-policy experience, thereby regularizing the\ntraining of the target policy ($\\pion$). This approach improves training\nstability and convergence speed by adaptively adjusting the learning step size.\nSecondly, we extend this idea to re-utilize zero-reward samples, which are\noften discarded by dynamic sampling strategies like DAPO's. By treating these\nsamples as a distinct batch guided by the expert policy, we further enhance\nsample efficiency. We provide a theoretical analysis for both methods,\ndemonstrating that their objective functions converge to the optimal solution\nwithin the established theoretical framework of reinforcement learning. The\nproposed mixed-policy framework effectively balances exploration and\nexploitation, promising more stable and efficient policy optimization.\n","authors":["Hongze Tan"],"pdf_url":"https://arxiv.org/pdf/2507.12931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12927v1","updated":"2025-07-17T09:08:41Z","published":"2025-07-17T09:08:41Z","title":"Trace Reconstruction with Language Models","summary":"  The general trace reconstruction problem seeks to recover an original\nsequence from its noisy copies independently corrupted by deletions,\ninsertions, and substitutions. This problem arises in applications such as DNA\ndata storage, a promising storage medium due to its high information density\nand longevity. However, errors introduced during DNA synthesis, storage, and\nsequencing require correction through algorithms and codes, with trace\nreconstruction often used as part of the data retrieval process. In this work,\nwe propose TReconLM, which leverages language models trained on next-token\nprediction for trace reconstruction. We pretrain language models on synthetic\ndata and fine-tune on real-world data to adapt to technology-specific error\npatterns. TReconLM outperforms state-of-the-art trace reconstruction\nalgorithms, including prior deep learning approaches, recovering a\nsubstantially higher fraction of sequences without error.\n","authors":["Franziska Weindel","Michael Girsch","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2507.12927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12913v1","updated":"2025-07-17T09:00:05Z","published":"2025-07-17T09:00:05Z","title":"Robust Explanations Through Uncertainty Decomposition: A Path to\n  Trustworthier AI","summary":"  Recent advancements in machine learning have emphasized the need for\ntransparency in model predictions, particularly as interpretability diminishes\nwhen using increasingly complex architectures. In this paper, we propose\nleveraging prediction uncertainty as a complementary approach to classical\nexplainability methods. Specifically, we distinguish between aleatoric\n(data-related) and epistemic (model-related) uncertainty to guide the selection\nof appropriate explanations. Epistemic uncertainty serves as a rejection\ncriterion for unreliable explanations and, in itself, provides insight into\ninsufficient training (a new form of explanation). Aleatoric uncertainty\ninforms the choice between feature-importance explanations and counterfactual\nexplanations. This leverages a framework of explainability methods driven by\nuncertainty quantification and disentanglement. Our experiments demonstrate the\nimpact of this uncertainty-aware approach on the robustness and attainability\nof explanations in both traditional machine learning and deep learning\nscenarios.\n","authors":["Chenrui Zhu","Louenas Bounia","Vu Linh Nguyen","Sébastien Destercke","Arthur Hoarau"],"pdf_url":"https://arxiv.org/pdf/2507.12913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12911v1","updated":"2025-07-17T08:58:24Z","published":"2025-07-17T08:58:24Z","title":"LaViPlan : Language-Guided Visual Path Planning with RLVR","summary":"  Out-of-distribution (OOD) scenarios in autonomous driving refer to situations\nthat deviate from the training domain, often leading to unexpected and\npotentially hazardous behavior from planners that lack prior exposure to such\ncases. Recently, Vision-Language Models (VLMs) have been introduced into\nautonomous driving research for their promising generalization capabilities in\nOOD settings. Early studies demonstrated that VLMs could recognize OOD\nscenarios and generate user-level decisions such as \"go straight\" or \"turn\nright.\" However, a new challenge has emerged due to the misalignment between\nthe VLM's high-level decisions or visual reasoning expressed in language, and\nthe low-level predicted trajectories interpreted as actions. In this paper, we\npropose LaViPlan, a framework that leverages Reinforcement Learning with\nVerifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics.\nThis approach addresses the vision-language-action misalignment observed in\nexisting VLMs fine-tuned via supervised learning, which can recognize driving\nscenarios but often produce context-unaware decisions. Experimental results\ndemonstrate that our method improves situational awareness and decision-making\nunder OOD conditions, highlighting its potential to mitigate the misalignment\nissue. This work introduces a promising post-training paradigm for VLM agents\nin the context of autonomous driving.\n","authors":["Hayeon Oh"],"pdf_url":"https://arxiv.org/pdf/2507.12911v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.12908v1","updated":"2025-07-17T08:51:28Z","published":"2025-07-17T08:51:28Z","title":"Fremer: Lightweight and Effective Frequency Transformer for Workload\n  Forecasting in Cloud Services","summary":"  Workload forecasting is pivotal in cloud service applications, such as\nauto-scaling and scheduling, with profound implications for operational\nefficiency. Although Transformer-based forecasting models have demonstrated\nremarkable success in general tasks, their computational efficiency often falls\nshort of the stringent requirements in large-scale cloud environments. Given\nthat most workload series exhibit complicated periodic patterns, addressing\nthese challenges in the frequency domain offers substantial advantages. To this\nend, we propose Fremer, an efficient and effective deep forecasting model.\nFremer fulfills three critical requirements: it demonstrates superior\nefficiency, outperforming most Transformer-based forecasting models; it\nachieves exceptional accuracy, surpassing all state-of-the-art (SOTA) models in\nworkload forecasting; and it exhibits robust performance for multi-period\nseries. Furthermore, we collect and open-source four high-quality, open-source\nworkload datasets derived from ByteDance's cloud services, encompassing\nworkload data from thousands of computing instances. Extensive experiments on\nboth our proprietary datasets and public benchmarks demonstrate that Fremer\nconsistently outperforms baseline models, achieving average improvements of\n5.5% in MSE, 4.7% in MAE, and 8.6% in SMAPE over SOTA models, while\nsimultaneously reducing parameter scale and computational costs. Additionally,\nin a proactive auto-scaling test based on Kubernetes, Fremer improves average\nlatency by 18.78% and reduces resource consumption by 2.35%, underscoring its\npractical efficacy in real-world applications.\n","authors":["Jiadong Chen","Hengyu Ye","Fuxin Jiang","Xiao He","Tieying Zhang","Jianjun Chen","Xiaofeng Gao"],"pdf_url":"https://arxiv.org/pdf/2507.12908v1.pdf","comment":"12 pages, 11 figures"},{"id":"http://arxiv.org/abs/2507.12900v1","updated":"2025-07-17T08:40:28Z","published":"2025-07-17T08:40:28Z","title":"Learning to Reject Low-Quality Explanations via User Feedback","summary":"  Machine Learning predictors are increasingly being employed in high-stakes\napplications such as credit scoring. Explanations help users unpack the reasons\nbehind their predictions, but are not always \"high quality''. That is,\nend-users may have difficulty interpreting or believing them, which can\ncomplicate trust assessment and downstream decision-making. We argue that\nclassifiers should have the option to refuse handling inputs whose predictions\ncannot be explained properly and introduce a framework for learning to reject\nlow-quality explanations (LtX) in which predictors are equipped with a rejector\nthat evaluates the quality of explanations. In this problem setting, the key\nchallenges are how to properly define and assess explanation quality and how to\ndesign a suitable rejector. Focusing on popular attribution techniques, we\nintroduce ULER (User-centric Low-quality Explanation Rejector), which learns a\nsimple rejector from human ratings and per-feature relevance judgments to\nmirror human judgments of explanation quality. Our experiments show that ULER\noutperforms both state-of-the-art and explanation-aware learning to reject\nstrategies at LtX on eight classification and regression benchmarks and on a\nnew human-annotated dataset, which we will publicly release to support future\nresearch.\n","authors":["Luca Stradiotti","Dario Pesenti","Stefano Teso","Jesse Davis"],"pdf_url":"https://arxiv.org/pdf/2507.12900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06187v2","updated":"2025-07-17T08:32:24Z","published":"2024-10-08T16:51:28Z","title":"A column generation algorithm with dynamic constraint aggregation for\n  minimum sum-of-squares clustering","summary":"  The minimum sum-of-squares clustering problem (MSSC), also known as $k$-means\nclustering, refers to the problem of partitioning $n$ data points into $k$\nclusters, with the objective of minimizing the total sum of squared Euclidean\ndistances between each point and the center of its assigned cluster. We propose\nan efficient algorithm for solving large-scale MSSC instances, which combines\ncolumn generation (CG) with dynamic constraint aggregation (DCA) to effectively\nreduce the number of constraints considered in the CG master problem. DCA was\noriginally conceived to reduce degeneracy in set partitioning problems by\nutilizing an aggregated restricted master problem obtained from a partition of\nthe set partitioning constraints into disjoint clusters. In this work, we\nexplore the use of DCA within a CG algorithm for MSSC exact solution. Our\nmethod is fine-tuned by a series of ablation studies on DCA design choices, and\nis demonstrated to significantly outperform existing state-of-the-art exact\napproaches available in the literature.\n","authors":["Antonio M. Sudoso","Daniel Aloise"],"pdf_url":"https://arxiv.org/pdf/2410.06187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12898v1","updated":"2025-07-17T08:31:55Z","published":"2025-07-17T08:31:55Z","title":"Generalist Bimanual Manipulation via Foundation Video Diffusion Models","summary":"  Bimanual robotic manipulation, which involves the coordinated control of two\nrobotic arms, is foundational for solving challenging tasks. Despite recent\nprogress in general-purpose manipulation, data scarcity and embodiment\nheterogeneity remain serious obstacles to further scaling up in bimanual\nsettings. In this paper, we introduce VIdeo Diffusion for Action Reasoning\n(VIDAR), a two-stage framework that leverages large-scale, diffusion-based\nvideo pre-training and a novel masked inverse dynamics model for action\nprediction. We pre-train the video diffusion model on 750K multi-view videos\nfrom three real-world bimanual robot platforms, utilizing a unified observation\nspace that encodes robot, camera, task, and scene contexts. Our masked inverse\ndynamics model learns masks to extract action-relevant information from\ngenerated trajectories without requiring pixel-level labels, and the masks can\neffectively generalize to unseen backgrounds. Our experiments demonstrate that\nwith only 20 minutes of human demonstrations on an unseen robot platform (only\n1% of typical data requirements), VIDAR generalizes to unseen tasks and\nbackgrounds with strong semantic understanding, surpassing state-of-the-art\nmethods. Our findings highlight the potential of video foundation models,\ncoupled with masked action prediction, to enable scalable and generalizable\nrobotic manipulation in diverse real-world settings.\n","authors":["Yao Feng","Hengkai Tan","Xinyi Mao","Guodong Liu","Shuhe Huang","Chendong Xiang","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.12898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12885v1","updated":"2025-07-17T08:10:55Z","published":"2025-07-17T08:10:55Z","title":"VAR-MATH: Probing True Mathematical Reasoning in Large Language Models\n  via Symbolic Multi-Instance Benchmarks","summary":"  Recent advances in reinforcement learning (RL) have led to substantial\nimprovements in the mathematical reasoning abilities of large language models\n(LLMs), as measured by standard benchmarks. However, these gains often persist\neven when models are trained with flawed signals, such as random or inverted\nrewards, raising a fundamental question: do such improvements reflect true\nreasoning, or are they merely artifacts of overfitting to benchmark-specific\npatterns? To address this question, we take an evaluation-centric perspective\nand identify two critical shortcomings in existing protocols. First,\n\\emph{benchmark contamination} arises from the public availability of test\nproblems, increasing the risk of data leakage. Second, \\emph{evaluation\nfragility} stems from the reliance on single-instance assessments, which are\nhighly sensitive to stochastic outputs and fail to capture reasoning\nconsistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic\nevaluation framework designed to probe genuine reasoning ability. By converting\nfixed numerical problems into symbolic templates and requiring models to solve\nmultiple instantiations of each, VAR-MATH enforces consistent reasoning across\nstructurally equivalent variants, thereby mitigating contamination and\nimproving evaluation robustness. We apply VAR-MATH to transform two popular\nbenchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and\nVAR-AIME24. Experimental results reveal substantial performance drops for\nRL-trained models on the variabilized versions, especially for smaller models,\nwith average declines of 48.0\\% on AMC23 and 58.3\\% on AIME24. These findings\nsuggest that many existing RL methods rely on superficial heuristics and fail\nto generalize beyond specific numerical forms. Overall, VAR-MATH offers a\nprincipled, contamination-resistant evaluation paradigm for mathematical\nreasoning.\n","authors":["Jian Yao","Ran Cheng","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2507.12885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12879v1","updated":"2025-07-17T07:58:16Z","published":"2025-07-17T07:58:16Z","title":"Autonomous Resource Management in Microservice Systems via Reinforcement\n  Learning","summary":"  This paper proposes a reinforcement learning-based method for microservice\nresource scheduling and optimization, aiming to address issues such as uneven\nresource allocation, high latency, and insufficient throughput in traditional\nmicroservice architectures. In microservice systems, as the number of services\nand the load increase, efficiently scheduling and allocating resources such as\ncomputing power, memory, and storage becomes a critical research challenge. To\naddress this, the paper employs an intelligent scheduling algorithm based on\nreinforcement learning. Through the interaction between the agent and the\nenvironment, the resource allocation strategy is continuously optimized. In the\nexperiments, the paper considers different resource conditions and load\nscenarios, evaluating the proposed method across multiple dimensions, including\nresponse time, throughput, resource utilization, and cost efficiency. The\nexperimental results show that the reinforcement learning-based scheduling\nmethod significantly improves system response speed and throughput under low\nload and high concurrency conditions, while also optimizing resource\nutilization and reducing energy consumption. Under multi-dimensional resource\nconditions, the proposed method can consider multiple objectives and achieve\noptimized resource scheduling. Compared to traditional static resource\nallocation methods, the reinforcement learning model demonstrates stronger\nadaptability and optimization capability. It can adjust resource allocation\nstrategies in real time, thereby maintaining good system performance in\ndynamically changing load and resource environments.\n","authors":["Yujun Zou","Nia Qi","Yingnan Deng","Zhihao Xue","Ming Gong","Wuyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.12879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12878v1","updated":"2025-07-17T07:55:34Z","published":"2025-07-17T07:55:34Z","title":"Bayesian Modeling and Estimation of Linear Time-Variant Systems using\n  Neural Networks and Gaussian Processes","summary":"  The identification of Linear Time-Variant (LTV) systems from input-output\ndata is a fundamental yet challenging ill-posed inverse problem. This work\nintroduces a unified Bayesian framework that models the system's impulse\nresponse, $h(t, \\tau)$, as a stochastic process. We decompose the response into\na posterior mean and a random fluctuation term, a formulation that provides a\nprincipled approach for quantifying uncertainty and naturally defines a new,\nuseful system class we term Linear Time-Invariant in Expectation (LTIE). To\nperform inference, we leverage modern machine learning techniques, including\nBayesian neural networks and Gaussian Processes, using scalable variational\ninference. We demonstrate through a series of experiments that our framework\ncan robustly infer the properties of an LTI system from a single noisy\nobservation, show superior data efficiency compared to classical methods in a\nsimulated ambient noise tomography problem, and successfully track a\ncontinuously varying LTV impulse response by using a structured Gaussian\nProcess prior. This work provides a flexible and robust methodology for\nuncertainty-aware system identification in dynamic environments.\n","authors":["Yaniv Shulman"],"pdf_url":"https://arxiv.org/pdf/2507.12878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12874v1","updated":"2025-07-17T07:48:36Z","published":"2025-07-17T07:48:36Z","title":"Topology-Aware Activation Functions in Neural Networks","summary":"  This study explores novel activation functions that enhance the ability of\nneural networks to manipulate data topology during training. Building on the\nlimitations of traditional activation functions like $\\mathrm{ReLU}$, we\npropose $\\mathrm{SmoothSplit}$ and $\\mathrm{ParametricSplit}$, which introduce\ntopology \"cutting\" capabilities. These functions enable networks to transform\ncomplex data manifolds effectively, improving performance in scenarios with\nlow-dimensional layers. Through experiments on synthetic and real-world\ndatasets, we demonstrate that $\\mathrm{ParametricSplit}$ outperforms\ntraditional activations in low-dimensional settings while maintaining\ncompetitive performance in higher-dimensional ones. Our findings highlight the\npotential of topology-aware activation functions in advancing neural network\narchitectures. The code is available via\nhttps://github.com/Snopoff/Topology-Aware-Activations.\n","authors":["Pavel Snopov","Oleg R. Musin"],"pdf_url":"https://arxiv.org/pdf/2507.12874v1.pdf","comment":"Accepted to ESANN 2025. Published in the ESANN 2025 proceedings"},{"id":"http://arxiv.org/abs/2507.12873v1","updated":"2025-07-17T07:48:05Z","published":"2025-07-17T07:48:05Z","title":"An Investigation of Ear-EEG Signals for a Novel Biometric Authentication\n  System","summary":"  This work explores the feasibility of biometric authentication using EEG\nsignals acquired through in-ear devices, commonly referred to as ear-EEG.\nTraditional EEG-based biometric systems, while secure, often suffer from low\nusability due to cumbersome scalp-based electrode setups. In this study, we\npropose a novel and practical framework leveraging ear-EEG signals as a\nuser-friendly alternative for everyday biometric authentication. The system\nextracts an original combination of temporal and spectral features from ear-EEG\nsignals and feeds them into a fully connected deep neural network for subject\nidentification. Experimental results on the only currently available ear-EEG\ndataset suitable for different purposes, including biometric authentication,\ndemonstrate promising performance, with an average accuracy of 82\\% in a\nsubject identification scenario. These findings confirm the potential of\near-EEG as a viable and deployable direction for next-generation real-world\nbiometric systems.\n","authors":["Danilo Avola","Giancarlo Crocetti","Gian Luca Foresti","Daniele Pannone","Claudio Piciarelli","Amedeo Ranaldi"],"pdf_url":"https://arxiv.org/pdf/2507.12873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12869v1","updated":"2025-07-17T07:40:50Z","published":"2025-07-17T07:40:50Z","title":"WhoFi: Deep Person Re-Identification via Wi-Fi Channel Signal Encoding","summary":"  Person Re-Identification is a key and challenging task in video surveillance.\nWhile traditional methods rely on visual data, issues like poor lighting,\nocclusion, and suboptimal angles often hinder performance. To address these\nchallenges, we introduce WhoFi, a novel pipeline that utilizes Wi-Fi signals\nfor person re-identification. Biometric features are extracted from Channel\nState Information (CSI) and processed through a modular Deep Neural Network\n(DNN) featuring a Transformer-based encoder. The network is trained using an\nin-batch negative loss function to learn robust and generalizable biometric\nsignatures. Experiments on the NTU-Fi dataset show that our approach achieves\ncompetitive results compared to state-of-the-art methods, confirming its\neffectiveness in identifying individuals via Wi-Fi signals.\n","authors":["Danilo Avola","Daniele Pannone","Dario Montagnini","Emad Emam"],"pdf_url":"https://arxiv.org/pdf/2507.12869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12856v1","updated":"2025-07-17T07:26:54Z","published":"2025-07-17T07:26:54Z","title":"Supervised Fine Tuning on Curated Data is Reinforcement Learning (and\n  can be improved)","summary":"  Behavior Cloning (BC) on curated (or filtered) data is the predominant\nparadigm for supervised fine-tuning (SFT) of large language models; as well as\nfor imitation learning of control policies. Here, we draw on a connection\nbetween this successful strategy and the theory and practice of finding optimal\npolicies via Reinforcement Learning (RL). Building on existing literature, we\nclarify that SFT can be understood as maximizing a lower bound on the RL\nobjective in a sparse reward setting. Giving support to its often observed good\nperformance. From this viewpoint, we realize that a small modification to SFT\nleads to an importance weighted variant that behaves closer to training with RL\nas it: i) optimizes a tighter bound to the RL objective and, ii) can improve\nperformance compared to SFT on curated data. We refer to this variant as\nimportance weighted supervised fine-tuning (iw-SFT). We show that it is easy to\nimplement and can be further generalized to training with quality scored data.\nThe resulting SFT variants are competitive with more advanced RL algorithms for\nlarge language models and for training policies in continuous control tasks.\nFor example achieving 66.7% on the AIME 2024 dataset.\n","authors":["Chongli Qin","Jost Tobias Springenberg"],"pdf_url":"https://arxiv.org/pdf/2507.12856v1.pdf","comment":"See project website for details and code at:\n  https://independentresearch.ai/posts/iwsft"},{"id":"http://arxiv.org/abs/2506.05710v3","updated":"2025-07-17T07:26:41Z","published":"2025-06-06T03:20:32Z","title":"Latent Diffusion Model Based Denoising Receiver for 6G Semantic\n  Communication: From Stochastic Differential Theory to Application","summary":"  In this paper, a novel semantic communication framework empowered by\ngenerative artificial intelligence (GAI) is proposed, to enhance the robustness\nagainst both channel noise and transmission data distribution shifts. A\ntheoretical foundation is established using stochastic differential equations\n(SDEs), from which a closed-form mapping between any signal-to-noise ratio\n(SNR) and the optimal denoising timestep is derived. Moreover, to address\ndistribution mismatch, a mathematical scaling method is introduced to align\nreceived semantic features with the training distribution of the GAI. Built on\nthis theoretical foundation, a latent diffusion model (LDM)-based semantic\ncommunication framework is proposed that combines a variational autoencoder for\nsemantic features extraction, where a pretrained diffusion model is used for\ndenoising. The proposed system is a training-free framework that supports\nzero-shot generalization, and achieves superior performance under low-SNR and\nout-of-distribution conditions, offering a scalable and robust solution for\nfuture 6G semantic communication systems. Experimental results demonstrate that\nthe proposed semantic communication framework achieves state-of-the-art\nperformance in both pixel-level accuracy and semantic perceptual quality,\nconsistently outperforming baselines across a wide range of SNRs and data\ndistributions without any fine-tuning or post-training.\n","authors":["Xiucheng Wang","Honggang Jia","Nan Cheng"],"pdf_url":"https://arxiv.org/pdf/2506.05710v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12854v1","updated":"2025-07-17T07:26:07Z","published":"2025-07-17T07:26:07Z","title":"Transformer-Based Person Identification via Wi-Fi CSI Amplitude and\n  Phase Perturbations","summary":"  Wi-Fi sensing is gaining momentum as a non-intrusive and privacy-preserving\nalternative to vision-based systems for human identification. However, person\nidentification through wireless signals, particularly without user motion,\nremains largely unexplored. Most prior wireless-based approaches rely on\nmovement patterns, such as walking gait, to extract biometric cues. In\ncontrast, we propose a transformer-based method that identifies individuals\nfrom Channel State Information (CSI) recorded while the subject remains\nstationary. CSI captures fine-grained amplitude and phase distortions induced\nby the unique interaction between the human body and the radio signal. To\nsupport evaluation, we introduce a dataset acquired with ESP32 devices in a\ncontrolled indoor environment, featuring six participants observed across\nmultiple orientations. A tailored preprocessing pipeline, including outlier\nremoval, smoothing, and phase calibration, enhances signal quality. Our\ndual-branch transformer architecture processes amplitude and phase modalities\nseparately and achieves 99.82\\% classification accuracy, outperforming\nconvolutional and multilayer perceptron baselines. These results demonstrate\nthe discriminative potential of CSI perturbations, highlighting their capacity\nto encode biometric traits in a consistent manner. They further confirm the\nviability of passive, device-free person identification using low-cost\ncommodity Wi-Fi hardware in real-world settings.\n","authors":["Danilo Avola","Andrea Bernardini","Francesco Danese","Mario Lezoche","Maurizio Mancini","Daniele Pannone","Amedeo Ranaldi"],"pdf_url":"https://arxiv.org/pdf/2507.12854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12269v2","updated":"2025-07-17T07:11:14Z","published":"2025-07-16T14:19:44Z","title":"Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust\n  Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in\n  Extremely Preterm Infants","summary":"  Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.\n","authors":["Sybelle Goedicke-Fritz","Michelle Bous","Annika Engel","Matthias Flotho","Pascal Hirsch","Hannah Wittig","Dino Milanovic","Dominik Mohr","Mathias Kaspar","Sogand Nemat","Dorothea Kerner","Arno Bücker","Andreas Keller","Sascha Meyer","Michael Zemlin","Philipp Flotho"],"pdf_url":"https://arxiv.org/pdf/2507.12269v2.pdf","comment":"S.G.-F., M.B., and A.E. contributed equally to this work and share\n  first authorship. M.Z. and P.F. contributed equally to this work and share\n  senior authorship"},{"id":"http://arxiv.org/abs/2407.17385v3","updated":"2025-07-17T07:10:43Z","published":"2024-07-24T16:07:57Z","title":"Formalising causal inference as prediction on a target population","summary":"  The standard approach to causal modelling especially in social and health\nsciences is the potential outcomes framework due to Neyman and Rubin. In this\nframework, observations are thought to be drawn from a distribution over\nvariables of interest, and the goal is to identify parameters of this\ndistribution. Even though the stated goal is often to inform decision making on\nsome target population, there is no straightforward way to include these target\npopulations in the framework. Instead of modelling the relationship between the\nobserved sample and the target population, the inductive assumptions in this\nframework take the form of abstract sampling and independence assumptions. In\nthis paper, we develop a version of this framework that construes causal\ninference as treatment-wise predictions for finite populations where all\nassumptions are testable in retrospect; this means that one can not only test\npredictions themselves (without any fundamental problem) but also investigate\nsources of error when they fail. Due to close connections to the original\nframework, established methods can still be be analysed under the new\nframework.\n","authors":["Benedikt Höltgen","Robert C. Williamson"],"pdf_url":"https://arxiv.org/pdf/2407.17385v3.pdf","comment":"Presented at the Humans, Algorithmic Decision-Making and Society\n  Workshop at ICML 2024"},{"id":"http://arxiv.org/abs/2411.02419v2","updated":"2025-07-17T07:09:03Z","published":"2024-10-21T11:37:58Z","title":"Dataset resulting from the user study on comprehensibility of\n  explainable AI algorithms","summary":"  This paper introduces a dataset that is the result of a user study on the\ncomprehensibility of explainable artificial intelligence (XAI) algorithms. The\nstudy participants were recruited from 149 candidates to form three groups\nrepresenting experts in the domain of mycology (DE), students with a data\nscience and visualization background (IT) and students from social sciences and\nhumanities (SSH). The main part of the dataset contains 39 transcripts of\ninterviews during which participants were asked to complete a series of tasks\nand questions related to the interpretation of explanations of decisions of a\nmachine learning model trained to distinguish between edible and inedible\nmushrooms. The transcripts were complemented with additional data that includes\nvisualizations of explanations presented to the user, results from thematic\nanalysis, recommendations of improvements of explanations provided by the\nparticipants, and the initial survey results that allow to determine the domain\nknowledge of the participant and data analysis literacy. The transcripts were\nmanually tagged to allow for automatic matching between the text and other data\nrelated to particular fragments. In the advent of the area of rapid development\nof XAI techniques, the need for a multidisciplinary qualitative evaluation of\nexplainability is one of the emerging topics in the community. Our dataset\nallows not only to reproduce the study we conducted, but also to open a wide\nrange of possibilities for the analysis of the material we gathered.\n","authors":["Szymon Bobek","Paloma Korycińska","Monika Krakowska","Maciej Mozolewski","Dorota Rak","Magdalena Zych","Magdalena Wójcik","Grzegorz J. Nalepa"],"pdf_url":"https://arxiv.org/pdf/2411.02419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12843v1","updated":"2025-07-17T07:08:54Z","published":"2025-07-17T07:08:54Z","title":"A Kernel Distribution Closeness Testing","summary":"  The distribution closeness testing (DCT) assesses whether the distance\nbetween a distribution pair is at least $\\epsilon$-far. Existing DCT methods\nmainly measure discrepancies between a distribution pair defined on discrete\none-dimensional spaces (e.g., using total variation), which limits their\napplications to complex data (e.g., images). To extend DCT to more types of\ndata, a natural idea is to introduce maximum mean discrepancy (MMD), a powerful\nmeasurement of the distributional discrepancy between two complex\ndistributions, into DCT scenarios. However, we find that MMD's value can be the\nsame for many pairs of distributions that have different norms in the same\nreproducing kernel Hilbert space (RKHS), making MMD less informative when\nassessing the closeness levels for multiple distribution pairs. To mitigate the\nissue, we design a new measurement of distributional discrepancy, norm-adaptive\nMMD (NAMMD), which scales MMD's value using the RKHS norms of distributions.\nBased on the asymptotic distribution of NAMMD, we finally propose the\nNAMMD-based DCT to assess the closeness levels of a distribution pair.\nTheoretically, we prove that NAMMD-based DCT has higher test power compared to\nMMD-based DCT, with bounded type-I error, which is also validated by extensive\nexperiments on many types of data (e.g., synthetic noise, real images).\nFurthermore, we also apply the proposed NAMMD for addressing the two-sample\ntesting problem and find NAMMD-based two-sample test has higher test power than\nthe MMD-based two-sample test in both theory and experiments.\n","authors":["Zhijian Zhou","Liuhua Peng","Xunye Tian","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2507.12843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.03331v2","updated":"2025-07-17T07:04:11Z","published":"2025-07-04T06:38:02Z","title":"Task-Specific Generative Dataset Distillation with Difficulty-Guided\n  Sampling","summary":"  To alleviate the reliance of deep neural networks on large-scale datasets,\ndataset distillation aims to generate compact, high-quality synthetic datasets\nthat can achieve comparable performance to the original dataset. The\nintegration of generative models has significantly advanced this field.\nHowever, existing approaches primarily focus on aligning the distilled dataset\nwith the original one, often overlooking task-specific information that can be\ncritical for optimal downstream performance. In this paper, focusing on the\ndownstream task of classification, we propose a task-specific sampling strategy\nfor generative dataset distillation that incorporates the concept of difficulty\nto consider the requirements of the target task better. The final dataset is\nsampled from a larger image pool with a sampling distribution obtained by\nmatching the difficulty distribution of the original dataset. A logarithmic\ntransformation is applied as a pre-processing step to correct for\ndistributional bias. The results of extensive experiments demonstrate the\neffectiveness of our method and suggest its potential for enhancing performance\non other downstream tasks. The code is available at\nhttps://github.com/SumomoTaku/DiffGuideSamp.\n","authors":["Mingzhuo Li","Guang Li","Jiafeng Mao","Linfeng Ye","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2507.03331v2.pdf","comment":"Accepted by The ICCV 2025 Workshop on Curated Data for Efficient\n  Learning"},{"id":"http://arxiv.org/abs/2407.17395v4","updated":"2025-07-17T06:59:59Z","published":"2024-07-24T16:17:14Z","title":"We should avoid the assumption of data-generating probability\n  distributions in social settings","summary":"  Machine Learning research, including work promoting fair or equitable\nalgorithms, heavily relies on the concept of a data-generating probability\ndistribution. The standard presumption is that since data points are 'sampled\nfrom' such a distribution, one can learn from observed data about this\ndistribution and, thus, predict future data points which are also drawn from\nit. We argue, however, that such true probability distributions do not exist\nand should not be dealt with uncritically. We show that alternative frameworks\nfocusing directly on relevant populations rather than abstract distributions\nare available and leave classical learning theory almost unchanged.\nFurthermore, we argue that the assumption of true probabilities or\ndata-generating distributions can be misleading and obscure both the choices\nmade and the goals pursued in machine learning practice. Based on these\nconsiderations, this position paper argues that, at least in social settings,\nmachine learning work should avoid assuming data-generating probability\ndistributions.\n","authors":["Benedikt Höltgen","Robert C. Williamson"],"pdf_url":"https://arxiv.org/pdf/2407.17395v4.pdf","comment":"Presented at the Humans, Algorithmic Decision-Making and Society\n  Workshop at ICML 2024"},{"id":"http://arxiv.org/abs/2507.12840v1","updated":"2025-07-17T06:59:52Z","published":"2025-07-17T06:59:52Z","title":"Bridging the Gap: Leveraging Retrieval-Augmented Generation to Better\n  Understand Public Concerns about Vaccines","summary":"  Vaccine hesitancy threatens public health, leading to delayed or rejected\nvaccines. Social media is a vital source for understanding public concerns, and\ntraditional methods like topic modelling often struggle to capture nuanced\nopinions. Though trained for query answering, large Language Models (LLMs)\noften miss current events and community concerns. Additionally, hallucinations\nin LLMs can compromise public health communication. To address these\nlimitations, we developed a tool (VaxPulse Query Corner) using the Retrieval\nAugmented Generation technique. It addresses complex queries about public\nvaccine concerns on various online platforms, aiding public health\nadministrators and stakeholders in understanding public concerns and\nimplementing targeted interventions to boost vaccine confidence. Analysing\n35,103 Shingrix social media posts, it achieved answer faithfulness (0.96) and\nrelevance (0.94).\n","authors":["Muhammad Javed","Sedigh Khademi Habibabadi","Christopher Palmer","Hazel Clothier","Jim Buttery","Gerardo Luis Dimaguila"],"pdf_url":"https://arxiv.org/pdf/2507.12840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12837v1","updated":"2025-07-17T06:52:56Z","published":"2025-07-17T06:52:56Z","title":"Understanding the Evolution of the Neural Tangent Kernel at the Edge of\n  Stability","summary":"  The study of Neural Tangent Kernels (NTKs) in deep learning has drawn\nincreasing attention in recent years. NTKs typically actively change during\ntraining and are related to feature learning. In parallel, recent work on\nGradient Descent (GD) has found a phenomenon called Edge of Stability (EoS), in\nwhich the largest eigenvalue of the NTK oscillates around a value inversely\nproportional to the step size. However, although follow-up works have explored\nthe underlying mechanism of such eigenvalue behavior in depth, the\nunderstanding of the behavior of the NTK eigenvectors during EoS is still\nmissing. This paper examines the dynamics of NTK eigenvectors during EoS in\ndetail. Across different architectures, we observe that larger learning rates\ncause the leading eigenvectors of the final NTK, as well as the full NTK\nmatrix, to have greater alignment with the training target. We then study the\nunderlying mechanism of this phenomenon and provide a theoretical analysis for\na two-layer linear network. Our study enhances the understanding of GD training\ndynamics in deep learning.\n","authors":["Kaiqi Jiang","Jeremy Cohen","Yuanzhi Li"],"pdf_url":"https://arxiv.org/pdf/2507.12837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12832v1","updated":"2025-07-17T06:45:47Z","published":"2025-07-17T06:45:47Z","title":"MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge:\n  Dataset, Methods, and Results","summary":"  Small Multi-Object Tracking (SMOT) is particularly challenging when targets\noccupy only a few dozen pixels, rendering detection and appearance-based\nassociation unreliable. Building on the success of the MVA2023 SOD4SB\nchallenge, this paper introduces the SMOT4SB challenge, which leverages\ntemporal information to address limitations of single-frame detection. Our\nthree main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV\nvideo sequences with 108,192 annotated frames under diverse real-world\nconditions, designed to capture motion entanglement where both camera and\ntargets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance\nwith HOTA to mitigate the sensitivity of IoU-based metrics to small\ndisplacements; and (3) a competitive MVA2025 challenge with 78 participants and\n308 submissions, where the winning method achieved a 5.1x improvement over the\nbaseline. This work lays a foundation for advancing SMOT in UAV scenarios with\napplications in bird strike avoidance, agriculture, fisheries, and ecological\nmonitoring.\n","authors":["Yuki Kondo","Norimichi Ukita","Riku Kanayama","Yuki Yoshida","Takayuki Yamaguchi","Xiang Yu","Guang Liang","Xinyao Liu","Guan-Zhang Wang","Wei-Ta Chu","Bing-Cheng Chuang","Jia-Hua Lee","Pin-Tseng Kuo","I-Hsuan Chu","Yi-Shein Hsiao","Cheng-Han Wu","Po-Yi Wu","Jui-Chien Tsou","Hsuan-Chi Liu","Chun-Yi Lee","Yuan-Fu Yang","Kosuke Shigematsu","Asuka Shin","Ba Tran"],"pdf_url":"https://arxiv.org/pdf/2507.12832v1.pdf","comment":"This paper is the official challenge report for SMOT4SB and is\n  published in the proceedings of MVA 2025 (19th International Conference on\n  Machine Vision and Applications). Official challenge page:\n  https://www.mva-org.jp/mva2025/challenge"},{"id":"http://arxiv.org/abs/2507.12825v1","updated":"2025-07-17T06:32:22Z","published":"2025-07-17T06:32:22Z","title":"Autoregressive Speech Enhancement via Acoustic Tokens","summary":"  In speech processing pipelines, improving the quality and intelligibility of\nreal-world recordings is crucial. While supervised regression is the primary\nmethod for speech enhancement, audio tokenization is emerging as a promising\nalternative for a smooth integration with other modalities. However, research\non speech enhancement using discrete representations is still limited. Previous\nwork has mainly focused on semantic tokens, which tend to discard key acoustic\ndetails such as speaker identity. Additionally, these studies typically employ\nnon-autoregressive models, assuming conditional independence of outputs and\noverlooking the potential improvements offered by autoregressive modeling. To\naddress these gaps we: 1) conduct a comprehensive study of the performance of\nacoustic tokens for speech enhancement, including the effect of bitrate and\nnoise strength; 2) introduce a novel transducer-based autoregressive\narchitecture specifically designed for this task. Experiments on VoiceBank and\nLibri1Mix datasets show that acoustic tokens outperform semantic tokens in\nterms of preserving speaker identity, and that our autoregressive approach can\nfurther improve performance. Nevertheless, we observe that discrete\nrepresentations still fall short compared to continuous ones, highlighting the\nneed for further research in this area.\n","authors":["Luca Della Libera","Cem Subakan","Mirco Ravanelli"],"pdf_url":"https://arxiv.org/pdf/2507.12825v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2507.12821v1","updated":"2025-07-17T06:28:14Z","published":"2025-07-17T06:28:14Z","title":"Assessing adaptive world models in machines with novel games","summary":"  Human intelligence exhibits a remarkable capacity for rapid adaptation and\neffective problem-solving in novel and unfamiliar contexts. We argue that this\nprofound adaptability is fundamentally linked to the efficient construction and\nrefinement of internal representations of the environment, commonly referred to\nas world models, and we refer to this adaptation mechanism as world model\ninduction. However, current understanding and evaluation of world models in\nartificial intelligence (AI) remains narrow, often focusing on static\nrepresentations learned from training on a massive corpora of data, instead of\nthe efficiency and efficacy of models in learning these representations through\ninteraction and exploration within a novel environment. In this Perspective, we\nprovide a view of world model induction drawing on decades of research in\ncognitive science on how humans learn and adapt so efficiently; we then call\nfor a new evaluation framework for assessing adaptive world models in AI.\nConcretely, we propose a new benchmarking paradigm based on suites of carefully\ndesigned games with genuine, deep and continually refreshing novelty in the\nunderlying game structures -- we refer to this kind of games as novel games. We\ndetail key desiderata for constructing these games and propose appropriate\nmetrics to explicitly challenge and evaluate the agent's ability for rapid\nworld model induction. We hope that this new evaluation framework will inspire\nfuture evaluation efforts on world models in AI and provide a crucial step\ntowards developing AI systems capable of the human-like rapid adaptation and\nrobust generalization -- a critical component of artificial general\nintelligence.\n","authors":["Lance Ying","Katherine M. Collins","Prafull Sharma","Cedric Colas","Kaiya Ivy Zhao","Adrian Weller","Zenna Tavares","Phillip Isola","Samuel J. Gershman","Jacob D. Andreas","Thomas L. Griffiths","Francois Chollet","Kelsey R. Allen","Joshua B. Tenenbaum"],"pdf_url":"https://arxiv.org/pdf/2507.12821v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2507.12818v1","updated":"2025-07-17T06:22:17Z","published":"2025-07-17T06:22:17Z","title":"Self Balancing Neural Network: A Novel Method to Estimate Average\n  Treatment Effect","summary":"  In observational studies, confounding variables affect both treatment and\noutcome. Moreover, instrumental variables also influence the treatment\nassignment mechanism. This situation sets the study apart from a standard\nrandomized controlled trial, where the treatment assignment is random. Due to\nthis situation, the estimated average treatment effect becomes biased. To\naddress this issue, a standard approach is to incorporate the estimated\npropensity score when estimating the average treatment effect. However, these\nmethods incur the risk of misspecification in propensity score models. To solve\nthis issue, a novel method called the \"Self balancing neural network\" (Sbnet),\nwhich lets the model itself obtain its pseudo propensity score from the\nbalancing net, is proposed in this study. The proposed method estimates the\naverage treatment effect by using the balancing net as a key part of the\nfeedforward neural network. This formulation resolves the estimation of the\naverage treatment effect in one step. Moreover, the multi-pseudo propensity\nscore framework, which is estimated from the diversified balancing net and used\nfor the estimation of the average treatment effect, is presented. Finally, the\nproposed methods are compared with state-of-the-art methods on three simulation\nsetups and real-world datasets. It has been shown that the proposed\nself-balancing neural network shows better performance than state-of-the-art\nmethods.\n","authors":["Atomsa Gemechu Abdisa","Yingchun Zhou","Yuqi Qiu"],"pdf_url":"https://arxiv.org/pdf/2507.12818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12815v1","updated":"2025-07-17T06:16:06Z","published":"2025-07-17T06:16:06Z","title":"From Novelty to Imitation: Self-Distilled Rewards for Offline\n  Reinforcement Learning","summary":"  Offline Reinforcement Learning (RL) aims to learn effective policies from a\nstatic dataset without requiring further agent-environment interactions.\nHowever, its practical adoption is often hindered by the need for explicit\nreward annotations, which can be costly to engineer or difficult to obtain\nretrospectively. To address this, we propose ReLOAD (Reinforcement Learning\nwith Offline Reward Annotation via Distillation), a novel reward annotation\nframework for offline RL. Unlike existing methods that depend on complex\nalignment procedures, our approach adapts Random Network Distillation (RND) to\ngenerate intrinsic rewards from expert demonstrations using a simple yet\neffective embedding discrepancy measure. First, we train a predictor network to\nmimic a fixed target network's embeddings based on expert state transitions.\nLater, the prediction error between these networks serves as a reward signal\nfor each transition in the static dataset. This mechanism provides a structured\nreward signal without requiring handcrafted reward annotations. We provide a\nformal theoretical construct that offers insights into how RND prediction\nerrors effectively serve as intrinsic rewards by distinguishing expert-like\ntransitions. Experiments on the D4RL benchmark demonstrate that ReLOAD enables\nrobust offline policy learning and achieves performance competitive with\ntraditional reward-annotated methods.\n","authors":["Gaurav Chaudhary","Laxmidhar Behera"],"pdf_url":"https://arxiv.org/pdf/2507.12815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12814v1","updated":"2025-07-17T06:14:19Z","published":"2025-07-17T06:14:19Z","title":"RONOM: Reduced-Order Neural Operator Modeling","summary":"  Time-dependent partial differential equations are ubiquitous in physics-based\nmodeling, but they remain computationally intensive in many-query scenarios,\nsuch as real-time forecasting, optimal control, and uncertainty quantification.\nReduced-order modeling (ROM) addresses these challenges by constructing a\nlow-dimensional surrogate model but relies on a fixed discretization, which\nlimits flexibility across varying meshes during evaluation. Operator learning\napproaches, such as neural operators, offer an alternative by parameterizing\nmappings between infinite-dimensional function spaces, enabling adaptation to\ndata across different resolutions. Whereas ROM provides rigorous numerical\nerror estimates, neural operator learning largely focuses on discretization\nconvergence and invariance without quantifying the error between the\ninfinite-dimensional and the discretized operators. This work introduces the\nreduced-order neural operator modeling (RONOM) framework, which bridges\nconcepts from ROM and operator learning. We establish a discretization error\nbound analogous to those in ROM, and get insights into RONOM's discretization\nconvergence and discretization robustness. Moreover, two numerical examples are\npresented that compare RONOM to existing neural operators for solving partial\ndifferential equations. The results demonstrate that RONOM using standard\nvector-to-vector neural networks achieves comparable performance in input\ngeneralization and superior performance in both spatial super-resolution and\ndiscretization robustness, while also offering novel insights into temporal\nsuper-resolution scenarios.\n","authors":["Sven Dummer","Dongwei Ye","Christoph Brune"],"pdf_url":"https://arxiv.org/pdf/2507.12814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.10638v2","updated":"2025-07-17T06:11:45Z","published":"2025-07-14T13:30:40Z","title":"ZClassifier: Temperature Tuning and Manifold Approximation via KL\n  Divergence on Logit Space","summary":"  We introduce a novel classification framework, ZClassifier, that replaces\nconventional deterministic logits with diagonal Gaussian-distributed logits.\nOur method simultaneously addresses temperature scaling and manifold\napproximation by minimizing the Kullback-Leibler (KL) divergence between the\npredicted Gaussian distributions and a unit isotropic Gaussian. This unifies\nuncertainty calibration and latent control in a principled probabilistic\nmanner, enabling a natural interpretation of class confidence and geometric\nconsistency. Experiments on CIFAR-10 show that ZClassifier improves over\nsoftmax classifiers in robustness, calibration, and latent separation.\n","authors":["Shim Soon Yong"],"pdf_url":"https://arxiv.org/pdf/2507.10638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09565v2","updated":"2025-07-17T06:11:39Z","published":"2025-07-13T10:18:15Z","title":"Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental\n  Health Narratives","summary":"  We introduce a dataset for classifying wellness dimensions in social media\nuser posts, covering six key aspects: physical, emotional, social,\nintellectual, spiritual, and vocational. The dataset is designed to capture\nthese dimensions in user-generated content, with a comprehensive annotation\nframework developed under the guidance of domain experts. This framework allows\nfor the classification of text spans into the appropriate wellness categories.\nWe evaluate both traditional machine learning models and advanced\ntransformer-based models for this multi-class classification task, with\nperformance assessed using precision, recall, and F1-score, averaged over\n10-fold cross-validation. Post-hoc explanations are applied to ensure the\ntransparency and interpretability of model decisions. The proposed dataset\ncontributes to region-specific wellness assessments in social media and paves\nthe way for personalized well-being evaluations and early intervention\nstrategies in mental health. We adhere to ethical considerations for\nconstructing and releasing our experiments and dataset publicly on Github.\n","authors":["Heba Shakeel","Tanvir Ahmad","Chandni Saxena"],"pdf_url":"https://arxiv.org/pdf/2507.09565v2.pdf","comment":"7 Pages"},{"id":"http://arxiv.org/abs/2407.19852v2","updated":"2025-07-17T06:06:45Z","published":"2024-07-29T10:10:03Z","title":"Quantum Long Short-Term Memory for Drug Discovery","summary":"  Quantum computing combined with machine learning (ML) is a highly promising\nresearch area, with numerous studies demonstrating that quantum machine\nlearning (QML) is expected to solve scientific problems more effectively than\nclassical ML. In this work, we present Quantum Long Short-Term Memory (QLSTM),\na QML architecture, and demonstrate its effectiveness in drug discovery. We\nevaluate QLSTM on five benchmark datasets (BBBP, BACE, SIDER, BCAP37, T-47D),\nand observe consistent performance gains over classical LSTM, with ROC-AUC\nimprovements ranging from 3% to over 6%. Furthermore, QLSTM exhibits improved\npredictive accuracy as the number of qubits increases, and faster convergence\nthan classical LSTM under the same training conditions. Notably, QLSTM\nmaintains strong robustness against quantum computer noise, outperforming\nnoise-free classical LSTM in certain settings. These findings highlight the\npotential of QLSTM as a scalable and noise-resilient model for scientific\napplications, particularly as quantum hardware continues to advance in qubit\ncapacity and fidelity.\n","authors":["Liang Zhang","Yin Xu","Mohan Wu","Liang Wang","Hua Xu"],"pdf_url":"https://arxiv.org/pdf/2407.19852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16700v2","updated":"2025-07-17T05:48:54Z","published":"2025-03-20T20:46:25Z","title":"Deep Q-Learning with Gradient Target Tracking","summary":"  This paper introduces Q-learning with gradient target tracking, a novel\nreinforcement learning framework that provides a learned continuous target\nupdate mechanism as an alternative to the conventional hard update paradigm. In\nthe standard deep Q-network (DQN), the target network is a copy of the online\nnetwork's weights, held fixed for a number of iterations before being\nperiodically replaced via a hard update. While this stabilizes training by\nproviding consistent targets, it introduces a new challenge: the hard update\nperiod must be carefully tuned to achieve optimal performance. To address this\nissue, we propose two gradient-based target update methods: DQN with asymmetric\ngradient target tracking (AGT2-DQN) and DQN with symmetric gradient target\ntracking (SGT2-DQN). These methods replace the conventional hard target updates\nwith continuous and structured updates using gradient descent, which\neffectively eliminates the need for manual tuning. We provide a theoretical\nanalysis proving the convergence of these methods in tabular settings.\nAdditionally, empirical evaluations demonstrate their advantages over standard\nDQN baselines, which suggest that gradient-based target updates can serve as an\neffective alternative to conventional target update mechanisms in Q-learning.\n","authors":["Donghwan Lee","Bum Geun Park","Taeho Lee"],"pdf_url":"https://arxiv.org/pdf/2503.16700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12808v1","updated":"2025-07-17T05:48:45Z","published":"2025-07-17T05:48:45Z","title":"Large Language Models' Internal Perception of Symbolic Music","summary":"  Large language models (LLMs) excel at modeling relationships between strings\nin natural language and have shown promise in extending to other symbolic\ndomains like coding or mathematics. However, the extent to which they\nimplicitly model symbolic music remains underexplored. This paper investigates\nhow LLMs represent musical concepts by generating symbolic music data from\ntextual prompts describing combinations of genres and styles, and evaluating\ntheir utility through recognition and generation tasks. We produce a dataset of\nLLM-generated MIDI files without relying on explicit musical training. We then\ntrain neural networks entirely on this LLM-generated MIDI dataset and perform\ngenre and style classification as well as melody completion, benchmarking their\nperformance against established models. Our results demonstrate that LLMs can\ninfer rudimentary musical structures and temporal relationships from text,\nhighlighting both their potential to implicitly encode musical patterns and\ntheir limitations due to a lack of explicit musical context, shedding light on\ntheir generative capabilities for symbolic music.\n","authors":["Andrew Shin","Kunitake Kaneko"],"pdf_url":"https://arxiv.org/pdf/2507.12808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12805v1","updated":"2025-07-17T05:46:08Z","published":"2025-07-17T05:46:08Z","title":"PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for\n  Large-Scale Genomics Database","summary":"  Learning-based lossless compressors play a crucial role in large-scale\ngenomic database backup, storage, transmission, and management. However, their\n1) inadequate compression ratio, 2) low compression \\& decompression\nthroughput, and 3) poor compression robustness limit their widespread adoption\nand application in both industry and academia. To solve those challenges, we\npropose a novel \\underline{P}arallel \\underline{M}ulti-\\underline{K}nowledge\n\\underline{L}earning-based \\underline{C}ompressor (PMKLC) with four crucial\ndesigns: 1) We propose an automated multi-knowledge learning-based compression\nframework as compressors' backbone to enhance compression ratio and robustness;\n2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression\nthroughput and computing resource usage; 3) we introduce data block\npartitioning and Step-wise Model Passing (SMP) mechanisms for parallel\nacceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet\nthe complex application scenarios, where the former runs on a\nresource-constrained single GPU and the latter is multi-GPU accelerated. We\nbenchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15\nreal-world datasets with different species and data sizes. Compared to\nbaselines on the testing datasets, PMKLC-S/M achieve the average compression\nratio improvement up to 73.609\\% and 73.480\\%, the average throughput\nimprovement up to 3.036$\\times$ and 10.710$\\times$, respectively. Besides,\nPMKLC-S/M also achieve the best robustness and competitive memory cost,\nindicating its greater stability against datasets with different probability\ndistribution perturbations, and its strong ability to run on memory-constrained\ndevices.\n","authors":["Hui Sun","Yanfeng Ding","Liping Yi","Huidong Ma","Gang Wang","Xiaoguang Liu","Cheng Zhong","Wentong Cai"],"pdf_url":"https://arxiv.org/pdf/2507.12805v1.pdf","comment":"Accepted via KDD-25"},{"id":"http://arxiv.org/abs/2507.12218v2","updated":"2025-07-17T05:39:25Z","published":"2025-07-16T13:23:39Z","title":"Physics-Informed Linear Model (PILM): Analytical Representations and\n  Application to Crustal Strain Rate Estimation","summary":"  Many physical systems are described by partial differential equations (PDEs),\nand solving these equations and estimating their coefficients or boundary\nconditions (BCs) from observational data play a crucial role in understanding\nthe associated phenomena. Recently, a machine learning approach known as\nphysics-informed neural network, which solves PDEs using neural networks by\nminimizing the sum of residuals from the PDEs, BCs, and data, has gained\nsignificant attention in the scientific community. In this study, we\ninvestigate a physics-informed linear model (PILM) that uses linear\ncombinations of basis functions to represent solutions, thereby enabling an\nanalytical representation of optimal solutions. The PILM was formulated and\nverified for illustrative forward and inverse problems including cases with\nuncertain BCs. Furthermore, the PILM was applied to estimate crustal strain\nrates using geodetic data. Specifically, physical regularization that enforces\nelastic equilibrium on the velocity fields was compared with mathematical\nregularization that imposes smoothness constraints. From a Bayesian\nperspective, mathematical regularization exhibited superior performance. The\nPILM provides an analytically solvable framework applicable to linear forward\nand inverse problems, underdetermined systems, and physical regularization.\n","authors":["Tomohisa Okazaki"],"pdf_url":"https://arxiv.org/pdf/2507.12218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12803v1","updated":"2025-07-17T05:39:15Z","published":"2025-07-17T05:39:15Z","title":"FLDmamba: Integrating Fourier and Laplace Transform Decomposition with\n  Mamba for Enhanced Time Series Prediction","summary":"  Time series prediction, a crucial task across various domains, faces\nsignificant challenges due to the inherent complexities of time series data,\nincluding non-stationarity, multi-scale periodicity, and transient dynamics,\nparticularly when tackling long-term predictions. While Transformer-based\narchitectures have shown promise, their quadratic complexity with sequence\nlength hinders their efficiency for long-term predictions. Recent advancements\nin State-Space Models, such as Mamba, offer a more efficient alternative for\nlong-term modeling, but they cannot capture multi-scale periodicity and\ntransient dynamics effectively. Meanwhile, they are susceptible to data noise\nissues in time series. This paper proposes a novel framework, FLDmamba (Fourier\nand Laplace Transform Decomposition Mamba), addressing these limitations.\nFLDmamba leverages the strengths of both Fourier and Laplace transforms to\neffectively capture both multi-scale periodicity, transient dynamics within\ntime series data, and improve the robustness of the model to the data noise\nissue. Our extensive experiments demonstrate that FLDmamba achieves superior\nperformance on time series prediction benchmarks, outperforming both\nTransformer-based and other Mamba-based architectures. To promote the\nreproducibility of our method, we have made both the code and data accessible\nvia the following\nURL:{\\href{https://github.com/AI4Science-WestlakeU/FLDmamba}{https://github.com/AI4Science-WestlakeU/\\model}.\n","authors":["Qianru Zhang","Chenglei Yu","Haixin Wang","Yudong Yan","Yuansheng Cao","Siu-Ming Yiu","Tailin Wu","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2507.12803v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2506.20495v2","updated":"2025-07-17T05:31:07Z","published":"2025-06-25T14:41:13Z","title":"ReCode: Updating Code API Knowledge with Reinforcement Learning","summary":"  Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode.\n","authors":["Haoze Wu","Yunzhi Yao","Wenhao Yu","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2506.20495v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.18699v2","updated":"2025-07-17T05:29:09Z","published":"2025-02-25T23:22:12Z","title":"MPO: An Efficient Post-Processing Framework for Mixing Diverse\n  Preference Alignment","summary":"  Reinforcement Learning from Human Feedback (RLHF) has shown promise in\naligning large language models (LLMs). Yet its reliance on a singular reward\nmodel often overlooks the diversity of human preferences. Recent approaches\naddress this limitation by leveraging multi-dimensional feedback to fine-tune\ncorresponding reward models and train LLMs using reinforcement learning.\nHowever, the process is costly and unstable, especially given the competing and\nheterogeneous nature of human preferences. In this paper, we propose Mixing\nPreference Optimization (MPO), a post-processing framework for aggregating\nsingle-objective policies as an alternative to both multi-objective RLHF\n(MORLHF) and MaxMin-RLHF. MPO avoids alignment from scratch. Instead, it\nlog-linearly combines existing policies into a unified one with the weight of\neach policy computed via a batch stochastic mirror descent. Empirical results\ndemonstrate that MPO achieves balanced performance across diverse preferences,\noutperforming or matching existing models with significantly reduced\ncomputational costs.\n","authors":["Tianze Wang","Dongnan Gui","Yifan Hu","Shuhang Lin","Linjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.18699v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2507.12787v1","updated":"2025-07-17T04:57:51Z","published":"2025-07-17T04:57:51Z","title":"Multi-Channel Graph Neural Network for Financial Risk Prediction of NEEQ\n  Enterprises","summary":"  With the continuous evolution of China's multi-level capital market, the\nNational Equities Exchange and Quotations (NEEQ), also known as the \"New Third\nBoard,\" has become a critical financing platform for small and medium-sized\nenterprises (SMEs). However, due to their limited scale and financial\nresilience, many NEEQ-listed companies face elevated risks of financial\ndistress. To address this issue, we propose a multi-channel deep learning\nframework that integrates structured financial indicators, textual disclosures,\nand enterprise relationship data for comprehensive financial risk prediction.\nSpecifically, we design a Triple-Channel Graph Isomorphism Network (GIN) that\nprocesses numeric, textual, and graph-based inputs separately. These\nmodality-specific representations are fused using an attention-based mechanism\nfollowed by a gating unit to enhance robustness and prediction accuracy.\nExperimental results on data from 7,731 real-world NEEQ companies demonstrate\nthat our model significantly outperforms traditional machine learning methods\nand single-modality baselines in terms of AUC, Precision, Recall, and F1 Score.\nThis work provides theoretical and practical insights into risk modeling for\nSMEs and offers a data-driven tool to support financial regulators and\ninvestors.\n","authors":["Jianyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.12787v1.pdf","comment":"10 pages, 4 figures. Submitted for conference review"},{"id":"http://arxiv.org/abs/2507.12780v1","updated":"2025-07-17T04:41:18Z","published":"2025-07-17T04:41:18Z","title":"Compact Vision Transformer by Reduction of Kernel Complexity","summary":"  Self-attention and transformer architectures have become foundational\ncomponents in modern deep learning. Recent efforts have integrated transformer\nblocks into compact neural architectures for computer vision, giving rise to\nvarious efficient vision transformers. In this work, we introduce Transformer\nwith Kernel Complexity Reduction, or KCR-Transformer, a compact transformer\nblock equipped with differentiable channel selection, guided by a novel and\nsharp theoretical generalization bound. KCR-Transformer performs input/output\nchannel selection in the MLP layers of transformer blocks to reduce the\ncomputational cost. Furthermore, we provide a rigorous theoretical analysis\nestablishing a tight generalization bound for networks equipped with\nKCR-Transformer blocks. Leveraging such strong theoretical results, the channel\npruning by KCR-Transformer is conducted in a generalization-aware manner,\nensuring that the resulting network retains a provably small generalization\nerror. Our KCR-Transformer is compatible with many popular and compact\ntransformer networks, such as ViT and Swin, and it reduces the FLOPs of the\nvision transformers while maintaining or even improving the prediction\naccuracy. In the experiments, we replace all the transformer blocks in the\nvision transformers with KCR-Transformer blocks, leading to KCR-Transformer\nnetworks with different backbones. The resulting TCR-Transformers achieve\nsuperior performance on various computer vision tasks, achieving even better\nperformance than the original models with even less FLOPs and parameters.\n","authors":["Yancheng Wang","Yingzhen Yang"],"pdf_url":"https://arxiv.org/pdf/2507.12780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04580v2","updated":"2025-07-17T04:32:51Z","published":"2024-11-07T10:06:23Z","title":"Demystifying MuZero Planning: Interpreting the Learned Model","summary":"  MuZero has achieved superhuman performance in various games by using a\ndynamics network to predict the environment dynamics for planning, without\nrelying on simulators. However, the latent states learned by the dynamics\nnetwork make its planning process opaque. This paper aims to demystify MuZero's\nmodel by interpreting the learned latent states. We incorporate observation\nreconstruction and state consistency into MuZero training and conduct an\nin-depth analysis to evaluate latent states across two board games: 9x9 Go and\nGomoku, and three Atari games: Breakout, Ms. Pacman, and Pong. Our findings\nreveal that while the dynamics network becomes less accurate over longer\nsimulations, MuZero still performs effectively by using planning to correct\nerrors. Our experiments also show that the dynamics network learns better\nlatent states in board games than in Atari games. These insights contribute to\na better understanding of MuZero and offer directions for future research to\nimprove the performance, robustness, and interpretability of the MuZero\nalgorithm. The code and data are available at\nhttps://rlg.iis.sinica.edu.tw/papers/demystifying-muzero-planning.\n","authors":["Hung Guei","Yan-Ru Ju","Wei-Yu Chen","Ti-Rong Wu"],"pdf_url":"https://arxiv.org/pdf/2411.04580v2.pdf","comment":"Accepted by IEEE Transactions on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2507.12774v1","updated":"2025-07-17T04:31:55Z","published":"2025-07-17T04:31:55Z","title":"A Comprehensive Survey of Electronic Health Record Modeling: From Deep\n  Learning Approaches to Large Language Models","summary":"  Artificial intelligence (AI) has demonstrated significant potential in\ntransforming healthcare through the analysis and modeling of electronic health\nrecords (EHRs). However, the inherent heterogeneity, temporal irregularity, and\ndomain-specific nature of EHR data present unique challenges that differ\nfundamentally from those in vision and natural language tasks. This survey\noffers a comprehensive overview of recent advancements at the intersection of\ndeep learning, large language models (LLMs), and EHR modeling. We introduce a\nunified taxonomy that spans five key design dimensions: data-centric\napproaches, neural architecture design, learning-focused strategies, multimodal\nlearning, and LLM-based modeling systems. Within each dimension, we review\nrepresentative methods addressing data quality enhancement, structural and\ntemporal representation, self-supervised learning, and integration with\nclinical knowledge. We further highlight emerging trends such as foundation\nmodels, LLM-driven clinical agents, and EHR-to-text translation for downstream\nreasoning. Finally, we discuss open challenges in benchmarking, explainability,\nclinical alignment, and generalization across diverse clinical settings. This\nsurvey aims to provide a structured roadmap for advancing AI-driven EHR\nmodeling and clinical decision support. For a comprehensive list of EHR-related\nmethods, kindly refer to https://survey-on-tabular-data.github.io/.\n","authors":["Weijieying Ren","Jingxi Zhu","Zehao Liu","Tianxiang Zhao","Vasant Honavar"],"pdf_url":"https://arxiv.org/pdf/2507.12774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12773v1","updated":"2025-07-17T04:26:57Z","published":"2025-07-17T04:26:57Z","title":"Sample-Constrained Black Box Optimization for Audio Personalization","summary":"  We consider the problem of personalizing audio to maximize user experience.\nBriefly, we aim to find a filter $h^*$, which applied to any music or speech,\nwill maximize the user's satisfaction. This is a black-box optimization problem\nsince the user's satisfaction function is unknown. Substantive work has been\ndone on this topic where the key idea is to play audio samples to the user,\neach shaped by a different filter $h_i$, and query the user for their\nsatisfaction scores $f(h_i)$. A family of ``surrogate\" functions is then\ndesigned to fit these scores and the optimization method gradually refines\nthese functions to arrive at the filter $\\hat{h}^*$ that maximizes\nsatisfaction. In certain applications, we observe that a second type of\nquerying is possible where users can tell us the individual elements $h^*[j]$\nof the optimal filter $h^*$. Consider an analogy from cooking where the goal is\nto cook a recipe that maximizes user satisfaction. A user can be asked to score\nvarious cooked recipes (e.g., tofu fried rice) or to score individual\ningredients (say, salt, sugar, rice, chicken, etc.). Given a budget of $B$\nqueries, where a query can be of either type, our goal is to find the recipe\nthat will maximize this user's satisfaction. Our proposal builds on Sparse\nGaussian Process Regression (GPR) and shows how a hybrid approach can\noutperform any one type of querying. Our results are validated through\nsimulations and real world experiments, where volunteers gave feedback on\nmusic/speech audio and were able to achieve high satisfaction levels. We\nbelieve this idea of hybrid querying opens new problems in black-box\noptimization and solutions can benefit other applications beyond audio\npersonalization.\n","authors":["Rajalaxmi Rajagopalan","Yu-Lin Wei","Romit Roy Choudhury"],"pdf_url":"https://arxiv.org/pdf/2507.12773v1.pdf","comment":"Published in AAAI 2024"},{"id":"http://arxiv.org/abs/2507.12768v1","updated":"2025-07-17T03:48:57Z","published":"2025-07-17T03:48:57Z","title":"AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation","summary":"  Vision-language-action (VLA) models have shown promise on task-conditioned\ncontrol in complex settings such as bimanual manipulation. However, the heavy\nreliance on task-specific human demonstrations limits their generalization and\nincurs high data acquisition costs. In this work, we present a new notion of\ntask-agnostic action paradigm that decouples action execution from\ntask-specific conditioning, enhancing scalability, efficiency, and\ncost-effectiveness. To address the data collection challenges posed by this\nparadigm -- such as low coverage density, behavioral redundancy, and safety\nrisks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a\nscalable self-supervised framework that accelerates collection by over $\n30\\times $ compared to human teleoperation. To further enable effective\nlearning from task-agnostic data, which often suffers from distribution\nmismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics\nmodel equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder\n(DAD). We additionally integrate a video-conditioned action validation module\nto verify the feasibility of learned policies across diverse manipulation\ntasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51%\nimprovement in test accuracy and achieves 30-40% higher success rates in\ndownstream tasks such as lifting, pick-and-place, and clicking, using\nreplay-based video validation. Project Page:\nhttps://embodiedfoundation.github.io/vidar_anypos\n","authors":["Hengkai Tan","Yao Feng","Xinyi Mao","Shuhe Huang","Guodong Liu","Zhongkai Hao","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2507.12768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12766v1","updated":"2025-07-17T03:43:18Z","published":"2025-07-17T03:43:18Z","title":"Layer Separation Deep Learning Model with Auxiliary Variables for\n  Partial Differential Equations","summary":"  In this paper, we propose a new optimization framework, the layer separation\n(LySep) model, to improve the deep learning-based methods in solving partial\ndifferential equations. Due to the highly non-convex nature of the loss\nfunction in deep learning, existing optimization algorithms often converge to\nsuboptimal local minima or suffer from gradient explosion or vanishing,\nresulting in poor performance. To address these issues, we introduce auxiliary\nvariables to separate the layers of deep neural networks. Specifically, the\noutput and its derivatives of each layer are represented by auxiliary\nvariables, effectively decomposing the deep architecture into a series of\nshallow architectures. New loss functions with auxiliary variables are\nestablished, in which only variables from two neighboring layers are coupled.\nCorresponding algorithms based on alternating directions are developed, where\nmany variables can be updated optimally in closed forms. Moreover, we provide\ntheoretical analyses demonstrating the consistency between the LySep model and\nthe original deep model. High-dimensional numerical results validate our theory\nand demonstrate the advantages of LySep in minimizing loss and reducing\nsolution error.\n","authors":["Yaru Liu","Yiqi Gu"],"pdf_url":"https://arxiv.org/pdf/2507.12766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09502v5","updated":"2025-07-17T03:40:22Z","published":"2024-11-14T15:13:13Z","title":"Golden Noise for Diffusion Models: A Learning Framework","summary":"  Text-to-image diffusion model is a popular paradigm that synthesizes\npersonalized images by providing a text prompt and a random Gaussian noise.\nWhile people observe that some noises are ``golden noises'' that can achieve\nbetter text-image alignment and higher human preference than others, we still\nlack a machine learning framework to obtain those golden noises. To learn\ngolden noises for diffusion sampling, we mainly make three contributions in\nthis paper. First, we identify a new concept termed the \\textit{noise prompt},\nwhich aims at turning a random Gaussian noise into a golden noise by adding a\nsmall desirable perturbation derived from the text prompt. Following the\nconcept, we first formulate the \\textit{noise prompt learning} framework that\nsystematically learns ``prompted'' golden noise associated with a text prompt\nfor diffusion models. Second, we design a noise prompt data collection pipeline\nand collect a large-scale \\textit{noise prompt dataset}~(NPD) that contains\n100k pairs of random noises and golden noises with the associated text prompts.\nWith the prepared NPD as the training dataset, we trained a small \\textit{noise\nprompt network}~(NPNet) that can directly learn to transform a random noise\ninto a golden noise. The learned golden noise perturbation can be considered as\na kind of prompt for noise, as it is rich in semantic information and tailored\nto the given text prompt. Third, our extensive experiments demonstrate the\nimpressive effectiveness and generalization of NPNet on improving the quality\nof synthesized images across various diffusion models, including SDXL,\nDreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and\nefficient controller that acts as a plug-and-play module with very limited\nadditional inference and computational costs, as it just provides a golden\nnoise instead of a random noise without accessing the original pipeline.\n","authors":["Zikai Zhou","Shitong Shao","Lichen Bai","Shufei Zhang","Zhiqiang Xu","Bo Han","Zeke Xie"],"pdf_url":"https://arxiv.org/pdf/2411.09502v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02838v2","updated":"2025-07-17T03:37:05Z","published":"2023-04-06T03:08:09Z","title":"TBDetector:Transformer-Based Detector for Advanced Persistent Threats\n  with Provenance Graph","summary":"  APT detection is difficult to detect due to the long-term latency, covert and\nslow multistage attack patterns of Advanced Persistent Threat (APT). To tackle\nthese issues, we propose TBDetector, a transformer-based advanced persistent\nthreat detection method for APT attack detection. Considering that provenance\ngraphs provide rich historical information and have the powerful attacks\nhistoric correlation ability to identify anomalous activities, TBDetector\nemploys provenance analysis for APT detection, which summarizes long-running\nsystem execution with space efficiency and utilizes transformer with\nself-attention based encoder-decoder to extract long-term contextual features\nof system states to detect slow-acting attacks. Furthermore, we further\nintroduce anomaly scores to investigate the anomaly of different system states,\nwhere each state is calculated with an anomaly score corresponding to its\nsimilarity score and isolation score. To evaluate the effectiveness of the\nproposed method, we have conducted experiments on five public datasets, i.e.,\nstreamspot, cadets, shellshock, clearscope, and wget_baseline. Experimental\nresults and comparisons with state-of-the-art methods have exhibited better\nperformance of our proposed method.\n","authors":["Nan Wang","Xuezhi Wen","Dalin Zhang","Xibin Zhao","Jiahui Ma","Mengxia Luo","Fan Xu","Sen Nie","Shi Wu","Jiqiang Liu"],"pdf_url":"https://arxiv.org/pdf/2304.02838v2.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2507.12762v1","updated":"2025-07-17T03:34:54Z","published":"2025-07-17T03:34:54Z","title":"World Model-Based End-to-End Scene Generation for Accident Anticipation\n  in Autonomous Driving","summary":"  Reliable anticipation of traffic accidents is essential for advancing\nautonomous driving systems. However, this objective is limited by two\nfundamental challenges: the scarcity of diverse, high-quality training data and\nthe frequent absence of crucial object-level cues due to environmental\ndisruptions or sensor deficiencies. To tackle these issues, we propose a\ncomprehensive framework combining generative scene augmentation with adaptive\ntemporal reasoning. Specifically, we develop a video generation pipeline that\nutilizes a world model guided by domain-informed prompts to create\nhigh-resolution, statistically consistent driving scenarios, particularly\nenriching the coverage of edge cases and complex interactions. In parallel, we\nconstruct a dynamic prediction model that encodes spatio-temporal relationships\nthrough strengthened graph convolutions and dilated temporal operators,\neffectively addressing data incompleteness and transient visual noise.\nFurthermore, we release a new benchmark dataset designed to better capture\ndiverse real-world driving risks. Extensive experiments on public and newly\nreleased datasets confirm that our framework enhances both the accuracy and\nlead time of accident anticipation, offering a robust solution to current data\nand modeling limitations in safety-critical autonomous driving applications.\n","authors":["Yanchen Guan","Haicheng Liao","Chengyue Wang","Xingcheng Liu","Jiaxun Zhang","Zhenning Li"],"pdf_url":"https://arxiv.org/pdf/2507.12762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06737v2","updated":"2025-07-17T03:27:40Z","published":"2025-03-09T19:33:01Z","title":"Faster and Space Efficient Indexing for Locality Sensitive Hashing","summary":"  This work suggests faster and space-efficient index construction algorithms\nfor LSH for Euclidean distance (\\textit{a.k.a.}~\\ELSH) and cosine similarity\n(\\textit{a.k.a.}~\\SRP). The index construction step of these LSHs relies on\ngrouping data points into several bins of hash tables based on their hashcode.\nTo generate an $m$-dimensional hashcode of the $d$-dimensional data point,\nthese LSHs first project the data point onto a $d$-dimensional random Gaussian\nvector and then discretise the resulting inner product. The time and space\ncomplexity of both \\ELSH~and \\SRP~for computing an $m$-sized hashcode of a\n$d$-dimensional vector is $O(md)$, which becomes impractical for large values\nof $m$ and $d$. To overcome this problem, we propose two alternative LSH\nhashcode generation algorithms, both for Euclidean distance and cosine\nsimilarity, namely, \\CSELSH, \\HCSELSH~and \\CSSRP, \\HCSSRP, respectively.\n\\CSELSH~and \\CSSRP~are based on count sketch \\cite{count_sketch} and\n\\HCSELSH~and \\HCSSRP~utilize higher-order count sketch \\cite{shi2019higher}.\nThese proposals significantly reduce the hashcode computation time from $O(md)$\nto $O(d)$. Additionally, both \\CSELSH~and \\CSSRP~reduce the space complexity\nfrom $O(md)$ to $O(d)$; ~and \\HCSELSH, \\HCSSRP~ reduce the space complexity\nfrom $O(md)$ to $O(N \\sqrt[N]{d})$ respectively, where $N\\geq 1$ denotes the\nsize of the input/reshaped tensor. Our proposals are backed by strong\nmathematical guarantees, and we validate their performance through simulations\non various real-world datasets.\n","authors":["Bhisham Dev Verma","Rameshwar Pratap"],"pdf_url":"https://arxiv.org/pdf/2503.06737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.16506v3","updated":"2025-07-17T03:22:43Z","published":"2025-04-23T08:33:34Z","title":"A Comprehensive Survey of Synthetic Tabular Data Generation","summary":"  Tabular data is one of the most prevalent and important data formats in\nreal-world applications such as healthcare, finance, and education. However,\nits effective use in machine learning is often constrained by data scarcity,\nprivacy concerns, and class imbalance. Synthetic tabular data generation has\nemerged as a powerful solution, leveraging generative models to learn\nunderlying data distributions and produce realistic, privacy-preserving\nsamples. Although this area has seen growing attention, most existing surveys\nfocus narrowly on specific methods (e.g., GANs or privacy-enhancing\ntechniques), lacking a unified and comprehensive view that integrates recent\nadvances such as diffusion models and large language models (LLMs).\n  In this survey, we present a structured and in-depth review of synthetic\ntabular data generation methods. Specifically, the survey is organized into\nthree core components: (1) Background, which covers the overall generation\npipeline, including problem definitions, synthetic tabular data generation\nmethods, post processing, and evaluation; (2) Generation Methods, where we\ncategorize existing approaches into traditional generation methods, diffusion\nmodel methods, and LLM-based methods, and compare them in terms of\narchitecture, generation quality, and applicability; and (3) Applications and\nChallenges, which summarizes practical use cases, highlights common datasets,\nand discusses open challenges such as heterogeneity, data fidelity, and privacy\nprotection.\n  This survey aims to provide researchers and practitioners with a holistic\nunderstanding of the field and to highlight key directions for future work in\nsynthetic tabular data generation.\n","authors":["Ruxue Shi","Yili Wang","Mengnan Du","Xu Shen","Yi Chang","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2504.16506v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12755v1","updated":"2025-07-17T03:16:28Z","published":"2025-07-17T03:16:28Z","title":"Domain-Enhanced Dual-Branch Model for Efficient and Interpretable\n  Accident Anticipation","summary":"  Developing precise and computationally efficient traffic accident\nanticipation system is crucial for contemporary autonomous driving\ntechnologies, enabling timely intervention and loss prevention. In this paper,\nwe propose an accident anticipation framework employing a dual-branch\narchitecture that effectively integrates visual information from dashcam videos\nwith structured textual data derived from accident reports. Furthermore, we\nintroduce a feature aggregation method that facilitates seamless integration of\nmultimodal inputs through large models (GPT-4o, Long-CLIP), complemented by\ntargeted prompt engineering strategies to produce actionable feedback and\nstandardized accident archives. Comprehensive evaluations conducted on\nbenchmark datasets (DAD, CCD, and A3D) validate the superior predictive\naccuracy, enhanced responsiveness, reduced computational overhead, and improved\ninterpretability of our approach, thus establishing a new benchmark for\nstate-of-the-art performance in traffic accident anticipation.\n","authors":["Yanchen Guan","Haicheng Liao","Chengyue Wang","Bonan Wang","Jiaxun Zhang","Jia Hu","Zhenning Li"],"pdf_url":"https://arxiv.org/pdf/2507.12755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12750v1","updated":"2025-07-17T03:08:26Z","published":"2025-07-17T03:08:26Z","title":"Multimodal-Guided Dynamic Dataset Pruning for Robust and Efficient\n  Data-Centric Learning","summary":"  Modern deep models are trained on large real-world datasets, where data\nquality varies and redundancy is common. Data-centric approaches such as\ndataset pruning have shown promise in improving training efficiency and model\nperformance. However, most existing methods rely on static heuristics or\ntask-specific metrics, limiting their robustness and generalizability across\ndomains. In this work, we introduce a dynamic dataset pruning framework that\nadaptively selects training samples based on both task-driven difficulty and\ncross-modality semantic consistency. By incorporating supervision from\npretrained multimodal foundation models, our approach captures training\ndynamics while effectively filtering out uninformative samples. Our work\nhighlights the potential of integrating cross-modality alignment for robust\nsample selection, advancing data-centric learning toward more efficient and\nrobust practices across application domains.\n","authors":["Suorong Yang","Peijia Li","Yujie Liu","Zhiming Xu","Peng Ye","Wanli Ouyang","Furao Shen","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2507.12750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15779v2","updated":"2025-07-17T02:52:37Z","published":"2025-03-20T01:41:28Z","title":"Learning Universal Human Mobility Patterns with a Foundation Model for\n  Cross-domain Data Fusion","summary":"  Human mobility modeling is critical for urban planning and transportation\nmanagement, yet existing approaches often lack the integration capabilities\nneeded to handle diverse data sources. We present a foundation model framework\nfor universal human mobility patterns that leverages cross-domain data fusion\nand large language models to address these limitations. Our approach integrates\nmulti-modal data of distinct nature and spatio-temporal resolution, including\ngeographical, mobility, socio-demographic, and traffic information, to\nconstruct a privacy-preserving and semantically enriched human travel\ntrajectory dataset. Our framework demonstrates adaptability through domain\ntransfer techniques that ensure transferability across diverse urban contexts,\nas evidenced in case studies of Los Angeles (LA) and Egypt. The framework\nemploys LLMs for semantic enrichment of trajectory data, enabling comprehensive\nunderstanding of mobility patterns. Quantitative evaluation shows that our\ngenerated synthetic dataset accurately reproduces mobility patterns observed in\nempirical data. The practical utility of this foundation model approach is\ndemonstrated through large-scale traffic simulations for LA County, where\nresults align well with observed traffic data. On California's I-405 corridor,\nthe simulation yields a Mean Absolute Percentage Error of 5.85% for traffic\nvolume and 4.36% for speed compared to Caltrans PeMS observations, illustrating\nthe framework's potential for intelligent transportation systems and urban\nmobility applications.\n","authors":["Haoxuan Ma","Xishun Liao","Yifan Liu","Qinhua Jiang","Chris Stanford","Shangqing Cao","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2503.15779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.11161v2","updated":"2025-07-17T02:30:27Z","published":"2025-07-15T10:09:55Z","title":"How does Labeling Error Impact Contrastive Learning? A Perspective from\n  Data Dimensionality Reduction","summary":"  In recent years, contrastive learning has achieved state-of-the-art\nperformance in the territory of self-supervised representation learning. Many\nprevious works have attempted to provide the theoretical understanding\nunderlying the success of contrastive learning. Almost all of them rely on a\ndefault assumption, i.e., the label consistency assumption, which may not hold\nin practice (the probability of failure is called labeling error) due to the\nstrength and randomness of common augmentation strategies, such as random\nresized crop (RRC). This paper investigates the theoretical impact of labeling\nerror on the downstream classification performance of contrastive learning. We\nfirst reveal several significant negative impacts of labeling error on\ndownstream classification risk. To mitigate these impacts, data dimensionality\nreduction method (e.g., singular value decomposition, SVD) is applied on\noriginal data to reduce false positive samples, and establish both theoretical\nand empirical evaluations. Moreover, it is also found that SVD acts as a\ndouble-edged sword, which may lead to the deterioration of downstream\nclassification accuracy due to the reduced connectivity of the augmentation\ngraph. Based on the above observations, we give the augmentation suggestion\nthat we should use some moderate embedding dimension (such as $512, 1024$ in\nour experiments), data inflation, weak augmentation, and SVD to ensure large\ngraph connectivity and small labeling error to improve model performance.\n","authors":["Jun Chen","Hong Chen","Yonghua Yu","Yiming Ying"],"pdf_url":"https://arxiv.org/pdf/2507.11161v2.pdf","comment":"Published as ICML2025 poster. The arXiv version is a modified version"},{"id":"http://arxiv.org/abs/2502.12086v3","updated":"2025-07-17T02:10:56Z","published":"2025-02-17T18:01:07Z","title":"Unifying Explainable Anomaly Detection and Root Cause Analysis in\n  Dynamical Systems","summary":"  Dynamical systems, prevalent in various scientific and engineering domains,\nare susceptible to anomalies that can significantly impact their performance\nand reliability. This paper addresses the critical challenges of anomaly\ndetection, root cause localization, and anomaly type classification in\ndynamical systems governed by ordinary differential equations (ODEs). We define\ntwo categories of anomalies: cyber anomalies, which propagate through\ninterconnected variables, and measurement anomalies, which remain localized to\nindividual variables. To address these challenges, we propose the Interpretable\nCausality Ordinary Differential Equation (ICODE) Networks, a model-intrinsic\nexplainable learning framework. ICODE leverages Neural ODEs for anomaly\ndetection while employing causality inference through an explanation channel to\nperform root cause analysis (RCA), elucidating why specific time periods are\nflagged as anomalous. ICODE is designed to simultaneously perform anomaly\ndetection, RCA, and anomaly type classification within a single, interpretable\nframework. Our approach is grounded in the hypothesis that anomalies alter the\nunderlying ODEs of the system, manifesting as changes in causal relationships\nbetween variables. We provide a theoretical analysis of how perturbations in\nlearned model parameters can be utilized to identify anomalies and their root\ncauses in time series data. Comprehensive experimental evaluations demonstrate\nthe efficacy of ICODE across various dynamical systems, showcasing its ability\nto accurately detect anomalies, classify their types, and pinpoint their\norigins.\n","authors":["Yue Sun","Rick S. Blum","Parv Venkitasubramaniam"],"pdf_url":"https://arxiv.org/pdf/2502.12086v3.pdf","comment":"Accepted by the AAAI-25 Workshop on Artificial Intelligence for Cyber\n  Security (AICS)"},{"id":"http://arxiv.org/abs/2503.07919v2","updated":"2025-07-17T01:50:49Z","published":"2025-03-10T23:50:30Z","title":"BEARCUBS: A benchmark for computer-using web agents","summary":"  Modern web agents possess computer use abilities that allow them to interact\nwith webpages by sending commands to a virtual keyboard and mouse. While such\nagents have considerable potential to assist human users with complex tasks,\nevaluating their capabilities in real-world settings poses a major challenge.\nTo this end, we introduce BEARCUBS, a \"small but mighty\" benchmark of 111\ninformation-seeking questions designed to evaluate a web agent's ability to\nsearch, browse, and identify factual information from the web. Unlike prior web\nagent benchmarks, solving BEARCUBS requires (1) accessing live web content\nrather than synthetic or simulated pages, which captures the unpredictability\nof real-world web interactions; and (2) performing a broad range of multimodal\ninteractions (e.g., video understanding, 3D navigation) that cannot be bypassed\nvia text-based workarounds. Each question in BEARCUBS has a corresponding\nshort, unambiguous answer and a human-validated browsing trajectory, allowing\nfor transparent evaluation of agent performance and strategies. A human study\nconfirms that BEARCUBS questions are solvable but non-trivial (84.7% human\naccuracy), revealing domain knowledge gaps and overlooked details as common\nfailure points. By contrast, state-of-the-art computer-using agents\nunderperform, with the best-scoring system (OpenAI's Operator) reaching only\n23.4% accuracy. These results highlight critical areas for improvement,\nincluding reliable source selection and more powerful multimodal capabilities.\nTo facilitate future research, BEARCUBS will be updated periodically to replace\ninvalid or contaminated questions, keeping the benchmark fresh for future\ngenerations of web agents.\n","authors":["Yixiao Song","Katherine Thai","Chau Minh Pham","Yapei Chang","Mazin Nadaf","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2503.07919v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2507.09958v2","updated":"2025-07-17T01:35:52Z","published":"2025-07-14T06:13:18Z","title":"Rethinking Inductive Bias in Geographically Neural Network Weighted\n  Regression","summary":"  Inductive bias is a key factor in spatial regression models, determining how\nwell a model can learn from limited data and capture spatial patterns. This\nwork revisits the inductive biases in Geographically Neural Network Weighted\nRegression (GNNWR) and identifies limitations in current approaches for\nmodeling spatial non-stationarity. While GNNWR extends traditional\nGeographically Weighted Regression by using neural networks to learn spatial\nweighting functions, existing implementations are often restricted by fixed\ndistance-based schemes and limited inductive bias. We propose to generalize\nGNNWR by incorporating concepts from convolutional neural networks, recurrent\nneural networks, and transformers, introducing local receptive fields,\nsequential context, and self-attention into spatial regression. Through\nextensive benchmarking on synthetic spatial datasets with varying\nheterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic\nmethods in capturing nonlinear and complex spatial relationships. Our results\nalso reveal that model performance depends strongly on data characteristics,\nwith local models excelling in highly heterogeneous or small-sample scenarios,\nand global models performing better with larger, more homogeneous data. These\nfindings highlight the importance of inductive bias in spatial modeling and\nsuggest future directions, including learnable spatial weighting functions,\nhybrid neural architectures, and improved interpretability for models handling\nnon-stationary spatial data.\n","authors":["Zhenyuan Chen"],"pdf_url":"https://arxiv.org/pdf/2507.09958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11737v4","updated":"2025-07-17T01:33:12Z","published":"2025-03-14T14:44:54Z","title":"Multi-View Node Pruning for Accurate Graph Representation","summary":"  Graph pooling, which compresses a whole graph into a smaller coarsened graph,\nis an essential component of graph representation learning. To efficiently\ncompress a given graph, graph pooling methods often drop their nodes with\nattention-based scoring with the task loss. However, this often results in\nsimply removing nodes with lower degrees without consideration of their\nfeature-level relevance to the given task. To fix this problem, we propose a\nMulti-View Pruning(MVP), a graph pruning method based on a multi-view framework\nand reconstruction loss. Given a graph, MVP first constructs multiple graphs\nfor different views either by utilizing the predefined modalities or by\nrandomly partitioning the input features, to consider the importance of each\nnode in diverse perspectives. Then, it learns the score for each node by\nconsidering both the reconstruction and the task loss. MVP can be incorporated\nwith any hierarchical pooling framework to score the nodes. We validate MVP on\nmultiple benchmark datasets by coupling it with two graph pooling methods, and\nshow that it significantly improves the performance of the base graph pooling\nmethod, outperforming all baselines. Further analysis shows that both the\nencoding of multiple views and the consideration of reconstruction loss are the\nkey to the success of MVP, and that it indeed identifies nodes that are less\nimportant according to domain knowledge.\n","authors":["Hanjin Kim","Jiseong Park","Seojin Kim","Jueun Choi","Doheon Lee","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2503.11737v4.pdf","comment":"Jiseong Park and Hanjin Kim are co-first author for this work"},{"id":"http://arxiv.org/abs/2408.02946v6","updated":"2025-07-17T01:19:42Z","published":"2024-08-06T04:14:29Z","title":"Scaling Trends for Data Poisoning in LLMs","summary":"  LLMs produce harmful and undesirable behavior when trained on datasets\ncontaining even a small fraction of poisoned data. We demonstrate that GPT\nmodels remain vulnerable to fine-tuning on poisoned data, even when safeguarded\nby moderation systems. Given the persistence of data poisoning vulnerabilities\nin today's most capable models, this paper investigates whether these risks\nincrease with model scaling. We evaluate three threat models -- malicious\nfine-tuning, imperfect data curation, and intentional data contamination --\nacross 24 frontier LLMs ranging from 1.5 to 72 billion parameters. Our\nexperiments reveal that larger LLMs are significantly more susceptible to data\npoisoning, learning harmful behaviors from even minimal exposure to harmful\ndata more quickly than smaller models. These findings underscore the need for\nleading AI companies to thoroughly red team fine-tuning APIs before public\nrelease and to develop more robust safeguards against data poisoning,\nparticularly as models continue to scale in size and capability.\n","authors":["Dillon Bowen","Brendan Murphy","Will Cai","David Khachaturov","Adam Gleave","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2408.02946v6.pdf","comment":"This arXiv version of the paper originally included an initial\n  investigation of jailbreak-tuning, which can produce 60+ percentage point\n  increases in vulnerability elicitation compared with standard data poisoning.\n  Jailbreak-tuning has now been separated into a full independent paper, which\n  can be found at arXiv:2507.11630"},{"id":"http://arxiv.org/abs/2507.12709v1","updated":"2025-07-17T01:06:39Z","published":"2025-07-17T01:06:39Z","title":"From SGD to Spectra: A Theory of Neural Network Weight Dynamics","summary":"  Deep neural networks have revolutionized machine learning, yet their training\ndynamics remain theoretically unclear-we develop a continuous-time,\nmatrix-valued stochastic differential equation (SDE) framework that rigorously\nconnects the microscopic dynamics of SGD to the macroscopic evolution of\nsingular-value spectra in weight matrices. We derive exact SDEs showing that\nsquared singular values follow Dyson Brownian motion with eigenvalue repulsion,\nand characterize stationary distributions as gamma-type densities with\npower-law tails, providing the first theoretical explanation for the\nempirically observed 'bulk+tail' spectral structure in trained networks.\nThrough controlled experiments on transformer and MLP architectures, we\nvalidate our theoretical predictions and demonstrate quantitative agreement\nbetween SDE-based forecasts and observed spectral evolution, providing a\nrigorous foundation for understanding why deep learning works.\n","authors":["Brian Richard Olsen","Sam Fatehmanesh","Frank Xiao","Adarsh Kumarappan","Anirudh Gajula"],"pdf_url":"https://arxiv.org/pdf/2507.12709v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2507.13312v1","updated":"2025-07-17T17:31:50Z","published":"2025-07-17T17:31:50Z","title":"Bidirectional Age of Incorrect Information: A Performance Metric for\n  Status Updates in Virtual Dynamic Environments","summary":"  Virtual dynamic environments (VDEs) such as the Metaverse and digital twins\n(DTs) require proper representation of the interacting entities to map their\ncharacteristics within the simulated or augmented space. Keeping these\nrepresentations accurate and up-to-date is crucial for seamless interaction and\nsystem reliability. In this paper, we propose bidirectional age of incorrect\ninformation (BAoII) to address this aspect. BAoII quantifies the time-dependent\npenalty paid by an entity in a VDE due to incorrect or outdated knowledge about\nitself and the overall dynamically changing space. This extends the concept of\nage of incorrect information for a bidirectional information exchange,\ncapturing that a VDE requires mutual awareness of the entity's own\nrepresentation, measured in the virtual space, and what the other entities\nshare about their representations. Using a continuous-time Markov chain model,\nwe derive a closed-form expression for long-term BAoII and identify a\ntransmission cost threshold for optimal update strategies. We describe a\ntrade-off between communication cost and information freshness and validate our\nmodel through numerical simulations, demonstrating the impact of BAoII on\nevaluating system performance and highlighting its relevance for real-time\ncollaboration in the Metaverse and DTs.\n","authors":["Chiara Schiavo","Manuele Favero","Alessandro Buratto","Leonardo Badia"],"pdf_url":"https://arxiv.org/pdf/2507.13312v1.pdf","comment":"8 pages, 8 figures, 1 table, Proc. IEEE Metacom"},{"id":"http://arxiv.org/abs/2507.13255v1","updated":"2025-07-17T16:04:55Z","published":"2025-07-17T16:04:55Z","title":"Automating Steering for Safe Multimodal Large Language Models","summary":"  Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.\n","authors":["Lyucheng Wu","Mengru Wang","Ziwen Xu","Tri Cao","Nay Oo","Bryan Hooi","Shumin Deng"],"pdf_url":"https://arxiv.org/pdf/2507.13255v1.pdf","comment":"Working in progress. 22 pages (8+ for main); 25 figures; 1 table"},{"id":"http://arxiv.org/abs/2507.13179v1","updated":"2025-07-17T14:45:56Z","published":"2025-07-17T14:45:56Z","title":"Predictability-Aware Motion Prediction for Edge XR via High-Order\n  Error-State Kalman Filtering","summary":"  As 6G networks are developed and defined, offloading of XR applications is\nemerging as one of the strong new use cases. The reduced 6G latency coupled\nwith edge processing infrastructure will for the first time provide a realistic\noffloading scenario in cellular networks where several computationally\nintensive functions, including rendering, can migrate from the user device and\ninto the network. A key advantage of doing so is the lowering of the battery\nneeds in the user devices and the possibility to design new devices with\nsmaller form factors.\n","authors":["Ziyu Zhong","Hector A Caltenco","Björn Landfeldt","Günter Alce"],"pdf_url":"https://arxiv.org/pdf/2507.13179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09647v2","updated":"2025-07-17T12:20:43Z","published":"2025-07-13T14:28:20Z","title":"KEN: Knowledge Augmentation and Emotion Guidance Network for Multimodal\n  Fake News Detection","summary":"  In recent years, the rampant spread of misinformation on social media has\nmade accurate detection of multimodal fake news a critical research focus.\nHowever, previous research has not adequately understood the semantics of\nimages, and models struggle to discern news authenticity with limited textual\ninformation. Meanwhile, treating all emotional types of news uniformly without\ntailored approaches further leads to performance degradation. Therefore, we\npropose a novel Knowledge Augmentation and Emotion Guidance Network (KEN). On\nthe one hand, we effectively leverage LVLM's powerful semantic understanding\nand extensive world knowledge. For images, the generated captions provide a\ncomprehensive understanding of image content and scenes, while for text, the\nretrieved evidence helps break the information silos caused by the closed and\nlimited text and context. On the other hand, we consider inter-class\ndifferences between different emotional types of news through balanced\nlearning, achieving fine-grained modeling of the relationship between emotional\ntypes and authenticity. Extensive experiments on two real-world datasets\ndemonstrate the superiority of our KEN.\n","authors":["Peican Zhu","Yubo Jing","Le Cheng","Keke Tang","Yangming Guo"],"pdf_url":"https://arxiv.org/pdf/2507.09647v2.pdf","comment":"Accepted by ACM MM 2025"},{"id":"http://arxiv.org/abs/2409.06690v2","updated":"2025-07-17T10:43:51Z","published":"2024-09-10T17:54:00Z","title":"Benchmarking Sub-Genre Classification For Mainstage Dance Music","summary":"  Music classification, a cornerstone of music information retrieval, supports\na wide array of applications. To address the lack of comprehensive datasets and\neffective methods for sub-genre classification in mainstage dance music, we\nintroduce a novel benchmark featuring a new dataset and baseline. Our dataset\nexpands the scope of sub-genres to reflect the diversity of recent mainstage\nlive sets performed by leading DJs at global music festivals, capturing the\nvibrant and rapidly evolving electronic dance music (EDM) scene that engages\nmillions of fans worldwide. We employ a continuous soft labeling approach to\naccommodate tracks blending multiple sub-genres, preserving their inherent\ncomplexity. Experiments demonstrate that even state-of-the-art multimodal large\nlanguage models (MLLMs) struggle with this task, while our specialized baseline\nmodels achieve high accuracy. This benchmark supports applications such as\nmusic recommendation, DJ set curation, and interactive multimedia systems, with\nvideo demos provided. Our code and data are all open-sourced at\nhttps://github.com/Gariscat/housex-v2.git}{https://github.com/Gariscat/housex-v2.git.\n","authors":["Hongzhi Shu","Xinglin Li","Hongyu Jiang","Minghao Fu","Xinyu Li"],"pdf_url":"https://arxiv.org/pdf/2409.06690v2.pdf","comment":"WASPAA 2025"},{"id":"http://arxiv.org/abs/2505.20770v2","updated":"2025-07-17T10:10:45Z","published":"2025-05-27T06:21:56Z","title":"Can Large Language Models Predict Audio Effects Parameters from Natural\n  Language?","summary":"  In music production, manipulating audio effects (Fx) parameters through\nnatural language has the potential to reduce technical barriers for\nnon-experts. We present LLM2Fx, a framework leveraging Large Language Models\n(LLMs) to predict Fx parameters directly from textual descriptions without\nrequiring task-specific training or fine-tuning. Our approach address the\ntext-to-effect parameter prediction (Text2Fx) task by mapping natural language\ndescriptions to the corresponding Fx parameters for equalization and\nreverberation. We demonstrate that LLMs can generate Fx parameters in a\nzero-shot manner that elucidates the relationship between timbre semantics and\naudio effects in music production. To enhance performance, we introduce three\ntypes of in-context examples: audio Digital Signal Processing (DSP) features,\nDSP function code, and few-shot examples. Our results demonstrate that\nLLM-based Fx parameter generation outperforms previous optimization approaches,\noffering competitive performance in translating natural language descriptions\nto appropriate Fx settings. Furthermore, LLMs can serve as text-driven\ninterfaces for audio production, paving the way for more intuitive and\naccessible music production tools.\n","authors":["Seungheon Doh","Junghyun Koo","Marco A. Martínez-Ramírez","Wei-Hsiang Liao","Juhan Nam","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2505.20770v2.pdf","comment":"Accepted for publication at The IEEE Workshop on Applications of\n  Signal Processing to Audio and Acoustics (WASPAA)"},{"id":"http://arxiv.org/abs/2507.12951v1","updated":"2025-07-17T09:45:49Z","published":"2025-07-17T09:45:49Z","title":"UniSLU: Unified Spoken Language Understanding from Heterogeneous\n  Cross-Task Datasets","summary":"  Spoken Language Understanding (SLU) plays a crucial role in speech-centric\nmultimedia applications, enabling machines to comprehend spoken language in\nscenarios such as meetings, interviews, and customer service interactions. SLU\nencompasses multiple tasks, including Automatic Speech Recognition (ASR),\nspoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).\nHowever, existing methods often rely on separate model architectures for\nindividual tasks such as spoken NER and SA, which increases system complexity,\nlimits cross-task interaction, and fails to fully exploit heterogeneous\ndatasets available across tasks. To address these limitations, we propose\nUniSLU, a unified framework that jointly models multiple SLU tasks within a\nsingle architecture. Specifically, we propose a unified representation for\ndiverse SLU tasks, enabling full utilization of heterogeneous datasets across\nmultiple tasks. Built upon this representation, we propose a unified generative\nmethod that jointly models ASR, spoken NER, and SA tasks, enhancing task\ninteractions and enabling seamless integration with large language models to\nharness their powerful generative capabilities. Extensive experiments on public\nSLU datasets demonstrate the effectiveness of our approach, achieving superior\nSLU performance compared to several benchmark methods, making it well-suited\nfor real-world speech-based multimedia scenarios. We will release all code and\nmodels at github to facilitate future research.\n","authors":["Zhichao Sheng","Shilin Zhou","Chen Gong","Zhenghua Li"],"pdf_url":"https://arxiv.org/pdf/2507.12951v1.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2507.12932v1","updated":"2025-07-17T09:12:36Z","published":"2025-07-17T09:12:36Z","title":"Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy\n  Protection against Voice Deepfakes","summary":"  The rapid advancement of voice deepfake technologies has raised serious\nconcerns about user audio privacy, as attackers increasingly exploit publicly\navailable voice data to generate convincing fake audio for malicious purposes\nsuch as identity theft, financial fraud, and misinformation campaigns. While\nexisting defense methods offer partial protection, they face critical\nlimitations, including weak adaptability to unseen user data, poor scalability\nto long audio, rigid reliance on white-box knowledge, and high computational\nand temporal costs during the encryption process. To address these challenges\nand defend against personalized voice deepfake threats, we propose Enkidu, a\nnovel user-oriented privacy-preserving framework that leverages universal\nfrequential perturbations generated through black-box knowledge and few-shot\ntraining on a small amount of user data. These highly malleable\nfrequency-domain noise patches enable real-time, lightweight protection with\nstrong generalization across variable-length audio and robust resistance to\nvoice deepfake attacks, all while preserving perceptual quality and speech\nintelligibility. Notably, Enkidu achieves over 50 to 200 times processing\nmemory efficiency (as low as 0.004 gigabytes) and 3 to 7000 times runtime\nefficiency (real-time coefficient as low as 0.004) compared to six\nstate-of-the-art countermeasures. Extensive experiments across six mainstream\ntext-to-speech models and five cutting-edge automated speaker verification\nmodels demonstrate the effectiveness, transferability, and practicality of\nEnkidu in defending against both vanilla and adaptive voice deepfake attacks.\n","authors":["Zhou Feng","Jiahao Chen","Chunyi Zhou","Yuwen Pu","Qingming Li","Tianyu Du","Shouling Ji"],"pdf_url":"https://arxiv.org/pdf/2507.12932v1.pdf","comment":"Accepted by ACM MM 2025"},{"id":"http://arxiv.org/abs/2408.16990v3","updated":"2025-07-17T04:31:17Z","published":"2024-08-30T03:36:22Z","title":"Music Grounding by Short Video","summary":"  Adding proper background music helps complete a short video to be shared.\nPrevious work tackles the task by video-to-music retrieval (V2MR), aiming to\nfind the most suitable music track from a collection to match the content of a\ngiven query video. In practice, however, music tracks are typically much longer\nthan the query video, necessitating (manual) trimming of the retrieved music to\na shorter segment that matches the video duration. In order to bridge the gap\nbetween the practical need for music moment localization and V2MR, we propose a\nnew task termed Music Grounding by Short Video (MGSV). To tackle the new task,\nwe introduce a new benchmark, MGSV-EC, which comprises a diverse set of 53k\nshort videos associated with 35k different music moments from 4k unique music\ntracks. Furthermore, we develop a new baseline method, MaDe, which performs\nboth video-to-music matching and music moment detection within a unified\nend-to-end deep network. Extensive experiments on MGSV-EC not only highlight\nthe challenging nature of MGSV but also set MaDe as a strong baseline.\n","authors":["Zijie Xin","Minquan Wang","Jingyu Liu","Ye Ma","Quan Chen","Peng Jiang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2408.16990v3.pdf","comment":"Accepted to ICCV2025"},{"id":"http://arxiv.org/abs/2506.16273v2","updated":"2025-07-17T03:38:12Z","published":"2025-06-19T12:46:55Z","title":"Fine-grained Image Retrieval via Dual-Vision Adaptation","summary":"  Fine-Grained Image Retrieval~(FGIR) faces challenges in learning\ndiscriminative visual representations to retrieve images with similar\nfine-grained features. Current leading FGIR solutions typically follow two\nregimes: enforce pairwise similarity constraints in the semantic embedding\nspace, or incorporate a localization sub-network to fine-tune the entire model.\nHowever, such two regimes tend to overfit the training data while forgetting\nthe knowledge gained from large-scale pre-training, thus reducing their\ngeneralization ability. In this paper, we propose a Dual-Vision Adaptation\n(DVA) approach for FGIR, which guides the frozen pre-trained model to perform\nFGIR through collaborative sample and feature adaptation. Specifically, we\ndesign Object-Perceptual Adaptation, which modifies input samples to help the\npre-trained model perceive critical objects and elements within objects that\nare helpful for category prediction. Meanwhile, we propose In-Context\nAdaptation, which introduces a small set of parameters for feature adaptation\nwithout modifying the pre-trained parameters. This makes the FGIR task using\nthese adjusted features closer to the task solved during the pre-training.\nAdditionally, to balance retrieval efficiency and performance, we propose\nDiscrimination Perception Transfer to transfer the discriminative knowledge in\nthe object-perceptual adaptation to the image encoder using the knowledge\ndistillation mechanism. Extensive experiments show that DVA has fewer learnable\nparameters and performs well on three in-distribution and three\nout-of-distribution fine-grained datasets.\n","authors":["Xin Jiang","Meiqi Cao","Hao Tang","Fei Shen","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2506.16273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12723v1","updated":"2025-07-17T02:02:39Z","published":"2025-07-17T02:02:39Z","title":"Cross-Modal Watermarking for Authentic Audio Recovery and Tamper\n  Localization in Synthesized Audiovisual Forgeries","summary":"  Recent advances in voice cloning and lip synchronization models have enabled\nSynthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are\nmanipulated to mimic a target speaker. This significantly increases the risk of\nmisinformation by making fake content seem real. To address this issue,\nexisting methods detect or localize manipulations but cannot recover the\nauthentic audio that conveys the semantic content of the message. This\nlimitation reduces their effectiveness in combating audiovisual misinformation.\nIn this work, we introduce the task of Authentic Audio Recovery (AAR) and\nTamper Localization in Audio (TLA) from SAVFs and propose a cross-modal\nwatermarking framework to embed authentic audio into visuals before\nmanipulation. This enables AAR, TLA, and a robust defense against\nmisinformation. Extensive experiments demonstrate the strong performance of our\nmethod in AAR and TLA against various manipulations, including voice cloning\nand lip synchronization.\n","authors":["Minyoung Kim","Sehwan Park","Sungmin Cha","Paul Hongsuck Seo"],"pdf_url":"https://arxiv.org/pdf/2507.12723v1.pdf","comment":"5 pages, 2 figures, Interspeech 2025"}]}}